Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:25<03:46, 25.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:41<02:41, 20.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:04<02:28, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:27<02:12, 22.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:44<01:41, 20.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:02<01:17, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:21<00:57, 19.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:37<00:36, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:57<00:18, 18.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:24<00:00, 21.22s/it]Generating: 100%|██████████| 10/10 [03:24<00:00, 20.42s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')", "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.04s/it]Extractor Estimating: 2it [00:17,  7.42s/it]Extractor Estimating: 3it [00:18,  4.36s/it]Extractor Estimating: 4it [00:19,  3.15s/it]Extractor Estimating: 5it [00:25,  4.16s/it]Extractor Estimating: 6it [00:26,  2.99s/it]Extractor Estimating: 7it [00:26,  2.26s/it]Extractor Estimating: 8it [00:27,  1.78s/it]Extractor Estimating: 9it [00:28,  1.44s/it]Extractor Estimating: 10it [00:29,  1.24s/it]Extractor Estimating: 11it [00:29,  1.12s/it]Extractor Estimating: 12it [00:30,  1.00s/it]Extractor Estimating: 13it [00:31,  1.09it/s]Extractor Estimating: 14it [00:32,  1.14it/s]Extractor Estimating: 15it [00:32,  1.20it/s]Extractor Estimating: 16it [00:33,  1.23it/s]Extractor Estimating: 17it [00:34,  1.26it/s]Extractor Estimating: 18it [00:35,  1.26it/s]Extractor Estimating: 19it [00:35,  1.27it/s]Extractor Estimating: 20it [00:36,  1.26it/s]Extractor Estimating: 21it [00:37,  1.29it/s]Extractor Estimating: 22it [00:38,  1.29it/s]Extractor Estimating: 23it [00:39,  1.27it/s]Extractor Estimating: 24it [00:39,  1.24it/s]Extractor Estimating: 25it [00:40,  1.28it/s]Extractor Estimating: 26it [00:41,  1.30it/s]Extractor Estimating: 27it [00:42,  1.30it/s]Extractor Estimating: 28it [00:42,  1.32it/s]Extractor Estimating: 29it [00:43,  1.36it/s]Extractor Estimating: 30it [00:44,  1.35it/s]Extractor Estimating: 31it [00:45,  1.34it/s]Extractor Estimating: 32it [00:45,  1.29it/s]Extractor Estimating: 33it [00:46,  1.32it/s]Extractor Estimating: 34it [00:47,  1.34it/s]Extractor Estimating: 35it [00:48,  1.30it/s]Extractor Estimating: 36it [00:48,  1.35it/s]Extractor Estimating: 37it [00:49,  1.33it/s]Extractor Estimating: 38it [00:50,  1.33it/s]Extractor Estimating: 39it [00:51,  1.35it/s]Extractor Estimating: 40it [00:51,  1.32it/s]Extractor Estimating: 41it [00:52,  1.28it/s]Extractor Estimating: 42it [00:53,  1.31it/s]Extractor Estimating: 43it [00:54,  1.35it/s]Extractor Estimating: 44it [00:54,  1.38it/s]Extractor Estimating: 45it [00:55,  1.38it/s]Extractor Estimating: 46it [00:56,  1.38it/s]Extractor Estimating: 47it [00:57,  1.36it/s]Extractor Estimating: 48it [00:57,  1.36it/s]Extractor Estimating: 49it [00:58,  1.34it/s]Extractor Estimating: 50it [00:59,  1.36it/s]Extractor Estimating: 51it [01:00,  1.34it/s]Extractor Estimating: 52it [01:00,  1.32it/s]Extractor Estimating: 53it [01:01,  1.33it/s]Extractor Estimating: 54it [01:02,  1.31it/s]Extractor Estimating: 55it [01:03,  1.32it/s]Extractor Estimating: 56it [01:03,  1.29it/s]Extractor Estimating: 57it [01:04,  1.26it/s]Extractor Estimating: 58it [01:05,  1.32it/s]Extractor Estimating: 59it [01:06,  1.34it/s]Extractor Estimating: 60it [01:06,  1.33it/s]Extractor Estimating: 61it [01:07,  1.29it/s]Extractor Estimating: 62it [01:08,  1.31it/s]Extractor Estimating: 63it [01:09,  1.34it/s]Extractor Estimating: 64it [01:09,  1.33it/s]Extractor Estimating: 65it [01:10,  1.32it/s]Extractor Estimating: 66it [01:11,  1.31it/s]Extractor Estimating: 67it [01:12,  1.29it/s]Extractor Estimating: 68it [01:13,  1.31it/s]Extractor Estimating: 69it [01:13,  1.32it/s]Extractor Estimating: 70it [01:14,  1.30it/s]Extractor Estimating: 71it [01:15,  1.21it/s]Extractor Estimating: 72it [01:16,  1.23it/s]Extractor Estimating: 73it [01:17,  1.26it/s]Extractor Estimating: 74it [01:17,  1.29it/s]Extractor Estimating: 75it [01:18,  1.31it/s]Extractor Estimating: 76it [01:19,  1.35it/s]Extractor Estimating: 77it [01:20,  1.29it/s]Extractor Estimating: 78it [01:20,  1.29it/s]Extractor Estimating: 79it [01:21,  1.35it/s]Extractor Estimating: 80it [01:22,  1.36it/s]Extractor Estimating: 81it [01:23,  1.35it/s]Extractor Estimating: 82it [01:23,  1.32it/s]Extractor Estimating: 83it [01:24,  1.30it/s]Extractor Estimating: 84it [01:25,  1.26it/s]Extractor Estimating: 85it [01:26,  1.25it/s]Extractor Estimating: 86it [01:27,  1.26it/s]Extractor Estimating: 87it [01:27,  1.25it/s]Extractor Estimating: 88it [01:28,  1.29it/s]Extractor Estimating: 89it [01:29,  1.28it/s]Extractor Estimating: 90it [01:30,  1.29it/s]Extractor Estimating: 91it [01:30,  1.27it/s]Extractor Estimating: 92it [01:31,  1.29it/s]Extractor Estimating: 93it [01:32,  1.28it/s]Extractor Estimating: 94it [01:33,  1.27it/s]Extractor Estimating: 95it [01:34,  1.26it/s]Extractor Estimating: 96it [01:34,  1.25it/s]Extractor Estimating: 97it [01:35,  1.16it/s]Extractor Estimating: 98it [01:36,  1.20it/s]Extractor Estimating: 99it [01:37,  1.21it/s]Extractor Estimating: 100it [01:38,  1.27it/s]Extractor Estimating: 101it [01:38,  1.28it/s]Extractor Estimating: 102it [01:39,  1.26it/s]Extractor Estimating: 103it [01:40,  1.28it/s]Extractor Estimating: 104it [01:41,  1.30it/s]Extractor Estimating: 105it [01:42,  1.32it/s]Extractor Estimating: 106it [01:42,  1.30it/s]Extractor Estimating: 107it [01:43,  1.29it/s]Extractor Estimating: 108it [01:44,  1.32it/s]Extractor Estimating: 109it [01:45,  1.32it/s]Extractor Estimating: 110it [01:45,  1.33it/s]Extractor Estimating: 111it [01:46,  1.32it/s]Extractor Estimating: 112it [01:47,  1.31it/s]Extractor Estimating: 113it [01:48,  1.29it/s]Extractor Estimating: 114it [01:48,  1.32it/s]Extractor Estimating: 115it [01:49,  1.32it/s]Extractor Estimating: 116it [01:50,  1.33it/s]Extractor Estimating: 117it [01:51,  1.33it/s]Extractor Estimating: 118it [01:51,  1.35it/s]Extractor Estimating: 119it [01:52,  1.35it/s]Extractor Estimating: 120it [01:53,  1.35it/s]Extractor Estimating: 121it [01:54,  1.35it/s]Extractor Estimating: 122it [01:54,  1.35it/s]Extractor Estimating: 123it [01:55,  1.33it/s]Extractor Estimating: 124it [01:56,  1.32it/s]Extractor Estimating: 125it [01:57,  1.34it/s]Extractor Estimating: 126it [01:57,  1.34it/s]Extractor Estimating: 127it [01:58,  1.29it/s]Extractor Estimating: 128it [01:59,  1.33it/s]Extractor Estimating: 129it [02:00,  1.29it/s]Extractor Estimating: 130it [02:01,  1.26it/s]Extractor Estimating: 131it [02:01,  1.24it/s]Extractor Estimating: 132it [02:02,  1.27it/s]Extractor Estimating: 133it [02:03,  1.26it/s]Extractor Estimating: 134it [02:04,  1.26it/s]Extractor Estimating: 135it [02:04,  1.28it/s]Extractor Estimating: 136it [02:05,  1.30it/s]Extractor Estimating: 137it [02:06,  1.26it/s]Extractor Estimating: 138it [02:07,  1.26it/s]Extractor Estimating: 139it [02:08,  1.23it/s]Extractor Estimating: 140it [02:08,  1.27it/s]Extractor Estimating: 141it [02:09,  1.32it/s]Extractor Estimating: 142it [02:10,  1.32it/s]Extractor Estimating: 143it [02:11,  1.30it/s]Extractor Estimating: 144it [02:12,  1.28it/s]Extractor Estimating: 145it [02:12,  1.27it/s]Extractor Estimating: 146it [02:13,  1.24it/s]Extractor Estimating: 147it [02:14,  1.25it/s]Extractor Estimating: 148it [02:15,  1.27it/s]Extractor Estimating: 149it [02:15,  1.30it/s]Extractor Estimating: 150it [02:16,  1.30it/s]Extractor Estimating: 151it [02:17,  1.32it/s]Extractor Estimating: 152it [02:18,  1.31it/s]Extractor Estimating: 153it [02:18,  1.32it/s]Extractor Estimating: 154it [02:19,  1.32it/s]Extractor Estimating: 155it [02:20,  1.32it/s]Extractor Estimating: 156it [02:21,  1.32it/s]Extractor Estimating: 157it [02:22,  1.30it/s]Extractor Estimating: 158it [02:22,  1.31it/s]Extractor Estimating: 159it [02:23,  1.32it/s]Extractor Estimating: 160it [02:24,  1.30it/s]Extractor Estimating: 161it [02:25,  1.30it/s]Extractor Estimating: 162it [02:25,  1.31it/s]Extractor Estimating: 163it [02:26,  1.33it/s]Extractor Estimating: 164it [02:27,  1.29it/s]Extractor Estimating: 165it [02:28,  1.27it/s]Extractor Estimating: 166it [02:28,  1.29it/s]Extractor Estimating: 167it [02:29,  1.29it/s]Extractor Estimating: 168it [02:30,  1.22it/s]Extractor Estimating: 169it [02:31,  1.24it/s]Extractor Estimating: 170it [02:32,  1.22it/s]Extractor Estimating: 171it [02:32,  1.27it/s]Extractor Estimating: 172it [02:33,  1.26it/s]Extractor Estimating: 173it [02:34,  1.31it/s]Extractor Estimating: 174it [02:35,  1.32it/s]Extractor Estimating: 175it [02:36,  1.31it/s]Extractor Estimating: 176it [02:36,  1.29it/s]Extractor Estimating: 177it [02:37,  1.30it/s]Extractor Estimating: 178it [02:38,  1.28it/s]Extractor Estimating: 179it [02:39,  1.28it/s]Extractor Estimating: 180it [02:39,  1.26it/s]Extractor Estimating: 181it [02:40,  1.30it/s]Extractor Estimating: 182it [02:41,  1.32it/s]Extractor Estimating: 183it [02:42,  1.28it/s]Extractor Estimating: 184it [02:43,  1.28it/s]Extractor Estimating: 185it [02:43,  1.31it/s]Extractor Estimating: 186it [02:44,  1.34it/s]Extractor Estimating: 187it [02:45,  1.34it/s]Extractor Estimating: 188it [02:45,  1.35it/s]Extractor Estimating: 189it [02:46,  1.33it/s]Extractor Estimating: 190it [02:47,  1.28it/s]Extractor Estimating: 191it [02:48,  1.32it/s]Extractor Estimating: 192it [02:49,  1.29it/s]Extractor Estimating: 193it [02:49,  1.29it/s]Extractor Estimating: 194it [02:50,  1.32it/s]Extractor Estimating: 195it [02:51,  1.32it/s]Extractor Estimating: 196it [02:52,  1.34it/s]Extractor Estimating: 197it [02:52,  1.34it/s]Extractor Estimating: 198it [02:53,  1.32it/s]Extractor Estimating: 199it [02:54,  1.27it/s]Extractor Estimating: 200it [02:55,  1.29it/s]Extractor Estimating: 201it [02:55,  1.28it/s]Extractor Estimating: 202it [02:56,  1.30it/s]Extractor Estimating: 203it [02:57,  1.32it/s]Extractor Estimating: 204it [02:58,  1.32it/s]Extractor Estimating: 205it [02:58,  1.33it/s]Extractor Estimating: 206it [02:59,  1.35it/s]Extractor Estimating: 207it [03:00,  1.34it/s]Extractor Estimating: 208it [03:01,  1.34it/s]Extractor Estimating: 209it [03:01,  1.32it/s]Extractor Estimating: 210it [03:02,  1.33it/s]Extractor Estimating: 211it [03:03,  1.31it/s]Extractor Estimating: 212it [03:04,  1.31it/s]Extractor Estimating: 213it [03:05,  1.28it/s]Extractor Estimating: 214it [03:05,  1.31it/s]Extractor Estimating: 215it [03:06,  1.29it/s]Extractor Estimating: 216it [03:07,  1.28it/s]Extractor Estimating: 217it [03:08,  1.29it/s]Extractor Estimating: 218it [03:08,  1.32it/s]Extractor Estimating: 219it [03:09,  1.34it/s]Extractor Estimating: 220it [03:10,  1.33it/s]Extractor Estimating: 221it [03:11,  1.31it/s]Extractor Estimating: 222it [03:11,  1.32it/s]Extractor Estimating: 223it [03:12,  1.34it/s]Extractor Estimating: 224it [03:13,  1.33it/s]Extractor Estimating: 225it [03:14,  1.32it/s]Extractor Estimating: 226it [03:14,  1.31it/s]Extractor Estimating: 227it [03:15,  1.32it/s]Extractor Estimating: 228it [03:16,  1.27it/s]Extractor Estimating: 229it [03:17,  1.30it/s]Extractor Estimating: 230it [03:18,  1.28it/s]Extractor Estimating: 231it [03:18,  1.26it/s]Extractor Estimating: 232it [03:19,  1.29it/s]Extractor Estimating: 233it [03:20,  1.28it/s]Extractor Estimating: 234it [03:21,  1.22it/s]Extractor Estimating: 235it [03:22,  1.24it/s]Extractor Estimating: 236it [03:22,  1.27it/s]Extractor Estimating: 237it [03:23,  1.28it/s]Extractor Estimating: 238it [03:24,  1.29it/s]Extractor Estimating: 239it [03:25,  1.25it/s]Extractor Estimating: 240it [03:26,  1.23it/s]Extractor Estimating: 241it [03:26,  1.23it/s]Extractor Estimating: 242it [03:27,  1.30it/s]Extractor Estimating: 243it [03:28,  1.30it/s]Extractor Estimating: 244it [03:29,  1.34it/s]Extractor Estimating: 245it [03:29,  1.37it/s]Extractor Estimating: 246it [03:30,  1.37it/s]Extractor Estimating: 247it [03:31,  1.36it/s]Extractor Estimating: 248it [03:31,  1.33it/s]Extractor Estimating: 249it [03:32,  1.35it/s]Extractor Estimating: 250it [03:33,  1.28it/s]Extractor Estimating: 250it [03:33,  1.17it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4993 mean pseudo reward: 0.9658052023780772
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 24175
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24275, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24275, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.523, loss:2719.1349
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.219, loss:1887.3849
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.223, loss:1624.9428
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.232, loss:1513.7146
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.243, loss:1451.5168
>> valid entity prec:0.5500, rec:0.3528, f1:0.4299
>> valid relation prec:0.5977, rec:0.0347, f1:0.0655
>> valid relation with NER prec:0.5977, rec:0.0347, f1:0.0655
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.020, loss:1402.5673
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.223, loss:1377.5351
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.228, loss:1276.7713
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.216, loss:1230.7284
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.225, loss:1179.9540
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4823, rec:0.3609, f1:0.4129
>> valid relation prec:0.3326, rec:0.0340, f1:0.0617
>> valid relation with NER prec:0.3326, rec:0.0340, f1:0.0617
g_step 1100, step 55, avg_time 3.018, loss:1134.0642
g_step 1200, step 155, avg_time 1.229, loss:1078.3326
g_step 1300, step 46, avg_time 1.218, loss:1065.1976
g_step 1400, step 146, avg_time 1.219, loss:1038.7765
g_step 1500, step 37, avg_time 1.222, loss:956.1727
>> valid entity prec:0.5135, rec:0.3170, f1:0.3920
>> valid relation prec:0.5099, rec:0.0395, f1:0.0732
>> valid relation with NER prec:0.5099, rec:0.0395, f1:0.0732
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 137, avg_time 3.010, loss:943.6558
g_step 1700, step 28, avg_time 1.227, loss:973.0944
g_step 1800, step 128, avg_time 1.224, loss:889.7570
g_step 1900, step 19, avg_time 1.225, loss:923.6160
g_step 2000, step 119, avg_time 1.233, loss:879.9948
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5132, rec:0.2999, f1:0.3785
>> valid relation prec:0.2681, rec:0.0137, f1:0.0261
>> valid relation with NER prec:0.2681, rec:0.0137, f1:0.0261
g_step 2100, step 10, avg_time 2.999, loss:878.7833
g_step 2200, step 110, avg_time 1.225, loss:859.2406
g_step 2300, step 1, avg_time 1.218, loss:811.7344
g_step 2400, step 101, avg_time 1.219, loss:784.9367
g_step 2500, step 201, avg_time 1.234, loss:806.0037
>> valid entity prec:0.4929, rec:0.4017, f1:0.4426
>> valid relation prec:0.2835, rec:0.0405, f1:0.0709
>> valid relation with NER prec:0.2835, rec:0.0405, f1:0.0709
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.025, loss:758.3499
g_step 2700, step 192, avg_time 1.225, loss:769.5189
g_step 2800, step 83, avg_time 1.215, loss:724.7393
g_step 2900, step 183, avg_time 1.237, loss:743.4511
g_step 3000, step 74, avg_time 1.213, loss:692.8151
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4730, rec:0.4673, f1:0.4701
>> valid relation prec:0.2503, rec:0.0399, f1:0.0688
>> valid relation with NER prec:0.2503, rec:0.0399, f1:0.0688
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 174, avg_time 3.033, loss:698.3378
g_step 3200, step 65, avg_time 1.216, loss:690.1367
g_step 3300, step 165, avg_time 1.235, loss:689.0135
g_step 3400, step 56, avg_time 1.237, loss:659.4944
g_step 3500, step 156, avg_time 1.210, loss:653.2291
>> valid entity prec:0.4721, rec:0.4125, f1:0.4402
>> valid relation prec:0.1806, rec:0.0296, f1:0.0509
>> valid relation with NER prec:0.1806, rec:0.0296, f1:0.0509
g_step 3600, step 47, avg_time 3.032, loss:632.3441
g_step 3700, step 147, avg_time 1.232, loss:633.6720
g_step 3800, step 38, avg_time 1.222, loss:617.7055
g_step 3900, step 138, avg_time 1.221, loss:610.8747
g_step 4000, step 29, avg_time 1.215, loss:581.8846
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4807, rec:0.4072, f1:0.4409
>> valid relation prec:0.1843, rec:0.0449, f1:0.0722
>> valid relation with NER prec:0.1843, rec:0.0449, f1:0.0722
g_step 4100, step 129, avg_time 3.041, loss:580.7656
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:54:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:54:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-54-34_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:54:35 - WARNING - datasets.builder -   Using custom data configuration default-7847192c2b2c32ec
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7847192c2b2c32ec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:54:36,214 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:54:36,215 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:54:36,215 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:54:36,216 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:54:36,230 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,234 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,234 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,235 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,235 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,235 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:54:36,235 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:54:36,417 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:54:39,526 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:54:39,530 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7847192c2b2c32ec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:54:39 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x148dd4f19050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.04ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.85ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.15ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.30ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.39ba/s]100%|██████████| 6/6 [00:01<00:00,  4.95ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.26ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.42ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.45ba/s]100%|██████████| 5/5 [00:01<00:00,  5.16ba/s]100%|██████████| 5/5 [00:01<00:00,  4.79ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.34ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.64ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.75ba/s]100%|██████████| 6/6 [00:00<00:00, 12.60ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.62ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.70ba/s]100%|██████████| 5/5 [00:00<00:00, 11.62ba/s]100%|██████████| 5/5 [00:00<00:00, 11.31ba/s]
[INFO|trainer.py:414] 2023-08-27 22:54:43,051 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:54:43,065 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:54:43,065 >>   Num examples = 5042
[INFO|trainer.py:1149] 2023-08-27 22:54:43,065 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:54:43,066 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:54:43,066 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:54:43,066 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:54:43,066 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<02:00,  3.27it/s]  1%|          | 2/395 [00:00<01:56,  3.36it/s]  1%|          | 3/395 [00:00<01:55,  3.39it/s]  1%|          | 4/395 [00:01<01:54,  3.40it/s]  1%|▏         | 5/395 [00:01<01:54,  3.41it/s]  2%|▏         | 6/395 [00:01<01:53,  3.41it/s]  2%|▏         | 7/395 [00:02<01:53,  3.42it/s]  2%|▏         | 8/395 [00:02<01:53,  3.42it/s]  2%|▏         | 9/395 [00:02<01:52,  3.42it/s]  3%|▎         | 10/395 [00:02<01:52,  3.41it/s]  3%|▎         | 11/395 [00:03<01:52,  3.41it/s]  3%|▎         | 12/395 [00:03<01:52,  3.41it/s]  3%|▎         | 13/395 [00:03<01:51,  3.41it/s]  4%|▎         | 14/395 [00:04<01:51,  3.41it/s]  4%|▍         | 15/395 [00:04<01:51,  3.41it/s]  4%|▍         | 16/395 [00:04<01:50,  3.42it/s]  4%|▍         | 17/395 [00:04<01:50,  3.42it/s]  5%|▍         | 18/395 [00:05<01:50,  3.42it/s]  5%|▍         | 19/395 [00:05<01:50,  3.42it/s]  5%|▌         | 20/395 [00:05<01:49,  3.42it/s]  5%|▌         | 21/395 [00:06<01:49,  3.42it/s]  6%|▌         | 22/395 [00:06<01:49,  3.42it/s]  6%|▌         | 23/395 [00:06<01:48,  3.42it/s]  6%|▌         | 24/395 [00:07<01:48,  3.42it/s]  6%|▋         | 25/395 [00:07<01:48,  3.42it/s]  7%|▋         | 26/395 [00:07<01:47,  3.42it/s]  7%|▋         | 27/395 [00:07<01:47,  3.42it/s]  7%|▋         | 28/395 [00:08<01:47,  3.42it/s]  7%|▋         | 29/395 [00:08<01:47,  3.42it/s]  8%|▊         | 30/395 [00:08<01:46,  3.42it/s]  8%|▊         | 31/395 [00:09<01:46,  3.42it/s]  8%|▊         | 32/395 [00:09<01:46,  3.41it/s]  8%|▊         | 33/395 [00:09<01:45,  3.42it/s]  9%|▊         | 34/395 [00:09<01:45,  3.42it/s]  9%|▉         | 35/395 [00:10<01:45,  3.42it/s]  9%|▉         | 36/395 [00:10<01:45,  3.41it/s]  9%|▉         | 37/395 [00:10<01:44,  3.41it/s] 10%|▉         | 38/395 [00:11<01:44,  3.42it/s] 10%|▉         | 39/395 [00:11<01:44,  3.41it/s] 10%|█         | 40/395 [00:11<01:44,  3.41it/s] 10%|█         | 41/395 [00:12<01:43,  3.42it/s] 11%|█         | 42/395 [00:12<01:43,  3.41it/s] 11%|█         | 43/395 [00:12<01:43,  3.41it/s] 11%|█         | 44/395 [00:12<01:42,  3.42it/s] 11%|█▏        | 45/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 46/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 47/395 [00:13<01:41,  3.41it/s] 12%|█▏        | 48/395 [00:14<01:41,  3.41it/s] 12%|█▏        | 49/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 50/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 51/395 [00:14<01:41,  3.40it/s] 13%|█▎        | 52/395 [00:15<01:40,  3.40it/s] 13%|█▎        | 53/395 [00:15<01:40,  3.41it/s] 14%|█▎        | 54/395 [00:15<01:39,  3.41it/s] 14%|█▍        | 55/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 56/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 57/395 [00:16<01:39,  3.41it/s] 15%|█▍        | 58/395 [00:16<01:38,  3.41it/s] 15%|█▍        | 59/395 [00:17<01:38,  3.41it/s] 15%|█▌        | 60/395 [00:17<01:38,  3.41it/s] 15%|█▌        | 61/395 [00:17<01:37,  3.41it/s] 16%|█▌        | 62/395 [00:18<01:37,  3.41it/s] 16%|█▌        | 63/395 [00:18<01:37,  3.41it/s] 16%|█▌        | 64/395 [00:18<01:36,  3.41it/s] 16%|█▋        | 65/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 66/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 67/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 68/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 69/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 70/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 71/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 72/395 [00:21<01:34,  3.41it/s] 18%|█▊        | 73/395 [00:21<01:34,  3.41it/s] 19%|█▊        | 74/395 [00:21<01:34,  3.41it/s] 19%|█▉        | 75/395 [00:21<01:33,  3.41it/s] 19%|█▉        | 76/395 [00:22<01:33,  3.41it/s] 19%|█▉        | 77/395 [00:22<01:33,  3.41it/s] 20%|█▉        | 78/395 [00:22<01:32,  3.41it/s] 20%|██        | 79/395 [00:23<01:27,  3.62it/s][INFO|trainer.py:2140] 2023-08-27 22:55:06,168 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:55:06,168 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:55:06,168 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 54.86it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.70it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.85it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.08it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.69it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.42it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.32it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.19it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.32it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.30it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.28it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.14it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.06it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.08it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.01it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.00it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.09it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.14it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.19it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.12it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.09it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.02it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.92it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.96it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.09it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.16it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.22it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.12it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.13it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.01it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.92it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.95it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.02it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.05it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.20it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.18it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.10it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.98it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.97it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.02it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.96it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.23it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.18it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.12it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.04it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.02it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.79it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.92it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.89it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.05it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.80it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.08it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.03it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.95it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.92it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.66it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.91it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.96it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.83it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.08it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.23it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.96it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.94it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.84it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.92it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.98it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.06it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.12it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.22it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.18it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.12it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.01it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.95it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.94it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.05it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.98it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.06it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.12it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.12it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.13it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.02it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.90it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.01it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.93it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.07it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.06it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.13it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.06it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.01it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.92it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.01it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.02it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.08it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.00it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.03it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.12it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.11it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.98it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.92it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.96it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.98it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.03it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.13it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.15it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.92it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.00it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.89it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.97it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.01it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.08it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:36<01:27,  3.62it/s]
100%|██████████| 577/577 [00:13<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:55:19,287 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-27 22:55:19,306 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:55:21,056 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:55:21,079 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:55:21,089 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:41<30:27,  5.80s/it] 21%|██        | 81/395 [00:42<21:43,  4.15s/it] 21%|██        | 82/395 [00:42<15:37,  2.99s/it] 21%|██        | 83/395 [00:42<11:22,  2.19s/it] 21%|██▏       | 84/395 [00:42<08:24,  1.62s/it] 22%|██▏       | 85/395 [00:43<06:19,  1.22s/it] 22%|██▏       | 86/395 [00:43<04:52,  1.06it/s] 22%|██▏       | 87/395 [00:43<03:51,  1.33it/s] 22%|██▏       | 88/395 [00:44<03:09,  1.62it/s] 23%|██▎       | 89/395 [00:44<02:39,  1.92it/s] 23%|██▎       | 90/395 [00:44<02:18,  2.20it/s] 23%|██▎       | 91/395 [00:45<02:04,  2.45it/s] 23%|██▎       | 92/395 [00:45<01:53,  2.66it/s] 24%|██▎       | 93/395 [00:45<01:46,  2.83it/s] 24%|██▍       | 94/395 [00:45<01:41,  2.97it/s] 24%|██▍       | 95/395 [00:46<01:37,  3.07it/s] 24%|██▍       | 96/395 [00:46<01:35,  3.14it/s] 25%|██▍       | 97/395 [00:46<01:33,  3.18it/s] 25%|██▍       | 98/395 [00:47<01:32,  3.22it/s] 25%|██▌       | 99/395 [00:47<01:31,  3.25it/s] 25%|██▌       | 100/395 [00:47<01:30,  3.27it/s] 26%|██▌       | 101/395 [00:48<01:29,  3.29it/s] 26%|██▌       | 102/395 [00:48<01:28,  3.30it/s] 26%|██▌       | 103/395 [00:48<01:28,  3.31it/s] 26%|██▋       | 104/395 [00:48<01:27,  3.31it/s] 27%|██▋       | 105/395 [00:49<01:27,  3.32it/s] 27%|██▋       | 106/395 [00:49<01:27,  3.32it/s] 27%|██▋       | 107/395 [00:49<01:26,  3.31it/s] 27%|██▋       | 108/395 [00:50<01:26,  3.32it/s] 28%|██▊       | 109/395 [00:50<01:26,  3.32it/s] 28%|██▊       | 110/395 [00:50<01:25,  3.32it/s] 28%|██▊       | 111/395 [00:51<01:25,  3.31it/s] 28%|██▊       | 112/395 [00:51<01:25,  3.32it/s] 29%|██▊       | 113/395 [00:51<01:24,  3.33it/s] 29%|██▉       | 114/395 [00:52<01:26,  3.25it/s] 29%|██▉       | 115/395 [00:52<01:25,  3.28it/s] 29%|██▉       | 116/395 [00:52<01:24,  3.29it/s] 30%|██▉       | 117/395 [00:52<01:24,  3.28it/s] 30%|██▉       | 118/395 [00:53<01:24,  3.29it/s] 30%|███       | 119/395 [00:53<01:23,  3.30it/s] 30%|███       | 120/395 [00:53<01:23,  3.31it/s] 31%|███       | 121/395 [00:54<01:22,  3.31it/s] 31%|███       | 122/395 [00:54<01:22,  3.32it/s] 31%|███       | 123/395 [00:54<01:21,  3.32it/s] 31%|███▏      | 124/395 [00:55<01:21,  3.32it/s] 32%|███▏      | 125/395 [00:55<01:21,  3.32it/s] 32%|███▏      | 126/395 [00:55<01:20,  3.33it/s] 32%|███▏      | 127/395 [00:55<01:20,  3.33it/s] 32%|███▏      | 128/395 [00:56<01:20,  3.33it/s] 33%|███▎      | 129/395 [00:56<01:20,  3.32it/s] 33%|███▎      | 130/395 [00:56<01:19,  3.32it/s] 33%|███▎      | 131/395 [00:57<01:19,  3.32it/s] 33%|███▎      | 132/395 [00:57<01:19,  3.33it/s] 34%|███▎      | 133/395 [00:57<01:18,  3.32it/s] 34%|███▍      | 134/395 [00:58<01:18,  3.33it/s] 34%|███▍      | 135/395 [00:58<01:18,  3.32it/s] 34%|███▍      | 136/395 [00:58<01:17,  3.33it/s] 35%|███▍      | 137/395 [00:58<01:17,  3.33it/s] 35%|███▍      | 138/395 [00:59<01:17,  3.33it/s] 35%|███▌      | 139/395 [00:59<01:17,  3.31it/s] 35%|███▌      | 140/395 [00:59<01:16,  3.31it/s] 36%|███▌      | 141/395 [01:00<01:16,  3.32it/s] 36%|███▌      | 142/395 [01:00<01:16,  3.32it/s] 36%|███▌      | 143/395 [01:00<01:15,  3.32it/s] 36%|███▋      | 144/395 [01:01<01:15,  3.32it/s] 37%|███▋      | 145/395 [01:01<01:15,  3.33it/s] 37%|███▋      | 146/395 [01:01<01:14,  3.33it/s] 37%|███▋      | 147/395 [01:01<01:14,  3.33it/s] 37%|███▋      | 148/395 [01:02<01:14,  3.33it/s] 38%|███▊      | 149/395 [01:02<01:14,  3.32it/s] 38%|███▊      | 150/395 [01:02<01:13,  3.33it/s] 38%|███▊      | 151/395 [01:03<01:13,  3.33it/s] 38%|███▊      | 152/395 [01:03<01:13,  3.33it/s] 39%|███▊      | 153/395 [01:03<01:12,  3.33it/s] 39%|███▉      | 154/395 [01:04<01:12,  3.33it/s] 39%|███▉      | 155/395 [01:04<01:12,  3.33it/s] 39%|███▉      | 156/395 [01:04<01:11,  3.33it/s] 40%|███▉      | 157/395 [01:04<01:11,  3.33it/s] 40%|████      | 158/395 [01:05<01:07,  3.53it/s][INFO|trainer.py:2140] 2023-08-27 22:55:48,281 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:55:48,281 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:55:48,281 >>   Batch size = 8
{'eval_loss': 0.9929807782173157, 'eval_runtime': 13.0962, 'eval_samples_per_second': 351.935, 'eval_steps_per_second': 44.059, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.16it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.99it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.28it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.54it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.94it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.40it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.18it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.03it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.11it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.16it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.25it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.30it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.25it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.09it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.91it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.77it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.97it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.10it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.17it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.08it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.99it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.89it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.79it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.89it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.99it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.19it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.18it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 43.95it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.08it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.91it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.88it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.94it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.04it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.23it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.21it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.07it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.00it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.86it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.82it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.94it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.91it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.06it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.25it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.21it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.14it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.94it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.86it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.92it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.91it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.98it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.16it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.20it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.01it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.89it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.83it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.85it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.88it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.93it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.21it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.14it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.91it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.87it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.79it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.84it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.96it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.16it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.26it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.22it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.97it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.86it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.83it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.92it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.95it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.06it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.20it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.24it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.16it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.91it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.79it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.90it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.93it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.97it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.08it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.26it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.08it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.85it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.82it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.95it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.11it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.17it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.21it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.08it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.94it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.75it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.97it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.90it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.07it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.07it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.05it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.06it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.94it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.88it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.06it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A                                                 
                                                 [A 40%|████      | 158/395 [01:18<01:07,  3.53it/s]
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:56:01,414 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-27 22:56:01,437 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:56:03,453 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:56:03,476 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:56:03,489 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:24<23:10,  5.89s/it] 41%|████      | 160/395 [01:24<16:30,  4.21s/it] 41%|████      | 161/395 [01:24<11:51,  3.04s/it] 41%|████      | 162/395 [01:25<08:36,  2.22s/it] 41%|████▏     | 163/395 [01:25<06:21,  1.64s/it] 42%|████▏     | 164/395 [01:25<04:46,  1.24s/it] 42%|████▏     | 165/395 [01:25<03:40,  1.04it/s] 42%|████▏     | 166/395 [01:26<02:54,  1.31it/s] 42%|████▏     | 167/395 [01:26<02:21,  1.61it/s] 43%|████▎     | 168/395 [01:26<01:59,  1.90it/s] 43%|████▎     | 169/395 [01:27<01:43,  2.18it/s] 43%|████▎     | 170/395 [01:27<01:32,  2.43it/s] 43%|████▎     | 171/395 [01:27<01:24,  2.64it/s] 44%|████▎     | 172/395 [01:28<01:19,  2.82it/s] 44%|████▍     | 173/395 [01:28<01:15,  2.95it/s] 44%|████▍     | 174/395 [01:28<01:12,  3.06it/s] 44%|████▍     | 175/395 [01:28<01:10,  3.14it/s] 45%|████▍     | 176/395 [01:29<01:08,  3.19it/s] 45%|████▍     | 177/395 [01:29<01:07,  3.23it/s] 45%|████▌     | 178/395 [01:29<01:06,  3.26it/s] 45%|████▌     | 179/395 [01:30<01:05,  3.28it/s] 46%|████▌     | 180/395 [01:30<01:05,  3.29it/s] 46%|████▌     | 181/395 [01:30<01:04,  3.30it/s] 46%|████▌     | 182/395 [01:31<01:04,  3.31it/s] 46%|████▋     | 183/395 [01:31<01:03,  3.31it/s] 47%|████▋     | 184/395 [01:31<01:03,  3.32it/s] 47%|████▋     | 185/395 [01:32<01:03,  3.32it/s] 47%|████▋     | 186/395 [01:32<01:02,  3.32it/s] 47%|████▋     | 187/395 [01:32<01:02,  3.33it/s] 48%|████▊     | 188/395 [01:32<01:02,  3.33it/s] 48%|████▊     | 189/395 [01:33<01:01,  3.33it/s] 48%|████▊     | 190/395 [01:33<01:01,  3.32it/s] 48%|████▊     | 191/395 [01:33<01:01,  3.32it/s] 49%|████▊     | 192/395 [01:34<01:01,  3.32it/s] 49%|████▉     | 193/395 [01:34<01:00,  3.33it/s] 49%|████▉     | 194/395 [01:34<01:00,  3.33it/s] 49%|████▉     | 195/395 [01:35<01:00,  3.32it/s] 50%|████▉     | 196/395 [01:35<00:59,  3.33it/s] 50%|████▉     | 197/395 [01:35<00:59,  3.33it/s] 50%|█████     | 198/395 [01:35<00:59,  3.33it/s] 50%|█████     | 199/395 [01:36<00:58,  3.33it/s] 51%|█████     | 200/395 [01:36<00:58,  3.32it/s] 51%|█████     | 201/395 [01:36<00:58,  3.32it/s] 51%|█████     | 202/395 [01:37<00:57,  3.33it/s] 51%|█████▏    | 203/395 [01:37<00:57,  3.33it/s] 52%|█████▏    | 204/395 [01:37<00:57,  3.33it/s] 52%|█████▏    | 205/395 [01:38<00:57,  3.33it/s] 52%|█████▏    | 206/395 [01:38<00:56,  3.33it/s] 52%|█████▏    | 207/395 [01:38<00:56,  3.33it/s] 53%|█████▎    | 208/395 [01:38<00:56,  3.33it/s] 53%|█████▎    | 209/395 [01:39<00:55,  3.33it/s] 53%|█████▎    | 210/395 [01:39<00:55,  3.32it/s] 53%|█████▎    | 211/395 [01:39<00:55,  3.32it/s] 54%|█████▎    | 212/395 [01:40<00:55,  3.32it/s] 54%|█████▍    | 213/395 [01:40<00:54,  3.32it/s] 54%|█████▍    | 214/395 [01:40<00:54,  3.33it/s] 54%|█████▍    | 215/395 [01:41<00:54,  3.33it/s] 55%|█████▍    | 216/395 [01:41<00:53,  3.32it/s] 55%|█████▍    | 217/395 [01:41<00:53,  3.32it/s] 55%|█████▌    | 218/395 [01:41<00:53,  3.32it/s] 55%|█████▌    | 219/395 [01:42<00:52,  3.33it/s] 56%|█████▌    | 220/395 [01:42<00:52,  3.32it/s] 56%|█████▌    | 221/395 [01:42<00:52,  3.32it/s] 56%|█████▌    | 222/395 [01:43<00:51,  3.33it/s] 56%|█████▋    | 223/395 [01:43<00:51,  3.33it/s] 57%|█████▋    | 224/395 [01:43<00:51,  3.33it/s] 57%|█████▋    | 225/395 [01:44<00:51,  3.33it/s] 57%|█████▋    | 226/395 [01:44<00:50,  3.33it/s] 57%|█████▋    | 227/395 [01:44<00:50,  3.33it/s] 58%|█████▊    | 228/395 [01:44<00:50,  3.33it/s] 58%|█████▊    | 229/395 [01:45<00:49,  3.33it/s] 58%|█████▊    | 230/395 [01:45<00:49,  3.33it/s] 58%|█████▊    | 231/395 [01:45<00:49,  3.33it/s] 59%|█████▊    | 232/395 [01:46<00:48,  3.33it/s] 59%|█████▉    | 233/395 [01:46<00:48,  3.33it/s] 59%|█████▉    | 234/395 [01:46<00:48,  3.33it/s] 59%|█████▉    | 235/395 [01:47<00:48,  3.33it/s] 60%|█████▉    | 236/395 [01:47<00:47,  3.33it/s] 60%|██████    | 237/395 [01:47<00:44,  3.54it/s][INFO|trainer.py:2140] 2023-08-27 22:56:30,642 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:56:30,642 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:56:30,642 >>   Batch size = 8
{'eval_loss': 0.9820364713668823, 'eval_runtime': 13.1005, 'eval_samples_per_second': 351.819, 'eval_steps_per_second': 44.044, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.16it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.52it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.16it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.16it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.75it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.41it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.24it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.16it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.31it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.09it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.18it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.09it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.91it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.99it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.02it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.93it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.18it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.25it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.08it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.08it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.97it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.81it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.96it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.94it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.05it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.13it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.20it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.15it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.03it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.87it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.85it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.87it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.01it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.08it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.12it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.07it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.14it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.64it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.77it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.88it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.82it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.01it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.07it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.09it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.21it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.13it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.85it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.87it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.89it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.94it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.86it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.01it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.05it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.07it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.11it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.99it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.00it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.01it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.01it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.03it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.01it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.20it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.15it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.05it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.98it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.07it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.08it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.95it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.11it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.08it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.07it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.04it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.00it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.97it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.13it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.05it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.18it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.13it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.12it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.93it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.92it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.87it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.93it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.04it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.14it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.12it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.16it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.98it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.96it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.02it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.96it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.12it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.05it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.08it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.03it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.02it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.94it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.98it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.95it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.16it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.09it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.01it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.91it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.99it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.91it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.93it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.31it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [02:00<00:44,  3.54it/s]
100%|██████████| 577/577 [00:13<00:00, 44.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:56:43,755 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-27 22:56:43,780 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:56:45,375 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:56:45,390 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:56:45,399 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:06<15:02,  5.75s/it] 61%|██████    | 239/395 [02:06<10:41,  4.11s/it] 61%|██████    | 240/395 [02:06<07:39,  2.97s/it] 61%|██████    | 241/395 [02:06<05:33,  2.16s/it] 61%|██████▏   | 242/395 [02:07<04:05,  1.60s/it] 62%|██████▏   | 243/395 [02:07<03:03,  1.21s/it] 62%|██████▏   | 244/395 [02:07<02:21,  1.07it/s] 62%|██████▏   | 245/395 [02:08<01:51,  1.35it/s] 62%|██████▏   | 246/395 [02:08<01:30,  1.65it/s] 63%|██████▎   | 247/395 [02:08<01:15,  1.95it/s] 63%|██████▎   | 248/395 [02:08<01:05,  2.24it/s] 63%|██████▎   | 249/395 [02:09<00:58,  2.50it/s] 63%|██████▎   | 250/395 [02:09<00:53,  2.70it/s] 64%|██████▎   | 251/395 [02:09<00:49,  2.88it/s] 64%|██████▍   | 252/395 [02:10<00:47,  3.03it/s] 64%|██████▍   | 253/395 [02:10<00:45,  3.13it/s] 64%|██████▍   | 254/395 [02:10<00:43,  3.21it/s] 65%|██████▍   | 255/395 [02:11<00:42,  3.27it/s] 65%|██████▍   | 256/395 [02:11<00:41,  3.31it/s] 65%|██████▌   | 257/395 [02:11<00:41,  3.34it/s] 65%|██████▌   | 258/395 [02:11<00:40,  3.36it/s] 66%|██████▌   | 259/395 [02:12<00:40,  3.38it/s] 66%|██████▌   | 260/395 [02:12<00:39,  3.39it/s] 66%|██████▌   | 261/395 [02:12<00:39,  3.39it/s] 66%|██████▋   | 262/395 [02:13<00:39,  3.39it/s] 67%|██████▋   | 263/395 [02:13<00:38,  3.40it/s] 67%|██████▋   | 264/395 [02:13<00:38,  3.40it/s] 67%|██████▋   | 265/395 [02:13<00:38,  3.40it/s] 67%|██████▋   | 266/395 [02:14<00:37,  3.41it/s] 68%|██████▊   | 267/395 [02:14<00:37,  3.41it/s] 68%|██████▊   | 268/395 [02:14<00:37,  3.41it/s] 68%|██████▊   | 269/395 [02:15<00:36,  3.41it/s] 68%|██████▊   | 270/395 [02:15<00:36,  3.41it/s] 69%|██████▊   | 271/395 [02:15<00:36,  3.41it/s] 69%|██████▉   | 272/395 [02:16<00:36,  3.41it/s] 69%|██████▉   | 273/395 [02:16<00:35,  3.41it/s] 69%|██████▉   | 274/395 [02:16<00:35,  3.41it/s] 70%|██████▉   | 275/395 [02:16<00:35,  3.41it/s] 70%|██████▉   | 276/395 [02:17<00:34,  3.41it/s] 70%|███████   | 277/395 [02:17<00:34,  3.41it/s] 70%|███████   | 278/395 [02:17<00:34,  3.41it/s] 71%|███████   | 279/395 [02:18<00:34,  3.41it/s] 71%|███████   | 280/395 [02:18<00:33,  3.41it/s] 71%|███████   | 281/395 [02:18<00:33,  3.41it/s] 71%|███████▏  | 282/395 [02:18<00:33,  3.41it/s] 72%|███████▏  | 283/395 [02:19<00:32,  3.40it/s] 72%|███████▏  | 284/395 [02:19<00:32,  3.41it/s] 72%|███████▏  | 285/395 [02:19<00:32,  3.41it/s] 72%|███████▏  | 286/395 [02:20<00:31,  3.41it/s] 73%|███████▎  | 287/395 [02:20<00:31,  3.41it/s] 73%|███████▎  | 288/395 [02:20<00:31,  3.41it/s] 73%|███████▎  | 289/395 [02:21<00:31,  3.41it/s] 73%|███████▎  | 290/395 [02:21<00:30,  3.41it/s] 74%|███████▎  | 291/395 [02:21<00:30,  3.41it/s] 74%|███████▍  | 292/395 [02:21<00:30,  3.41it/s] 74%|███████▍  | 293/395 [02:22<00:29,  3.41it/s] 74%|███████▍  | 294/395 [02:22<00:29,  3.41it/s] 75%|███████▍  | 295/395 [02:22<00:29,  3.41it/s] 75%|███████▍  | 296/395 [02:23<00:29,  3.41it/s] 75%|███████▌  | 297/395 [02:23<00:28,  3.41it/s] 75%|███████▌  | 298/395 [02:23<00:28,  3.41it/s] 76%|███████▌  | 299/395 [02:23<00:28,  3.41it/s] 76%|███████▌  | 300/395 [02:24<00:27,  3.41it/s] 76%|███████▌  | 301/395 [02:24<00:27,  3.41it/s] 76%|███████▋  | 302/395 [02:24<00:27,  3.41it/s] 77%|███████▋  | 303/395 [02:25<00:26,  3.41it/s] 77%|███████▋  | 304/395 [02:25<00:26,  3.41it/s] 77%|███████▋  | 305/395 [02:25<00:26,  3.41it/s] 77%|███████▋  | 306/395 [02:26<00:26,  3.41it/s] 78%|███████▊  | 307/395 [02:26<00:25,  3.41it/s] 78%|███████▊  | 308/395 [02:26<00:25,  3.41it/s] 78%|███████▊  | 309/395 [02:26<00:25,  3.40it/s] 78%|███████▊  | 310/395 [02:27<00:24,  3.40it/s] 79%|███████▊  | 311/395 [02:27<00:24,  3.41it/s] 79%|███████▉  | 312/395 [02:27<00:24,  3.41it/s] 79%|███████▉  | 313/395 [02:28<00:24,  3.41it/s] 79%|███████▉  | 314/395 [02:28<00:23,  3.41it/s] 80%|███████▉  | 315/395 [02:28<00:23,  3.41it/s] 80%|████████  | 316/395 [02:28<00:21,  3.62it/s][INFO|trainer.py:2140] 2023-08-27 22:57:11,951 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:57:11,951 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:57:11,951 >>   Batch size = 8
{'eval_loss': 0.98235023021698, 'eval_runtime': 13.0978, 'eval_samples_per_second': 351.89, 'eval_steps_per_second': 44.053, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.06it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.67it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.97it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.07it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.81it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.40it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.20it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.25it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.36it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.16it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.97it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.94it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.91it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.03it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.92it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.04it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.20it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.24it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.08it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.90it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.01it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.95it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.91it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.14it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.21it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.14it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.08it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.93it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.92it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.00it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.96it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.93it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.19it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.25it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.17it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.08it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.98it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.85it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.85it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.00it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.19it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.21it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.17it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.11it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.91it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.90it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.77it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.79it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.09it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.22it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.13it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.06it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.97it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.80it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.87it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.94it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.09it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.15it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.13it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.97it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.90it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.89it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.96it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.13it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.14it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.05it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.18it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.12it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.95it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.97it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.96it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.02it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.22it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.14it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.16it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.18it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.06it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.78it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.77it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.85it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.89it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.08it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.17it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.23it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.18it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.12it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.97it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.93it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.92it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.00it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.07it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.12it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.21it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.03it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.03it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.97it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.95it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.92it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.99it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.05it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.21it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.15it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.13it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.99it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.96it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.89it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.93it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [02:41<00:21,  3.62it/s]
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:57:25,075 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-27 22:57:25,095 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:57:26,521 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:57:26,536 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:57:26,547 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [02:47<07:29,  5.76s/it] 81%|████████  | 318/395 [02:47<05:17,  4.12s/it] 81%|████████  | 319/395 [02:48<03:46,  2.97s/it] 81%|████████  | 320/395 [02:48<02:42,  2.17s/it] 81%|████████▏ | 321/395 [02:48<01:59,  1.61s/it] 82%|████████▏ | 322/395 [02:48<01:28,  1.22s/it] 82%|████████▏ | 323/395 [02:49<01:07,  1.06it/s] 82%|████████▏ | 324/395 [02:49<00:53,  1.33it/s] 82%|████████▏ | 325/395 [02:49<00:43,  1.63it/s] 83%|████████▎ | 326/395 [02:50<00:35,  1.92it/s] 83%|████████▎ | 327/395 [02:50<00:30,  2.20it/s] 83%|████████▎ | 328/395 [02:50<00:27,  2.45it/s] 83%|████████▎ | 329/395 [02:51<00:24,  2.66it/s] 84%|████████▎ | 330/395 [02:51<00:23,  2.82it/s] 84%|████████▍ | 331/395 [02:51<00:21,  2.96it/s] 84%|████████▍ | 332/395 [02:51<00:20,  3.06it/s] 84%|████████▍ | 333/395 [02:52<00:20,  3.06it/s] 85%|████████▍ | 334/395 [02:52<00:19,  3.14it/s] 85%|████████▍ | 335/395 [02:52<00:18,  3.19it/s] 85%|████████▌ | 336/395 [02:53<00:18,  3.23it/s] 85%|████████▌ | 337/395 [02:53<00:17,  3.26it/s] 86%|████████▌ | 338/395 [02:53<00:17,  3.28it/s] 86%|████████▌ | 339/395 [02:54<00:16,  3.29it/s] 86%|████████▌ | 340/395 [02:54<00:16,  3.29it/s] 86%|████████▋ | 341/395 [02:54<00:16,  3.30it/s] 87%|████████▋ | 342/395 [02:54<00:15,  3.31it/s] 87%|████████▋ | 343/395 [02:55<00:15,  3.32it/s] 87%|████████▋ | 344/395 [02:55<00:15,  3.32it/s] 87%|████████▋ | 345/395 [02:55<00:15,  3.33it/s] 88%|████████▊ | 346/395 [02:56<00:14,  3.33it/s] 88%|████████▊ | 347/395 [02:56<00:14,  3.33it/s] 88%|████████▊ | 348/395 [02:56<00:14,  3.33it/s] 88%|████████▊ | 349/395 [02:57<00:13,  3.33it/s] 89%|████████▊ | 350/395 [02:57<00:13,  3.32it/s] 89%|████████▉ | 351/395 [02:57<00:13,  3.33it/s] 89%|████████▉ | 352/395 [02:57<00:12,  3.33it/s] 89%|████████▉ | 353/395 [02:58<00:12,  3.33it/s] 90%|████████▉ | 354/395 [02:58<00:12,  3.33it/s] 90%|████████▉ | 355/395 [02:58<00:11,  3.33it/s] 90%|█████████ | 356/395 [02:59<00:11,  3.33it/s] 90%|█████████ | 357/395 [02:59<00:11,  3.33it/s] 91%|█████████ | 358/395 [02:59<00:11,  3.33it/s] 91%|█████████ | 359/395 [03:00<00:10,  3.34it/s] 91%|█████████ | 360/395 [03:00<00:10,  3.33it/s] 91%|█████████▏| 361/395 [03:00<00:10,  3.33it/s] 92%|█████████▏| 362/395 [03:00<00:09,  3.33it/s] 92%|█████████▏| 363/395 [03:01<00:09,  3.33it/s] 92%|█████████▏| 364/395 [03:01<00:09,  3.33it/s] 92%|█████████▏| 365/395 [03:01<00:09,  3.33it/s] 93%|█████████▎| 366/395 [03:02<00:08,  3.33it/s] 93%|█████████▎| 367/395 [03:02<00:08,  3.33it/s] 93%|█████████▎| 368/395 [03:02<00:08,  3.33it/s] 93%|█████████▎| 369/395 [03:03<00:07,  3.33it/s] 94%|█████████▎| 370/395 [03:03<00:07,  3.33it/s] 94%|█████████▍| 371/395 [03:03<00:07,  3.33it/s] 94%|█████████▍| 372/395 [03:03<00:06,  3.33it/s] 94%|█████████▍| 373/395 [03:04<00:06,  3.33it/s] 95%|█████████▍| 374/395 [03:04<00:06,  3.33it/s] 95%|█████████▍| 375/395 [03:04<00:06,  3.33it/s] 95%|█████████▌| 376/395 [03:05<00:05,  3.33it/s] 95%|█████████▌| 377/395 [03:05<00:05,  3.33it/s] 96%|█████████▌| 378/395 [03:05<00:05,  3.33it/s] 96%|█████████▌| 379/395 [03:06<00:04,  3.33it/s] 96%|█████████▌| 380/395 [03:06<00:04,  3.32it/s] 96%|█████████▋| 381/395 [03:06<00:04,  3.32it/s] 97%|█████████▋| 382/395 [03:06<00:03,  3.32it/s] 97%|█████████▋| 383/395 [03:07<00:03,  3.32it/s] 97%|█████████▋| 384/395 [03:07<00:03,  3.32it/s] 97%|█████████▋| 385/395 [03:07<00:03,  3.32it/s] 98%|█████████▊| 386/395 [03:08<00:02,  3.32it/s] 98%|█████████▊| 387/395 [03:08<00:02,  3.32it/s] 98%|█████████▊| 388/395 [03:08<00:02,  3.33it/s] 98%|█████████▊| 389/395 [03:09<00:01,  3.33it/s] 99%|█████████▊| 390/395 [03:09<00:01,  3.31it/s] 99%|█████████▉| 391/395 [03:09<00:01,  3.32it/s] 99%|█████████▉| 392/395 [03:09<00:00,  3.32it/s] 99%|█████████▉| 393/395 [03:10<00:00,  3.33it/s]100%|█████████▉| 394/395 [03:10<00:00,  3.33it/s]100%|██████████| 395/395 [03:10<00:00,  3.53it/s][INFO|trainer.py:2140] 2023-08-27 22:57:53,898 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:57:53,898 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:57:53,898 >>   Batch size = 8
{'eval_loss': 0.9843268990516663, 'eval_runtime': 13.0949, 'eval_samples_per_second': 351.968, 'eval_steps_per_second': 44.063, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.36it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.81it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.04it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.15it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.71it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.41it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.25it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.16it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.23it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.25it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.09it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.96it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.98it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.88it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.87it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.97it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.16it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.13it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.16it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.07it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.11it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.93it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.96it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.96it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.99it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.24it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.28it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.18it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.08it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.93it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.93it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.90it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.94it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.08it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.23it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.17it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.12it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.02it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.93it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.97it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.94it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.07it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.18it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.27it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.13it/s][A
 41%|████      | 237/577 [00:05<00:07, 43.90it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.04it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.00it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.00it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.95it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.08it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.16it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.19it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.95it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.03it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.04it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.89it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.97it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.08it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.01it/s][A
 54%|█████▍    | 312/577 [00:07<00:05, 44.27it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.06it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.13it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.95it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.96it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.01it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.12it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.22it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.14it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.15it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.99it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.97it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.01it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.00it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.03it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.18it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.15it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.05it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.07it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.06it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.02it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.03it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.06it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.05it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.23it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.03it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.02it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.04it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.05it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.87it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.07it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.11it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.18it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.24it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.95it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.99it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.00it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.96it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.04it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.08it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.26it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.19it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.15it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.02it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.96it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.71it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.89it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.92it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.10it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.22it/s][A                                                 
                                                 [A100%|██████████| 395/395 [03:23<00:00,  3.53it/s]
100%|██████████| 577/577 [00:13<00:00, 44.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:58:07,008 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-27 22:58:07,029 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:58:09,053 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:58:09,069 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:58:09,084 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:58:12,317 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:58:12,320 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158 (score: 0.9820364713668823).
                                                 100%|██████████| 395/395 [03:31<00:00,  3.53it/s]100%|██████████| 395/395 [03:31<00:00,  1.87it/s]
[INFO|trainer.py:1894] 2023-08-27 22:58:14,070 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:58:14,092 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:58:15,739 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:58:15,757 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:58:15,772 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:58:15,953 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   train_loss               =     0.7301
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   train_runtime            = 0:03:31.00
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   train_samples            =       5042
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   train_samples_per_second =    119.478
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:15,954 >>   train_steps_per_second   =      1.872
{'eval_loss': 0.9862211346626282, 'eval_runtime': 13.0914, 'eval_samples_per_second': 352.064, 'eval_steps_per_second': 44.075, 'epoch': 5.0}
{'train_runtime': 211.0017, 'train_samples_per_second': 119.478, 'train_steps_per_second': 1.872, 'train_loss': 0.7301434866989716, 'epoch': 5.0}
08/27/2023 22:58:15 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:58:16,000 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:58:16,000 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-27 22:58:16,000 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.55it/s]  2%|▏         | 12/577 [00:00<00:11, 48.17it/s]  3%|▎         | 17/577 [00:00<00:12, 46.49it/s]  4%|▍         | 22/577 [00:00<00:12, 45.85it/s]  5%|▍         | 27/577 [00:00<00:12, 45.28it/s]  6%|▌         | 32/577 [00:00<00:12, 44.95it/s]  6%|▋         | 37/577 [00:00<00:12, 44.86it/s]  7%|▋         | 42/577 [00:00<00:12, 44.43it/s]  8%|▊         | 47/577 [00:01<00:12, 44.03it/s]  9%|▉         | 52/577 [00:01<00:11, 44.04it/s] 10%|▉         | 57/577 [00:01<00:11, 44.10it/s] 11%|█         | 62/577 [00:01<00:11, 44.21it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.17it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.24it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.23it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.12it/s] 15%|█▌        | 87/577 [00:01<00:11, 43.88it/s] 16%|█▌        | 92/577 [00:02<00:11, 43.67it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.84it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.98it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.15it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.24it/s] 20%|██        | 117/577 [00:02<00:10, 44.21it/s] 21%|██        | 122/577 [00:02<00:10, 44.11it/s] 22%|██▏       | 127/577 [00:02<00:10, 43.92it/s] 23%|██▎       | 132/577 [00:02<00:10, 43.85it/s] 24%|██▎       | 137/577 [00:03<00:10, 43.80it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.78it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.93it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.11it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.25it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.19it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.05it/s] 30%|██▉       | 172/577 [00:03<00:09, 43.91it/s] 31%|███       | 177/577 [00:03<00:09, 43.89it/s] 32%|███▏      | 182/577 [00:04<00:09, 43.79it/s] 32%|███▏      | 187/577 [00:04<00:08, 43.85it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.96it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.00it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.29it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.22it/s] 37%|███▋      | 212/577 [00:04<00:08, 43.99it/s] 38%|███▊      | 217/577 [00:04<00:08, 43.99it/s] 38%|███▊      | 222/577 [00:05<00:08, 43.88it/s] 39%|███▉      | 227/577 [00:05<00:07, 43.80it/s] 40%|████      | 232/577 [00:05<00:07, 43.93it/s] 41%|████      | 237/577 [00:05<00:07, 44.01it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.09it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.10it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.18it/s] 45%|████▍     | 257/577 [00:05<00:07, 43.74it/s] 45%|████▌     | 262/577 [00:05<00:07, 43.87it/s] 46%|████▋     | 267/577 [00:06<00:07, 43.92it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.03it/s] 48%|████▊     | 277/577 [00:06<00:06, 44.00it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.08it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.20it/s] 51%|█████     | 292/577 [00:06<00:06, 44.23it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.08it/s] 52%|█████▏    | 302/577 [00:06<00:06, 43.92it/s] 53%|█████▎    | 307/577 [00:06<00:06, 43.82it/s] 54%|█████▍    | 312/577 [00:07<00:06, 43.79it/s] 55%|█████▍    | 317/577 [00:07<00:05, 43.96it/s] 56%|█████▌    | 322/577 [00:07<00:05, 43.99it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.17it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.23it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.28it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.01it/s] 60%|██████    | 347/577 [00:07<00:05, 43.82it/s] 61%|██████    | 352/577 [00:07<00:05, 43.79it/s] 62%|██████▏   | 357/577 [00:08<00:05, 43.89it/s] 63%|██████▎   | 362/577 [00:08<00:04, 43.89it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.03it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.12it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.28it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.21it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.02it/s] 68%|██████▊   | 392/577 [00:08<00:04, 43.82it/s] 69%|██████▉   | 397/577 [00:08<00:04, 43.92it/s] 70%|██████▉   | 402/577 [00:09<00:03, 43.96it/s] 71%|███████   | 407/577 [00:09<00:03, 43.91it/s] 71%|███████▏  | 412/577 [00:09<00:03, 44.02it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.13it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.05it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.13it/s] 75%|███████▍  | 432/577 [00:09<00:03, 43.98it/s] 76%|███████▌  | 437/577 [00:09<00:03, 43.89it/s] 77%|███████▋  | 442/577 [00:10<00:03, 43.84it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.01it/s] 78%|███████▊  | 452/577 [00:10<00:02, 43.97it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.10it/s] 80%|████████  | 462/577 [00:10<00:02, 44.21it/s] 81%|████████  | 467/577 [00:10<00:02, 44.17it/s] 82%|████████▏ | 472/577 [00:10<00:02, 43.99it/s] 83%|████████▎ | 477/577 [00:10<00:02, 43.95it/s] 84%|████████▎ | 482/577 [00:10<00:02, 43.90it/s] 84%|████████▍ | 487/577 [00:11<00:02, 43.90it/s] 85%|████████▌ | 492/577 [00:11<00:01, 43.93it/s] 86%|████████▌ | 497/577 [00:11<00:01, 43.90it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.01it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.15it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.11it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.05it/s] 90%|█████████ | 522/577 [00:11<00:01, 43.96it/s] 91%|█████████▏| 527/577 [00:11<00:01, 43.90it/s] 92%|█████████▏| 532/577 [00:12<00:01, 43.84it/s] 93%|█████████▎| 537/577 [00:12<00:00, 43.98it/s] 94%|█████████▍| 542/577 [00:12<00:00, 43.96it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.03it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.07it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.11it/s] 97%|█████████▋| 562/577 [00:12<00:00, 43.97it/s] 98%|█████████▊| 567/577 [00:12<00:00, 43.94it/s] 99%|█████████▉| 572/577 [00:12<00:00, 43.99it/s]100%|██████████| 577/577 [00:13<00:00, 44.07it/s]100%|██████████| 577/577 [00:13<00:00, 44.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:58:29,100 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   eval_loss               =      0.982
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   eval_runtime            = 0:00:13.09
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   eval_samples_per_second =    351.834
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   eval_steps_per_second   =     44.046
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:58:29,100 >>   perplexity              =     2.6699
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_2', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:22<03:19, 22.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:38<02:31, 19.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:01<02:25, 20.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:24<02:09, 21.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:41<01:40, 20.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:59<01:16, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:18<00:57, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:35<00:36, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:55<00:18, 18.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:20<00:00, 20.90s/it]Generating: 100%|██████████| 10/10 [03:20<00:00, 20.06s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_5_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.20s/it]Extractor Estimating: 2it [00:17,  7.48s/it]Extractor Estimating: 3it [00:18,  4.40s/it]Extractor Estimating: 4it [00:19,  3.18s/it]Extractor Estimating: 5it [00:25,  4.19s/it]Extractor Estimating: 6it [00:26,  3.02s/it]Extractor Estimating: 7it [00:27,  2.28s/it]Extractor Estimating: 8it [00:27,  1.79s/it]Extractor Estimating: 9it [00:28,  1.45s/it]Extractor Estimating: 10it [00:29,  1.24s/it]Extractor Estimating: 11it [00:30,  1.12s/it]Extractor Estimating: 12it [00:30,  1.00s/it]Extractor Estimating: 13it [00:31,  1.08it/s]Extractor Estimating: 14it [00:32,  1.14it/s]Extractor Estimating: 15it [00:33,  1.20it/s]Extractor Estimating: 16it [00:33,  1.23it/s]Extractor Estimating: 17it [00:34,  1.26it/s]Extractor Estimating: 18it [00:35,  1.27it/s]Extractor Estimating: 19it [00:36,  1.28it/s]Extractor Estimating: 20it [00:37,  1.27it/s]Extractor Estimating: 21it [00:37,  1.30it/s]Extractor Estimating: 22it [00:38,  1.29it/s]Extractor Estimating: 23it [00:39,  1.27it/s]Extractor Estimating: 24it [00:40,  1.24it/s]Extractor Estimating: 25it [00:40,  1.28it/s]Extractor Estimating: 26it [00:41,  1.30it/s]Extractor Estimating: 27it [00:42,  1.30it/s]Extractor Estimating: 28it [00:43,  1.33it/s]Extractor Estimating: 29it [00:43,  1.36it/s]Extractor Estimating: 30it [00:44,  1.34it/s]Extractor Estimating: 31it [00:45,  1.34it/s]Extractor Estimating: 32it [00:46,  1.29it/s]Extractor Estimating: 33it [00:46,  1.32it/s]Extractor Estimating: 34it [00:47,  1.34it/s]Extractor Estimating: 35it [00:48,  1.30it/s]Extractor Estimating: 36it [00:49,  1.35it/s]Extractor Estimating: 37it [00:49,  1.33it/s]Extractor Estimating: 38it [00:50,  1.34it/s]Extractor Estimating: 39it [00:51,  1.36it/s]Extractor Estimating: 40it [00:52,  1.33it/s]Extractor Estimating: 41it [00:52,  1.29it/s]Extractor Estimating: 42it [00:53,  1.31it/s]Extractor Estimating: 43it [00:54,  1.35it/s]Extractor Estimating: 44it [00:55,  1.37it/s]Extractor Estimating: 45it [00:55,  1.37it/s]Extractor Estimating: 46it [00:56,  1.37it/s]Extractor Estimating: 47it [00:57,  1.35it/s]Extractor Estimating: 48it [00:58,  1.36it/s]Extractor Estimating: 49it [00:58,  1.34it/s]Extractor Estimating: 50it [00:59,  1.36it/s]Extractor Estimating: 51it [01:00,  1.33it/s]Extractor Estimating: 52it [01:01,  1.29it/s]Extractor Estimating: 53it [01:01,  1.31it/s]Extractor Estimating: 54it [01:02,  1.30it/s]Extractor Estimating: 55it [01:03,  1.31it/s]Extractor Estimating: 56it [01:04,  1.27it/s]Extractor Estimating: 57it [01:05,  1.24it/s]Extractor Estimating: 58it [01:05,  1.30it/s]Extractor Estimating: 59it [01:06,  1.33it/s]Extractor Estimating: 60it [01:07,  1.32it/s]Extractor Estimating: 61it [01:08,  1.29it/s]Extractor Estimating: 62it [01:08,  1.31it/s]Extractor Estimating: 63it [01:09,  1.34it/s]Extractor Estimating: 64it [01:10,  1.32it/s]Extractor Estimating: 65it [01:11,  1.31it/s]Extractor Estimating: 66it [01:11,  1.30it/s]Extractor Estimating: 67it [01:12,  1.29it/s]Extractor Estimating: 68it [01:13,  1.31it/s]Extractor Estimating: 69it [01:14,  1.32it/s]Extractor Estimating: 70it [01:14,  1.30it/s]Extractor Estimating: 71it [01:15,  1.29it/s]Extractor Estimating: 72it [01:16,  1.29it/s]Extractor Estimating: 73it [01:17,  1.30it/s]Extractor Estimating: 74it [01:17,  1.32it/s]Extractor Estimating: 75it [01:18,  1.33it/s]Extractor Estimating: 76it [01:19,  1.37it/s]Extractor Estimating: 77it [01:20,  1.30it/s]Extractor Estimating: 78it [01:21,  1.29it/s]Extractor Estimating: 79it [01:21,  1.35it/s]Extractor Estimating: 80it [01:22,  1.36it/s]Extractor Estimating: 81it [01:23,  1.35it/s]Extractor Estimating: 82it [01:23,  1.32it/s]Extractor Estimating: 83it [01:24,  1.30it/s]Extractor Estimating: 84it [01:25,  1.26it/s]Extractor Estimating: 85it [01:26,  1.26it/s]Extractor Estimating: 86it [01:27,  1.26it/s]Extractor Estimating: 87it [01:28,  1.25it/s]Extractor Estimating: 88it [01:28,  1.29it/s]Extractor Estimating: 89it [01:29,  1.28it/s]Extractor Estimating: 90it [01:30,  1.29it/s]Extractor Estimating: 91it [01:31,  1.27it/s]Extractor Estimating: 92it [01:31,  1.29it/s]Extractor Estimating: 93it [01:32,  1.28it/s]Extractor Estimating: 94it [01:33,  1.27it/s]Extractor Estimating: 95it [01:34,  1.26it/s]Extractor Estimating: 96it [01:35,  1.25it/s]Extractor Estimating: 97it [01:36,  1.17it/s]Extractor Estimating: 98it [01:36,  1.20it/s]Extractor Estimating: 99it [01:37,  1.21it/s]Extractor Estimating: 100it [01:38,  1.28it/s]Extractor Estimating: 101it [01:39,  1.28it/s]Extractor Estimating: 102it [01:39,  1.26it/s]Extractor Estimating: 103it [01:40,  1.28it/s]Extractor Estimating: 104it [01:41,  1.30it/s]Extractor Estimating: 105it [01:42,  1.32it/s]Extractor Estimating: 106it [01:42,  1.30it/s]Extractor Estimating: 107it [01:43,  1.29it/s]Extractor Estimating: 108it [01:44,  1.32it/s]Extractor Estimating: 109it [01:45,  1.32it/s]Extractor Estimating: 110it [01:45,  1.33it/s]Extractor Estimating: 111it [01:46,  1.32it/s]Extractor Estimating: 112it [01:47,  1.31it/s]Extractor Estimating: 113it [01:48,  1.29it/s]Extractor Estimating: 114it [01:49,  1.32it/s]Extractor Estimating: 115it [01:49,  1.32it/s]Extractor Estimating: 116it [01:50,  1.34it/s]Extractor Estimating: 117it [01:51,  1.34it/s]Extractor Estimating: 118it [01:51,  1.36it/s]Extractor Estimating: 119it [01:52,  1.35it/s]Extractor Estimating: 120it [01:53,  1.35it/s]Extractor Estimating: 121it [01:54,  1.35it/s]Extractor Estimating: 122it [01:54,  1.35it/s]Extractor Estimating: 123it [01:55,  1.33it/s]Extractor Estimating: 124it [01:56,  1.32it/s]Extractor Estimating: 125it [01:57,  1.34it/s]Extractor Estimating: 126it [01:57,  1.33it/s]Extractor Estimating: 127it [01:58,  1.29it/s]Extractor Estimating: 128it [01:59,  1.33it/s]Extractor Estimating: 129it [02:00,  1.29it/s]Extractor Estimating: 130it [02:01,  1.26it/s]Extractor Estimating: 131it [02:02,  1.24it/s]Extractor Estimating: 132it [02:02,  1.27it/s]Extractor Estimating: 133it [02:03,  1.26it/s]Extractor Estimating: 134it [02:04,  1.26it/s]Extractor Estimating: 135it [02:05,  1.29it/s]Extractor Estimating: 136it [02:05,  1.30it/s]Extractor Estimating: 137it [02:06,  1.26it/s]Extractor Estimating: 138it [02:07,  1.26it/s]Extractor Estimating: 139it [02:08,  1.23it/s]Extractor Estimating: 140it [02:09,  1.26it/s]Extractor Estimating: 141it [02:09,  1.30it/s]Extractor Estimating: 142it [02:10,  1.31it/s]Extractor Estimating: 143it [02:11,  1.29it/s]Extractor Estimating: 144it [02:12,  1.27it/s]Extractor Estimating: 145it [02:13,  1.25it/s]Extractor Estimating: 146it [02:13,  1.22it/s]Extractor Estimating: 147it [02:14,  1.25it/s]Extractor Estimating: 148it [02:15,  1.26it/s]Extractor Estimating: 149it [02:16,  1.30it/s]Extractor Estimating: 150it [02:16,  1.29it/s]Extractor Estimating: 151it [02:17,  1.32it/s]Extractor Estimating: 152it [02:18,  1.32it/s]Extractor Estimating: 153it [02:19,  1.33it/s]Extractor Estimating: 154it [02:19,  1.32it/s]Extractor Estimating: 155it [02:20,  1.31it/s]Extractor Estimating: 156it [02:21,  1.32it/s]Extractor Estimating: 157it [02:22,  1.30it/s]Extractor Estimating: 158it [02:22,  1.31it/s]Extractor Estimating: 159it [02:23,  1.32it/s]Extractor Estimating: 160it [02:24,  1.31it/s]Extractor Estimating: 161it [02:25,  1.30it/s]Extractor Estimating: 162it [02:25,  1.31it/s]Extractor Estimating: 163it [02:26,  1.33it/s]Extractor Estimating: 164it [02:27,  1.30it/s]Extractor Estimating: 165it [02:28,  1.28it/s]Extractor Estimating: 166it [02:29,  1.29it/s]Extractor Estimating: 167it [02:29,  1.26it/s]Extractor Estimating: 168it [02:30,  1.19it/s]Extractor Estimating: 169it [02:31,  1.22it/s]Extractor Estimating: 170it [02:32,  1.22it/s]Extractor Estimating: 171it [02:33,  1.27it/s]Extractor Estimating: 172it [02:33,  1.27it/s]Extractor Estimating: 173it [02:34,  1.32it/s]Extractor Estimating: 174it [02:35,  1.32it/s]Extractor Estimating: 175it [02:36,  1.31it/s]Extractor Estimating: 176it [02:37,  1.29it/s]Extractor Estimating: 177it [02:37,  1.29it/s]Extractor Estimating: 178it [02:38,  1.28it/s]Extractor Estimating: 179it [02:39,  1.28it/s]Extractor Estimating: 180it [02:40,  1.25it/s]Extractor Estimating: 181it [02:40,  1.29it/s]Extractor Estimating: 182it [02:41,  1.31it/s]Extractor Estimating: 183it [02:42,  1.28it/s]Extractor Estimating: 184it [02:43,  1.29it/s]Extractor Estimating: 185it [02:43,  1.31it/s]Extractor Estimating: 186it [02:44,  1.34it/s]Extractor Estimating: 187it [02:45,  1.34it/s]Extractor Estimating: 188it [02:46,  1.35it/s]Extractor Estimating: 189it [02:46,  1.33it/s]Extractor Estimating: 190it [02:47,  1.29it/s]Extractor Estimating: 191it [02:48,  1.32it/s]Extractor Estimating: 192it [02:49,  1.29it/s]Extractor Estimating: 193it [02:50,  1.30it/s]Extractor Estimating: 194it [02:50,  1.32it/s]Extractor Estimating: 195it [02:51,  1.33it/s]Extractor Estimating: 196it [02:52,  1.34it/s]Extractor Estimating: 197it [02:52,  1.35it/s]Extractor Estimating: 198it [02:53,  1.32it/s]Extractor Estimating: 199it [02:54,  1.26it/s]Extractor Estimating: 200it [02:55,  1.29it/s]Extractor Estimating: 201it [02:56,  1.29it/s]Extractor Estimating: 202it [02:56,  1.30it/s]Extractor Estimating: 203it [02:57,  1.32it/s]Extractor Estimating: 204it [02:58,  1.32it/s]Extractor Estimating: 205it [02:59,  1.33it/s]Extractor Estimating: 206it [02:59,  1.35it/s]Extractor Estimating: 207it [03:00,  1.34it/s]Extractor Estimating: 208it [03:01,  1.34it/s]Extractor Estimating: 209it [03:02,  1.32it/s]Extractor Estimating: 210it [03:02,  1.33it/s]Extractor Estimating: 211it [03:03,  1.31it/s]Extractor Estimating: 212it [03:04,  1.31it/s]Extractor Estimating: 213it [03:05,  1.29it/s]Extractor Estimating: 214it [03:05,  1.31it/s]Extractor Estimating: 215it [03:06,  1.29it/s]Extractor Estimating: 216it [03:07,  1.27it/s]Extractor Estimating: 217it [03:08,  1.29it/s]Extractor Estimating: 218it [03:09,  1.31it/s]Extractor Estimating: 219it [03:09,  1.33it/s]Extractor Estimating: 220it [03:10,  1.33it/s]Extractor Estimating: 221it [03:11,  1.31it/s]Extractor Estimating: 222it [03:12,  1.32it/s]Extractor Estimating: 223it [03:12,  1.34it/s]Extractor Estimating: 224it [03:13,  1.33it/s]Extractor Estimating: 225it [03:14,  1.32it/s]Extractor Estimating: 226it [03:15,  1.31it/s]Extractor Estimating: 227it [03:15,  1.32it/s]Extractor Estimating: 228it [03:16,  1.26it/s]Extractor Estimating: 229it [03:17,  1.29it/s]Extractor Estimating: 230it [03:18,  1.26it/s]Extractor Estimating: 231it [03:19,  1.24it/s]Extractor Estimating: 232it [03:19,  1.27it/s]Extractor Estimating: 233it [03:20,  1.26it/s]Extractor Estimating: 234it [03:21,  1.21it/s]Extractor Estimating: 235it [03:22,  1.24it/s]Extractor Estimating: 236it [03:23,  1.27it/s]Extractor Estimating: 237it [03:23,  1.28it/s]Extractor Estimating: 238it [03:24,  1.28it/s]Extractor Estimating: 239it [03:25,  1.24it/s]Extractor Estimating: 240it [03:26,  1.23it/s]Extractor Estimating: 241it [03:27,  1.23it/s]Extractor Estimating: 242it [03:27,  1.30it/s]Extractor Estimating: 243it [03:28,  1.30it/s]Extractor Estimating: 244it [03:29,  1.34it/s]Extractor Estimating: 245it [03:29,  1.37it/s]Extractor Estimating: 246it [03:30,  1.37it/s]Extractor Estimating: 247it [03:31,  1.36it/s]Extractor Estimating: 248it [03:32,  1.33it/s]Extractor Estimating: 249it [03:32,  1.36it/s]Extractor Estimating: 250it [03:33,  1.28it/s]Extractor Estimating: 250it [03:33,  1.17it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5110 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 24542
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24642, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_5_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24642, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.542, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.345, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.256, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.260, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.254, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.837, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.278, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.241, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.264, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.242, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.795, loss:nan
g_step 1200, step 135, avg_time 1.238, loss:nan
g_step 1300, step 22, avg_time 1.237, loss:nan
g_step 1400, step 122, avg_time 1.238, loss:nan
g_step 1500, step 9, avg_time 1.257, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.767, loss:nan
g_step 1700, step 209, avg_time 1.249, loss:nan
g_step 1800, step 96, avg_time 1.245, loss:nan
g_step 1900, step 196, avg_time 1.243, loss:nan
g_step 2000, step 83, avg_time 1.263, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.760, loss:nan
g_step 2200, step 70, avg_time 1.241, loss:nan
g_step 2300, step 170, avg_time 1.239, loss:nan
g_step 2400, step 57, avg_time 1.264, loss:nan
g_step 2500, step 157, avg_time 1.228, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.751, loss:nan
g_step 2700, step 144, avg_time 1.241, loss:nan
g_step 2800, step 31, avg_time 1.251, loss:nan
g_step 2900, step 131, avg_time 1.233, loss:nan
g_step 3000, step 18, avg_time 1.248, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.786, loss:nan
g_step 3200, step 5, avg_time 1.234, loss:nan
g_step 3300, step 105, avg_time 1.245, loss:nan
g_step 3400, step 205, avg_time 1.243, loss:nan
g_step 3500, step 92, avg_time 1.229, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.779, loss:nan
g_step 3700, step 79, avg_time 1.273, loss:nan
g_step 3800, step 179, avg_time 1.245, loss:nan
g_step 3900, step 66, avg_time 1.246, loss:nan
g_step 4000, step 166, avg_time 1.244, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.762, loss:nan
g_step 4200, step 153, avg_time 1.246, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:56:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:56:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-56-21_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:56:22 - WARNING - datasets.builder -   Using custom data configuration default-64dde3b60e95fdb9
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-64dde3b60e95fdb9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:56:22,468 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:56:22,469 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:56:22,469 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:56:22,470 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:56:22,479 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:22,484 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:56:22,600 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:56:25,791 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:56:25,793 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-64dde3b60e95fdb9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:56:25 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x151fd2f24200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.13ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.14ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.67ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.98ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.18ba/s]100%|██████████| 6/6 [00:01<00:00,  4.35ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.14ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.44ba/s]100%|██████████| 5/5 [00:01<00:00,  5.15ba/s]100%|██████████| 5/5 [00:01<00:00,  4.76ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.88ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.91ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.74ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.70ba/s]100%|██████████| 6/6 [00:00<00:00, 10.99ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.90ba/s] 40%|████      | 2/5 [00:00<00:00,  8.88ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.00ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.29ba/s]100%|██████████| 5/5 [00:00<00:00,  9.87ba/s]
[INFO|trainer.py:414] 2023-08-28 00:56:29,636 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:56:29,651 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:56:29,651 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:56:29,651 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:56:29,651 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:56:29,651 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:56:29,651 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:56:29,651 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:57,  3.42it/s]  0%|          | 2/405 [00:00<01:54,  3.51it/s]  1%|          | 3/405 [00:00<01:53,  3.54it/s]  1%|          | 4/405 [00:01<01:52,  3.55it/s]  1%|          | 5/405 [00:01<01:52,  3.56it/s]  1%|▏         | 6/405 [00:01<01:51,  3.57it/s]  2%|▏         | 7/405 [00:01<01:51,  3.57it/s]  2%|▏         | 8/405 [00:02<01:51,  3.57it/s]  2%|▏         | 9/405 [00:02<01:51,  3.56it/s]  2%|▏         | 10/405 [00:02<01:50,  3.57it/s]  3%|▎         | 11/405 [00:03<01:50,  3.57it/s]  3%|▎         | 12/405 [00:03<01:49,  3.57it/s]  3%|▎         | 13/405 [00:03<01:49,  3.58it/s]  3%|▎         | 14/405 [00:03<01:49,  3.58it/s]  4%|▎         | 15/405 [00:04<01:49,  3.57it/s]  4%|▍         | 16/405 [00:04<01:48,  3.57it/s]  4%|▍         | 17/405 [00:04<01:48,  3.57it/s]  4%|▍         | 18/405 [00:05<01:48,  3.57it/s]  5%|▍         | 19/405 [00:05<01:48,  3.57it/s]  5%|▍         | 20/405 [00:05<01:47,  3.57it/s]  5%|▌         | 21/405 [00:05<01:47,  3.57it/s]  5%|▌         | 22/405 [00:06<01:47,  3.57it/s]  6%|▌         | 23/405 [00:06<01:46,  3.57it/s]  6%|▌         | 24/405 [00:06<01:46,  3.57it/s]  6%|▌         | 25/405 [00:07<01:46,  3.57it/s]  6%|▋         | 26/405 [00:07<01:46,  3.57it/s]  7%|▋         | 27/405 [00:07<01:45,  3.57it/s]  7%|▋         | 28/405 [00:07<01:45,  3.57it/s]  7%|▋         | 29/405 [00:08<01:45,  3.57it/s]  7%|▋         | 30/405 [00:08<01:45,  3.57it/s]  8%|▊         | 31/405 [00:08<01:44,  3.57it/s]  8%|▊         | 32/405 [00:08<01:44,  3.57it/s]  8%|▊         | 33/405 [00:09<01:44,  3.57it/s]  8%|▊         | 34/405 [00:09<01:43,  3.57it/s]  9%|▊         | 35/405 [00:09<01:43,  3.57it/s]  9%|▉         | 36/405 [00:10<01:43,  3.57it/s]  9%|▉         | 37/405 [00:10<01:43,  3.57it/s]  9%|▉         | 38/405 [00:10<01:42,  3.57it/s] 10%|▉         | 39/405 [00:10<01:42,  3.57it/s] 10%|▉         | 40/405 [00:11<01:42,  3.57it/s] 10%|█         | 41/405 [00:11<01:41,  3.57it/s] 10%|█         | 42/405 [00:11<01:41,  3.57it/s] 11%|█         | 43/405 [00:12<01:41,  3.57it/s] 11%|█         | 44/405 [00:12<01:41,  3.57it/s] 11%|█         | 45/405 [00:12<01:40,  3.57it/s] 11%|█▏        | 46/405 [00:12<01:40,  3.57it/s] 12%|█▏        | 47/405 [00:13<01:40,  3.57it/s] 12%|█▏        | 48/405 [00:13<01:39,  3.57it/s] 12%|█▏        | 49/405 [00:13<01:39,  3.57it/s] 12%|█▏        | 50/405 [00:14<01:39,  3.57it/s] 13%|█▎        | 51/405 [00:14<01:39,  3.57it/s] 13%|█▎        | 52/405 [00:14<01:39,  3.55it/s] 13%|█▎        | 53/405 [00:14<01:39,  3.55it/s] 13%|█▎        | 54/405 [00:15<01:38,  3.55it/s] 14%|█▎        | 55/405 [00:15<01:38,  3.55it/s] 14%|█▍        | 56/405 [00:15<01:38,  3.56it/s] 14%|█▍        | 57/405 [00:15<01:37,  3.55it/s] 14%|█▍        | 58/405 [00:16<01:37,  3.56it/s] 15%|█▍        | 59/405 [00:16<01:37,  3.56it/s] 15%|█▍        | 60/405 [00:16<01:36,  3.56it/s] 15%|█▌        | 61/405 [00:17<01:36,  3.56it/s] 15%|█▌        | 62/405 [00:17<01:36,  3.56it/s] 16%|█▌        | 63/405 [00:17<01:35,  3.57it/s] 16%|█▌        | 64/405 [00:17<01:35,  3.57it/s] 16%|█▌        | 65/405 [00:18<01:35,  3.57it/s] 16%|█▋        | 66/405 [00:18<01:35,  3.57it/s] 17%|█▋        | 67/405 [00:18<01:34,  3.57it/s] 17%|█▋        | 68/405 [00:19<01:34,  3.57it/s] 17%|█▋        | 69/405 [00:19<01:34,  3.57it/s] 17%|█▋        | 70/405 [00:19<01:33,  3.57it/s] 18%|█▊        | 71/405 [00:19<01:33,  3.57it/s] 18%|█▊        | 72/405 [00:20<01:33,  3.57it/s] 18%|█▊        | 73/405 [00:20<01:32,  3.57it/s] 18%|█▊        | 74/405 [00:20<01:32,  3.57it/s] 19%|█▊        | 75/405 [00:21<01:32,  3.57it/s] 19%|█▉        | 76/405 [00:21<01:32,  3.57it/s] 19%|█▉        | 77/405 [00:21<01:31,  3.57it/s] 19%|█▉        | 78/405 [00:21<01:31,  3.57it/s] 20%|█▉        | 79/405 [00:22<01:31,  3.57it/s] 20%|█▉        | 80/405 [00:22<01:31,  3.57it/s] 20%|██        | 81/405 [00:22<01:22,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 00:56:52,275 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:56:52,275 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:56:52,275 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.79it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.28it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.31it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.33it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.87it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.57it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.45it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.18it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.25it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.26it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.42it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.38it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.26it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.10it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.13it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.05it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.02it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.06it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.25it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.30it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.33it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.22it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.13it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.12it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.99it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.11it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.23it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.27it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.26it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.19it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.09it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.07it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.06it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.08it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.18it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.17it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.25it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.19it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.12it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.06it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.06it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.14it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.06it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.15it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.27it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.23it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.22it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.19it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.05it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.11it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.15it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.13it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.14it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.27it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.17it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.12it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.05it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.03it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.05it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.09it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.08it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.13it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.19it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.15it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.05it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.08it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.07it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.99it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.12it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.18it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.22it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.21it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.08it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.92it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.06it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.13it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.22it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.14it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.17it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.14it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.19it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.12it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.12it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.06it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.23it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.19it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.23it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.05it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.20it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.14it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.10it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.17it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.22it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.25it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.12it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.12it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.21it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.14it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.11it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.96it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.07it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.24it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.23it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.09it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.11it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.14it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.07it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.03it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:35<01:22,  3.94it/s]
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:57:05,366 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:57:05,384 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:57:07,348 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:57:07,361 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:57:07,371 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:38<26:15,  4.88s/it] 20%|██        | 83/405 [00:38<18:46,  3.50s/it] 21%|██        | 84/405 [00:38<13:34,  2.54s/it] 21%|██        | 85/405 [00:39<09:55,  1.86s/it] 21%|██        | 86/405 [00:39<07:22,  1.39s/it] 21%|██▏       | 87/405 [00:39<05:36,  1.06s/it] 22%|██▏       | 88/405 [00:40<04:22,  1.21it/s] 22%|██▏       | 89/405 [00:40<03:29,  1.50it/s] 22%|██▏       | 90/405 [00:40<02:53,  1.81it/s] 22%|██▏       | 91/405 [00:40<02:28,  2.12it/s] 23%|██▎       | 92/405 [00:41<02:10,  2.40it/s] 23%|██▎       | 93/405 [00:41<01:57,  2.65it/s] 23%|██▎       | 94/405 [00:41<01:48,  2.86it/s] 23%|██▎       | 95/405 [00:42<01:42,  3.02it/s] 24%|██▎       | 96/405 [00:42<01:38,  3.14it/s] 24%|██▍       | 97/405 [00:42<01:34,  3.24it/s] 24%|██▍       | 98/405 [00:42<01:32,  3.31it/s] 24%|██▍       | 99/405 [00:43<01:31,  3.36it/s] 25%|██▍       | 100/405 [00:43<01:29,  3.40it/s] 25%|██▍       | 101/405 [00:43<01:28,  3.43it/s] 25%|██▌       | 102/405 [00:44<01:27,  3.44it/s] 25%|██▌       | 103/405 [00:44<01:27,  3.46it/s] 26%|██▌       | 104/405 [00:44<01:26,  3.47it/s] 26%|██▌       | 105/405 [00:44<01:26,  3.47it/s] 26%|██▌       | 106/405 [00:45<01:25,  3.48it/s] 26%|██▋       | 107/405 [00:45<01:25,  3.48it/s] 27%|██▋       | 108/405 [00:45<01:25,  3.48it/s] 27%|██▋       | 109/405 [00:46<01:25,  3.48it/s] 27%|██▋       | 110/405 [00:46<01:24,  3.47it/s] 27%|██▋       | 111/405 [00:46<01:24,  3.48it/s] 28%|██▊       | 112/405 [00:46<01:24,  3.48it/s] 28%|██▊       | 113/405 [00:47<01:23,  3.48it/s] 28%|██▊       | 114/405 [00:47<01:23,  3.48it/s] 28%|██▊       | 115/405 [00:47<01:23,  3.49it/s] 29%|██▊       | 116/405 [00:48<01:22,  3.48it/s] 29%|██▉       | 117/405 [00:48<01:22,  3.48it/s] 29%|██▉       | 118/405 [00:48<01:22,  3.49it/s] 29%|██▉       | 119/405 [00:48<01:22,  3.48it/s] 30%|██▉       | 120/405 [00:49<01:21,  3.49it/s] 30%|██▉       | 121/405 [00:49<01:21,  3.47it/s] 30%|███       | 122/405 [00:49<01:21,  3.48it/s] 30%|███       | 123/405 [00:50<01:21,  3.48it/s] 31%|███       | 124/405 [00:50<01:20,  3.48it/s] 31%|███       | 125/405 [00:50<01:20,  3.46it/s] 31%|███       | 126/405 [00:50<01:20,  3.47it/s] 31%|███▏      | 127/405 [00:51<01:19,  3.48it/s] 32%|███▏      | 128/405 [00:51<01:19,  3.48it/s] 32%|███▏      | 129/405 [00:51<01:22,  3.35it/s] 32%|███▏      | 130/405 [00:52<01:21,  3.38it/s] 32%|███▏      | 131/405 [00:52<01:20,  3.41it/s] 33%|███▎      | 132/405 [00:52<01:19,  3.42it/s] 33%|███▎      | 133/405 [00:52<01:19,  3.44it/s] 33%|███▎      | 134/405 [00:53<01:18,  3.45it/s] 33%|███▎      | 135/405 [00:53<01:17,  3.47it/s] 34%|███▎      | 136/405 [00:53<01:17,  3.47it/s] 34%|███▍      | 137/405 [00:54<01:17,  3.47it/s] 34%|███▍      | 138/405 [00:54<01:16,  3.47it/s] 34%|███▍      | 139/405 [00:54<01:16,  3.48it/s] 35%|███▍      | 140/405 [00:54<01:16,  3.48it/s] 35%|███▍      | 141/405 [00:55<01:15,  3.48it/s] 35%|███▌      | 142/405 [00:55<01:15,  3.48it/s] 35%|███▌      | 143/405 [00:55<01:15,  3.48it/s] 36%|███▌      | 144/405 [00:56<01:15,  3.48it/s] 36%|███▌      | 145/405 [00:56<01:14,  3.48it/s] 36%|███▌      | 146/405 [00:56<01:14,  3.48it/s] 36%|███▋      | 147/405 [00:56<01:14,  3.48it/s] 37%|███▋      | 148/405 [00:57<01:13,  3.48it/s] 37%|███▋      | 149/405 [00:57<01:13,  3.48it/s] 37%|███▋      | 150/405 [00:57<01:13,  3.49it/s] 37%|███▋      | 151/405 [00:58<01:12,  3.49it/s] 38%|███▊      | 152/405 [00:58<01:12,  3.49it/s] 38%|███▊      | 153/405 [00:58<01:12,  3.49it/s] 38%|███▊      | 154/405 [00:58<01:11,  3.49it/s] 38%|███▊      | 155/405 [00:59<01:12,  3.47it/s] 39%|███▊      | 156/405 [00:59<01:11,  3.47it/s] 39%|███▉      | 157/405 [00:59<01:11,  3.48it/s] 39%|███▉      | 158/405 [01:00<01:10,  3.48it/s] 39%|███▉      | 159/405 [01:00<01:10,  3.48it/s] 40%|███▉      | 160/405 [01:00<01:10,  3.48it/s] 40%|███▉      | 161/405 [01:01<01:10,  3.48it/s] 40%|████      | 162/405 [01:01<01:03,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 00:57:30,859 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:57:30,859 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:57:30,859 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0618, 'eval_samples_per_second': 352.861, 'eval_steps_per_second': 44.175, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.72it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.48it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.74it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.64it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.00it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.43it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.17it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.83it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.98it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.18it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.29it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.43it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.33it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.12it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.96it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.86it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.78it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.87it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.01it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.17it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.29it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.18it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.09it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.98it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.85it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.80it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.93it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.03it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.15it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.19it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.21it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.21it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.07it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.93it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.82it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.94it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.03it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.09it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.18it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.19it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.08it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.06it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.99it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.95it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.08it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.14it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.07it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.12it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.16it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.01it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.15it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.96it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.98it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.16it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.09it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.98it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.08it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.96it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.03it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.09it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.08it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.18it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.06it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.04it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.90it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.93it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.97it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.07it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.14it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.13it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.07it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.04it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.96it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.99it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.05it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.10it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.02it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.04it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.10it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.01it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.00it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.96it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.91it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.01it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.03it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.95it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.04it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.06it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.00it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.99it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.94it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.01it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.03it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.11it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.11it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.99it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.04it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.05it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.88it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.98it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.03it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.07it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.07it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.06it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.10it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:14<01:03,  3.85it/s]
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:57:43,978 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:57:43,998 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:57:45,925 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:57:45,939 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:57:45,948 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:16<19:45,  4.90s/it] 40%|████      | 164/405 [01:17<14:07,  3.52s/it] 41%|████      | 165/405 [01:17<10:11,  2.55s/it] 41%|████      | 166/405 [01:17<07:28,  1.88s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:18<05:34,  1.41s/it] 41%|████▏     | 168/405 [01:18<04:13,  1.07s/it] 42%|████▏     | 169/405 [01:18<03:17,  1.20it/s] 42%|████▏     | 170/405 [01:18<02:37,  1.49it/s] 42%|████▏     | 171/405 [01:19<02:10,  1.80it/s] 42%|████▏     | 172/405 [01:19<01:50,  2.11it/s] 43%|████▎     | 173/405 [01:19<01:37,  2.39it/s] 43%|████▎     | 174/405 [01:20<01:27,  2.64it/s] 43%|████▎     | 175/405 [01:20<01:20,  2.84it/s] 43%|████▎     | 176/405 [01:20<01:16,  3.01it/s] 44%|████▎     | 177/405 [01:20<01:12,  3.13it/s] 44%|████▍     | 178/405 [01:21<01:10,  3.23it/s] 44%|████▍     | 179/405 [01:21<01:08,  3.31it/s] 44%|████▍     | 180/405 [01:21<01:06,  3.36it/s] 45%|████▍     | 181/405 [01:22<01:06,  3.39it/s] 45%|████▍     | 182/405 [01:22<01:05,  3.42it/s] 45%|████▌     | 183/405 [01:22<01:04,  3.44it/s] 45%|████▌     | 184/405 [01:23<01:03,  3.46it/s] 46%|████▌     | 185/405 [01:23<01:03,  3.47it/s] 46%|████▌     | 186/405 [01:23<01:02,  3.48it/s] 46%|████▌     | 187/405 [01:23<01:02,  3.48it/s] 46%|████▋     | 188/405 [01:24<01:02,  3.48it/s] 47%|████▋     | 189/405 [01:24<01:01,  3.49it/s] 47%|████▋     | 190/405 [01:24<01:01,  3.49it/s] 47%|████▋     | 191/405 [01:25<01:01,  3.49it/s] 47%|████▋     | 192/405 [01:25<01:01,  3.49it/s] 48%|████▊     | 193/405 [01:25<01:00,  3.49it/s] 48%|████▊     | 194/405 [01:25<01:00,  3.49it/s] 48%|████▊     | 195/405 [01:26<01:00,  3.49it/s] 48%|████▊     | 196/405 [01:26<00:59,  3.49it/s] 49%|████▊     | 197/405 [01:26<00:59,  3.47it/s] 49%|████▉     | 198/405 [01:27<00:59,  3.47it/s] 49%|████▉     | 199/405 [01:27<00:59,  3.47it/s] 49%|████▉     | 200/405 [01:27<00:58,  3.48it/s] 50%|████▉     | 201/405 [01:27<00:58,  3.48it/s] 50%|████▉     | 202/405 [01:28<00:58,  3.48it/s] 50%|█████     | 203/405 [01:28<00:57,  3.48it/s] 50%|█████     | 204/405 [01:28<00:57,  3.48it/s] 51%|█████     | 205/405 [01:29<00:57,  3.49it/s] 51%|█████     | 206/405 [01:29<00:57,  3.49it/s] 51%|█████     | 207/405 [01:29<00:56,  3.49it/s] 51%|█████▏    | 208/405 [01:29<00:56,  3.48it/s] 52%|█████▏    | 209/405 [01:30<00:56,  3.48it/s] 52%|█████▏    | 210/405 [01:30<00:55,  3.48it/s] 52%|█████▏    | 211/405 [01:30<00:55,  3.49it/s] 52%|█████▏    | 212/405 [01:31<00:55,  3.49it/s] 53%|█████▎    | 213/405 [01:31<00:55,  3.49it/s] 53%|█████▎    | 214/405 [01:31<00:54,  3.48it/s] 53%|█████▎    | 215/405 [01:31<00:54,  3.48it/s] 53%|█████▎    | 216/405 [01:32<00:54,  3.48it/s] 54%|█████▎    | 217/405 [01:32<00:53,  3.49it/s] 54%|█████▍    | 218/405 [01:32<00:53,  3.49it/s] 54%|█████▍    | 219/405 [01:33<00:53,  3.48it/s] 54%|█████▍    | 220/405 [01:33<00:53,  3.48it/s] 55%|█████▍    | 221/405 [01:33<00:52,  3.48it/s] 55%|█████▍    | 222/405 [01:33<00:52,  3.49it/s] 55%|█████▌    | 223/405 [01:34<00:52,  3.50it/s] 55%|█████▌    | 224/405 [01:34<00:51,  3.52it/s] 56%|█████▌    | 225/405 [01:34<00:51,  3.53it/s] 56%|█████▌    | 226/405 [01:35<00:50,  3.54it/s] 56%|█████▌    | 227/405 [01:35<00:50,  3.54it/s] 56%|█████▋    | 228/405 [01:35<00:49,  3.55it/s] 57%|█████▋    | 229/405 [01:35<00:49,  3.55it/s] 57%|█████▋    | 230/405 [01:36<00:49,  3.54it/s] 57%|█████▋    | 231/405 [01:36<00:49,  3.55it/s] 57%|█████▋    | 232/405 [01:36<00:48,  3.55it/s] 58%|█████▊    | 233/405 [01:37<00:48,  3.54it/s] 58%|█████▊    | 234/405 [01:37<00:48,  3.52it/s] 58%|█████▊    | 235/405 [01:37<00:48,  3.51it/s] 58%|█████▊    | 236/405 [01:37<00:48,  3.50it/s] 59%|█████▊    | 237/405 [01:38<00:48,  3.50it/s] 59%|█████▉    | 238/405 [01:38<00:47,  3.49it/s] 59%|█████▉    | 239/405 [01:38<00:47,  3.49it/s] 59%|█████▉    | 240/405 [01:39<00:47,  3.49it/s] 60%|█████▉    | 241/405 [01:39<00:47,  3.47it/s] 60%|█████▉    | 242/405 [01:39<00:46,  3.48it/s] 60%|██████    | 243/405 [01:39<00:42,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 00:58:09,451 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:58:09,451 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:58:09,451 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0936, 'eval_samples_per_second': 352.005, 'eval_steps_per_second': 44.067, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.66it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.47it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.96it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.60it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.09it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.55it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.18it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.83it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.88it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.15it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.21it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.24it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.28it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.35it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.17it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.92it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.70it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.78it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.07it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.15it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.14it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.27it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.22it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.76it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.64it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.74it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.96it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.15it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.14it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.26it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.25it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.69it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.72it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.90it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.97it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.15it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.25it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.23it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.20it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.99it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.79it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.80it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.84it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.97it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.17it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.34it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.28it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.14it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.93it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.88it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.77it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.85it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.01it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.19it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.38it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.28it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.92it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.92it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.82it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.77it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.85it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.09it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.33it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.29it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.00it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.94it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.83it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.84it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.90it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.05it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.28it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.29it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.22it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.98it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.91it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.87it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.82it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.95it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.29it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.33it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.08it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.92it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.84it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.76it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.84it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.90it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.10it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.27it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.29it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.13it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.27it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.08it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.00it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.03it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.02it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.19it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.28it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.28it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.09it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.09it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.89it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.29it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.60it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.87it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.99it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.09it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.08it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.96it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.99it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.85it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.89it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [01:52<00:42,  3.83it/s]
100%|██████████| 577/577 [00:13<00:00, 43.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:58:22,575 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:58:22,598 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:58:24,501 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:58:24,511 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:58:24,521 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [01:55<13:04,  4.87s/it] 60%|██████    | 245/405 [01:55<09:19,  3.50s/it] 61%|██████    | 246/405 [01:55<06:42,  2.53s/it] 61%|██████    | 247/405 [01:56<04:53,  1.86s/it] 61%|██████    | 248/405 [01:56<03:37,  1.39s/it] 61%|██████▏   | 249/405 [01:56<02:44,  1.06s/it] 62%|██████▏   | 250/405 [01:57<02:08,  1.21it/s] 62%|██████▏   | 251/405 [01:57<01:42,  1.51it/s] 62%|██████▏   | 252/405 [01:57<01:24,  1.81it/s] 62%|██████▏   | 253/405 [01:58<01:11,  2.12it/s] 63%|██████▎   | 254/405 [01:58<01:02,  2.40it/s] 63%|██████▎   | 255/405 [01:58<00:56,  2.65it/s] 63%|██████▎   | 256/405 [01:58<00:52,  2.86it/s] 63%|██████▎   | 257/405 [01:59<00:49,  3.02it/s] 64%|██████▎   | 258/405 [01:59<00:46,  3.15it/s] 64%|██████▍   | 259/405 [01:59<00:45,  3.24it/s] 64%|██████▍   | 260/405 [02:00<00:43,  3.31it/s] 64%|██████▍   | 261/405 [02:00<00:42,  3.36it/s] 65%|██████▍   | 262/405 [02:00<00:42,  3.40it/s] 65%|██████▍   | 263/405 [02:00<00:41,  3.41it/s] 65%|██████▌   | 264/405 [02:01<00:41,  3.44it/s] 65%|██████▌   | 265/405 [02:01<00:40,  3.45it/s] 66%|██████▌   | 266/405 [02:01<00:40,  3.46it/s] 66%|██████▌   | 267/405 [02:02<00:39,  3.47it/s] 66%|██████▌   | 268/405 [02:02<00:39,  3.47it/s] 66%|██████▋   | 269/405 [02:02<00:39,  3.48it/s] 67%|██████▋   | 270/405 [02:02<00:38,  3.48it/s] 67%|██████▋   | 271/405 [02:03<00:38,  3.48it/s] 67%|██████▋   | 272/405 [02:03<00:38,  3.48it/s] 67%|██████▋   | 273/405 [02:03<00:37,  3.49it/s] 68%|██████▊   | 274/405 [02:04<00:37,  3.47it/s] 68%|██████▊   | 275/405 [02:04<00:37,  3.48it/s] 68%|██████▊   | 276/405 [02:04<00:37,  3.48it/s] 68%|██████▊   | 277/405 [02:04<00:36,  3.48it/s] 69%|██████▊   | 278/405 [02:05<00:36,  3.49it/s] 69%|██████▉   | 279/405 [02:05<00:36,  3.49it/s] 69%|██████▉   | 280/405 [02:05<00:35,  3.49it/s] 69%|██████▉   | 281/405 [02:06<00:35,  3.49it/s] 70%|██████▉   | 282/405 [02:06<00:35,  3.49it/s] 70%|██████▉   | 283/405 [02:06<00:35,  3.49it/s] 70%|███████   | 284/405 [02:06<00:34,  3.49it/s] 70%|███████   | 285/405 [02:07<00:34,  3.47it/s] 71%|███████   | 286/405 [02:07<00:34,  3.48it/s] 71%|███████   | 287/405 [02:07<00:33,  3.48it/s] 71%|███████   | 288/405 [02:08<00:33,  3.48it/s] 71%|███████▏  | 289/405 [02:08<00:33,  3.49it/s] 72%|███████▏  | 290/405 [02:08<00:32,  3.49it/s] 72%|███████▏  | 291/405 [02:08<00:32,  3.49it/s] 72%|███████▏  | 292/405 [02:09<00:32,  3.49it/s] 72%|███████▏  | 293/405 [02:09<00:32,  3.49it/s] 73%|███████▎  | 294/405 [02:09<00:31,  3.49it/s] 73%|███████▎  | 295/405 [02:10<00:31,  3.49it/s] 73%|███████▎  | 296/405 [02:10<00:31,  3.48it/s] 73%|███████▎  | 297/405 [02:10<00:31,  3.48it/s] 74%|███████▎  | 298/405 [02:10<00:30,  3.48it/s] 74%|███████▍  | 299/405 [02:11<00:30,  3.48it/s] 74%|███████▍  | 300/405 [02:11<00:30,  3.48it/s] 74%|███████▍  | 301/405 [02:11<00:29,  3.49it/s] 75%|███████▍  | 302/405 [02:12<00:29,  3.49it/s] 75%|███████▍  | 303/405 [02:12<00:29,  3.49it/s] 75%|███████▌  | 304/405 [02:12<00:28,  3.49it/s] 75%|███████▌  | 305/405 [02:12<00:28,  3.49it/s] 76%|███████▌  | 306/405 [02:13<00:28,  3.49it/s] 76%|███████▌  | 307/405 [02:13<00:28,  3.46it/s] 76%|███████▌  | 308/405 [02:13<00:27,  3.47it/s] 76%|███████▋  | 309/405 [02:14<00:27,  3.48it/s] 77%|███████▋  | 310/405 [02:14<00:27,  3.48it/s] 77%|███████▋  | 311/405 [02:14<00:27,  3.48it/s] 77%|███████▋  | 312/405 [02:14<00:26,  3.48it/s] 77%|███████▋  | 313/405 [02:15<00:26,  3.48it/s] 78%|███████▊  | 314/405 [02:15<00:26,  3.48it/s] 78%|███████▊  | 315/405 [02:15<00:25,  3.49it/s] 78%|███████▊  | 316/405 [02:16<00:25,  3.48it/s] 78%|███████▊  | 317/405 [02:16<00:25,  3.49it/s] 79%|███████▊  | 318/405 [02:16<00:25,  3.48it/s] 79%|███████▉  | 319/405 [02:16<00:24,  3.48it/s] 79%|███████▉  | 320/405 [02:17<00:24,  3.48it/s] 79%|███████▉  | 321/405 [02:17<00:24,  3.48it/s] 80%|███████▉  | 322/405 [02:17<00:23,  3.48it/s] 80%|███████▉  | 323/405 [02:18<00:23,  3.49it/s] 80%|████████  | 324/405 [02:18<00:21,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 00:58:47,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:58:47,949 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:58:47,949 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.1002, 'eval_samples_per_second': 351.827, 'eval_steps_per_second': 44.045, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.98it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.37it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.70it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.62it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.86it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.50it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.21it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.99it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.08it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.20it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.19it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.43it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.32it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.18it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.03it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.91it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.99it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.06it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.30it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.32it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.23it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.05it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.98it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.90it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.83it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.96it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.08it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.29it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.29it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.28it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.15it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.92it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.84it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.80it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.97it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.15it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.30it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.32it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.22it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.10it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.05it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.90it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.85it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.95it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.11it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.17it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.29it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.23it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.23it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.07it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.94it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.84it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.04it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.09it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.23it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.22it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.21it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.09it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.97it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.86it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.91it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.02it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.04it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.29it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.01it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.92it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.91it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.91it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.04it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.18it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.25it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.16it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.99it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.92it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.86it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.82it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.98it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.16it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.19it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.28it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.20it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.95it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.81it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.95it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.95it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.10it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.23it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.17it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.12it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.02it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.84it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.92it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.02it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.06it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.15it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.25it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.15it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.12it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.00it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.99it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.96it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.08it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.09it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.22it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.16it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.99it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.01it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.93it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [02:31<00:21,  3.85it/s]
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:59:01,053 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:59:01,075 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:59:02,981 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:59:02,997 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:59:03,005 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [02:33<06:29,  4.87s/it] 80%|████████  | 326/405 [02:34<04:35,  3.49s/it] 81%|████████  | 327/405 [02:34<03:17,  2.53s/it] 81%|████████  | 328/405 [02:34<02:23,  1.86s/it] 81%|████████  | 329/405 [02:35<01:45,  1.39s/it] 81%|████████▏ | 330/405 [02:35<01:19,  1.06s/it] 82%|████████▏ | 331/405 [02:35<01:01,  1.21it/s] 82%|████████▏ | 332/405 [02:35<00:48,  1.51it/s] 82%|████████▏ | 333/405 [02:36<00:39,  1.82it/s] 82%|████████▏ | 334/405 [02:36<00:33,  2.12it/s] 83%|████████▎ | 335/405 [02:36<00:29,  2.40it/s] 83%|████████▎ | 336/405 [02:37<00:26,  2.65it/s] 83%|████████▎ | 337/405 [02:37<00:23,  2.85it/s] 83%|████████▎ | 338/405 [02:37<00:22,  3.00it/s] 84%|████████▎ | 339/405 [02:37<00:21,  3.13it/s] 84%|████████▍ | 340/405 [02:38<00:20,  3.22it/s] 84%|████████▍ | 341/405 [02:38<00:19,  3.30it/s] 84%|████████▍ | 342/405 [02:38<00:18,  3.36it/s] 85%|████████▍ | 343/405 [02:39<00:18,  3.39it/s] 85%|████████▍ | 344/405 [02:39<00:17,  3.43it/s] 85%|████████▌ | 345/405 [02:39<00:17,  3.45it/s] 85%|████████▌ | 346/405 [02:39<00:17,  3.46it/s] 86%|████████▌ | 347/405 [02:40<00:16,  3.47it/s] 86%|████████▌ | 348/405 [02:40<00:16,  3.48it/s] 86%|████████▌ | 349/405 [02:40<00:16,  3.47it/s] 86%|████████▋ | 350/405 [02:41<00:15,  3.48it/s] 87%|████████▋ | 351/405 [02:41<00:15,  3.48it/s] 87%|████████▋ | 352/405 [02:41<00:15,  3.48it/s] 87%|████████▋ | 353/405 [02:41<00:14,  3.49it/s] 87%|████████▋ | 354/405 [02:42<00:14,  3.51it/s] 88%|████████▊ | 355/405 [02:42<00:14,  3.53it/s] 88%|████████▊ | 356/405 [02:42<00:13,  3.54it/s] 88%|████████▊ | 357/405 [02:43<00:13,  3.54it/s] 88%|████████▊ | 358/405 [02:43<00:13,  3.55it/s] 89%|████████▊ | 359/405 [02:43<00:12,  3.55it/s] 89%|████████▉ | 360/405 [02:43<00:12,  3.54it/s] 89%|████████▉ | 361/405 [02:44<00:12,  3.55it/s] 89%|████████▉ | 362/405 [02:44<00:12,  3.55it/s] 90%|████████▉ | 363/405 [02:44<00:11,  3.55it/s] 90%|████████▉ | 364/405 [02:45<00:11,  3.55it/s] 90%|█████████ | 365/405 [02:45<00:11,  3.56it/s] 90%|█████████ | 366/405 [02:45<00:10,  3.56it/s] 91%|█████████ | 367/405 [02:45<00:10,  3.56it/s] 91%|█████████ | 368/405 [02:46<00:10,  3.56it/s] 91%|█████████ | 369/405 [02:46<00:10,  3.56it/s] 91%|█████████▏| 370/405 [02:46<00:09,  3.56it/s] 92%|█████████▏| 371/405 [02:47<00:09,  3.55it/s] 92%|█████████▏| 372/405 [02:47<00:09,  3.55it/s] 92%|█████████▏| 373/405 [02:47<00:09,  3.55it/s] 92%|█████████▏| 374/405 [02:47<00:08,  3.55it/s] 93%|█████████▎| 375/405 [02:48<00:08,  3.55it/s] 93%|█████████▎| 376/405 [02:48<00:08,  3.55it/s] 93%|█████████▎| 377/405 [02:48<00:07,  3.55it/s] 93%|█████████▎| 378/405 [02:48<00:07,  3.55it/s] 94%|█████████▎| 379/405 [02:49<00:07,  3.55it/s] 94%|█████████▍| 380/405 [02:49<00:07,  3.56it/s] 94%|█████████▍| 381/405 [02:49<00:06,  3.56it/s] 94%|█████████▍| 382/405 [02:50<00:06,  3.55it/s] 95%|█████████▍| 383/405 [02:50<00:06,  3.56it/s] 95%|█████████▍| 384/405 [02:50<00:05,  3.52it/s] 95%|█████████▌| 385/405 [02:50<00:05,  3.53it/s] 95%|█████████▌| 386/405 [02:51<00:05,  3.54it/s] 96%|█████████▌| 387/405 [02:51<00:05,  3.55it/s] 96%|█████████▌| 388/405 [02:51<00:04,  3.46it/s] 96%|█████████▌| 389/405 [02:52<00:04,  3.49it/s] 96%|█████████▋| 390/405 [02:52<00:04,  3.51it/s] 97%|█████████▋| 391/405 [02:52<00:03,  3.53it/s] 97%|█████████▋| 392/405 [02:52<00:03,  3.54it/s] 97%|█████████▋| 393/405 [02:53<00:03,  3.53it/s] 97%|█████████▋| 394/405 [02:53<00:03,  3.54it/s] 98%|█████████▊| 395/405 [02:53<00:02,  3.54it/s] 98%|█████████▊| 396/405 [02:54<00:02,  3.55it/s] 98%|█████████▊| 397/405 [02:54<00:02,  3.55it/s] 98%|█████████▊| 398/405 [02:54<00:01,  3.55it/s] 99%|█████████▊| 399/405 [02:54<00:01,  3.56it/s] 99%|█████████▉| 400/405 [02:55<00:01,  3.55it/s] 99%|█████████▉| 401/405 [02:55<00:01,  3.56it/s] 99%|█████████▉| 402/405 [02:55<00:00,  3.56it/s]100%|█████████▉| 403/405 [02:56<00:00,  3.56it/s]100%|█████████▉| 404/405 [02:56<00:00,  3.56it/s]100%|██████████| 405/405 [02:56<00:00,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 00:59:26,166 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:59:26,166 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:59:26,166 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0832, 'eval_samples_per_second': 352.283, 'eval_steps_per_second': 44.102, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.74it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.48it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.34it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.36it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.77it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.49it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.10it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.19it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.35it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.37it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.33it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.09it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.05it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.92it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.97it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.98it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.15it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.17it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.21it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.06it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.97it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.98it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.04it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.11it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.23it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.30it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.20it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.12it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.97it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.94it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.99it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.90it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.08it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.23it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.23it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.17it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.02it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.95it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.98it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.00it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.93it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.05it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.23it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.18it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.06it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.02it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.94it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.99it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.07it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.17it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.30it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.12it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.98it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.06it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.08it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.97it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.13it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.13it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.06it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.21it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.04it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.99it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.02it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.04it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.09it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.10it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.11it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.12it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.08it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.13it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.07it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.11it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.09it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.01it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.05it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.05it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.10it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.09it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.11it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.83it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.95it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.92it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.86it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.93it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.04it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.13it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.11it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.05it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.02it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.04it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.11it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.13it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.13it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.14it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.13it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.02it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.93it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.03it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.06it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.06it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.14it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.15it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.19it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.08it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.93it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.06it/s][A                                                 
                                                 [A100%|██████████| 405/405 [03:09<00:00,  3.94it/s]
100%|██████████| 577/577 [00:13<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:59:39,270 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:59:39,284 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:59:41,159 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:59:41,172 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:59:41,182 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:59:41,474 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:59:41,474 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81 (score: 1.12112557888031).
                                                 100%|██████████| 405/405 [03:13<00:00,  3.94it/s]100%|██████████| 405/405 [03:13<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 00:59:43,269 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:59:43,282 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:59:45,141 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:59:45,154 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:59:45,163 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:59:45,346 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,346 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,346 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,347 >>   train_runtime            = 0:03:13.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,347 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,347 >>   train_samples_per_second =    133.254
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:45,347 >>   train_steps_per_second   =      2.092
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.082, 'eval_samples_per_second': 352.317, 'eval_steps_per_second': 44.107, 'epoch': 5.0}
{'train_runtime': 193.6152, 'train_samples_per_second': 133.254, 'train_steps_per_second': 2.092, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:59:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:59:45,382 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:59:45,382 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 00:59:45,382 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.74it/s]  2%|▏         | 12/577 [00:00<00:11, 48.56it/s]  3%|▎         | 17/577 [00:00<00:11, 46.94it/s]  4%|▍         | 22/577 [00:00<00:12, 46.16it/s]  5%|▍         | 27/577 [00:00<00:12, 45.79it/s]  6%|▌         | 32/577 [00:00<00:11, 45.43it/s]  6%|▋         | 37/577 [00:00<00:11, 45.27it/s]  7%|▋         | 42/577 [00:00<00:11, 44.70it/s]  8%|▊         | 47/577 [00:01<00:12, 44.14it/s]  9%|▉         | 52/577 [00:01<00:11, 43.95it/s] 10%|▉         | 57/577 [00:01<00:11, 44.03it/s] 11%|█         | 62/577 [00:01<00:11, 44.16it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.31it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.54it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.66it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.57it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.32it/s] 16%|█▌        | 92/577 [00:02<00:11, 44.05it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.90it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.94it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.12it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.25it/s] 20%|██        | 117/577 [00:02<00:10, 44.45it/s] 21%|██        | 122/577 [00:02<00:10, 44.57it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.57it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.23it/s] 24%|██▎       | 137/577 [00:03<00:09, 44.07it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.94it/s] 25%|██▌       | 147/577 [00:03<00:09, 44.02it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.08it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.29it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.30it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.44it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.49it/s] 31%|███       | 177/577 [00:03<00:09, 44.35it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.06it/s] 32%|███▏      | 187/577 [00:04<00:08, 44.05it/s] 33%|███▎      | 192/577 [00:04<00:08, 44.03it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.17it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.19it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.29it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.42it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.39it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.25it/s] 39%|███▉      | 227/577 [00:05<00:07, 44.00it/s] 40%|████      | 232/577 [00:05<00:07, 44.10it/s] 41%|████      | 237/577 [00:05<00:07, 44.06it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.16it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.20it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.31it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.36it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.39it/s] 46%|████▋     | 267/577 [00:06<00:07, 44.19it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.09it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.92it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.03it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.04it/s] 51%|█████     | 292/577 [00:06<00:06, 44.25it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.36it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.37it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.36it/s] 54%|█████▍    | 312/577 [00:07<00:05, 44.21it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.16it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.15it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.04it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.15it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.27it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.34it/s] 60%|██████    | 347/577 [00:07<00:05, 44.33it/s] 61%|██████    | 352/577 [00:07<00:05, 44.27it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.13it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.02it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.10it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.15it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.21it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.31it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.32it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.19it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.17it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.15it/s] 71%|███████   | 407/577 [00:09<00:03, 43.99it/s] 71%|███████▏  | 412/577 [00:09<00:03, 44.06it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.08it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.15it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.32it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.28it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.25it/s] 77%|███████▋  | 442/577 [00:09<00:03, 44.32it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.09it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.00it/s] 79%|███████▉  | 457/577 [00:10<00:02, 43.97it/s] 80%|████████  | 462/577 [00:10<00:02, 44.07it/s] 81%|████████  | 467/577 [00:10<00:02, 44.20it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.31it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.31it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.31it/s] 84%|████████▍ | 487/577 [00:10<00:02, 44.23it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.16it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.07it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.06it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.25it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.35it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.29it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.29it/s] 92%|█████████▏| 532/577 [00:12<00:01, 44.18it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.01it/s] 94%|█████████▍| 542/577 [00:12<00:00, 44.02it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.06it/s] 96%|█████████▌| 552/577 [00:12<00:00, 43.58it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.39it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.40it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.39it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.34it/s]100%|██████████| 577/577 [00:13<00:00, 44.20it/s]100%|██████████| 577/577 [00:13<00:00, 44.31it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:59:58,424 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,424 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,424 >>   eval_loss               =     1.1211
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,424 >>   eval_runtime            = 0:00:13.04
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,425 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,425 >>   eval_samples_per_second =      353.4
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,425 >>   eval_steps_per_second   =     44.242
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:59:58,425 >>   perplexity              =     3.0683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:05,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:05,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:05,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:05,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:05,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:00:05,617 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:00:05,618 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:00:06,207 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:00:07,276 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:00:07,276 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:10,243 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:10,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:10,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:10,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:10,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:00:10,907 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:00:10,909 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:00:11,487 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:00:11,648 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:00:11,648 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-81
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-324
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-162
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-243
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/checkpoint-405
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.26it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.42it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.41it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:12,  1.42it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.45it/s]Extractor Predicting: 21it [00:15,  1.42it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:16,  1.40it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.42it/s]Extractor Predicting: 27it [00:19,  1.46it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.46it/s]Extractor Predicting: 30it [00:21,  1.47it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.44it/s]Extractor Predicting: 33it [00:23,  1.44it/s]Extractor Predicting: 34it [00:24,  1.40it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.45it/s]Extractor Predicting: 37it [00:26,  1.44it/s]Extractor Predicting: 38it [00:26,  1.42it/s]Extractor Predicting: 39it [00:27,  1.40it/s]Extractor Predicting: 40it [00:28,  1.40it/s]Extractor Predicting: 41it [00:29,  1.37it/s]Extractor Predicting: 42it [00:29,  1.30it/s]Extractor Predicting: 43it [00:30,  1.31it/s]Extractor Predicting: 44it [00:31,  1.31it/s]Extractor Predicting: 45it [00:32,  1.31it/s]Extractor Predicting: 46it [00:32,  1.32it/s]Extractor Predicting: 47it [00:33,  1.32it/s]Extractor Predicting: 48it [00:34,  1.33it/s]Extractor Predicting: 49it [00:35,  1.39it/s]Extractor Predicting: 50it [00:35,  1.38it/s]Extractor Predicting: 51it [00:36,  1.40it/s]Extractor Predicting: 52it [00:37,  1.38it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:38,  1.36it/s]Extractor Predicting: 55it [00:39,  1.36it/s]Extractor Predicting: 56it [00:40,  1.40it/s]Extractor Predicting: 57it [00:40,  1.38it/s]Extractor Predicting: 58it [00:41,  1.36it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:43,  1.40it/s]Extractor Predicting: 61it [00:43,  1.37it/s]Extractor Predicting: 62it [00:44,  1.35it/s]Extractor Predicting: 63it [00:45,  1.34it/s]Extractor Predicting: 64it [00:46,  1.33it/s]Extractor Predicting: 65it [00:46,  1.34it/s]Extractor Predicting: 66it [00:47,  1.38it/s]Extractor Predicting: 67it [00:48,  1.38it/s]Extractor Predicting: 68it [00:48,  1.42it/s]Extractor Predicting: 69it [00:49,  1.43it/s]Extractor Predicting: 70it [00:50,  1.42it/s]Extractor Predicting: 71it [00:50,  1.44it/s]Extractor Predicting: 72it [00:51,  1.49it/s]Extractor Predicting: 73it [00:52,  1.46it/s]Extractor Predicting: 74it [00:52,  1.48it/s]Extractor Predicting: 75it [00:53,  1.48it/s]Extractor Predicting: 76it [00:54,  1.44it/s]Extractor Predicting: 77it [00:55,  1.43it/s]Extractor Predicting: 78it [00:55,  1.45it/s]Extractor Predicting: 79it [00:56,  1.44it/s]Extractor Predicting: 80it [00:57,  1.45it/s]Extractor Predicting: 81it [00:57,  1.43it/s]Extractor Predicting: 82it [00:58,  1.42it/s]Extractor Predicting: 83it [00:59,  1.41it/s]Extractor Predicting: 84it [00:59,  1.46it/s]Extractor Predicting: 85it [01:00,  1.52it/s]Extractor Predicting: 86it [01:01,  1.51it/s]Extractor Predicting: 87it [01:01,  1.53it/s]Extractor Predicting: 88it [01:02,  1.45it/s]Extractor Predicting: 89it [01:03,  1.44it/s]Extractor Predicting: 90it [01:04,  1.42it/s]Extractor Predicting: 91it [01:04,  1.43it/s]Extractor Predicting: 92it [01:05,  1.44it/s]Extractor Predicting: 93it [01:06,  1.44it/s]Extractor Predicting: 94it [01:06,  1.45it/s]Extractor Predicting: 95it [01:07,  1.44it/s]Extractor Predicting: 96it [01:08,  1.45it/s]Extractor Predicting: 97it [01:08,  1.45it/s]Extractor Predicting: 98it [01:09,  1.41it/s]Extractor Predicting: 99it [01:10,  1.42it/s]Extractor Predicting: 100it [01:11,  1.39it/s]Extractor Predicting: 101it [01:11,  1.39it/s]Extractor Predicting: 102it [01:12,  1.41it/s]Extractor Predicting: 103it [01:13,  1.39it/s]Extractor Predicting: 104it [01:13,  1.42it/s]Extractor Predicting: 105it [01:14,  1.45it/s]Extractor Predicting: 106it [01:15,  1.40it/s]Extractor Predicting: 107it [01:15,  1.44it/s]Extractor Predicting: 108it [01:16,  1.46it/s]Extractor Predicting: 109it [01:17,  1.44it/s]Extractor Predicting: 110it [01:18,  1.44it/s]Extractor Predicting: 111it [01:18,  1.41it/s]Extractor Predicting: 112it [01:19,  1.41it/s]Extractor Predicting: 113it [01:20,  1.42it/s]Extractor Predicting: 114it [01:20,  1.42it/s]Extractor Predicting: 115it [01:21,  1.42it/s]Extractor Predicting: 116it [01:22,  1.43it/s]Extractor Predicting: 117it [01:22,  1.46it/s]Extractor Predicting: 118it [01:23,  1.46it/s]Extractor Predicting: 119it [01:24,  1.51it/s]Extractor Predicting: 120it [01:24,  1.49it/s]Extractor Predicting: 121it [01:25,  1.47it/s]Extractor Predicting: 122it [01:26,  1.45it/s]Extractor Predicting: 123it [01:27,  1.44it/s]Extractor Predicting: 124it [01:27,  1.45it/s]Extractor Predicting: 125it [01:28,  1.42it/s]Extractor Predicting: 126it [01:29,  1.46it/s]Extractor Predicting: 127it [01:29,  1.48it/s]Extractor Predicting: 128it [01:30,  1.45it/s]Extractor Predicting: 129it [01:31,  1.44it/s]Extractor Predicting: 130it [01:31,  1.41it/s]Extractor Predicting: 131it [01:32,  1.41it/s]Extractor Predicting: 132it [01:33,  1.43it/s]Extractor Predicting: 133it [01:34,  1.31it/s]Extractor Predicting: 134it [01:34,  1.33it/s]Extractor Predicting: 135it [01:35,  1.34it/s]Extractor Predicting: 136it [01:36,  1.35it/s]Extractor Predicting: 137it [01:37,  1.38it/s]Extractor Predicting: 138it [01:37,  1.38it/s]Extractor Predicting: 139it [01:38,  1.36it/s]Extractor Predicting: 140it [01:39,  1.35it/s]Extractor Predicting: 141it [01:40,  1.34it/s]Extractor Predicting: 142it [01:40,  1.32it/s]Extractor Predicting: 143it [01:41,  1.35it/s]Extractor Predicting: 144it [01:42,  1.33it/s]Extractor Predicting: 145it [01:43,  1.33it/s]Extractor Predicting: 146it [01:43,  1.30it/s]Extractor Predicting: 147it [01:44,  1.31it/s]Extractor Predicting: 148it [01:45,  1.33it/s]Extractor Predicting: 149it [01:46,  1.35it/s]Extractor Predicting: 150it [01:46,  1.35it/s]Extractor Predicting: 151it [01:47,  1.38it/s]Extractor Predicting: 152it [01:48,  1.37it/s]Extractor Predicting: 153it [01:48,  1.40it/s]Extractor Predicting: 154it [01:49,  1.39it/s]Extractor Predicting: 155it [01:50,  1.39it/s]Extractor Predicting: 156it [01:51,  1.37it/s]Extractor Predicting: 157it [01:51,  1.39it/s]Extractor Predicting: 158it [01:52,  1.40it/s]Extractor Predicting: 159it [01:53,  1.38it/s]Extractor Predicting: 160it [01:54,  1.38it/s]Extractor Predicting: 161it [01:54,  1.41it/s]Extractor Predicting: 162it [01:55,  1.41it/s]Extractor Predicting: 163it [01:56,  1.36it/s]Extractor Predicting: 164it [01:56,  1.36it/s]Extractor Predicting: 165it [01:57,  1.37it/s]Extractor Predicting: 166it [01:58,  1.35it/s]Extractor Predicting: 167it [01:59,  1.37it/s]Extractor Predicting: 168it [01:59,  1.37it/s]Extractor Predicting: 169it [02:00,  1.38it/s]Extractor Predicting: 170it [02:01,  1.38it/s]Extractor Predicting: 171it [02:01,  1.66it/s]Extractor Predicting: 171it [02:01,  1.41it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:20,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:20,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:20,922 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:20,922 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:20,922 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:02:21,533 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:02:21,534 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:02:22,119 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:02:23,153 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:02:23,153 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:25,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:25,978 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:25,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:25,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:25,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:02:26,597 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:02:26,598 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:02:27,172 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:02:27,340 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:02:27,340 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.34it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.44it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:15,  1.45it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:17,  1.39it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.44it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.42it/s]Extractor Predicting: 31it [00:21,  1.41it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:25,  1.41it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:26,  1.42it/s]Extractor Predicting: 39it [00:27,  1.41it/s]Extractor Predicting: 40it [00:28,  1.43it/s]Extractor Predicting: 41it [00:28,  1.44it/s]Extractor Predicting: 42it [00:29,  1.44it/s]Extractor Predicting: 43it [00:30,  1.44it/s]Extractor Predicting: 44it [00:30,  1.46it/s]Extractor Predicting: 45it [00:31,  1.46it/s]Extractor Predicting: 46it [00:32,  1.48it/s]Extractor Predicting: 47it [00:33,  1.43it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:34,  1.45it/s]Extractor Predicting: 50it [00:35,  1.44it/s]Extractor Predicting: 51it [00:35,  1.45it/s]Extractor Predicting: 52it [00:36,  1.46it/s]Extractor Predicting: 53it [00:37,  1.45it/s]Extractor Predicting: 54it [00:37,  1.43it/s]Extractor Predicting: 55it [00:38,  1.41it/s]Extractor Predicting: 56it [00:39,  1.43it/s]Extractor Predicting: 57it [00:39,  1.44it/s]Extractor Predicting: 58it [00:40,  1.45it/s]Extractor Predicting: 59it [00:41,  1.45it/s]Extractor Predicting: 60it [00:42,  1.42it/s]Extractor Predicting: 61it [00:42,  1.42it/s]Extractor Predicting: 62it [00:43,  1.40it/s]Extractor Predicting: 63it [00:44,  1.44it/s]Extractor Predicting: 64it [00:44,  1.43it/s]Extractor Predicting: 65it [00:45,  1.45it/s]Extractor Predicting: 66it [00:46,  1.43it/s]Extractor Predicting: 67it [00:46,  1.45it/s]Extractor Predicting: 68it [00:47,  1.46it/s]Extractor Predicting: 69it [00:48,  1.48it/s]Extractor Predicting: 70it [00:48,  1.45it/s]Extractor Predicting: 71it [00:49,  1.40it/s]Extractor Predicting: 72it [00:50,  1.42it/s]Extractor Predicting: 73it [00:51,  1.41it/s]Extractor Predicting: 74it [00:51,  1.44it/s]Extractor Predicting: 75it [00:52,  1.43it/s]Extractor Predicting: 76it [00:53,  1.47it/s]Extractor Predicting: 77it [00:53,  1.50it/s]Extractor Predicting: 78it [00:54,  1.50it/s]Extractor Predicting: 79it [00:55,  1.47it/s]Extractor Predicting: 80it [00:55,  1.49it/s]Extractor Predicting: 81it [00:56,  1.47it/s]Extractor Predicting: 82it [00:57,  1.48it/s]Extractor Predicting: 83it [00:57,  1.47it/s]Extractor Predicting: 84it [00:58,  1.45it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [00:59,  1.45it/s]Extractor Predicting: 87it [01:00,  1.43it/s]Extractor Predicting: 88it [01:01,  1.45it/s]Extractor Predicting: 89it [01:02,  1.45it/s]Extractor Predicting: 90it [01:02,  1.42it/s]Extractor Predicting: 91it [01:03,  1.43it/s]Extractor Predicting: 92it [01:04,  1.42it/s]Extractor Predicting: 93it [01:04,  1.43it/s]Extractor Predicting: 94it [01:05,  1.42it/s]Extractor Predicting: 95it [01:06,  1.41it/s]Extractor Predicting: 96it [01:07,  1.38it/s]Extractor Predicting: 97it [01:07,  1.42it/s]Extractor Predicting: 98it [01:08,  1.42it/s]Extractor Predicting: 99it [01:09,  1.50it/s]Extractor Predicting: 100it [01:09,  1.50it/s]Extractor Predicting: 101it [01:10,  1.47it/s]Extractor Predicting: 102it [01:11,  1.47it/s]Extractor Predicting: 103it [01:11,  1.47it/s]Extractor Predicting: 104it [01:12,  1.47it/s]Extractor Predicting: 105it [01:13,  1.46it/s]Extractor Predicting: 106it [01:13,  1.48it/s]Extractor Predicting: 107it [01:14,  1.50it/s]Extractor Predicting: 108it [01:15,  1.47it/s]Extractor Predicting: 109it [01:15,  1.43it/s]Extractor Predicting: 110it [01:16,  1.26it/s]Extractor Predicting: 111it [01:17,  1.27it/s]Extractor Predicting: 112it [01:18,  1.30it/s]Extractor Predicting: 113it [01:19,  1.36it/s]Extractor Predicting: 114it [01:19,  1.39it/s]Extractor Predicting: 115it [01:20,  1.39it/s]Extractor Predicting: 116it [01:21,  1.39it/s]Extractor Predicting: 117it [01:21,  1.39it/s]Extractor Predicting: 118it [01:22,  1.36it/s]Extractor Predicting: 119it [01:23,  1.38it/s]Extractor Predicting: 120it [01:24,  1.35it/s]Extractor Predicting: 121it [01:24,  1.35it/s]Extractor Predicting: 122it [01:25,  1.37it/s]Extractor Predicting: 123it [01:26,  1.39it/s]Extractor Predicting: 124it [01:27,  1.39it/s]Extractor Predicting: 125it [01:27,  1.76it/s]Extractor Predicting: 125it [01:27,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:00,807 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:00,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:00,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:00,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:00,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:04:01,429 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:04:01,430 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:04:02,007 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:04:03,039 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:04:03,039 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:05,859 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:05,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:05,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:05,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:05,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:04:06,508 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:04:06,508 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:04:07,072 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:04:07,243 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:04:07,243 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.77it/s]Extractor Predicting: 6it [00:04,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 01:04:11,793 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:04:11,794 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:04:11,797 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:04:11,797 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 01:04:11,803 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:04:14,842 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 01:04:14,845 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 01:04:14,862 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:04:14,863 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:04:14,868 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:04:14,873 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 01:04:15,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:15,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:16,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:17,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:18,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:19,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:20,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:21,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:22,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:22,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:23,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:24,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:25,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:26,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:27,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:27,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:28,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:29,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:30,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:31,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:32,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:32,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:33,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:34,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:35,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:20<03:06, 20.77s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:35,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:36,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:37,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:38,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:38,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:39,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:40,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:40,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:41,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:42,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:43,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:43,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:44,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:45,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:46,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:46,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:47,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:48,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:49,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:49,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:50,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:51,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:26, 18.26s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:52,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:53,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:54,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:54,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:55,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:56,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:57,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:57,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:58,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:59,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:00,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:00,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:01,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:02,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:03,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:03,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:04,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:05,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:06,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:07,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:07,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:08,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:09,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:10,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:10,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:11,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:12,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:13,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:13,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:20, 20.10s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:14,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:15,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:16,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:16,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:17,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:18,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:19,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:20,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:21,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:21,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:22,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:23,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:24,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:25,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:25,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:26,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:27,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:28,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:29,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:29,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:30,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:32,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:32,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:33,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:34,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:35,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:36,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:21<02:06, 21.01s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:37,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:37,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:38,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:39,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:39,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:40,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:41,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:42,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:42,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:43,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:44,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:45,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:46,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:46,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:47,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:48,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:48,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:49,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:50,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:50,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:51,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:52,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:53,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:37, 19.52s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:53,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:54,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:55,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:56,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:57,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:57,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:58,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:59,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:00,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:00,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:01,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:02,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:03,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:04,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:04,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:05,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:06,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:07,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:08,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:08,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:09,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:10,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:56<01:14, 18.72s/it][WARNING|generation_utils.py:914] 2023-08-28 01:06:11,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:11,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:12,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:13,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:14,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:14,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:15,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:16,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:17,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:18,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:18,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:19,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:20,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:21,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:22,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:23,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:24,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:24,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:25,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:26,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:27,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:28,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:29,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:15<00:56, 18.82s/it][WARNING|generation_utils.py:914] 2023-08-28 01:06:30,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:30,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:31,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:32,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:33,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:33,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:34,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:35,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:36,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:36,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:37,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:38,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:39,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:39,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:40,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:41,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:42,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:43,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:43,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:44,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:45,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:31<00:35, 17.93s/it][WARNING|generation_utils.py:914] 2023-08-28 01:06:46,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:47,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:47,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:48,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:49,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:50,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:50,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:51,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:52,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:53,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:53,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:54,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:55,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:56,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:57,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:58,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:58,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:59,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:01,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:02,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:02,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:03,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:04,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:05,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:50<00:18, 18.47s/it][WARNING|generation_utils.py:914] 2023-08-28 01:07:05,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:06,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:07,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:07,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:08,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:09,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:11,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:12,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:12,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:13,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:14,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:15,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:15,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:16,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:17,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:17,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:18,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:19,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:20,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:21,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:22,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:23,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:23,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:24,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:25,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:26,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:27,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:27,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:28,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:29,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:07:30,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:15<00:00, 20.53s/it]Generating: 100%|██████████| 10/10 [03:15<00:00, 19.59s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:35,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:35,767 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:35,767 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:35,767 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:35,767 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:07:36,404 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:07:36,404 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:07:36,978 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:07:38,097 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:07:38,097 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:41,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:41,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:41,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:41,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:07:41,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:07:41,722 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:07:41,723 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:07:42,299 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:07:42,482 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:07:42,482 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.14it/s]Extractor Estimating: 2it [00:01,  1.19it/s]Extractor Estimating: 3it [00:02,  1.27it/s]Extractor Estimating: 4it [00:03,  1.28it/s]Extractor Estimating: 5it [00:04,  1.12it/s]Extractor Estimating: 6it [00:05,  1.19it/s]Extractor Estimating: 7it [00:05,  1.22it/s]Extractor Estimating: 8it [00:06,  1.26it/s]Extractor Estimating: 9it [00:07,  1.31it/s]Extractor Estimating: 10it [00:08,  1.28it/s]Extractor Estimating: 11it [00:08,  1.25it/s]Extractor Estimating: 12it [00:09,  1.27it/s]Extractor Estimating: 13it [00:10,  1.29it/s]Extractor Estimating: 14it [00:11,  1.29it/s]Extractor Estimating: 15it [00:11,  1.30it/s]Extractor Estimating: 16it [00:12,  1.30it/s]Extractor Estimating: 17it [00:13,  1.31it/s]Extractor Estimating: 18it [00:14,  1.30it/s]Extractor Estimating: 19it [00:15,  1.29it/s]Extractor Estimating: 20it [00:15,  1.28it/s]Extractor Estimating: 21it [00:16,  1.30it/s]Extractor Estimating: 22it [00:17,  1.29it/s]Extractor Estimating: 23it [00:18,  1.27it/s]Extractor Estimating: 24it [00:19,  1.24it/s]Extractor Estimating: 25it [00:19,  1.27it/s]Extractor Estimating: 26it [00:20,  1.25it/s]Extractor Estimating: 27it [00:21,  1.26it/s]Extractor Estimating: 28it [00:22,  1.29it/s]Extractor Estimating: 29it [00:22,  1.33it/s]Extractor Estimating: 30it [00:23,  1.31it/s]Extractor Estimating: 31it [00:24,  1.29it/s]Extractor Estimating: 32it [00:25,  1.31it/s]Extractor Estimating: 33it [00:25,  1.33it/s]Extractor Estimating: 34it [00:26,  1.35it/s]Extractor Estimating: 35it [00:27,  1.30it/s]Extractor Estimating: 36it [00:28,  1.35it/s]Extractor Estimating: 37it [00:28,  1.32it/s]Extractor Estimating: 38it [00:29,  1.33it/s]Extractor Estimating: 39it [00:30,  1.35it/s]Extractor Estimating: 40it [00:31,  1.32it/s]Extractor Estimating: 41it [00:31,  1.28it/s]Extractor Estimating: 42it [00:32,  1.30it/s]Extractor Estimating: 43it [00:33,  1.34it/s]Extractor Estimating: 44it [00:34,  1.37it/s]Extractor Estimating: 45it [00:34,  1.37it/s]Extractor Estimating: 46it [00:35,  1.37it/s]Extractor Estimating: 47it [00:36,  1.35it/s]Extractor Estimating: 48it [00:37,  1.36it/s]Extractor Estimating: 49it [00:37,  1.34it/s]Extractor Estimating: 50it [00:38,  1.36it/s]Extractor Estimating: 51it [00:39,  1.34it/s]Extractor Estimating: 52it [00:40,  1.32it/s]Extractor Estimating: 53it [00:40,  1.33it/s]Extractor Estimating: 54it [00:41,  1.31it/s]Extractor Estimating: 55it [00:42,  1.32it/s]Extractor Estimating: 56it [00:43,  1.29it/s]Extractor Estimating: 57it [00:43,  1.26it/s]Extractor Estimating: 58it [00:44,  1.32it/s]Extractor Estimating: 59it [00:45,  1.34it/s]Extractor Estimating: 60it [00:46,  1.33it/s]Extractor Estimating: 61it [00:46,  1.30it/s]Extractor Estimating: 62it [00:47,  1.32it/s]Extractor Estimating: 63it [00:48,  1.35it/s]Extractor Estimating: 64it [00:49,  1.33it/s]Extractor Estimating: 65it [00:49,  1.32it/s]Extractor Estimating: 66it [00:50,  1.31it/s]Extractor Estimating: 67it [00:51,  1.30it/s]Extractor Estimating: 68it [00:52,  1.31it/s]Extractor Estimating: 69it [00:53,  1.32it/s]Extractor Estimating: 70it [00:53,  1.30it/s]Extractor Estimating: 71it [00:54,  1.29it/s]Extractor Estimating: 72it [00:55,  1.29it/s]Extractor Estimating: 73it [00:56,  1.30it/s]Extractor Estimating: 74it [00:56,  1.32it/s]Extractor Estimating: 75it [00:57,  1.33it/s]Extractor Estimating: 76it [00:58,  1.37it/s]Extractor Estimating: 77it [00:59,  1.31it/s]Extractor Estimating: 78it [00:59,  1.29it/s]Extractor Estimating: 79it [01:00,  1.35it/s]Extractor Estimating: 80it [01:01,  1.36it/s]Extractor Estimating: 81it [01:02,  1.34it/s]Extractor Estimating: 82it [01:02,  1.31it/s]Extractor Estimating: 83it [01:03,  1.30it/s]Extractor Estimating: 84it [01:04,  1.26it/s]Extractor Estimating: 85it [01:05,  1.25it/s]Extractor Estimating: 86it [01:06,  1.26it/s]Extractor Estimating: 87it [01:07,  1.16it/s]Extractor Estimating: 88it [01:07,  1.22it/s]Extractor Estimating: 89it [01:08,  1.22it/s]Extractor Estimating: 90it [01:09,  1.24it/s]Extractor Estimating: 91it [01:10,  1.24it/s]Extractor Estimating: 92it [01:10,  1.27it/s]Extractor Estimating: 93it [01:11,  1.26it/s]Extractor Estimating: 94it [01:12,  1.25it/s]Extractor Estimating: 95it [01:13,  1.25it/s]Extractor Estimating: 96it [01:14,  1.24it/s]Extractor Estimating: 97it [01:15,  1.23it/s]Extractor Estimating: 98it [01:15,  1.25it/s]Extractor Estimating: 99it [01:16,  1.25it/s]Extractor Estimating: 100it [01:17,  1.30it/s]Extractor Estimating: 101it [01:18,  1.29it/s]Extractor Estimating: 102it [01:18,  1.27it/s]Extractor Estimating: 103it [01:19,  1.29it/s]Extractor Estimating: 104it [01:20,  1.30it/s]Extractor Estimating: 105it [01:21,  1.31it/s]Extractor Estimating: 106it [01:21,  1.30it/s]Extractor Estimating: 107it [01:22,  1.29it/s]Extractor Estimating: 108it [01:23,  1.31it/s]Extractor Estimating: 109it [01:24,  1.32it/s]Extractor Estimating: 110it [01:24,  1.32it/s]Extractor Estimating: 111it [01:25,  1.31it/s]Extractor Estimating: 112it [01:26,  1.31it/s]Extractor Estimating: 113it [01:27,  1.29it/s]Extractor Estimating: 114it [01:28,  1.31it/s]Extractor Estimating: 115it [01:28,  1.32it/s]Extractor Estimating: 116it [01:29,  1.33it/s]Extractor Estimating: 117it [01:30,  1.33it/s]Extractor Estimating: 118it [01:31,  1.35it/s]Extractor Estimating: 119it [01:31,  1.35it/s]Extractor Estimating: 120it [01:32,  1.35it/s]Extractor Estimating: 121it [01:33,  1.35it/s]Extractor Estimating: 122it [01:33,  1.35it/s]Extractor Estimating: 123it [01:34,  1.33it/s]Extractor Estimating: 124it [01:35,  1.32it/s]Extractor Estimating: 125it [01:36,  1.33it/s]Extractor Estimating: 126it [01:37,  1.33it/s]Extractor Estimating: 127it [01:37,  1.30it/s]Extractor Estimating: 128it [01:38,  1.33it/s]Extractor Estimating: 129it [01:39,  1.29it/s]Extractor Estimating: 130it [01:40,  1.26it/s]Extractor Estimating: 131it [01:41,  1.24it/s]Extractor Estimating: 132it [01:41,  1.27it/s]Extractor Estimating: 133it [01:42,  1.26it/s]Extractor Estimating: 134it [01:43,  1.26it/s]Extractor Estimating: 135it [01:44,  1.29it/s]Extractor Estimating: 136it [01:44,  1.30it/s]Extractor Estimating: 137it [01:45,  1.26it/s]Extractor Estimating: 138it [01:46,  1.26it/s]Extractor Estimating: 139it [01:47,  1.23it/s]Extractor Estimating: 140it [01:48,  1.27it/s]Extractor Estimating: 141it [01:48,  1.32it/s]Extractor Estimating: 142it [01:49,  1.32it/s]Extractor Estimating: 143it [01:50,  1.31it/s]Extractor Estimating: 144it [01:51,  1.28it/s]Extractor Estimating: 145it [01:51,  1.27it/s]Extractor Estimating: 146it [01:52,  1.24it/s]Extractor Estimating: 147it [01:53,  1.26it/s]Extractor Estimating: 148it [01:54,  1.27it/s]Extractor Estimating: 149it [01:55,  1.30it/s]Extractor Estimating: 150it [01:55,  1.30it/s]Extractor Estimating: 151it [01:56,  1.33it/s]Extractor Estimating: 152it [01:57,  1.32it/s]Extractor Estimating: 153it [01:58,  1.33it/s]Extractor Estimating: 154it [01:58,  1.32it/s]Extractor Estimating: 155it [01:59,  1.32it/s]Extractor Estimating: 156it [02:00,  1.31it/s]Extractor Estimating: 157it [02:01,  1.29it/s]Extractor Estimating: 158it [02:01,  1.30it/s]Extractor Estimating: 159it [02:02,  1.32it/s]Extractor Estimating: 160it [02:03,  1.30it/s]Extractor Estimating: 161it [02:04,  1.29it/s]Extractor Estimating: 162it [02:05,  1.22it/s]Extractor Estimating: 163it [02:05,  1.26it/s]Extractor Estimating: 164it [02:06,  1.24it/s]Extractor Estimating: 165it [02:07,  1.24it/s]Extractor Estimating: 166it [02:08,  1.26it/s]Extractor Estimating: 167it [02:09,  1.26it/s]Extractor Estimating: 168it [02:09,  1.27it/s]Extractor Estimating: 169it [02:10,  1.28it/s]Extractor Estimating: 170it [02:11,  1.25it/s]Extractor Estimating: 171it [02:12,  1.30it/s]Extractor Estimating: 172it [02:12,  1.28it/s]Extractor Estimating: 173it [02:13,  1.32it/s]Extractor Estimating: 174it [02:14,  1.33it/s]Extractor Estimating: 175it [02:15,  1.31it/s]Extractor Estimating: 176it [02:15,  1.29it/s]Extractor Estimating: 177it [02:16,  1.29it/s]Extractor Estimating: 178it [02:17,  1.28it/s]Extractor Estimating: 179it [02:18,  1.28it/s]Extractor Estimating: 180it [02:19,  1.25it/s]Extractor Estimating: 181it [02:19,  1.30it/s]Extractor Estimating: 182it [02:20,  1.31it/s]Extractor Estimating: 183it [02:21,  1.28it/s]Extractor Estimating: 184it [02:22,  1.29it/s]Extractor Estimating: 185it [02:22,  1.31it/s]Extractor Estimating: 186it [02:23,  1.34it/s]Extractor Estimating: 187it [02:24,  1.34it/s]Extractor Estimating: 188it [02:25,  1.35it/s]Extractor Estimating: 189it [02:25,  1.33it/s]Extractor Estimating: 190it [02:26,  1.28it/s]Extractor Estimating: 191it [02:27,  1.32it/s]Extractor Estimating: 192it [02:28,  1.29it/s]Extractor Estimating: 193it [02:29,  1.29it/s]Extractor Estimating: 194it [02:29,  1.31it/s]Extractor Estimating: 195it [02:30,  1.32it/s]Extractor Estimating: 196it [02:31,  1.33it/s]Extractor Estimating: 197it [02:32,  1.33it/s]Extractor Estimating: 198it [02:32,  1.32it/s]Extractor Estimating: 199it [02:33,  1.26it/s]Extractor Estimating: 200it [02:34,  1.29it/s]Extractor Estimating: 201it [02:35,  1.29it/s]Extractor Estimating: 202it [02:35,  1.31it/s]Extractor Estimating: 203it [02:36,  1.32it/s]Extractor Estimating: 204it [02:37,  1.32it/s]Extractor Estimating: 205it [02:38,  1.32it/s]Extractor Estimating: 206it [02:38,  1.35it/s]Extractor Estimating: 207it [02:39,  1.33it/s]Extractor Estimating: 208it [02:40,  1.34it/s]Extractor Estimating: 209it [02:41,  1.32it/s]Extractor Estimating: 210it [02:41,  1.33it/s]Extractor Estimating: 211it [02:42,  1.31it/s]Extractor Estimating: 212it [02:43,  1.30it/s]Extractor Estimating: 213it [02:44,  1.29it/s]Extractor Estimating: 214it [02:45,  1.31it/s]Extractor Estimating: 215it [02:45,  1.29it/s]Extractor Estimating: 216it [02:46,  1.27it/s]Extractor Estimating: 217it [02:47,  1.29it/s]Extractor Estimating: 218it [02:48,  1.31it/s]Extractor Estimating: 219it [02:48,  1.33it/s]Extractor Estimating: 220it [02:49,  1.33it/s]Extractor Estimating: 221it [02:50,  1.31it/s]Extractor Estimating: 222it [02:51,  1.32it/s]Extractor Estimating: 223it [02:51,  1.34it/s]Extractor Estimating: 224it [02:52,  1.33it/s]Extractor Estimating: 225it [02:53,  1.31it/s]Extractor Estimating: 226it [02:54,  1.30it/s]Extractor Estimating: 227it [02:54,  1.31it/s]Extractor Estimating: 228it [02:55,  1.27it/s]Extractor Estimating: 229it [02:56,  1.30it/s]Extractor Estimating: 230it [02:57,  1.28it/s]Extractor Estimating: 231it [02:58,  1.26it/s]Extractor Estimating: 232it [02:58,  1.30it/s]Extractor Estimating: 233it [02:59,  1.28it/s]Extractor Estimating: 234it [03:00,  1.31it/s]Extractor Estimating: 235it [03:01,  1.32it/s]Extractor Estimating: 236it [03:01,  1.32it/s]Extractor Estimating: 237it [03:02,  1.32it/s]Extractor Estimating: 238it [03:03,  1.32it/s]Extractor Estimating: 239it [03:04,  1.18it/s]Extractor Estimating: 240it [03:05,  1.18it/s]Extractor Estimating: 241it [03:06,  1.20it/s]Extractor Estimating: 242it [03:06,  1.27it/s]Extractor Estimating: 243it [03:07,  1.28it/s]Extractor Estimating: 244it [03:08,  1.31it/s]Extractor Estimating: 245it [03:08,  1.34it/s]Extractor Estimating: 246it [03:09,  1.34it/s]Extractor Estimating: 247it [03:10,  1.34it/s]Extractor Estimating: 248it [03:11,  1.31it/s]Extractor Estimating: 249it [03:11,  1.34it/s]Extractor Estimating: 250it [03:12,  1.26it/s]Extractor Estimating: 250it [03:12,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:13,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:13,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:13,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:13,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:13,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:11:13,779 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:11:13,780 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:11:14,355 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:11:15,466 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:11:15,471 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:18,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:18,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:18,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:18,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:11:18,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:11:19,030 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:11:19,056 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:11:19,664 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:11:19,849 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:11:19,850 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 03:03:26,438 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 03:03:26,472 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5212 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 23618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23718, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.255, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.267, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.251, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.258, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.275, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.798, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.264, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.267, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.250, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.264, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.795, loss:nan
g_step 1200, step 110, avg_time 1.279, loss:nan
g_step 1300, step 210, avg_time 1.247, loss:nan
g_step 1400, step 92, avg_time 1.260, loss:nan
g_step 1500, step 192, avg_time 1.269, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.783, loss:nan
g_step 1700, step 174, avg_time 1.262, loss:nan
g_step 1800, step 56, avg_time 1.257, loss:nan
g_step 1900, step 156, avg_time 1.242, loss:nan
g_step 2000, step 38, avg_time 1.262, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.787, loss:nan
g_step 2200, step 20, avg_time 1.260, loss:nan
g_step 2300, step 120, avg_time 1.258, loss:nan
g_step 2400, step 2, avg_time 1.264, loss:nan
g_step 2500, step 102, avg_time 1.264, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.804, loss:nan
g_step 2700, step 84, avg_time 1.271, loss:nan
g_step 2800, step 184, avg_time 1.254, loss:nan
g_step 2900, step 66, avg_time 1.250, loss:nan
g_step 3000, step 166, avg_time 1.259, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.806, loss:nan
g_step 3200, step 148, avg_time 1.266, loss:nan
g_step 3300, step 30, avg_time 1.259, loss:nan
g_step 3400, step 130, avg_time 1.254, loss:nan
g_step 3500, step 12, avg_time 1.270, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.778, loss:nan
g_step 3700, step 212, avg_time 1.277, loss:nan
g_step 3800, step 94, avg_time 1.263, loss:nan
g_step 3900, step 194, avg_time 1.260, loss:nan
g_step 4000, step 76, avg_time 1.273, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.780, loss:nan
g_step 4200, step 58, avg_time 1.257, loss:nan
g_step 4300, step 158, avg_time 1.251, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:03:26 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:03:26 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-03-26_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:03:27 - WARNING - datasets.builder -   Using custom data configuration default-bba2f25f1927cbf6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bba2f25f1927cbf6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:03:27,663 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:03:27,664 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:03:27,664 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:03:27,665 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:03:27,677 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,680 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,681 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:03:27,681 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:03:27,821 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:03:30,963 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:03:30,965 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bba2f25f1927cbf6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.08ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.90ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.21ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.36ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.46ba/s]100%|██████████| 6/6 [00:01<00:00,  4.84ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.19ba/s] 40%|████      | 2/5 [00:00<00:00,  4.39ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.46ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.49ba/s]100%|██████████| 5/5 [00:01<00:00,  5.21ba/s]100%|██████████| 5/5 [00:01<00:00,  4.82ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.89ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.62ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.91ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.23ba/s]100%|██████████| 6/6 [00:00<00:00, 11.84ba/s]100%|██████████| 6/6 [00:00<00:00, 10.49ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.63ba/s] 40%|████      | 2/5 [00:00<00:00,  8.89ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.64ba/s]100%|██████████| 5/5 [00:00<00:00, 10.27ba/s]
[INFO|trainer.py:414] 2023-08-28 03:03:34,783 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:03:34,798 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:03:34,798 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 03:03:34,798 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:03:34,798 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:03:34,798 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:03:34,798 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:03:34,798 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.47it/s]  0%|          | 2/410 [00:00<01:55,  3.54it/s]  1%|          | 3/410 [00:00<01:54,  3.56it/s]  1%|          | 4/410 [00:01<01:53,  3.57it/s]  1%|          | 5/410 [00:01<01:53,  3.57it/s]  1%|▏         | 6/410 [00:01<01:53,  3.57it/s]  2%|▏         | 7/410 [00:01<01:53,  3.56it/s]  2%|▏         | 8/410 [00:02<01:52,  3.57it/s]  2%|▏         | 9/410 [00:02<01:52,  3.57it/s]  2%|▏         | 10/410 [00:02<01:52,  3.56it/s]  3%|▎         | 11/410 [00:03<01:51,  3.56it/s]  3%|▎         | 12/410 [00:03<01:51,  3.57it/s]  3%|▎         | 13/410 [00:03<01:51,  3.57it/s]  3%|▎         | 14/410 [00:03<01:50,  3.57it/s]  4%|▎         | 15/410 [00:04<01:50,  3.57it/s]  4%|▍         | 16/410 [00:04<01:50,  3.58it/s]  4%|▍         | 17/410 [00:04<01:49,  3.58it/s]  4%|▍         | 18/410 [00:05<01:50,  3.56it/s]  5%|▍         | 19/410 [00:05<01:50,  3.54it/s]  5%|▍         | 20/410 [00:05<01:50,  3.52it/s]  5%|▌         | 21/410 [00:05<01:51,  3.50it/s]  5%|▌         | 22/410 [00:06<01:50,  3.50it/s]  6%|▌         | 23/410 [00:06<01:50,  3.50it/s]  6%|▌         | 24/410 [00:06<01:50,  3.50it/s]  6%|▌         | 25/410 [00:07<01:50,  3.50it/s]  6%|▋         | 26/410 [00:07<01:49,  3.49it/s]  7%|▋         | 27/410 [00:07<01:49,  3.50it/s]  7%|▋         | 28/410 [00:07<01:49,  3.49it/s]  7%|▋         | 29/410 [00:08<01:49,  3.49it/s]  7%|▋         | 30/410 [00:08<01:48,  3.50it/s]  8%|▊         | 31/410 [00:08<01:48,  3.50it/s]  8%|▊         | 32/410 [00:09<02:10,  2.89it/s]  8%|▊         | 33/410 [00:09<02:02,  3.07it/s]  8%|▊         | 34/410 [00:09<01:57,  3.20it/s]  9%|▊         | 35/410 [00:10<01:53,  3.30it/s]  9%|▉         | 36/410 [00:10<01:50,  3.38it/s]  9%|▉         | 37/410 [00:10<01:48,  3.44it/s]  9%|▉         | 38/410 [00:10<01:47,  3.48it/s] 10%|▉         | 39/410 [00:11<01:45,  3.50it/s] 10%|▉         | 40/410 [00:11<01:44,  3.52it/s] 10%|█         | 41/410 [00:11<01:44,  3.54it/s] 10%|█         | 42/410 [00:12<01:44,  3.53it/s] 10%|█         | 43/410 [00:12<01:43,  3.55it/s] 11%|█         | 44/410 [00:12<01:42,  3.56it/s] 11%|█         | 45/410 [00:12<01:42,  3.56it/s] 11%|█         | 46/410 [00:13<01:42,  3.56it/s] 11%|█▏        | 47/410 [00:13<01:41,  3.57it/s] 12%|█▏        | 48/410 [00:13<01:41,  3.57it/s] 12%|█▏        | 49/410 [00:14<01:41,  3.57it/s] 12%|█▏        | 50/410 [00:14<01:40,  3.57it/s] 12%|█▏        | 51/410 [00:14<01:40,  3.57it/s] 13%|█▎        | 52/410 [00:14<01:40,  3.57it/s] 13%|█▎        | 53/410 [00:15<01:40,  3.56it/s] 13%|█▎        | 54/410 [00:15<01:39,  3.56it/s] 13%|█▎        | 55/410 [00:15<01:39,  3.57it/s] 14%|█▎        | 56/410 [00:15<01:39,  3.56it/s] 14%|█▍        | 57/410 [00:16<01:38,  3.57it/s] 14%|█▍        | 58/410 [00:16<01:38,  3.57it/s] 14%|█▍        | 59/410 [00:16<01:38,  3.57it/s] 15%|█▍        | 60/410 [00:17<01:38,  3.56it/s] 15%|█▍        | 61/410 [00:17<01:38,  3.56it/s] 15%|█▌        | 62/410 [00:17<01:37,  3.56it/s] 15%|█▌        | 63/410 [00:17<01:37,  3.56it/s] 16%|█▌        | 64/410 [00:18<01:37,  3.55it/s] 16%|█▌        | 65/410 [00:18<01:37,  3.55it/s] 16%|█▌        | 66/410 [00:18<01:36,  3.55it/s] 16%|█▋        | 67/410 [00:19<01:36,  3.56it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.56it/s] 17%|█▋        | 69/410 [00:19<01:35,  3.56it/s] 17%|█▋        | 70/410 [00:19<01:35,  3.57it/s] 17%|█▋        | 71/410 [00:20<01:35,  3.57it/s] 18%|█▊        | 72/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 74/410 [00:21<01:34,  3.57it/s] 18%|█▊        | 75/410 [00:21<01:34,  3.56it/s] 19%|█▊        | 76/410 [00:21<01:33,  3.56it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.56it/s] 19%|█▉        | 78/410 [00:22<01:33,  3.57it/s] 19%|█▉        | 79/410 [00:22<01:32,  3.57it/s] 20%|█▉        | 80/410 [00:22<01:32,  3.57it/s] 20%|█▉        | 81/410 [00:23<01:32,  3.57it/s] 20%|██        | 82/410 [00:23<01:28,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 03:03:58,048 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:03:58,048 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:03:58,048 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.76it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.35it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.52it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.40it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.97it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.62it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.40it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.36it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.51it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.64it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.61it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.37it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.29it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.23it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.14it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.07it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.15it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.35it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.41it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.42it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.33it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.18it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.19it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.10it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.09it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.21it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.32it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.44it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.27it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.22it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.22it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.22it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.11it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.11it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.10it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.29it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.46it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.40it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.30it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.35it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.26it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.12it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.12it/s][A
 38%|███▊      | 222/577 [00:04<00:08, 44.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.26it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.42it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.36it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.28it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.24it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.22it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.20it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.14it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.13it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.32it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.35it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.28it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.34it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.17it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.24it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.22it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.15it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.22it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.30it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.29it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.28it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.20it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.14it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.11it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.20it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.32it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.30it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.20it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.18it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.23it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.07it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.16it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.20it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.19it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.24it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.36it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.38it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.30it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.27it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.17it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.20it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.19it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.24it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.23it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.32it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.26it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.29it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.16it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.18it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.23it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.21it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.06it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.21it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.37it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.35it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.33it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.17it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.17it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.22it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.16it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.12it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.29it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.38it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.31it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.21it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.24it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.23it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:13<00:00, 44.23it/s][A 20%|██        | 82/410 [00:36<01:28,  3.71it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:04:11,095 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 03:04:11,113 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:04:13,235 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:04:13,256 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:04:13,271 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:39<26:56,  4.94s/it] 20%|██        | 84/410 [00:39<19:16,  3.55s/it] 21%|██        | 85/410 [00:39<13:54,  2.57s/it] 21%|██        | 86/410 [00:39<10:10,  1.88s/it] 21%|██        | 87/410 [00:40<07:33,  1.40s/it] 21%|██▏       | 88/410 [00:40<05:44,  1.07s/it] 22%|██▏       | 89/410 [00:40<04:27,  1.20it/s] 22%|██▏       | 90/410 [00:41<03:34,  1.49it/s] 22%|██▏       | 91/410 [00:41<02:57,  1.80it/s] 22%|██▏       | 92/410 [00:41<02:31,  2.10it/s] 23%|██▎       | 93/410 [00:41<02:13,  2.37it/s] 23%|██▎       | 94/410 [00:42<02:00,  2.62it/s] 23%|██▎       | 95/410 [00:42<01:51,  2.83it/s] 23%|██▎       | 96/410 [00:42<01:44,  3.01it/s] 24%|██▎       | 97/410 [00:43<01:39,  3.14it/s] 24%|██▍       | 98/410 [00:43<01:36,  3.23it/s] 24%|██▍       | 99/410 [00:43<01:34,  3.31it/s] 24%|██▍       | 100/410 [00:43<01:32,  3.36it/s] 25%|██▍       | 101/410 [00:44<01:30,  3.40it/s] 25%|██▍       | 102/410 [00:44<01:29,  3.43it/s] 25%|██▌       | 103/410 [00:44<01:29,  3.44it/s] 25%|██▌       | 104/410 [00:45<01:28,  3.45it/s] 26%|██▌       | 105/410 [00:45<01:28,  3.46it/s] 26%|██▌       | 106/410 [00:45<01:27,  3.47it/s] 26%|██▌       | 107/410 [00:45<01:27,  3.48it/s] 26%|██▋       | 108/410 [00:46<01:26,  3.48it/s] 27%|██▋       | 109/410 [00:46<01:26,  3.48it/s] 27%|██▋       | 110/410 [00:46<01:26,  3.48it/s] 27%|██▋       | 111/410 [00:47<01:25,  3.49it/s] 27%|██▋       | 112/410 [00:47<01:25,  3.48it/s] 28%|██▊       | 113/410 [00:47<01:25,  3.49it/s] 28%|██▊       | 114/410 [00:47<01:24,  3.49it/s] 28%|██▊       | 115/410 [00:48<01:24,  3.48it/s] 28%|██▊       | 116/410 [00:48<01:24,  3.48it/s] 29%|██▊       | 117/410 [00:48<01:24,  3.48it/s] 29%|██▉       | 118/410 [00:49<01:23,  3.48it/s] 29%|██▉       | 119/410 [00:49<01:23,  3.49it/s] 29%|██▉       | 120/410 [00:49<01:23,  3.49it/s] 30%|██▉       | 121/410 [00:50<01:22,  3.49it/s] 30%|██▉       | 122/410 [00:50<01:22,  3.49it/s] 30%|███       | 123/410 [00:50<01:22,  3.48it/s] 30%|███       | 124/410 [00:50<01:22,  3.48it/s] 30%|███       | 125/410 [00:51<01:21,  3.49it/s] 31%|███       | 126/410 [00:51<01:22,  3.46it/s] 31%|███       | 127/410 [00:51<01:23,  3.39it/s] 31%|███       | 128/410 [00:52<01:22,  3.41it/s] 31%|███▏      | 129/410 [00:52<01:21,  3.44it/s] 32%|███▏      | 130/410 [00:52<01:21,  3.45it/s] 32%|███▏      | 131/410 [00:52<01:20,  3.46it/s] 32%|███▏      | 132/410 [00:53<01:20,  3.47it/s] 32%|███▏      | 133/410 [00:53<01:19,  3.48it/s] 33%|███▎      | 134/410 [00:53<01:19,  3.48it/s] 33%|███▎      | 135/410 [00:54<01:18,  3.48it/s] 33%|███▎      | 136/410 [00:54<01:18,  3.49it/s] 33%|███▎      | 137/410 [00:54<01:18,  3.48it/s] 34%|███▎      | 138/410 [00:54<01:17,  3.49it/s] 34%|███▍      | 139/410 [00:55<01:17,  3.49it/s] 34%|███▍      | 140/410 [00:55<01:17,  3.49it/s] 34%|███▍      | 141/410 [00:55<01:17,  3.49it/s] 35%|███▍      | 142/410 [00:56<01:16,  3.49it/s] 35%|███▍      | 143/410 [00:56<01:16,  3.48it/s] 35%|███▌      | 144/410 [00:56<01:16,  3.48it/s] 35%|███▌      | 145/410 [00:56<01:16,  3.48it/s] 36%|███▌      | 146/410 [00:57<01:15,  3.49it/s] 36%|███▌      | 147/410 [00:57<01:15,  3.49it/s] 36%|███▌      | 148/410 [00:57<01:15,  3.49it/s] 36%|███▋      | 149/410 [00:58<01:14,  3.49it/s] 37%|███▋      | 150/410 [00:58<01:14,  3.49it/s] 37%|███▋      | 151/410 [00:58<01:14,  3.49it/s] 37%|███▋      | 152/410 [00:58<01:13,  3.49it/s] 37%|███▋      | 153/410 [00:59<01:13,  3.49it/s] 38%|███▊      | 154/410 [00:59<01:13,  3.48it/s] 38%|███▊      | 155/410 [00:59<01:13,  3.48it/s] 38%|███▊      | 156/410 [01:00<01:12,  3.49it/s] 38%|███▊      | 157/410 [01:00<01:12,  3.48it/s] 39%|███▊      | 158/410 [01:00<01:12,  3.49it/s] 39%|███▉      | 159/410 [01:00<01:12,  3.49it/s] 39%|███▉      | 160/410 [01:01<01:11,  3.49it/s] 39%|███▉      | 161/410 [01:01<01:11,  3.49it/s] 40%|███▉      | 162/410 [01:01<01:11,  3.49it/s] 40%|███▉      | 163/410 [01:02<01:10,  3.48it/s] 40%|████      | 164/410 [01:02<01:07,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 03:04:37,141 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:04:37,141 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:04:37,141 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0335, 'eval_samples_per_second': 353.628, 'eval_steps_per_second': 44.271, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.54it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.55it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.86it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.67it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.83it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.52it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.09it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.14it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.34it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.35it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.55it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.42it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.19it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.05it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.00it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.97it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.08it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.15it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.29it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.34it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.14it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.05it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.01it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.98it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 44.00it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.10it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.27it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.38it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.26it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.13it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.01it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.03it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.00it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.10it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.17it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.33it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.33it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.24it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.99it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.00it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.99it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.19it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.20it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.34it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.31it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.24it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.03it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.04it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.98it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.06it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.32it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.35it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.22it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.13it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.01it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.06it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.02it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.16it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.22it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.34it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.22it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.17it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.11it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.07it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.07it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.11it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.16it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.21it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.21it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.12it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.07it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.02it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.10it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.13it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.16it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.21it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.27it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.13it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.06it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.07it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.10it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.04it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.09it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.19it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.26it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.18it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.07it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.08it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.12it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.01it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.21it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.34it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.25it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.13it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.16it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.05it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.99it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.16it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.22it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.26it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.05it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.13it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.03it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.21it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:15<01:07,  3.63it/s]
100%|██████████| 577/577 [00:13<00:00, 44.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:04:50,229 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 03:04:50,248 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:04:52,177 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:04:52,193 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:04:52,203 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:17<19:57,  4.89s/it] 40%|████      | 166/410 [01:18<14:15,  3.51s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:18<10:17,  2.54s/it] 41%|████      | 168/410 [01:18<07:31,  1.87s/it] 41%|████      | 169/410 [01:19<05:35,  1.39s/it] 41%|████▏     | 170/410 [01:19<04:14,  1.06s/it] 42%|████▏     | 171/410 [01:19<03:17,  1.21it/s] 42%|████▏     | 172/410 [01:19<02:38,  1.50it/s] 42%|████▏     | 173/410 [01:20<02:10,  1.81it/s] 42%|████▏     | 174/410 [01:20<01:51,  2.12it/s] 43%|████▎     | 175/410 [01:20<01:37,  2.40it/s] 43%|████▎     | 176/410 [01:21<01:28,  2.65it/s] 43%|████▎     | 177/410 [01:21<01:21,  2.84it/s] 43%|████▎     | 178/410 [01:21<01:17,  3.01it/s] 44%|████▎     | 179/410 [01:22<01:13,  3.14it/s] 44%|████▍     | 180/410 [01:22<01:11,  3.24it/s] 44%|████▍     | 181/410 [01:22<01:09,  3.31it/s] 44%|████▍     | 182/410 [01:22<01:07,  3.36it/s] 45%|████▍     | 183/410 [01:23<01:06,  3.40it/s] 45%|████▍     | 184/410 [01:23<01:06,  3.42it/s] 45%|████▌     | 185/410 [01:23<01:05,  3.44it/s] 45%|████▌     | 186/410 [01:24<01:04,  3.46it/s] 46%|████▌     | 187/410 [01:24<01:04,  3.47it/s] 46%|████▌     | 188/410 [01:24<01:03,  3.47it/s] 46%|████▌     | 189/410 [01:24<01:03,  3.48it/s] 46%|████▋     | 190/410 [01:25<01:03,  3.48it/s] 47%|████▋     | 191/410 [01:25<01:02,  3.48it/s] 47%|████▋     | 192/410 [01:25<01:02,  3.48it/s] 47%|████▋     | 193/410 [01:26<01:02,  3.48it/s] 47%|████▋     | 194/410 [01:26<01:01,  3.49it/s] 48%|████▊     | 195/410 [01:26<01:01,  3.49it/s] 48%|████▊     | 196/410 [01:26<01:01,  3.48it/s] 48%|████▊     | 197/410 [01:27<01:01,  3.47it/s] 48%|████▊     | 198/410 [01:27<01:01,  3.47it/s] 49%|████▊     | 199/410 [01:27<01:00,  3.48it/s] 49%|████▉     | 200/410 [01:28<01:00,  3.48it/s] 49%|████▉     | 201/410 [01:28<00:59,  3.48it/s] 49%|████▉     | 202/410 [01:28<00:59,  3.49it/s] 50%|████▉     | 203/410 [01:28<00:59,  3.49it/s] 50%|████▉     | 204/410 [01:29<00:59,  3.48it/s] 50%|█████     | 205/410 [01:29<00:58,  3.49it/s] 50%|█████     | 206/410 [01:29<00:58,  3.49it/s] 50%|█████     | 207/410 [01:30<00:58,  3.49it/s] 51%|█████     | 208/410 [01:30<00:58,  3.47it/s] 51%|█████     | 209/410 [01:30<00:57,  3.48it/s] 51%|█████     | 210/410 [01:30<00:57,  3.48it/s] 51%|█████▏    | 211/410 [01:31<00:57,  3.49it/s] 52%|█████▏    | 212/410 [01:31<00:56,  3.48it/s] 52%|█████▏    | 213/410 [01:31<00:56,  3.48it/s] 52%|█████▏    | 214/410 [01:32<00:56,  3.48it/s] 52%|█████▏    | 215/410 [01:32<00:56,  3.48it/s] 53%|█████▎    | 216/410 [01:32<00:55,  3.48it/s] 53%|█████▎    | 217/410 [01:32<00:55,  3.49it/s] 53%|█████▎    | 218/410 [01:33<00:55,  3.48it/s] 53%|█████▎    | 219/410 [01:33<00:55,  3.45it/s] 54%|█████▎    | 220/410 [01:33<00:54,  3.46it/s] 54%|█████▍    | 221/410 [01:34<00:54,  3.47it/s] 54%|█████▍    | 222/410 [01:34<00:54,  3.47it/s] 54%|█████▍    | 223/410 [01:34<00:53,  3.48it/s] 55%|█████▍    | 224/410 [01:34<00:53,  3.48it/s] 55%|█████▍    | 225/410 [01:35<00:53,  3.48it/s] 55%|█████▌    | 226/410 [01:35<00:52,  3.48it/s] 55%|█████▌    | 227/410 [01:35<00:52,  3.49it/s] 56%|█████▌    | 228/410 [01:36<00:52,  3.48it/s] 56%|█████▌    | 229/410 [01:36<00:51,  3.48it/s] 56%|█████▌    | 230/410 [01:36<00:51,  3.48it/s] 56%|█████▋    | 231/410 [01:36<00:51,  3.48it/s] 57%|█████▋    | 232/410 [01:37<00:51,  3.48it/s] 57%|█████▋    | 233/410 [01:37<00:51,  3.47it/s] 57%|█████▋    | 234/410 [01:37<00:50,  3.47it/s] 57%|█████▋    | 235/410 [01:38<00:50,  3.47it/s] 58%|█████▊    | 236/410 [01:38<00:50,  3.47it/s] 58%|█████▊    | 237/410 [01:38<00:49,  3.48it/s] 58%|█████▊    | 238/410 [01:38<00:49,  3.48it/s] 58%|█████▊    | 239/410 [01:39<00:49,  3.48it/s] 59%|█████▊    | 240/410 [01:39<00:48,  3.48it/s] 59%|█████▉    | 241/410 [01:39<00:48,  3.47it/s] 59%|█████▉    | 242/410 [01:40<00:48,  3.48it/s] 59%|█████▉    | 243/410 [01:40<00:47,  3.48it/s] 60%|█████▉    | 244/410 [01:40<00:47,  3.48it/s] 60%|█████▉    | 245/410 [01:40<00:47,  3.48it/s] 60%|██████    | 246/410 [01:41<00:45,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 03:05:16,013 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:05:16,014 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:05:16,014 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0606, 'eval_samples_per_second': 352.893, 'eval_steps_per_second': 44.179, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.57it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.40it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.81it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.83it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.10it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.63it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.14it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.98it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.03it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.27it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.31it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.45it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.30it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.18it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.97it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.89it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.89it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.00it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.21it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.31it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.41it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.40it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.17it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.88it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.96it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.03it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.14it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.23it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.33it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.30it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.17it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.99it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.90it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.97it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.01it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.15it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.21it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.27it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.33it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.21it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.10it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.00it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.02it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.03it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.13it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.28it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.17it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.14it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.91it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.00it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.06it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.01it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.22it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.28it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.28it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.20it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.18it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.10it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.08it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.96it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.00it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.17it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.36it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.30it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.84it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.92it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.01it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.02it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.05it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.17it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.36it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.36it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.34it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.20it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.11it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.01it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.02it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.02it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.19it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.30it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.31it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.21it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.11it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.04it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.03it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.92it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.11it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.22it/s][A
 81%|████████  | 467/577 [00:10<00:02, 41.73it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 42.81it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.32it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.46it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.58it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.59it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.66it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.99it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.96it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.99it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.14it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.14it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.17it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.05it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.08it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.17it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.11it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.24it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.14it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.08it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [01:54<00:45,  3.62it/s]
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:05:29,116 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 03:05:29,139 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:05:30,638 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:05:30,654 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:05:30,663 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:56<12:54,  4.75s/it] 60%|██████    | 248/410 [01:56<09:12,  3.41s/it] 61%|██████    | 249/410 [01:56<06:38,  2.47s/it] 61%|██████    | 250/410 [01:57<04:50,  1.82s/it] 61%|██████    | 251/410 [01:57<03:36,  1.36s/it] 61%|██████▏   | 252/410 [01:57<02:44,  1.04s/it] 62%|██████▏   | 253/410 [01:58<02:07,  1.23it/s] 62%|██████▏   | 254/410 [01:58<01:42,  1.53it/s] 62%|██████▏   | 255/410 [01:58<01:24,  1.83it/s] 62%|██████▏   | 256/410 [01:58<01:11,  2.14it/s] 63%|██████▎   | 257/410 [01:59<01:03,  2.42it/s] 63%|██████▎   | 258/410 [01:59<00:57,  2.66it/s] 63%|██████▎   | 259/410 [01:59<00:52,  2.87it/s] 63%|██████▎   | 260/410 [02:00<00:49,  3.03it/s] 64%|██████▎   | 261/410 [02:00<00:47,  3.15it/s] 64%|██████▍   | 262/410 [02:00<00:45,  3.25it/s] 64%|██████▍   | 263/410 [02:01<00:44,  3.30it/s] 64%|██████▍   | 264/410 [02:01<00:43,  3.36it/s] 65%|██████▍   | 265/410 [02:01<00:42,  3.39it/s] 65%|██████▍   | 266/410 [02:01<00:42,  3.42it/s] 65%|██████▌   | 267/410 [02:02<00:41,  3.44it/s] 65%|██████▌   | 268/410 [02:02<00:41,  3.46it/s] 66%|██████▌   | 269/410 [02:02<00:40,  3.47it/s] 66%|██████▌   | 270/410 [02:03<00:40,  3.47it/s] 66%|██████▌   | 271/410 [02:03<00:39,  3.48it/s] 66%|██████▋   | 272/410 [02:03<00:39,  3.48it/s] 67%|██████▋   | 273/410 [02:03<00:39,  3.49it/s] 67%|██████▋   | 274/410 [02:04<00:39,  3.47it/s] 67%|██████▋   | 275/410 [02:04<00:38,  3.48it/s] 67%|██████▋   | 276/410 [02:04<00:38,  3.48it/s] 68%|██████▊   | 277/410 [02:05<00:38,  3.48it/s] 68%|██████▊   | 278/410 [02:05<00:37,  3.49it/s] 68%|██████▊   | 279/410 [02:05<00:37,  3.49it/s] 68%|██████▊   | 280/410 [02:05<00:37,  3.49it/s] 69%|██████▊   | 281/410 [02:06<00:37,  3.49it/s] 69%|██████▉   | 282/410 [02:06<00:36,  3.49it/s] 69%|██████▉   | 283/410 [02:06<00:36,  3.48it/s] 69%|██████▉   | 284/410 [02:07<00:36,  3.49it/s] 70%|██████▉   | 285/410 [02:07<00:36,  3.46it/s] 70%|██████▉   | 286/410 [02:07<00:35,  3.47it/s] 70%|███████   | 287/410 [02:07<00:35,  3.47it/s] 70%|███████   | 288/410 [02:08<00:35,  3.48it/s] 70%|███████   | 289/410 [02:08<00:34,  3.48it/s] 71%|███████   | 290/410 [02:08<00:34,  3.48it/s] 71%|███████   | 291/410 [02:09<00:34,  3.48it/s] 71%|███████   | 292/410 [02:09<00:33,  3.49it/s] 71%|███████▏  | 293/410 [02:09<00:33,  3.49it/s] 72%|███████▏  | 294/410 [02:09<00:33,  3.49it/s] 72%|███████▏  | 295/410 [02:10<00:32,  3.49it/s] 72%|███████▏  | 296/410 [02:10<00:32,  3.47it/s] 72%|███████▏  | 297/410 [02:10<00:32,  3.48it/s] 73%|███████▎  | 298/410 [02:11<00:32,  3.48it/s] 73%|███████▎  | 299/410 [02:11<00:31,  3.48it/s] 73%|███████▎  | 300/410 [02:11<00:31,  3.49it/s] 73%|███████▎  | 301/410 [02:11<00:31,  3.49it/s] 74%|███████▎  | 302/410 [02:12<00:30,  3.49it/s] 74%|███████▍  | 303/410 [02:12<00:30,  3.49it/s] 74%|███████▍  | 304/410 [02:12<00:30,  3.49it/s] 74%|███████▍  | 305/410 [02:13<00:30,  3.49it/s] 75%|███████▍  | 306/410 [02:13<00:29,  3.49it/s] 75%|███████▍  | 307/410 [02:13<00:29,  3.48it/s] 75%|███████▌  | 308/410 [02:13<00:29,  3.48it/s] 75%|███████▌  | 309/410 [02:14<00:28,  3.49it/s] 76%|███████▌  | 310/410 [02:14<00:28,  3.49it/s] 76%|███████▌  | 311/410 [02:14<00:28,  3.49it/s] 76%|███████▌  | 312/410 [02:15<00:28,  3.49it/s] 76%|███████▋  | 313/410 [02:15<00:27,  3.49it/s] 77%|███████▋  | 314/410 [02:15<00:27,  3.49it/s] 77%|███████▋  | 315/410 [02:15<00:27,  3.49it/s] 77%|███████▋  | 316/410 [02:16<00:26,  3.49it/s] 77%|███████▋  | 317/410 [02:16<00:26,  3.49it/s] 78%|███████▊  | 318/410 [02:16<00:26,  3.47it/s] 78%|███████▊  | 319/410 [02:17<00:26,  3.47it/s] 78%|███████▊  | 320/410 [02:17<00:25,  3.48it/s] 78%|███████▊  | 321/410 [02:17<00:25,  3.48it/s] 79%|███████▊  | 322/410 [02:17<00:25,  3.48it/s] 79%|███████▉  | 323/410 [02:18<00:24,  3.48it/s] 79%|███████▉  | 324/410 [02:18<00:24,  3.48it/s] 79%|███████▉  | 325/410 [02:18<00:24,  3.48it/s] 80%|███████▉  | 326/410 [02:19<00:24,  3.49it/s] 80%|███████▉  | 327/410 [02:19<00:23,  3.48it/s] 80%|████████  | 328/410 [02:19<00:22,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 03:05:54,429 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:05:54,430 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:05:54,430 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0839, 'eval_samples_per_second': 352.266, 'eval_steps_per_second': 44.1, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.70it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.33it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.89it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.96it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.20it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.52it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.09it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.07it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.13it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.27it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.44it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.52it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.55it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.38it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.14it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.02it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.99it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.09it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.17it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.33it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.40it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.39it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.21it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.94it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.91it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.95it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.18it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.35it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.41it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.38it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.18it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.06it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.97it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.85it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.97it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.38it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.34it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.31it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.19it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.02it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.96it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.82it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.95it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.22it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.35it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.40it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.30it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.33it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.14it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.01it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.89it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.01it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.21it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.34it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.31it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.27it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.23it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.11it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.99it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.96it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.03it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.22it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.33it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.30it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.21it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.01it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.11it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.03it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.05it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.23it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.27it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.22it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 39.45it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 40.95it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 42.02it/s][A
 70%|██████▉   | 402/577 [00:09<00:04, 42.64it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.26it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.61it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.91it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.94it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.67it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.63it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.82it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.09it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.17it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.29it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.38it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.34it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.83it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.84it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.95it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.22it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.30it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.37it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.42it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.32it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.11it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.89it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.87it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.97it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.21it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.29it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.37it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.30it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.11it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.89it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.94it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.97it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A 80%|████████  | 328/410 [02:32<00:22,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:06:07,549 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 03:06:07,580 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:06:09,318 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:06:09,330 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:06:09,343 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:35<06:32,  4.84s/it] 80%|████████  | 330/410 [02:35<04:38,  3.48s/it] 81%|████████  | 331/410 [02:35<03:19,  2.52s/it] 81%|████████  | 332/410 [02:35<02:24,  1.85s/it] 81%|████████  | 333/410 [02:36<01:46,  1.38s/it] 81%|████████▏ | 334/410 [02:36<01:20,  1.05s/it] 82%|████████▏ | 335/410 [02:36<01:01,  1.22it/s] 82%|████████▏ | 336/410 [02:37<00:49,  1.51it/s] 82%|████████▏ | 337/410 [02:37<00:40,  1.82it/s] 82%|████████▏ | 338/410 [02:37<00:33,  2.12it/s] 83%|████████▎ | 339/410 [02:37<00:29,  2.40it/s] 83%|████████▎ | 340/410 [02:38<00:26,  2.65it/s] 83%|████████▎ | 341/410 [02:38<00:24,  2.85it/s] 83%|████████▎ | 342/410 [02:38<00:22,  3.02it/s] 84%|████████▎ | 343/410 [02:39<00:21,  3.15it/s] 84%|████████▍ | 344/410 [02:39<00:20,  3.24it/s] 84%|████████▍ | 345/410 [02:39<00:19,  3.31it/s] 84%|████████▍ | 346/410 [02:40<00:19,  3.36it/s] 85%|████████▍ | 347/410 [02:40<00:18,  3.40it/s] 85%|████████▍ | 348/410 [02:40<00:18,  3.43it/s] 85%|████████▌ | 349/410 [02:40<00:17,  3.44it/s] 85%|████████▌ | 350/410 [02:41<00:17,  3.45it/s] 86%|████████▌ | 351/410 [02:41<00:17,  3.46it/s] 86%|████████▌ | 352/410 [02:41<00:16,  3.47it/s] 86%|████████▌ | 353/410 [02:42<00:16,  3.47it/s] 86%|████████▋ | 354/410 [02:42<00:16,  3.48it/s] 87%|████████▋ | 355/410 [02:42<00:15,  3.48it/s] 87%|████████▋ | 356/410 [02:42<00:15,  3.48it/s] 87%|████████▋ | 357/410 [02:43<00:15,  3.49it/s] 87%|████████▋ | 358/410 [02:43<00:14,  3.48it/s] 88%|████████▊ | 359/410 [02:43<00:14,  3.49it/s] 88%|████████▊ | 360/410 [02:44<00:14,  3.49it/s] 88%|████████▊ | 361/410 [02:44<00:14,  3.47it/s] 88%|████████▊ | 362/410 [02:44<00:13,  3.48it/s] 89%|████████▊ | 363/410 [02:44<00:13,  3.48it/s] 89%|████████▉ | 364/410 [02:45<00:13,  3.48it/s] 89%|████████▉ | 365/410 [02:45<00:12,  3.48it/s] 89%|████████▉ | 366/410 [02:45<00:12,  3.48it/s] 90%|████████▉ | 367/410 [02:46<00:12,  3.48it/s] 90%|████████▉ | 368/410 [02:46<00:12,  3.49it/s] 90%|█████████ | 369/410 [02:46<00:11,  3.49it/s] 90%|█████████ | 370/410 [02:46<00:11,  3.48it/s] 90%|█████████ | 371/410 [02:47<00:11,  3.49it/s] 91%|█████████ | 372/410 [02:47<00:10,  3.48it/s] 91%|█████████ | 373/410 [02:47<00:10,  3.48it/s] 91%|█████████ | 374/410 [02:48<00:10,  3.48it/s] 91%|█████████▏| 375/410 [02:48<00:10,  3.48it/s] 92%|█████████▏| 376/410 [02:48<00:09,  3.49it/s] 92%|█████████▏| 377/410 [02:48<00:09,  3.49it/s] 92%|█████████▏| 378/410 [02:49<00:09,  3.48it/s] 92%|█████████▏| 379/410 [02:49<00:08,  3.49it/s] 93%|█████████▎| 380/410 [02:49<00:08,  3.49it/s] 93%|█████████▎| 381/410 [02:50<00:08,  3.49it/s] 93%|█████████▎| 382/410 [02:50<00:08,  3.49it/s] 93%|█████████▎| 383/410 [02:50<00:09,  2.86it/s] 94%|█████████▎| 384/410 [02:51<00:08,  3.02it/s] 94%|█████████▍| 385/410 [02:51<00:07,  3.14it/s] 94%|█████████▍| 386/410 [02:51<00:07,  3.24it/s] 94%|█████████▍| 387/410 [02:52<00:07,  3.23it/s] 95%|█████████▍| 388/410 [02:52<00:06,  3.30it/s] 95%|█████████▍| 389/410 [02:52<00:06,  3.35it/s] 95%|█████████▌| 390/410 [02:52<00:05,  3.39it/s] 95%|█████████▌| 391/410 [02:53<00:05,  3.42it/s] 96%|█████████▌| 392/410 [02:53<00:05,  3.44it/s] 96%|█████████▌| 393/410 [02:53<00:04,  3.44it/s] 96%|█████████▌| 394/410 [02:54<00:04,  3.46it/s] 96%|█████████▋| 395/410 [02:54<00:04,  3.47it/s] 97%|█████████▋| 396/410 [02:54<00:04,  3.47it/s] 97%|█████████▋| 397/410 [02:54<00:03,  3.48it/s] 97%|█████████▋| 398/410 [02:55<00:03,  3.48it/s] 97%|█████████▋| 399/410 [02:55<00:03,  3.48it/s] 98%|█████████▊| 400/410 [02:55<00:02,  3.48it/s] 98%|█████████▊| 401/410 [02:56<00:02,  3.48it/s] 98%|█████████▊| 402/410 [02:56<00:02,  3.49it/s] 98%|█████████▊| 403/410 [02:56<00:02,  3.48it/s] 99%|█████████▊| 404/410 [02:56<00:01,  3.48it/s] 99%|█████████▉| 405/410 [02:57<00:01,  3.49it/s] 99%|█████████▉| 406/410 [02:57<00:01,  3.49it/s] 99%|█████████▉| 407/410 [02:57<00:00,  3.48it/s]100%|█████████▉| 408/410 [02:58<00:00,  3.49it/s]100%|█████████▉| 409/410 [02:58<00:00,  3.49it/s]100%|██████████| 410/410 [02:58<00:00,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 03:06:33,375 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:06:33,375 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:06:33,375 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0971, 'eval_samples_per_second': 351.909, 'eval_steps_per_second': 44.055, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.65it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.41it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.82it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.90it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.30it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.66it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.28it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.02it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.10it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.49it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.55it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.47it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.38it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.09it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.95it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.82it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.97it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.09it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.27it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.46it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.45it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.33it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.09it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.87it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.87it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.00it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.14it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.37it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.36it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.43it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.16it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.94it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.88it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.91it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.93it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.25it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.32it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.41it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.35it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.21it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.08it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.91it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.85it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.99it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.27it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.42it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.37it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.27it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.18it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.00it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.92it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.95it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.01it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.24it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.34it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.38it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.32it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.22it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.00it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.80it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.18it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.28it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.38it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.34it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.22it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.18it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.10it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.10it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.10it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.08it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.19it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.28it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.37it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.27it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.10it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.07it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.02it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.12it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.06it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.19it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.23it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.34it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.25it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.09it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 43.96it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.02it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.07it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.22it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.23it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.28it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.27it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.21it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.20it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.07it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.13it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.21it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.26it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.19it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.18it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.15it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.17it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.10it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.12it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.21it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.26it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.25it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.26it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.19it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.15it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.17it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A100%|██████████| 410/410 [03:11<00:00,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:06:46,443 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 03:06:46,462 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:06:48,436 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:06:48,452 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:06:48,465 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:06:48,792 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:06:48,792 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82 (score: 1.12112557888031).
                                                 100%|██████████| 410/410 [03:15<00:00,  3.62it/s]100%|██████████| 410/410 [03:15<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 03:06:50,789 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 03:06:50,804 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:06:52,679 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:06:52,692 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:06:52,705 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:06:52,900 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   train_runtime            = 0:03:15.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   train_samples_per_second =    133.701
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:06:52,900 >>   train_steps_per_second   =      2.092
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0546, 'eval_samples_per_second': 353.054, 'eval_steps_per_second': 44.199, 'epoch': 5.0}
{'train_runtime': 195.9598, 'train_samples_per_second': 133.701, 'train_steps_per_second': 2.092, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:06:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:06:52,941 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:06:52,941 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 03:06:52,941 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.24it/s]  2%|▏         | 12/577 [00:00<00:11, 48.33it/s]  3%|▎         | 17/577 [00:00<00:11, 46.86it/s]  4%|▍         | 22/577 [00:00<00:12, 46.20it/s]  5%|▍         | 27/577 [00:00<00:12, 45.75it/s]  6%|▌         | 32/577 [00:00<00:11, 45.49it/s]  6%|▋         | 37/577 [00:00<00:11, 45.37it/s]  7%|▋         | 42/577 [00:00<00:11, 44.90it/s]  8%|▊         | 47/577 [00:01<00:11, 44.22it/s]  9%|▉         | 52/577 [00:01<00:11, 44.00it/s] 10%|▉         | 57/577 [00:01<00:11, 44.03it/s] 11%|█         | 62/577 [00:01<00:11, 44.20it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.31it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.56it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.66it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.74it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.48it/s] 16%|█▌        | 92/577 [00:02<00:10, 44.14it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.96it/s] 18%|█▊        | 102/577 [00:02<00:10, 44.07it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.22it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.34it/s] 20%|██        | 117/577 [00:02<00:10, 44.53it/s] 21%|██        | 122/577 [00:02<00:10, 44.64it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.68it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.31it/s] 24%|██▎       | 137/577 [00:03<00:09, 44.09it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.99it/s] 25%|██▌       | 147/577 [00:03<00:09, 44.04it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.16it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.35it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.45it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.62it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.47it/s] 31%|███       | 177/577 [00:03<00:09, 44.26it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.07it/s] 32%|███▏      | 187/577 [00:04<00:08, 44.16it/s] 33%|███▎      | 192/577 [00:04<00:08, 44.08it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.29it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.39it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.46it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.57it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.40it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.22it/s] 39%|███▉      | 227/577 [00:05<00:07, 44.14it/s] 40%|████      | 232/577 [00:05<00:07, 44.03it/s] 41%|████      | 237/577 [00:05<00:07, 44.07it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.23it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.38it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.57it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.56it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.29it/s] 46%|████▋     | 267/577 [00:05<00:07, 44.22it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.18it/s] 48%|████▊     | 277/577 [00:06<00:06, 44.11it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.07it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.18it/s] 51%|█████     | 292/577 [00:06<00:06, 44.39it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.51it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.46it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.29it/s] 54%|█████▍    | 312/577 [00:07<00:06, 44.16it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.10it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.04it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.14it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.21it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.37it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.40it/s] 60%|██████    | 347/577 [00:07<00:05, 44.44it/s] 61%|██████    | 352/577 [00:07<00:05, 44.40it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.24it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.09it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.16it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.23it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.20it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.35it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.43it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.42it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.30it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.23it/s] 71%|███████   | 407/577 [00:09<00:03, 44.15it/s] 71%|███████▏  | 412/577 [00:09<00:03, 44.21it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.14it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.09it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.31it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.39it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.44it/s] 77%|███████▋  | 442/577 [00:09<00:03, 44.35it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.18it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.25it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.26it/s] 80%|████████  | 462/577 [00:10<00:02, 44.19it/s] 81%|████████  | 467/577 [00:10<00:02, 44.12it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.22it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.37it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.42it/s] 84%|████████▍ | 487/577 [00:10<00:02, 44.33it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.19it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.27it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.21it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.22it/s] 89%|████████▊ | 512/577 [00:11<00:01, 43.95it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.29it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.37it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.45it/s] 92%|█████████▏| 532/577 [00:11<00:01, 44.40it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.32it/s] 94%|█████████▍| 542/577 [00:12<00:00, 44.40it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.35it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.32it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.16it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.21it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.33it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.37it/s]100%|██████████| 577/577 [00:12<00:00, 44.36it/s]100%|██████████| 577/577 [00:12<00:00, 44.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:07:05,958 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   eval_loss               =     1.1211
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   eval_runtime            = 0:00:13.01
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   eval_samples_per_second =    354.091
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   eval_steps_per_second   =     44.329
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:07:05,958 >>   perplexity              =     3.0683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:12,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:12,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:12,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:12,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:12,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:07:13,323 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:07:13,324 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:07:13,875 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:07:14,905 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:07:14,906 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:18,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:18,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:18,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:18,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:18,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:07:18,699 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:07:18,700 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:07:19,078 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:07:19,252 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:07:19,252 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/checkpoint-82
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.43it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.35it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.42it/s]Extractor Predicting: 14it [00:10,  1.42it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:12,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.44it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.33it/s]Extractor Predicting: 23it [00:16,  1.33it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.43it/s]Extractor Predicting: 28it [00:20,  1.41it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:21,  1.45it/s]Extractor Predicting: 31it [00:22,  1.42it/s]Extractor Predicting: 32it [00:22,  1.43it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:24,  1.38it/s]Extractor Predicting: 35it [00:24,  1.41it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:27,  1.40it/s]Extractor Predicting: 39it [00:27,  1.38it/s]Extractor Predicting: 40it [00:28,  1.39it/s]Extractor Predicting: 41it [00:29,  1.35it/s]Extractor Predicting: 42it [00:30,  1.37it/s]Extractor Predicting: 43it [00:30,  1.36it/s]Extractor Predicting: 44it [00:31,  1.34it/s]Extractor Predicting: 45it [00:32,  1.34it/s]Extractor Predicting: 46it [00:33,  1.33it/s]Extractor Predicting: 47it [00:33,  1.32it/s]Extractor Predicting: 48it [00:34,  1.34it/s]Extractor Predicting: 49it [00:35,  1.39it/s]Extractor Predicting: 50it [00:35,  1.38it/s]Extractor Predicting: 51it [00:36,  1.40it/s]Extractor Predicting: 52it [00:37,  1.37it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:38,  1.37it/s]Extractor Predicting: 55it [00:39,  1.37it/s]Extractor Predicting: 56it [00:40,  1.40it/s]Extractor Predicting: 57it [00:41,  1.38it/s]Extractor Predicting: 58it [00:41,  1.37it/s]Extractor Predicting: 59it [00:42,  1.41it/s]Extractor Predicting: 60it [00:43,  1.40it/s]Extractor Predicting: 61it [00:43,  1.38it/s]Extractor Predicting: 62it [00:44,  1.36it/s]Extractor Predicting: 63it [00:45,  1.35it/s]Extractor Predicting: 64it [00:46,  1.33it/s]Extractor Predicting: 65it [00:46,  1.34it/s]Extractor Predicting: 66it [00:47,  1.39it/s]Extractor Predicting: 67it [00:48,  1.39it/s]Extractor Predicting: 68it [00:48,  1.43it/s]Extractor Predicting: 69it [00:49,  1.43it/s]Extractor Predicting: 70it [00:50,  1.42it/s]Extractor Predicting: 71it [00:51,  1.45it/s]Extractor Predicting: 72it [00:51,  1.49it/s]Extractor Predicting: 73it [00:52,  1.46it/s]Extractor Predicting: 74it [00:53,  1.48it/s]Extractor Predicting: 75it [00:53,  1.47it/s]Extractor Predicting: 76it [00:54,  1.44it/s]Extractor Predicting: 77it [00:55,  1.43it/s]Extractor Predicting: 78it [00:55,  1.45it/s]Extractor Predicting: 79it [00:56,  1.45it/s]Extractor Predicting: 80it [00:57,  1.44it/s]Extractor Predicting: 81it [00:57,  1.43it/s]Extractor Predicting: 82it [00:58,  1.42it/s]Extractor Predicting: 83it [00:59,  1.42it/s]Extractor Predicting: 84it [01:00,  1.46it/s]Extractor Predicting: 85it [01:00,  1.52it/s]Extractor Predicting: 86it [01:01,  1.50it/s]Extractor Predicting: 87it [01:01,  1.52it/s]Extractor Predicting: 88it [01:02,  1.44it/s]Extractor Predicting: 89it [01:03,  1.44it/s]Extractor Predicting: 90it [01:04,  1.42it/s]Extractor Predicting: 91it [01:04,  1.43it/s]Extractor Predicting: 92it [01:05,  1.43it/s]Extractor Predicting: 93it [01:06,  1.44it/s]Extractor Predicting: 94it [01:06,  1.44it/s]Extractor Predicting: 95it [01:07,  1.44it/s]Extractor Predicting: 96it [01:08,  1.45it/s]Extractor Predicting: 97it [01:08,  1.45it/s]Extractor Predicting: 98it [01:09,  1.41it/s]Extractor Predicting: 99it [01:10,  1.41it/s]Extractor Predicting: 100it [01:11,  1.38it/s]Extractor Predicting: 101it [01:11,  1.38it/s]Extractor Predicting: 102it [01:12,  1.40it/s]Extractor Predicting: 103it [01:13,  1.38it/s]Extractor Predicting: 104it [01:14,  1.41it/s]Extractor Predicting: 105it [01:14,  1.44it/s]Extractor Predicting: 106it [01:15,  1.39it/s]Extractor Predicting: 107it [01:16,  1.43it/s]Extractor Predicting: 108it [01:16,  1.46it/s]Extractor Predicting: 109it [01:17,  1.43it/s]Extractor Predicting: 110it [01:18,  1.43it/s]Extractor Predicting: 111it [01:18,  1.40it/s]Extractor Predicting: 112it [01:19,  1.40it/s]Extractor Predicting: 113it [01:20,  1.30it/s]Extractor Predicting: 114it [01:21,  1.33it/s]Extractor Predicting: 115it [01:21,  1.35it/s]Extractor Predicting: 116it [01:22,  1.37it/s]Extractor Predicting: 117it [01:23,  1.41it/s]Extractor Predicting: 118it [01:24,  1.42it/s]Extractor Predicting: 119it [01:24,  1.47it/s]Extractor Predicting: 120it [01:25,  1.46it/s]Extractor Predicting: 121it [01:26,  1.45it/s]Extractor Predicting: 122it [01:26,  1.43it/s]Extractor Predicting: 123it [01:27,  1.42it/s]Extractor Predicting: 124it [01:28,  1.44it/s]Extractor Predicting: 125it [01:28,  1.40it/s]Extractor Predicting: 126it [01:29,  1.44it/s]Extractor Predicting: 127it [01:30,  1.46it/s]Extractor Predicting: 128it [01:30,  1.43it/s]Extractor Predicting: 129it [01:31,  1.42it/s]Extractor Predicting: 130it [01:32,  1.39it/s]Extractor Predicting: 131it [01:33,  1.39it/s]Extractor Predicting: 132it [01:33,  1.41it/s]Extractor Predicting: 133it [01:34,  1.42it/s]Extractor Predicting: 134it [01:35,  1.40it/s]Extractor Predicting: 135it [01:35,  1.39it/s]Extractor Predicting: 136it [01:36,  1.39it/s]Extractor Predicting: 137it [01:37,  1.41it/s]Extractor Predicting: 138it [01:38,  1.40it/s]Extractor Predicting: 139it [01:38,  1.37it/s]Extractor Predicting: 140it [01:39,  1.36it/s]Extractor Predicting: 141it [01:40,  1.36it/s]Extractor Predicting: 142it [01:41,  1.34it/s]Extractor Predicting: 143it [01:41,  1.36it/s]Extractor Predicting: 144it [01:42,  1.34it/s]Extractor Predicting: 145it [01:43,  1.35it/s]Extractor Predicting: 146it [01:44,  1.31it/s]Extractor Predicting: 147it [01:44,  1.32it/s]Extractor Predicting: 148it [01:45,  1.35it/s]Extractor Predicting: 149it [01:46,  1.37it/s]Extractor Predicting: 150it [01:47,  1.36it/s]Extractor Predicting: 151it [01:47,  1.39it/s]Extractor Predicting: 152it [01:48,  1.38it/s]Extractor Predicting: 153it [01:49,  1.40it/s]Extractor Predicting: 154it [01:49,  1.39it/s]Extractor Predicting: 155it [01:50,  1.39it/s]Extractor Predicting: 156it [01:51,  1.37it/s]Extractor Predicting: 157it [01:52,  1.39it/s]Extractor Predicting: 158it [01:52,  1.39it/s]Extractor Predicting: 159it [01:53,  1.38it/s]Extractor Predicting: 160it [01:54,  1.38it/s]Extractor Predicting: 161it [01:54,  1.41it/s]Extractor Predicting: 162it [01:55,  1.40it/s]Extractor Predicting: 163it [01:56,  1.37it/s]Extractor Predicting: 164it [01:57,  1.36it/s]Extractor Predicting: 165it [01:57,  1.37it/s]Extractor Predicting: 166it [01:58,  1.35it/s]Extractor Predicting: 167it [01:59,  1.36it/s]Extractor Predicting: 168it [02:00,  1.37it/s]Extractor Predicting: 169it [02:00,  1.38it/s]Extractor Predicting: 170it [02:01,  1.38it/s]Extractor Predicting: 171it [02:01,  1.65it/s]Extractor Predicting: 171it [02:01,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:28,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:28,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:28,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:28,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:28,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:09:29,269 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:09:29,270 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:09:29,820 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:09:30,882 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:09:30,882 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:33,708 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:33,712 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:33,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:33,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:33,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:09:34,381 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:09:34,381 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:09:34,935 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:09:35,105 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:09:35,105 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.34it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.43it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.33it/s]Extractor Predicting: 15it [00:10,  1.34it/s]Extractor Predicting: 16it [00:11,  1.37it/s]Extractor Predicting: 17it [00:12,  1.40it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:13,  1.45it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:17,  1.41it/s]Extractor Predicting: 25it [00:17,  1.43it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:19,  1.47it/s]Extractor Predicting: 28it [00:19,  1.47it/s]Extractor Predicting: 29it [00:20,  1.45it/s]Extractor Predicting: 30it [00:21,  1.43it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.42it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:26,  1.39it/s]Extractor Predicting: 39it [00:27,  1.39it/s]Extractor Predicting: 40it [00:28,  1.41it/s]Extractor Predicting: 41it [00:28,  1.42it/s]Extractor Predicting: 42it [00:29,  1.43it/s]Extractor Predicting: 43it [00:30,  1.43it/s]Extractor Predicting: 44it [00:31,  1.44it/s]Extractor Predicting: 45it [00:31,  1.44it/s]Extractor Predicting: 46it [00:32,  1.46it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:33,  1.42it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.41it/s]Extractor Predicting: 51it [00:35,  1.42it/s]Extractor Predicting: 52it [00:36,  1.42it/s]Extractor Predicting: 53it [00:37,  1.41it/s]Extractor Predicting: 54it [00:38,  1.39it/s]Extractor Predicting: 55it [00:38,  1.38it/s]Extractor Predicting: 56it [00:39,  1.40it/s]Extractor Predicting: 57it [00:40,  1.41it/s]Extractor Predicting: 58it [00:40,  1.41it/s]Extractor Predicting: 59it [00:41,  1.43it/s]Extractor Predicting: 60it [00:42,  1.39it/s]Extractor Predicting: 61it [00:43,  1.40it/s]Extractor Predicting: 62it [00:43,  1.38it/s]Extractor Predicting: 63it [00:44,  1.42it/s]Extractor Predicting: 64it [00:45,  1.41it/s]Extractor Predicting: 65it [00:45,  1.43it/s]Extractor Predicting: 66it [00:46,  1.42it/s]Extractor Predicting: 67it [00:47,  1.44it/s]Extractor Predicting: 68it [00:48,  1.45it/s]Extractor Predicting: 69it [00:48,  1.46it/s]Extractor Predicting: 70it [00:49,  1.44it/s]Extractor Predicting: 71it [00:50,  1.39it/s]Extractor Predicting: 72it [00:50,  1.41it/s]Extractor Predicting: 73it [00:51,  1.40it/s]Extractor Predicting: 74it [00:52,  1.43it/s]Extractor Predicting: 75it [00:52,  1.43it/s]Extractor Predicting: 76it [00:53,  1.46it/s]Extractor Predicting: 77it [00:54,  1.49it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:55,  1.46it/s]Extractor Predicting: 80it [00:56,  1.48it/s]Extractor Predicting: 81it [00:56,  1.47it/s]Extractor Predicting: 82it [00:57,  1.47it/s]Extractor Predicting: 83it [00:58,  1.47it/s]Extractor Predicting: 84it [00:59,  1.44it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.46it/s]Extractor Predicting: 87it [01:01,  1.44it/s]Extractor Predicting: 88it [01:01,  1.45it/s]Extractor Predicting: 89it [01:02,  1.45it/s]Extractor Predicting: 90it [01:03,  1.42it/s]Extractor Predicting: 91it [01:03,  1.42it/s]Extractor Predicting: 92it [01:04,  1.42it/s]Extractor Predicting: 93it [01:05,  1.43it/s]Extractor Predicting: 94it [01:06,  1.42it/s]Extractor Predicting: 95it [01:06,  1.41it/s]Extractor Predicting: 96it [01:07,  1.38it/s]Extractor Predicting: 97it [01:08,  1.41it/s]Extractor Predicting: 98it [01:08,  1.43it/s]Extractor Predicting: 99it [01:09,  1.50it/s]Extractor Predicting: 100it [01:10,  1.50it/s]Extractor Predicting: 101it [01:10,  1.47it/s]Extractor Predicting: 102it [01:11,  1.48it/s]Extractor Predicting: 103it [01:12,  1.47it/s]Extractor Predicting: 104it [01:12,  1.48it/s]Extractor Predicting: 105it [01:13,  1.47it/s]Extractor Predicting: 106it [01:14,  1.48it/s]Extractor Predicting: 107it [01:14,  1.50it/s]Extractor Predicting: 108it [01:15,  1.36it/s]Extractor Predicting: 109it [01:16,  1.35it/s]Extractor Predicting: 110it [01:17,  1.31it/s]Extractor Predicting: 111it [01:18,  1.30it/s]Extractor Predicting: 112it [01:18,  1.32it/s]Extractor Predicting: 113it [01:19,  1.38it/s]Extractor Predicting: 114it [01:20,  1.40it/s]Extractor Predicting: 115it [01:20,  1.39it/s]Extractor Predicting: 116it [01:21,  1.39it/s]Extractor Predicting: 117it [01:22,  1.39it/s]Extractor Predicting: 118it [01:23,  1.36it/s]Extractor Predicting: 119it [01:23,  1.37it/s]Extractor Predicting: 120it [01:24,  1.34it/s]Extractor Predicting: 121it [01:25,  1.33it/s]Extractor Predicting: 122it [01:26,  1.35it/s]Extractor Predicting: 123it [01:26,  1.37it/s]Extractor Predicting: 124it [01:27,  1.37it/s]Extractor Predicting: 125it [01:27,  1.73it/s]Extractor Predicting: 125it [01:27,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:09,368 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:09,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:09,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:09,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:09,373 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:11:09,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:11:09,984 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:11:10,573 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:11:11,628 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:11:11,628 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:14,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:14,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:14,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:14,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:11:14,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:11:15,234 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:11:15,235 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:11:15,807 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:11:15,963 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:11:15,963 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.16it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.78it/s]Extractor Predicting: 6it [00:04,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:11:20,483 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:11:20,484 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:11:20,489 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:11:20,489 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:11:20,491 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:11:23,490 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:11:23,493 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:11:23,513 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:11:23,514 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:11:23,521 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:11:23,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:11:23,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:24,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:25,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:26,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:26,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:28,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:29,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:29,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:30,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:31,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:32,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:33,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:33,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:34,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:35,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:36,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:37,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:38,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:39,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:39,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:40,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:41,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:42,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:43,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:43,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:20<03:07, 20.82s/it][WARNING|generation_utils.py:914] 2023-08-28 03:11:44,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:45,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:46,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:46,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:47,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:48,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:48,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:49,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:50,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:50,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:51,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:52,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:53,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:54,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:54,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:55,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:56,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:57,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:57,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:58,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:11:59,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:00,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:26, 18.26s/it][WARNING|generation_utils.py:914] 2023-08-28 03:12:01,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:01,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:02,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:03,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:04,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:05,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:05,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:06,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:07,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:08,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:08,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:09,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:10,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:11,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:11,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:12,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:13,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:14,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:14,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:15,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:16,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:17,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:18,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:18,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:19,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:20,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:21,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:21,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:22,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:20, 20.08s/it][WARNING|generation_utils.py:914] 2023-08-28 03:12:23,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:23,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:24,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:25,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:26,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:27,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:28,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:28,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:29,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:30,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:31,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:32,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:32,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:33,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:34,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:35,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:36,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:36,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:37,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:38,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:39,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:40,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:41,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:42,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:43,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:43,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:44,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:21<02:05, 20.97s/it][WARNING|generation_utils.py:914] 2023-08-28 03:12:45,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:46,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:47,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:47,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:48,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:49,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:50,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:50,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:51,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:52,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:53,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:53,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:54,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:55,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:56,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:57,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:58,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:58,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:12:59,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:00,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:01,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:01,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:37, 19.51s/it][WARNING|generation_utils.py:914] 2023-08-28 03:13:02,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:03,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:04,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:04,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:05,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:06,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:07,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:07,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:08,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:09,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:10,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:11,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:12,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:12,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:13,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:14,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:15,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:15,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:16,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:17,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:18,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:18,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:55<01:14, 18.72s/it][WARNING|generation_utils.py:914] 2023-08-28 03:13:19,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:20,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:21,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:21,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:22,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:23,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:24,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:25,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:25,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:26,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:27,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:27,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:28,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:29,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:30,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:31,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:31,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:32,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:33,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:34,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:35,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:36,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:36,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:37,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:14<00:56, 18.74s/it][WARNING|generation_utils.py:914] 2023-08-28 03:13:38,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:39,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:40,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:40,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:41,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:42,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:43,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:43,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:44,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:45,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:46,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:46,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:47,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:48,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:49,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:49,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:50,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:51,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:52,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:52,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:53,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:30<00:35, 17.86s/it][WARNING|generation_utils.py:914] 2023-08-28 03:13:54,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:55,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:56,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:56,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:57,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:58,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:59,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:13:59,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:00,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:01,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:02,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:02,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:03,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:05,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:06,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:07,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:08,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:08,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:09,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:10,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:11,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:11,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:12,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:13,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:50<00:18, 18.38s/it][WARNING|generation_utils.py:914] 2023-08-28 03:14:14,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:14,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:15,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:16,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:17,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:17,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:18,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:19,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:20,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:21,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:21,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:22,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:23,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:23,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:24,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:25,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:26,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:26,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:27,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:28,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:29,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:30,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:31,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:31,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:32,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:33,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:33,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:34,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:35,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:36,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:37,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:37,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:38,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:15<00:00, 20.53s/it]Generating: 100%|██████████| 10/10 [03:15<00:00, 19.56s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:45,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:45,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:45,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:45,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:45,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:14:46,080 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:14:46,081 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:14:46,645 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:14:47,763 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:14:47,763 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:50,613 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:50,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:50,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:50,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:50,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:14:51,265 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:14:51,265 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:14:51,863 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:14:52,043 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:14:52,043 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.14it/s]Extractor Estimating: 2it [00:01,  1.19it/s]Extractor Estimating: 3it [00:02,  1.26it/s]Extractor Estimating: 4it [00:03,  1.28it/s]Extractor Estimating: 5it [00:04,  1.10it/s]Extractor Estimating: 6it [00:05,  1.18it/s]Extractor Estimating: 7it [00:05,  1.21it/s]Extractor Estimating: 8it [00:06,  1.25it/s]Extractor Estimating: 9it [00:07,  1.30it/s]Extractor Estimating: 10it [00:08,  1.29it/s]Extractor Estimating: 11it [00:08,  1.24it/s]Extractor Estimating: 12it [00:09,  1.27it/s]Extractor Estimating: 13it [00:10,  1.29it/s]Extractor Estimating: 14it [00:11,  1.29it/s]Extractor Estimating: 15it [00:11,  1.30it/s]Extractor Estimating: 16it [00:12,  1.29it/s]Extractor Estimating: 17it [00:13,  1.30it/s]Extractor Estimating: 18it [00:14,  1.29it/s]Extractor Estimating: 19it [00:15,  1.28it/s]Extractor Estimating: 20it [00:15,  1.27it/s]Extractor Estimating: 21it [00:16,  1.29it/s]Extractor Estimating: 22it [00:17,  1.28it/s]Extractor Estimating: 23it [00:18,  1.26it/s]Extractor Estimating: 24it [00:19,  1.23it/s]Extractor Estimating: 25it [00:19,  1.25it/s]Extractor Estimating: 26it [00:20,  1.26it/s]Extractor Estimating: 27it [00:21,  1.26it/s]Extractor Estimating: 28it [00:22,  1.30it/s]Extractor Estimating: 29it [00:22,  1.34it/s]Extractor Estimating: 30it [00:23,  1.33it/s]Extractor Estimating: 31it [00:24,  1.33it/s]Extractor Estimating: 32it [00:25,  1.34it/s]Extractor Estimating: 33it [00:25,  1.36it/s]Extractor Estimating: 34it [00:26,  1.37it/s]Extractor Estimating: 35it [00:27,  1.31it/s]Extractor Estimating: 36it [00:28,  1.36it/s]Extractor Estimating: 37it [00:29,  1.26it/s]Extractor Estimating: 38it [00:29,  1.28it/s]Extractor Estimating: 39it [00:30,  1.31it/s]Extractor Estimating: 40it [00:31,  1.29it/s]Extractor Estimating: 41it [00:32,  1.25it/s]Extractor Estimating: 42it [00:32,  1.28it/s]Extractor Estimating: 43it [00:33,  1.31it/s]Extractor Estimating: 44it [00:34,  1.34it/s]Extractor Estimating: 45it [00:35,  1.34it/s]Extractor Estimating: 46it [00:35,  1.34it/s]Extractor Estimating: 47it [00:36,  1.32it/s]Extractor Estimating: 48it [00:37,  1.34it/s]Extractor Estimating: 49it [00:38,  1.31it/s]Extractor Estimating: 50it [00:38,  1.34it/s]Extractor Estimating: 51it [00:39,  1.31it/s]Extractor Estimating: 52it [00:40,  1.29it/s]Extractor Estimating: 53it [00:41,  1.30it/s]Extractor Estimating: 54it [00:41,  1.29it/s]Extractor Estimating: 55it [00:42,  1.31it/s]Extractor Estimating: 56it [00:43,  1.27it/s]Extractor Estimating: 57it [00:44,  1.24it/s]Extractor Estimating: 58it [00:45,  1.30it/s]Extractor Estimating: 59it [00:45,  1.32it/s]Extractor Estimating: 60it [00:46,  1.31it/s]Extractor Estimating: 61it [00:47,  1.27it/s]Extractor Estimating: 62it [00:48,  1.29it/s]Extractor Estimating: 63it [00:48,  1.32it/s]Extractor Estimating: 64it [00:49,  1.31it/s]Extractor Estimating: 65it [00:50,  1.29it/s]Extractor Estimating: 66it [00:51,  1.28it/s]Extractor Estimating: 67it [00:52,  1.28it/s]Extractor Estimating: 68it [00:52,  1.29it/s]Extractor Estimating: 69it [00:53,  1.30it/s]Extractor Estimating: 70it [00:54,  1.29it/s]Extractor Estimating: 71it [00:55,  1.28it/s]Extractor Estimating: 72it [00:55,  1.28it/s]Extractor Estimating: 73it [00:56,  1.28it/s]Extractor Estimating: 74it [00:57,  1.30it/s]Extractor Estimating: 75it [00:58,  1.32it/s]Extractor Estimating: 76it [00:58,  1.36it/s]Extractor Estimating: 77it [00:59,  1.29it/s]Extractor Estimating: 78it [01:00,  1.28it/s]Extractor Estimating: 79it [01:01,  1.34it/s]Extractor Estimating: 80it [01:01,  1.35it/s]Extractor Estimating: 81it [01:02,  1.34it/s]Extractor Estimating: 82it [01:03,  1.31it/s]Extractor Estimating: 83it [01:04,  1.29it/s]Extractor Estimating: 84it [01:05,  1.25it/s]Extractor Estimating: 85it [01:05,  1.25it/s]Extractor Estimating: 86it [01:06,  1.26it/s]Extractor Estimating: 87it [01:07,  1.24it/s]Extractor Estimating: 88it [01:08,  1.28it/s]Extractor Estimating: 89it [01:09,  1.28it/s]Extractor Estimating: 90it [01:09,  1.28it/s]Extractor Estimating: 91it [01:10,  1.27it/s]Extractor Estimating: 92it [01:11,  1.29it/s]Extractor Estimating: 93it [01:12,  1.28it/s]Extractor Estimating: 94it [01:12,  1.27it/s]Extractor Estimating: 95it [01:13,  1.26it/s]Extractor Estimating: 96it [01:14,  1.25it/s]Extractor Estimating: 97it [01:15,  1.24it/s]Extractor Estimating: 98it [01:16,  1.25it/s]Extractor Estimating: 99it [01:17,  1.25it/s]Extractor Estimating: 100it [01:17,  1.30it/s]Extractor Estimating: 101it [01:18,  1.30it/s]Extractor Estimating: 102it [01:19,  1.27it/s]Extractor Estimating: 103it [01:20,  1.29it/s]Extractor Estimating: 104it [01:20,  1.30it/s]Extractor Estimating: 105it [01:21,  1.32it/s]Extractor Estimating: 106it [01:22,  1.30it/s]Extractor Estimating: 107it [01:23,  1.29it/s]Extractor Estimating: 108it [01:23,  1.32it/s]Extractor Estimating: 109it [01:24,  1.18it/s]Extractor Estimating: 110it [01:25,  1.22it/s]Extractor Estimating: 111it [01:26,  1.24it/s]Extractor Estimating: 112it [01:27,  1.25it/s]Extractor Estimating: 113it [01:28,  1.24it/s]Extractor Estimating: 114it [01:28,  1.28it/s]Extractor Estimating: 115it [01:29,  1.29it/s]Extractor Estimating: 116it [01:30,  1.31it/s]Extractor Estimating: 117it [01:31,  1.31it/s]Extractor Estimating: 118it [01:31,  1.33it/s]Extractor Estimating: 119it [01:32,  1.33it/s]Extractor Estimating: 120it [01:33,  1.33it/s]Extractor Estimating: 121it [01:34,  1.32it/s]Extractor Estimating: 122it [01:34,  1.33it/s]Extractor Estimating: 123it [01:35,  1.31it/s]Extractor Estimating: 124it [01:36,  1.30it/s]Extractor Estimating: 125it [01:37,  1.31it/s]Extractor Estimating: 126it [01:37,  1.31it/s]Extractor Estimating: 127it [01:38,  1.27it/s]Extractor Estimating: 128it [01:39,  1.31it/s]Extractor Estimating: 129it [01:40,  1.27it/s]Extractor Estimating: 130it [01:41,  1.24it/s]Extractor Estimating: 131it [01:41,  1.22it/s]Extractor Estimating: 132it [01:42,  1.25it/s]Extractor Estimating: 133it [01:43,  1.24it/s]Extractor Estimating: 134it [01:44,  1.24it/s]Extractor Estimating: 135it [01:45,  1.26it/s]Extractor Estimating: 136it [01:45,  1.28it/s]Extractor Estimating: 137it [01:46,  1.24it/s]Extractor Estimating: 138it [01:47,  1.24it/s]Extractor Estimating: 139it [01:48,  1.21it/s]Extractor Estimating: 140it [01:49,  1.24it/s]Extractor Estimating: 141it [01:49,  1.29it/s]Extractor Estimating: 142it [01:50,  1.30it/s]Extractor Estimating: 143it [01:51,  1.29it/s]Extractor Estimating: 144it [01:52,  1.27it/s]Extractor Estimating: 145it [01:53,  1.26it/s]Extractor Estimating: 146it [01:53,  1.23it/s]Extractor Estimating: 147it [01:54,  1.25it/s]Extractor Estimating: 148it [01:55,  1.27it/s]Extractor Estimating: 149it [01:56,  1.30it/s]Extractor Estimating: 150it [01:56,  1.30it/s]Extractor Estimating: 151it [01:57,  1.33it/s]Extractor Estimating: 152it [01:58,  1.32it/s]Extractor Estimating: 153it [01:59,  1.32it/s]Extractor Estimating: 154it [01:59,  1.32it/s]Extractor Estimating: 155it [02:00,  1.32it/s]Extractor Estimating: 156it [02:01,  1.32it/s]Extractor Estimating: 157it [02:02,  1.30it/s]Extractor Estimating: 158it [02:02,  1.31it/s]Extractor Estimating: 159it [02:03,  1.32it/s]Extractor Estimating: 160it [02:04,  1.30it/s]Extractor Estimating: 161it [02:05,  1.30it/s]Extractor Estimating: 162it [02:06,  1.31it/s]Extractor Estimating: 163it [02:06,  1.33it/s]Extractor Estimating: 164it [02:07,  1.30it/s]Extractor Estimating: 165it [02:08,  1.28it/s]Extractor Estimating: 166it [02:09,  1.29it/s]Extractor Estimating: 167it [02:09,  1.29it/s]Extractor Estimating: 168it [02:10,  1.30it/s]Extractor Estimating: 169it [02:11,  1.30it/s]Extractor Estimating: 170it [02:12,  1.27it/s]Extractor Estimating: 171it [02:12,  1.32it/s]Extractor Estimating: 172it [02:13,  1.29it/s]Extractor Estimating: 173it [02:14,  1.33it/s]Extractor Estimating: 174it [02:15,  1.34it/s]Extractor Estimating: 175it [02:15,  1.32it/s]Extractor Estimating: 176it [02:16,  1.29it/s]Extractor Estimating: 177it [02:17,  1.30it/s]Extractor Estimating: 178it [02:18,  1.28it/s]Extractor Estimating: 179it [02:19,  1.27it/s]Extractor Estimating: 180it [02:20,  1.22it/s]Extractor Estimating: 181it [02:20,  1.27it/s]Extractor Estimating: 182it [02:21,  1.29it/s]Extractor Estimating: 183it [02:22,  1.26it/s]Extractor Estimating: 184it [02:23,  1.28it/s]Extractor Estimating: 185it [02:24,  1.21it/s]Extractor Estimating: 186it [02:24,  1.27it/s]Extractor Estimating: 187it [02:25,  1.28it/s]Extractor Estimating: 188it [02:26,  1.30it/s]Extractor Estimating: 189it [02:26,  1.30it/s]Extractor Estimating: 190it [02:27,  1.25it/s]Extractor Estimating: 191it [02:28,  1.29it/s]Extractor Estimating: 192it [02:29,  1.27it/s]Extractor Estimating: 193it [02:30,  1.27it/s]Extractor Estimating: 194it [02:30,  1.30it/s]Extractor Estimating: 195it [02:31,  1.31it/s]Extractor Estimating: 196it [02:32,  1.33it/s]Extractor Estimating: 197it [02:33,  1.33it/s]Extractor Estimating: 198it [02:33,  1.31it/s]Extractor Estimating: 199it [02:34,  1.25it/s]Extractor Estimating: 200it [02:35,  1.28it/s]Extractor Estimating: 201it [02:36,  1.27it/s]Extractor Estimating: 202it [02:37,  1.29it/s]Extractor Estimating: 203it [02:37,  1.31it/s]Extractor Estimating: 204it [02:38,  1.30it/s]Extractor Estimating: 205it [02:39,  1.31it/s]Extractor Estimating: 206it [02:40,  1.34it/s]Extractor Estimating: 207it [02:40,  1.32it/s]Extractor Estimating: 208it [02:41,  1.33it/s]Extractor Estimating: 209it [02:42,  1.31it/s]Extractor Estimating: 210it [02:43,  1.32it/s]Extractor Estimating: 211it [02:43,  1.30it/s]Extractor Estimating: 212it [02:44,  1.30it/s]Extractor Estimating: 213it [02:45,  1.28it/s]Extractor Estimating: 214it [02:46,  1.30it/s]Extractor Estimating: 215it [02:47,  1.28it/s]Extractor Estimating: 216it [02:47,  1.27it/s]Extractor Estimating: 217it [02:48,  1.28it/s]Extractor Estimating: 218it [02:49,  1.31it/s]Extractor Estimating: 219it [02:50,  1.33it/s]Extractor Estimating: 220it [02:50,  1.32it/s]Extractor Estimating: 221it [02:51,  1.30it/s]Extractor Estimating: 222it [02:52,  1.32it/s]Extractor Estimating: 223it [02:53,  1.34it/s]Extractor Estimating: 224it [02:53,  1.33it/s]Extractor Estimating: 225it [02:54,  1.31it/s]Extractor Estimating: 226it [02:55,  1.30it/s]Extractor Estimating: 227it [02:56,  1.31it/s]Extractor Estimating: 228it [02:57,  1.27it/s]Extractor Estimating: 229it [02:57,  1.30it/s]Extractor Estimating: 230it [02:58,  1.28it/s]Extractor Estimating: 231it [02:59,  1.25it/s]Extractor Estimating: 232it [03:00,  1.29it/s]Extractor Estimating: 233it [03:00,  1.27it/s]Extractor Estimating: 234it [03:01,  1.31it/s]Extractor Estimating: 235it [03:02,  1.31it/s]Extractor Estimating: 236it [03:03,  1.32it/s]Extractor Estimating: 237it [03:03,  1.31it/s]Extractor Estimating: 238it [03:04,  1.32it/s]Extractor Estimating: 239it [03:05,  1.26it/s]Extractor Estimating: 240it [03:06,  1.24it/s]Extractor Estimating: 241it [03:07,  1.24it/s]Extractor Estimating: 242it [03:07,  1.31it/s]Extractor Estimating: 243it [03:08,  1.30it/s]Extractor Estimating: 244it [03:09,  1.33it/s]Extractor Estimating: 245it [03:10,  1.37it/s]Extractor Estimating: 246it [03:10,  1.36it/s]Extractor Estimating: 247it [03:11,  1.35it/s]Extractor Estimating: 248it [03:12,  1.32it/s]Extractor Estimating: 249it [03:13,  1.34it/s]Extractor Estimating: 250it [03:13,  1.26it/s]Extractor Estimating: 250it [03:13,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:23,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:23,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:23,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:23,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:23,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:18:24,191 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:18:24,192 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:18:24,767 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:18:25,872 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:18:25,872 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:28,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:28,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:28,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:28,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:28,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:18:29,520 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:18:29,520 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:18:30,091 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:18:30,272 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:18:30,272 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:13:59,269 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:13:59,272 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5283 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 22444
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22544, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22544, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.280, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.306, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.278, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.282, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.299, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.821, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.263, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.291, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.287, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.283, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.836, loss:nan
g_step 1200, step 95, avg_time 1.276, loss:nan
g_step 1300, step 195, avg_time 1.302, loss:nan
g_step 1400, step 74, avg_time 1.284, loss:nan
g_step 1500, step 174, avg_time 1.295, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.822, loss:nan
g_step 1700, step 153, avg_time 1.300, loss:nan
g_step 1800, step 32, avg_time 1.286, loss:nan
g_step 1900, step 132, avg_time 1.291, loss:nan
g_step 2000, step 11, avg_time 1.283, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.820, loss:nan
g_step 2200, step 211, avg_time 1.293, loss:nan
g_step 2300, step 90, avg_time 1.298, loss:nan
g_step 2400, step 190, avg_time 1.283, loss:nan
g_step 2500, step 69, avg_time 1.289, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.820, loss:nan
g_step 2700, step 48, avg_time 1.303, loss:nan
g_step 2800, step 148, avg_time 1.295, loss:nan
g_step 2900, step 27, avg_time 1.283, loss:nan
g_step 3000, step 127, avg_time 1.272, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.836, loss:nan
g_step 3200, step 106, avg_time 1.281, loss:nan
g_step 3300, step 206, avg_time 1.293, loss:nan
g_step 3400, step 85, avg_time 1.302, loss:nan
g_step 3500, step 185, avg_time 1.284, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.830, loss:nan
g_step 3700, step 164, avg_time 1.295, loss:nan
g_step 3800, step 43, avg_time 1.288, loss:nan
g_step 3900, step 143, avg_time 1.285, loss:nan
g_step 4000, step 22, avg_time 1.288, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.847, loss:nan
g_step 4200, step 1, avg_time 1.266, loss:nan
g_step 4300, step 101, avg_time 1.301, loss:nan
g_step 4400, step 201, avg_time 1.291, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:13:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:13:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-13-59_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:14:00 - WARNING - datasets.builder -   Using custom data configuration default-44ff662030733f9b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-44ff662030733f9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:14:00,575 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:14:00,576 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:14:00,577 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:14:00,578 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:14:00,588 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:14:00,594 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:14:00,746 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:14:03,865 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:14:03,868 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-44ff662030733f9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.21ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.97ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.26ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.40ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.48ba/s]100%|██████████| 6/6 [00:01<00:00,  4.86ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.18ba/s] 40%|████      | 2/5 [00:00<00:00,  4.37ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.45ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.50ba/s]100%|██████████| 5/5 [00:01<00:00,  5.23ba/s]100%|██████████| 5/5 [00:01<00:00,  4.83ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.71ba/s] 33%|███▎      | 2/6 [00:00<00:00,  9.40ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.24ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.46ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.58ba/s]100%|██████████| 6/6 [00:00<00:00, 10.71ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.91ba/s] 40%|████      | 2/5 [00:00<00:00,  8.26ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.74ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.03ba/s]100%|██████████| 5/5 [00:00<00:00,  9.60ba/s]
[INFO|trainer.py:414] 2023-08-28 05:14:07,581 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:14:07,597 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:14:07,597 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 05:14:07,597 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:14:07,597 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:14:07,597 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:14:07,597 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:14:07,597 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<01:59,  3.47it/s]  0%|          | 2/415 [00:00<01:56,  3.53it/s]  1%|          | 3/415 [00:00<01:55,  3.55it/s]  1%|          | 4/415 [00:01<01:55,  3.56it/s]  1%|          | 5/415 [00:01<01:54,  3.57it/s]  1%|▏         | 6/415 [00:01<01:54,  3.57it/s]  2%|▏         | 7/415 [00:01<01:54,  3.55it/s]  2%|▏         | 8/415 [00:02<01:55,  3.54it/s]  2%|▏         | 9/415 [00:02<01:54,  3.53it/s]  2%|▏         | 10/415 [00:02<01:55,  3.50it/s]  3%|▎         | 11/415 [00:03<01:55,  3.50it/s]  3%|▎         | 12/415 [00:03<01:54,  3.51it/s]  3%|▎         | 13/415 [00:03<01:54,  3.51it/s]  3%|▎         | 14/415 [00:03<01:54,  3.51it/s]  4%|▎         | 15/415 [00:04<01:54,  3.51it/s]  4%|▍         | 16/415 [00:04<01:53,  3.51it/s]  4%|▍         | 17/415 [00:04<01:53,  3.51it/s]  4%|▍         | 18/415 [00:05<01:53,  3.51it/s]  5%|▍         | 19/415 [00:05<01:52,  3.51it/s]  5%|▍         | 20/415 [00:05<01:52,  3.51it/s]  5%|▌         | 21/415 [00:05<01:52,  3.49it/s]  5%|▌         | 22/415 [00:06<01:52,  3.50it/s]  6%|▌         | 23/415 [00:06<01:51,  3.50it/s]  6%|▌         | 24/415 [00:06<01:51,  3.51it/s]  6%|▌         | 25/415 [00:07<01:51,  3.51it/s]  6%|▋         | 26/415 [00:07<01:50,  3.51it/s]  7%|▋         | 27/415 [00:07<01:50,  3.51it/s]  7%|▋         | 28/415 [00:07<01:50,  3.51it/s]  7%|▋         | 29/415 [00:08<01:49,  3.51it/s]  7%|▋         | 30/415 [00:08<01:49,  3.51it/s]  7%|▋         | 31/415 [00:08<01:49,  3.51it/s]  8%|▊         | 32/415 [00:09<01:49,  3.51it/s]  8%|▊         | 33/415 [00:09<01:48,  3.50it/s]  8%|▊         | 34/415 [00:09<01:48,  3.50it/s]  8%|▊         | 35/415 [00:09<01:48,  3.51it/s]  9%|▊         | 36/415 [00:10<01:48,  3.51it/s]  9%|▉         | 37/415 [00:10<01:47,  3.51it/s]  9%|▉         | 38/415 [00:10<01:47,  3.51it/s]  9%|▉         | 39/415 [00:11<01:47,  3.51it/s] 10%|▉         | 40/415 [00:11<01:46,  3.51it/s] 10%|▉         | 41/415 [00:11<01:46,  3.51it/s] 10%|█         | 42/415 [00:11<01:46,  3.51it/s] 10%|█         | 43/415 [00:12<01:46,  3.50it/s] 11%|█         | 44/415 [00:12<01:46,  3.50it/s] 11%|█         | 45/415 [00:12<01:45,  3.50it/s] 11%|█         | 46/415 [00:13<01:45,  3.51it/s] 11%|█▏        | 47/415 [00:13<01:44,  3.51it/s] 12%|█▏        | 48/415 [00:13<01:44,  3.51it/s] 12%|█▏        | 49/415 [00:13<01:44,  3.51it/s] 12%|█▏        | 50/415 [00:14<01:44,  3.49it/s] 12%|█▏        | 51/415 [00:14<01:44,  3.50it/s] 13%|█▎        | 52/415 [00:14<01:43,  3.50it/s] 13%|█▎        | 53/415 [00:15<01:43,  3.50it/s] 13%|█▎        | 54/415 [00:15<01:43,  3.49it/s] 13%|█▎        | 55/415 [00:15<01:43,  3.49it/s] 13%|█▎        | 56/415 [00:15<01:42,  3.50it/s] 14%|█▎        | 57/415 [00:16<01:42,  3.50it/s] 14%|█▍        | 58/415 [00:16<01:41,  3.50it/s] 14%|█▍        | 59/415 [00:16<01:41,  3.50it/s] 14%|█▍        | 60/415 [00:17<01:41,  3.50it/s] 15%|█▍        | 61/415 [00:17<01:40,  3.51it/s] 15%|█▍        | 62/415 [00:17<01:40,  3.50it/s] 15%|█▌        | 63/415 [00:17<01:40,  3.51it/s] 15%|█▌        | 64/415 [00:18<01:40,  3.51it/s] 16%|█▌        | 65/415 [00:18<01:40,  3.49it/s] 16%|█▌        | 66/415 [00:18<01:39,  3.50it/s] 16%|█▌        | 67/415 [00:19<01:39,  3.50it/s] 16%|█▋        | 68/415 [00:19<01:39,  3.50it/s] 17%|█▋        | 69/415 [00:19<01:38,  3.50it/s] 17%|█▋        | 70/415 [00:19<01:38,  3.51it/s] 17%|█▋        | 71/415 [00:20<01:38,  3.51it/s] 17%|█▋        | 72/415 [00:20<01:37,  3.51it/s] 18%|█▊        | 73/415 [00:20<01:37,  3.51it/s] 18%|█▊        | 74/415 [00:21<01:37,  3.50it/s] 18%|█▊        | 75/415 [00:21<01:37,  3.50it/s] 18%|█▊        | 76/415 [00:21<01:39,  3.40it/s] 19%|█▊        | 77/415 [00:21<01:38,  3.43it/s] 19%|█▉        | 78/415 [00:22<01:37,  3.45it/s] 19%|█▉        | 79/415 [00:22<01:36,  3.46it/s] 19%|█▉        | 80/415 [00:22<01:36,  3.48it/s] 20%|█▉        | 81/415 [00:23<01:35,  3.48it/s] 20%|█▉        | 82/415 [00:23<01:35,  3.49it/s] 20%|██        | 83/415 [00:23<01:30,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 05:14:31,244 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:14:31,244 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:14:31,244 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.48it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.86it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.24it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.87it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.18it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.73it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.46it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.30it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.37it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.55it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.73it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.83it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.69it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.40it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.23it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.18it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.24it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.29it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.43it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.56it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.55it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.70it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.54it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.24it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.22it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.15it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.26it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.45it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.61it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.64it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.63it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.44it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.26it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.18it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.16it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.34it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.48it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.56it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.71it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.52it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.39it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.20it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.14it/s][A
 38%|███▊      | 222/577 [00:04<00:08, 44.15it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.25it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.43it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.44it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.60it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.47it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.41it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.24it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.24it/s][A
 46%|████▋     | 267/577 [00:05<00:07, 44.25it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.28it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.38it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.51it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.58it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.49it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.35it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.35it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.18it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.11it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.30it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.46it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.55it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.51it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.40it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.33it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.21it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.24it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.27it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.30it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.45it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.58it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.53it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.39it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.40it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.28it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.27it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.21it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.29it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.44it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.55it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.49it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.41it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.30it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.30it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.28it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.22it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.30it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.42it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.48it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.44it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.39it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.18it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.22it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.21it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.08it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.25it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.41it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.49it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.44it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.35it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.33it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.26it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.22it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.07it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.33it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.46it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.49it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.47it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.36it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.32it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.28it/s][A
100%|██████████| 577/577 [00:12<00:00, 44.26it/s][A                                                
                                                 [A 20%|██        | 83/415 [00:36<01:30,  3.69it/s]
100%|██████████| 577/577 [00:12<00:00, 44.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:14:44,269 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 05:14:44,290 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:14:45,954 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:14:45,980 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:14:45,993 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:38<26:24,  4.79s/it] 20%|██        | 85/415 [00:39<18:54,  3.44s/it] 21%|██        | 86/415 [00:39<13:39,  2.49s/it] 21%|██        | 87/415 [00:39<10:00,  1.83s/it] 21%|██        | 88/415 [00:40<07:26,  1.37s/it] 21%|██▏       | 89/415 [00:40<05:39,  1.04s/it] 22%|██▏       | 90/415 [00:40<04:24,  1.23it/s] 22%|██▏       | 91/415 [00:40<03:32,  1.52it/s] 22%|██▏       | 92/415 [00:41<02:56,  1.83it/s] 22%|██▏       | 93/415 [00:41<02:30,  2.14it/s] 23%|██▎       | 94/415 [00:41<02:12,  2.42it/s] 23%|██▎       | 95/415 [00:42<02:00,  2.66it/s] 23%|██▎       | 96/415 [00:42<01:51,  2.87it/s] 23%|██▎       | 97/415 [00:42<01:44,  3.03it/s] 24%|██▎       | 98/415 [00:42<01:40,  3.16it/s] 24%|██▍       | 99/415 [00:43<01:37,  3.26it/s] 24%|██▍       | 100/415 [00:43<01:34,  3.32it/s] 24%|██▍       | 101/415 [00:43<01:32,  3.38it/s] 25%|██▍       | 102/415 [00:44<01:31,  3.41it/s] 25%|██▍       | 103/415 [00:44<01:30,  3.43it/s] 25%|██▌       | 104/415 [00:44<01:30,  3.45it/s] 25%|██▌       | 105/415 [00:44<01:29,  3.45it/s] 26%|██▌       | 106/415 [00:45<01:29,  3.47it/s] 26%|██▌       | 107/415 [00:45<01:28,  3.48it/s] 26%|██▌       | 108/415 [00:45<01:28,  3.49it/s] 26%|██▋       | 109/415 [00:46<01:27,  3.49it/s] 27%|██▋       | 110/415 [00:46<01:27,  3.50it/s] 27%|██▋       | 111/415 [00:46<01:26,  3.50it/s] 27%|██▋       | 112/415 [00:46<01:26,  3.50it/s] 27%|██▋       | 113/415 [00:47<01:26,  3.50it/s] 27%|██▋       | 114/415 [00:47<01:25,  3.50it/s] 28%|██▊       | 115/415 [00:47<01:25,  3.51it/s] 28%|██▊       | 116/415 [00:48<01:25,  3.49it/s] 28%|██▊       | 117/415 [00:48<01:25,  3.50it/s] 28%|██▊       | 118/415 [00:48<01:24,  3.50it/s] 29%|██▊       | 119/415 [00:48<01:24,  3.50it/s] 29%|██▉       | 120/415 [00:49<01:24,  3.50it/s] 29%|██▉       | 121/415 [00:49<01:23,  3.50it/s] 29%|██▉       | 122/415 [00:49<01:23,  3.50it/s] 30%|██▉       | 123/415 [00:50<01:23,  3.50it/s] 30%|██▉       | 124/415 [00:50<01:23,  3.50it/s] 30%|███       | 125/415 [00:50<01:22,  3.50it/s] 30%|███       | 126/415 [00:50<01:22,  3.50it/s] 31%|███       | 127/415 [00:51<01:22,  3.49it/s] 31%|███       | 128/415 [00:51<01:22,  3.49it/s] 31%|███       | 129/415 [00:51<01:21,  3.49it/s] 31%|███▏      | 130/415 [00:52<01:21,  3.49it/s] 32%|███▏      | 131/415 [00:52<01:21,  3.50it/s] 32%|███▏      | 132/415 [00:52<01:20,  3.50it/s] 32%|███▏      | 133/415 [00:52<01:20,  3.49it/s] 32%|███▏      | 134/415 [00:53<01:20,  3.50it/s] 33%|███▎      | 135/415 [00:53<01:19,  3.50it/s] 33%|███▎      | 136/415 [00:53<01:19,  3.50it/s] 33%|███▎      | 137/415 [00:54<01:19,  3.50it/s] 33%|███▎      | 138/415 [00:54<01:19,  3.50it/s] 33%|███▎      | 139/415 [00:54<01:18,  3.50it/s] 34%|███▎      | 140/415 [00:54<01:18,  3.50it/s] 34%|███▍      | 141/415 [00:55<01:18,  3.50it/s] 34%|███▍      | 142/415 [00:55<01:17,  3.50it/s] 34%|███▍      | 143/415 [00:55<01:17,  3.50it/s] 35%|███▍      | 144/415 [00:56<01:17,  3.49it/s] 35%|███▍      | 145/415 [00:56<01:17,  3.49it/s] 35%|███▌      | 146/415 [00:56<01:17,  3.49it/s] 35%|███▌      | 147/415 [00:56<01:16,  3.49it/s] 36%|███▌      | 148/415 [00:57<01:16,  3.50it/s] 36%|███▌      | 149/415 [00:57<01:16,  3.50it/s] 36%|███▌      | 150/415 [00:57<01:15,  3.50it/s] 36%|███▋      | 151/415 [00:58<01:15,  3.50it/s] 37%|███▋      | 152/415 [00:58<01:15,  3.49it/s] 37%|███▋      | 153/415 [00:58<01:15,  3.49it/s] 37%|███▋      | 154/415 [00:58<01:14,  3.49it/s] 37%|███▋      | 155/415 [00:59<01:16,  3.39it/s] 38%|███▊      | 156/415 [00:59<01:15,  3.42it/s] 38%|███▊      | 157/415 [00:59<01:14,  3.44it/s] 38%|███▊      | 158/415 [01:00<01:14,  3.45it/s] 38%|███▊      | 159/415 [01:00<01:13,  3.47it/s] 39%|███▊      | 160/415 [01:00<01:13,  3.47it/s] 39%|███▉      | 161/415 [01:01<01:13,  3.47it/s] 39%|███▉      | 162/415 [01:01<01:13,  3.46it/s] 39%|███▉      | 163/415 [01:01<01:12,  3.47it/s] 40%|███▉      | 164/415 [01:01<01:12,  3.47it/s] 40%|███▉      | 165/415 [01:02<01:11,  3.47it/s] 40%|████      | 166/415 [01:02<01:09,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 05:15:10,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:15:10,033 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:15:10,033 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 12.9963, 'eval_samples_per_second': 354.639, 'eval_steps_per_second': 44.397, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.50it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.34it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.87it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.87it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.13it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.61it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.01it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.79it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.14it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.26it/s][A
 10%|▉         | 57/577 [00:01<00:11, 43.80it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.45it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.56it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.58it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.19it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.80it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.72it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.72it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.10it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.25it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.46it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.21it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.23it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.14it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.91it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.92it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.98it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.21it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.40it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.52it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.52it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.31it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.11it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.94it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.97it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.99it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.36it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.46it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.40it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.29it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.07it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.11it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.19it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.33it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.42it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.39it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.21it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.07it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.99it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.01it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.12it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.22it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.32it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.30it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.37it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.34it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.10it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.01it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.05it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.03it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.16it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.33it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.38it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.37it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.12it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.98it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.13it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.15it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.21it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.23it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.29it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.44it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.35it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.17it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.14it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.08it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.07it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.17it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.26it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.36it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.35it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.28it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.21it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.14it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.09it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.13it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.21it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.31it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.31it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.36it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.20it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.18it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.15it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.12it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.06it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.14it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.27it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.27it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.27it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.11it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.11it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.17it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.23it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.24it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.27it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.33it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.29it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.25it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.26it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.10it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.92it/s][A                                                 
                                                 [A 40%|████      | 166/415 [01:15<01:09,  3.58it/s]
100%|██████████| 577/577 [00:13<00:00, 43.92it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:15:23,103 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 05:15:23,130 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:15:25,221 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:15:25,237 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:15:25,247 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:18<20:21,  4.93s/it] 40%|████      | 168/415 [01:18<14:33,  3.53s/it] 41%|████      | 169/415 [01:18<10:29,  2.56s/it] 41%|████      | 170/415 [01:19<07:39,  1.88s/it] 41%|████      | 171/415 [01:19<05:41,  1.40s/it] 41%|████▏     | 172/415 [01:19<04:19,  1.07s/it] 42%|████▏     | 173/415 [01:19<03:21,  1.20it/s] 42%|████▏     | 174/415 [01:20<02:40,  1.50it/s] 42%|████▏     | 175/415 [01:20<02:12,  1.81it/s] 42%|████▏     | 176/415 [01:20<01:53,  2.12it/s] 43%|████▎     | 177/415 [01:21<01:39,  2.39it/s] 43%|████▎     | 178/415 [01:21<01:29,  2.64it/s] 43%|████▎     | 179/415 [01:21<01:22,  2.85it/s] 43%|████▎     | 180/415 [01:21<01:17,  3.02it/s] 44%|████▎     | 181/415 [01:22<01:14,  3.15it/s] 44%|████▍     | 182/415 [01:22<01:11,  3.25it/s] 44%|████▍     | 183/415 [01:22<01:09,  3.32it/s] 44%|████▍     | 184/415 [01:23<01:08,  3.37it/s] 45%|████▍     | 185/415 [01:23<01:07,  3.40it/s] 45%|████▍     | 186/415 [01:23<01:06,  3.43it/s] 45%|████▌     | 187/415 [01:23<01:06,  3.44it/s] 45%|████▌     | 188/415 [01:24<01:05,  3.46it/s] 46%|████▌     | 189/415 [01:24<01:05,  3.47it/s] 46%|████▌     | 190/415 [01:24<01:04,  3.48it/s] 46%|████▌     | 191/415 [01:25<01:04,  3.48it/s] 46%|████▋     | 192/415 [01:25<01:03,  3.49it/s] 47%|████▋     | 193/415 [01:25<01:03,  3.49it/s] 47%|████▋     | 194/415 [01:25<01:03,  3.49it/s] 47%|████▋     | 195/415 [01:26<01:02,  3.49it/s] 47%|████▋     | 196/415 [01:26<01:02,  3.49it/s] 47%|████▋     | 197/415 [01:26<01:02,  3.49it/s] 48%|████▊     | 198/415 [01:27<01:02,  3.49it/s] 48%|████▊     | 199/415 [01:27<01:01,  3.49it/s] 48%|████▊     | 200/415 [01:27<01:01,  3.49it/s] 48%|████▊     | 201/415 [01:27<01:01,  3.49it/s] 49%|████▊     | 202/415 [01:28<01:01,  3.49it/s] 49%|████▉     | 203/415 [01:28<01:00,  3.49it/s] 49%|████▉     | 204/415 [01:28<01:00,  3.49it/s] 49%|████▉     | 205/415 [01:29<01:00,  3.49it/s] 50%|████▉     | 206/415 [01:29<00:59,  3.49it/s] 50%|████▉     | 207/415 [01:29<00:59,  3.49it/s] 50%|█████     | 208/415 [01:29<00:59,  3.49it/s] 50%|█████     | 209/415 [01:30<00:59,  3.48it/s] 51%|█████     | 210/415 [01:30<00:58,  3.49it/s] 51%|█████     | 211/415 [01:30<00:58,  3.48it/s] 51%|█████     | 212/415 [01:31<00:58,  3.49it/s] 51%|█████▏    | 213/415 [01:31<00:57,  3.49it/s] 52%|█████▏    | 214/415 [01:31<00:57,  3.49it/s] 52%|█████▏    | 215/415 [01:31<00:57,  3.49it/s] 52%|█████▏    | 216/415 [01:32<00:56,  3.50it/s] 52%|█████▏    | 217/415 [01:32<00:56,  3.50it/s] 53%|█████▎    | 218/415 [01:32<00:56,  3.50it/s] 53%|█████▎    | 219/415 [01:33<00:56,  3.49it/s] 53%|█████▎    | 220/415 [01:33<00:55,  3.49it/s] 53%|█████▎    | 221/415 [01:33<00:55,  3.49it/s] 53%|█████▎    | 222/415 [01:33<00:55,  3.49it/s] 54%|█████▎    | 223/415 [01:34<00:54,  3.49it/s] 54%|█████▍    | 224/415 [01:34<00:54,  3.49it/s] 54%|█████▍    | 225/415 [01:34<00:54,  3.49it/s] 54%|█████▍    | 226/415 [01:35<00:54,  3.49it/s] 55%|█████▍    | 227/415 [01:35<00:53,  3.49it/s] 55%|█████▍    | 228/415 [01:35<00:53,  3.50it/s] 55%|█████▌    | 229/415 [01:35<00:53,  3.49it/s] 55%|█████▌    | 230/415 [01:36<00:52,  3.49it/s] 56%|█████▌    | 231/415 [01:36<00:52,  3.49it/s] 56%|█████▌    | 232/415 [01:36<00:52,  3.49it/s] 56%|█████▌    | 233/415 [01:37<00:52,  3.49it/s] 56%|█████▋    | 234/415 [01:37<00:51,  3.49it/s] 57%|█████▋    | 235/415 [01:37<00:51,  3.49it/s] 57%|█████▋    | 236/415 [01:37<00:51,  3.49it/s] 57%|█████▋    | 237/415 [01:38<00:51,  3.49it/s] 57%|█████▋    | 238/415 [01:38<00:50,  3.49it/s] 58%|█████▊    | 239/415 [01:38<00:50,  3.49it/s] 58%|█████▊    | 240/415 [01:39<00:50,  3.49it/s] 58%|█████▊    | 241/415 [01:39<00:49,  3.49it/s] 58%|█████▊    | 242/415 [01:39<00:49,  3.48it/s] 59%|█████▊    | 243/415 [01:39<00:49,  3.49it/s] 59%|█████▉    | 244/415 [01:40<00:49,  3.49it/s] 59%|█████▉    | 245/415 [01:40<00:48,  3.49it/s] 59%|█████▉    | 246/415 [01:40<00:48,  3.49it/s] 60%|█████▉    | 247/415 [01:41<00:48,  3.49it/s] 60%|█████▉    | 248/415 [01:41<00:47,  3.49it/s] 60%|██████    | 249/415 [01:41<00:44,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 05:15:49,239 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:15:49,239 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:15:49,239 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0526, 'eval_samples_per_second': 353.111, 'eval_steps_per_second': 44.206, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.21it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.23it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.76it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.56it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.85it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.32it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.05it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.99it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.05it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.34it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.36it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.48it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.31it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.11it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.03it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.95it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.00it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.04it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.19it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.07it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.29it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.25it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.08it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.08it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.91it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.97it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.12it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.16it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.33it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.37it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.27it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.10it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.96it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.92it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.02it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.24it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.37it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.26it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.22it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.11it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.05it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.98it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.90it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.93it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.16it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.32it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.33it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.08it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.03it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.99it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.89it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.00it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.02it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.23it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.38it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.36it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.20it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.98it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.90it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.88it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 42.64it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.19it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.63it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.70it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.89it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.83it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.75it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.94it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.87it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.95it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.20it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.26it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.23it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.18it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.05it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.04it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.96it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.02it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.03it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.21it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.38it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.26it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.23it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.21it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.10it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.04it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.00it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.06it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.21it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.30it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.11it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.96it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.09it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.04it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.99it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.20it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.31it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.27it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.22it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.08it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.11it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.07it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.99it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.06it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.11it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.26it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.19it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.13it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.16it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.15it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.15it/s][A 60%|██████    | 249/415 [01:54<00:44,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:16:02,332 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 05:16:02,348 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:16:04,225 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:16:04,238 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:16:04,246 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [01:57<13:21,  4.86s/it] 60%|██████    | 251/415 [01:57<09:31,  3.49s/it] 61%|██████    | 252/415 [01:57<06:52,  2.53s/it] 61%|██████    | 253/415 [01:58<05:00,  1.85s/it] 61%|██████    | 254/415 [01:58<03:42,  1.38s/it] 61%|██████▏   | 255/415 [01:58<02:48,  1.06s/it] 62%|██████▏   | 256/415 [01:58<02:11,  1.21it/s] 62%|██████▏   | 257/415 [01:59<01:44,  1.51it/s] 62%|██████▏   | 258/415 [01:59<01:26,  1.82it/s] 62%|██████▏   | 259/415 [01:59<01:13,  2.12it/s] 63%|██████▎   | 260/415 [02:00<01:04,  2.41it/s] 63%|██████▎   | 261/415 [02:00<00:58,  2.65it/s] 63%|██████▎   | 262/415 [02:00<00:53,  2.86it/s] 63%|██████▎   | 263/415 [02:00<00:50,  3.01it/s] 64%|██████▎   | 264/415 [02:01<00:48,  3.14it/s] 64%|██████▍   | 265/415 [02:01<00:46,  3.24it/s] 64%|██████▍   | 266/415 [02:01<00:44,  3.31it/s] 64%|██████▍   | 267/415 [02:02<00:44,  3.36it/s] 65%|██████▍   | 268/415 [02:02<00:43,  3.40it/s] 65%|██████▍   | 269/415 [02:02<00:42,  3.43it/s] 65%|██████▌   | 270/415 [02:02<00:42,  3.45it/s] 65%|██████▌   | 271/415 [02:03<00:41,  3.47it/s] 66%|██████▌   | 272/415 [02:03<00:41,  3.48it/s] 66%|██████▌   | 273/415 [02:03<00:40,  3.48it/s] 66%|██████▌   | 274/415 [02:04<00:40,  3.45it/s] 66%|██████▋   | 275/415 [02:04<00:40,  3.46it/s] 67%|██████▋   | 276/415 [02:04<00:40,  3.47it/s] 67%|██████▋   | 277/415 [02:04<00:39,  3.48it/s] 67%|██████▋   | 278/415 [02:05<00:39,  3.48it/s] 67%|██████▋   | 279/415 [02:05<00:39,  3.49it/s] 67%|██████▋   | 280/415 [02:05<00:38,  3.49it/s] 68%|██████▊   | 281/415 [02:06<00:38,  3.49it/s] 68%|██████▊   | 282/415 [02:06<00:38,  3.49it/s] 68%|██████▊   | 283/415 [02:06<00:37,  3.49it/s] 68%|██████▊   | 284/415 [02:06<00:37,  3.49it/s] 69%|██████▊   | 285/415 [02:07<00:37,  3.49it/s] 69%|██████▉   | 286/415 [02:07<00:36,  3.49it/s] 69%|██████▉   | 287/415 [02:07<00:36,  3.49it/s] 69%|██████▉   | 288/415 [02:08<00:36,  3.49it/s] 70%|██████▉   | 289/415 [02:08<00:36,  3.50it/s] 70%|██████▉   | 290/415 [02:08<00:35,  3.49it/s] 70%|███████   | 291/415 [02:08<00:35,  3.50it/s] 70%|███████   | 292/415 [02:09<00:35,  3.50it/s] 71%|███████   | 293/415 [02:09<00:34,  3.50it/s] 71%|███████   | 294/415 [02:09<00:34,  3.50it/s] 71%|███████   | 295/415 [02:10<00:34,  3.50it/s] 71%|███████▏  | 296/415 [02:10<00:34,  3.49it/s] 72%|███████▏  | 297/415 [02:10<00:33,  3.49it/s] 72%|███████▏  | 298/415 [02:10<00:33,  3.50it/s] 72%|███████▏  | 299/415 [02:11<00:33,  3.50it/s] 72%|███████▏  | 300/415 [02:11<00:32,  3.50it/s] 73%|███████▎  | 301/415 [02:11<00:32,  3.50it/s] 73%|███████▎  | 302/415 [02:12<00:32,  3.50it/s] 73%|███████▎  | 303/415 [02:12<00:32,  3.50it/s] 73%|███████▎  | 304/415 [02:12<00:31,  3.50it/s] 73%|███████▎  | 305/415 [02:12<00:31,  3.50it/s] 74%|███████▎  | 306/415 [02:13<00:31,  3.50it/s] 74%|███████▍  | 307/415 [02:13<00:31,  3.48it/s] 74%|███████▍  | 308/415 [02:13<00:30,  3.49it/s] 74%|███████▍  | 309/415 [02:14<00:30,  3.49it/s] 75%|███████▍  | 310/415 [02:14<00:30,  3.49it/s] 75%|███████▍  | 311/415 [02:14<00:29,  3.49it/s] 75%|███████▌  | 312/415 [02:14<00:29,  3.50it/s] 75%|███████▌  | 313/415 [02:15<00:29,  3.49it/s] 76%|███████▌  | 314/415 [02:15<00:28,  3.50it/s] 76%|███████▌  | 315/415 [02:15<00:28,  3.50it/s] 76%|███████▌  | 316/415 [02:16<00:28,  3.49it/s] 76%|███████▋  | 317/415 [02:16<00:28,  3.50it/s] 77%|███████▋  | 318/415 [02:16<00:27,  3.48it/s] 77%|███████▋  | 319/415 [02:16<00:27,  3.48it/s] 77%|███████▋  | 320/415 [02:17<00:27,  3.49it/s] 77%|███████▋  | 321/415 [02:17<00:26,  3.49it/s] 78%|███████▊  | 322/415 [02:17<00:26,  3.49it/s] 78%|███████▊  | 323/415 [02:18<00:26,  3.49it/s] 78%|███████▊  | 324/415 [02:18<00:26,  3.49it/s] 78%|███████▊  | 325/415 [02:18<00:25,  3.49it/s] 79%|███████▊  | 326/415 [02:18<00:25,  3.49it/s] 79%|███████▉  | 327/415 [02:19<00:25,  3.50it/s] 79%|███████▉  | 328/415 [02:19<00:24,  3.49it/s] 79%|███████▉  | 329/415 [02:19<00:24,  3.48it/s] 80%|███████▉  | 330/415 [02:20<00:24,  3.49it/s] 80%|███████▉  | 331/415 [02:20<00:24,  3.49it/s] 80%|████████  | 332/415 [02:20<00:22,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 05:16:28,247 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:16:28,248 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:16:28,248 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0824, 'eval_samples_per_second': 352.304, 'eval_steps_per_second': 44.105, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.32it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.47it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.80it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.57it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.86it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.23it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.01it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.01it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.07it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.14it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.15it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.40it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.41it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.24it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.06it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.86it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.86it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.14it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.20it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.23it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.32it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.20it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.02it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.96it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.94it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.99it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.23it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.29it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.28it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.32it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.17it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.97it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.94it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.97it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.10it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.32it/s][A
 34%|███▍      | 197/577 [00:04<00:09, 41.56it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 42.49it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.06it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.12it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.46it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.63it/s][A
 39%|███▉      | 227/577 [00:05<00:08, 43.74it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.98it/s][A
 41%|████      | 237/577 [00:05<00:07, 43.82it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.87it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.20it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.13it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.09it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.98it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.95it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.01it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.03it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.98it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.21it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.17it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.22it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.17it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.07it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.95it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.97it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.19it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.23it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.17it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.07it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.95it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.96it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.86it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.04it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.05it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.03it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.13it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.18it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 44.10it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.97it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.99it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.05it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.18it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.29it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.08it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.20it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.20it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.10it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.93it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.03it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.03it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.13it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.26it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.26it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.23it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.14it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.05it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.89it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.97it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.10it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.13it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.16it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.21it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.23it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.92it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.97it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.10it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.13it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.19it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.19it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.19it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.03it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.03it/s][A 80%|████████  | 332/415 [02:33<00:22,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:16:41,366 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 05:16:41,386 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:16:43,316 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:16:43,333 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:16:43,343 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [02:36<06:41,  4.90s/it] 80%|████████  | 334/415 [02:36<04:44,  3.51s/it] 81%|████████  | 335/415 [02:36<03:23,  2.54s/it] 81%|████████  | 336/415 [02:37<02:27,  1.86s/it] 81%|████████  | 337/415 [02:37<01:48,  1.39s/it] 81%|████████▏ | 338/415 [02:37<01:21,  1.06s/it] 82%|████████▏ | 339/415 [02:38<01:02,  1.21it/s] 82%|████████▏ | 340/415 [02:38<00:49,  1.51it/s] 82%|████████▏ | 341/415 [02:38<00:40,  1.83it/s] 82%|████████▏ | 342/415 [02:38<00:34,  2.14it/s] 83%|████████▎ | 343/415 [02:39<00:29,  2.42it/s] 83%|████████▎ | 344/415 [02:39<00:26,  2.68it/s] 83%|████████▎ | 345/415 [02:39<00:24,  2.90it/s] 83%|████████▎ | 346/415 [02:39<00:22,  3.07it/s] 84%|████████▎ | 347/415 [02:40<00:21,  3.20it/s] 84%|████████▍ | 348/415 [02:40<00:20,  3.30it/s] 84%|████████▍ | 349/415 [02:40<00:19,  3.38it/s] 84%|████████▍ | 350/415 [02:41<00:18,  3.43it/s] 85%|████████▍ | 351/415 [02:41<00:18,  3.47it/s] 85%|████████▍ | 352/415 [02:41<00:18,  3.50it/s] 85%|████████▌ | 353/415 [02:41<00:17,  3.52it/s] 85%|████████▌ | 354/415 [02:42<00:17,  3.52it/s] 86%|████████▌ | 355/415 [02:42<00:16,  3.54it/s] 86%|████████▌ | 356/415 [02:42<00:16,  3.55it/s] 86%|████████▌ | 357/415 [02:43<00:16,  3.55it/s] 86%|████████▋ | 358/415 [02:43<00:16,  3.56it/s] 87%|████████▋ | 359/415 [02:43<00:15,  3.56it/s] 87%|████████▋ | 360/415 [02:43<00:15,  3.56it/s] 87%|████████▋ | 361/415 [02:44<00:15,  3.56it/s] 87%|████████▋ | 362/415 [02:44<00:14,  3.56it/s] 87%|████████▋ | 363/415 [02:44<00:14,  3.56it/s] 88%|████████▊ | 364/415 [02:45<00:14,  3.56it/s] 88%|████████▊ | 365/415 [02:45<00:14,  3.56it/s] 88%|████████▊ | 366/415 [02:45<00:13,  3.56it/s] 88%|████████▊ | 367/415 [02:45<00:13,  3.56it/s] 89%|████████▊ | 368/415 [02:46<00:13,  3.56it/s] 89%|████████▉ | 369/415 [02:46<00:12,  3.57it/s] 89%|████████▉ | 370/415 [02:46<00:12,  3.57it/s] 89%|████████▉ | 371/415 [02:47<00:12,  3.57it/s] 90%|████████▉ | 372/415 [02:47<00:12,  3.57it/s] 90%|████████▉ | 373/415 [02:47<00:11,  3.57it/s] 90%|█████████ | 374/415 [02:47<00:11,  3.57it/s] 90%|█████████ | 375/415 [02:48<00:11,  3.56it/s] 91%|█████████ | 376/415 [02:48<00:10,  3.55it/s] 91%|█████████ | 377/415 [02:48<00:10,  3.55it/s] 91%|█████████ | 378/415 [02:49<00:10,  3.43it/s] 91%|█████████▏| 379/415 [02:49<00:10,  3.45it/s] 92%|█████████▏| 380/415 [02:49<00:10,  3.48it/s] 92%|█████████▏| 381/415 [02:49<00:09,  3.51it/s] 92%|█████████▏| 382/415 [02:50<00:09,  3.53it/s] 92%|█████████▏| 383/415 [02:50<00:09,  3.54it/s] 93%|█████████▎| 384/415 [02:50<00:08,  3.55it/s] 93%|█████████▎| 385/415 [02:50<00:08,  3.55it/s] 93%|█████████▎| 386/415 [02:51<00:08,  3.56it/s] 93%|█████████▎| 387/415 [02:51<00:07,  3.55it/s] 93%|█████████▎| 388/415 [02:51<00:07,  3.56it/s] 94%|█████████▎| 389/415 [02:52<00:07,  3.56it/s] 94%|█████████▍| 390/415 [02:52<00:07,  3.56it/s] 94%|█████████▍| 391/415 [02:52<00:06,  3.56it/s] 94%|█████████▍| 392/415 [02:52<00:06,  3.57it/s] 95%|█████████▍| 393/415 [02:53<00:06,  3.57it/s] 95%|█████████▍| 394/415 [02:53<00:05,  3.57it/s] 95%|█████████▌| 395/415 [02:53<00:05,  3.57it/s] 95%|█████████▌| 396/415 [02:54<00:05,  3.57it/s] 96%|█████████▌| 397/415 [02:54<00:05,  3.57it/s] 96%|█████████▌| 398/415 [02:54<00:04,  3.55it/s] 96%|█████████▌| 399/415 [02:54<00:04,  3.55it/s] 96%|█████████▋| 400/415 [02:55<00:04,  3.56it/s] 97%|█████████▋| 401/415 [02:55<00:03,  3.56it/s] 97%|█████████▋| 402/415 [02:55<00:03,  3.56it/s] 97%|█████████▋| 403/415 [02:56<00:03,  3.56it/s] 97%|█████████▋| 404/415 [02:56<00:03,  3.56it/s] 98%|█████████▊| 405/415 [02:56<00:02,  3.56it/s] 98%|█████████▊| 406/415 [02:56<00:02,  3.56it/s] 98%|█████████▊| 407/415 [02:57<00:02,  3.56it/s] 98%|█████████▊| 408/415 [02:57<00:01,  3.56it/s] 99%|█████████▊| 409/415 [02:57<00:01,  3.56it/s] 99%|█████████▉| 410/415 [02:57<00:01,  3.56it/s] 99%|█████████▉| 411/415 [02:58<00:01,  3.56it/s] 99%|█████████▉| 412/415 [02:58<00:00,  3.57it/s]100%|█████████▉| 413/415 [02:58<00:00,  3.57it/s]100%|█████████▉| 414/415 [02:59<00:00,  3.55it/s]100%|██████████| 415/415 [02:59<00:00,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 05:17:06,959 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:17:06,959 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:17:06,959 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0991, 'eval_samples_per_second': 351.855, 'eval_steps_per_second': 44.049, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.90it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.39it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.49it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.46it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.81it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.39it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.06it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.05it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.12it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.30it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.37it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.42it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.35it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.17it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.98it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.82it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.85it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.98it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.18it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.25it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.38it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.37it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.26it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.78it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.93it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.98it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.17it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.32it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.41it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.33it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.35it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.14it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.93it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.92it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.90it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.18it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.38it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.29it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.41it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.24it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.12it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.04it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.91it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.92it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.20it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.42it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.47it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.33it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.21it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.91it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.00it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.12it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.05it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.25it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.34it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.39it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.26it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.05it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.02it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.91it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.01it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.98it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.18it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.32it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.22it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.85it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.90it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.88it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.03it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.08it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.01it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.11it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.25it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.25it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.06it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.14it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.12it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.00it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.19it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.25it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.26it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.27it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.05it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.97it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.17it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.16it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.14it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.23it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.33it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.24it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.11it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.10it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.14it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.29it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.33it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.28it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.03it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.17it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.17it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.01it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.02it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.15it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.15it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.23it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.02it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.01it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.05it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.08it/s][A                                                 
                                                 [A100%|██████████| 415/415 [03:12<00:00,  3.74it/s]
100%|██████████| 577/577 [00:13<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:17:20,046 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 05:17:20,072 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:17:21,976 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:17:21,997 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:17:22,008 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:17:22,337 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:17:22,337 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83 (score: 1.12112557888031).
                                                 100%|██████████| 415/415 [03:16<00:00,  3.74it/s]100%|██████████| 415/415 [03:16<00:00,  2.11it/s]
[INFO|trainer.py:1894] 2023-08-28 05:17:24,155 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 05:17:24,169 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:17:25,953 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:17:25,974 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:17:25,979 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:17:26,163 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   train_runtime            = 0:03:16.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   train_samples_per_second =    134.826
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:26,164 >>   train_steps_per_second   =      2.111
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.065, 'eval_samples_per_second': 352.776, 'eval_steps_per_second': 44.164, 'epoch': 5.0}
{'train_runtime': 196.5494, 'train_samples_per_second': 134.826, 'train_steps_per_second': 2.111, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:17:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:17:26,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:17:26,203 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 05:17:26,203 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.87it/s]  2%|▏         | 12/577 [00:00<00:11, 48.39it/s]  3%|▎         | 17/577 [00:00<00:11, 46.81it/s]  4%|▍         | 22/577 [00:00<00:12, 46.08it/s]  5%|▍         | 27/577 [00:00<00:12, 45.78it/s]  6%|▌         | 32/577 [00:00<00:12, 45.39it/s]  6%|▋         | 37/577 [00:00<00:11, 45.27it/s]  7%|▋         | 42/577 [00:00<00:11, 44.75it/s]  8%|▊         | 47/577 [00:01<00:11, 44.27it/s]  9%|▉         | 52/577 [00:01<00:11, 44.03it/s] 10%|▉         | 57/577 [00:01<00:11, 44.10it/s] 11%|█         | 62/577 [00:01<00:11, 44.26it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.25it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.54it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.60it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.56it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.40it/s] 16%|█▌        | 92/577 [00:02<00:10, 44.12it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.91it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.99it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.08it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s] 20%|██        | 117/577 [00:02<00:10, 44.40it/s] 21%|██        | 122/577 [00:02<00:10, 44.58it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.51it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.33it/s] 24%|██▎       | 137/577 [00:03<00:09, 44.20it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.98it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.99it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.01it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.21it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.34it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.55it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.57it/s] 31%|███       | 177/577 [00:03<00:09, 44.34it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.12it/s] 32%|███▏      | 187/577 [00:04<00:08, 44.11it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.86it/s] 34%|███▍      | 197/577 [00:04<00:08, 43.99it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.13it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.33it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.54it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.46it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.35it/s] 39%|███▉      | 227/577 [00:05<00:07, 44.16it/s] 40%|████      | 232/577 [00:05<00:07, 44.07it/s] 41%|████      | 237/577 [00:05<00:07, 44.06it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.18it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.19it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.36it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.52it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.49it/s] 46%|████▋     | 267/577 [00:06<00:06, 44.31it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.08it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.91it/s] 49%|████▉     | 282/577 [00:06<00:06, 43.92it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.10it/s] 51%|█████     | 292/577 [00:06<00:06, 44.15it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.28it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.45it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.51it/s] 54%|█████▍    | 312/577 [00:07<00:05, 44.36it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.15it/s] 56%|█████▌    | 322/577 [00:07<00:05, 43.95it/s] 57%|█████▋    | 327/577 [00:07<00:05, 43.97it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.15it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.22it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.28it/s] 60%|██████    | 347/577 [00:07<00:05, 44.35it/s] 61%|██████    | 352/577 [00:07<00:05, 44.48it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.37it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.09it/s] 64%|██████▎   | 367/577 [00:08<00:04, 43.98it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.02it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.16it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.22it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.39it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.42it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.35it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.32it/s] 71%|███████   | 407/577 [00:09<00:03, 44.14it/s] 71%|███████▏  | 412/577 [00:09<00:03, 44.06it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.01it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.23it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.38it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.46it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.39it/s] 77%|███████▋  | 442/577 [00:09<00:03, 44.33it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.23it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.09it/s] 79%|███████▉  | 457/577 [00:10<00:02, 43.97it/s] 80%|████████  | 462/577 [00:10<00:02, 43.94it/s] 81%|████████  | 467/577 [00:10<00:02, 44.21it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.18it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.47it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.37it/s] 84%|████████▍ | 487/577 [00:10<00:02, 44.36it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.20it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.09it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.08it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.16it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.36it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.26it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.29it/s] 92%|█████████▏| 532/577 [00:11<00:01, 44.32it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.26it/s] 94%|█████████▍| 542/577 [00:12<00:00, 44.14it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.05it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.16it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.21it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.33it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.44it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.18it/s]100%|██████████| 577/577 [00:13<00:00, 44.31it/s]100%|██████████| 577/577 [00:13<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:17:39,238 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   eval_loss               =     1.1211
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   eval_runtime            = 0:00:13.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   eval_samples_per_second =    353.594
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   eval_steps_per_second   =     44.266
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:17:39,238 >>   perplexity              =     3.0683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:45,865 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:45,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:45,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:45,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:45,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:17:46,468 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:17:46,469 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:17:47,054 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:17:48,127 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:17:48,127 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:50,969 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:50,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:50,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:50,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:50,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:17:51,685 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:17:51,687 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:17:52,246 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:17:52,414 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:17:52,414 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-166
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-332
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-83
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-415
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/checkpoint-249
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.43it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.41it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.44it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:16,  1.39it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:19,  1.45it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.46it/s]Extractor Predicting: 31it [00:22,  1.42it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.42it/s]Extractor Predicting: 34it [00:24,  1.38it/s]Extractor Predicting: 35it [00:24,  1.41it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:26,  1.41it/s]Extractor Predicting: 39it [00:27,  1.38it/s]Extractor Predicting: 40it [00:28,  1.38it/s]Extractor Predicting: 41it [00:29,  1.35it/s]Extractor Predicting: 42it [00:30,  1.29it/s]Extractor Predicting: 43it [00:30,  1.31it/s]Extractor Predicting: 44it [00:31,  1.30it/s]Extractor Predicting: 45it [00:32,  1.31it/s]Extractor Predicting: 46it [00:33,  1.32it/s]Extractor Predicting: 47it [00:33,  1.32it/s]Extractor Predicting: 48it [00:34,  1.33it/s]Extractor Predicting: 49it [00:35,  1.38it/s]Extractor Predicting: 50it [00:35,  1.38it/s]Extractor Predicting: 51it [00:36,  1.40it/s]Extractor Predicting: 52it [00:37,  1.37it/s]Extractor Predicting: 53it [00:38,  1.35it/s]Extractor Predicting: 54it [00:38,  1.36it/s]Extractor Predicting: 55it [00:39,  1.36it/s]Extractor Predicting: 56it [00:40,  1.39it/s]Extractor Predicting: 57it [00:41,  1.37it/s]Extractor Predicting: 58it [00:41,  1.35it/s]Extractor Predicting: 59it [00:42,  1.39it/s]Extractor Predicting: 60it [00:43,  1.38it/s]Extractor Predicting: 61it [00:44,  1.36it/s]Extractor Predicting: 62it [00:44,  1.34it/s]Extractor Predicting: 63it [00:45,  1.33it/s]Extractor Predicting: 64it [00:46,  1.31it/s]Extractor Predicting: 65it [00:47,  1.33it/s]Extractor Predicting: 66it [00:47,  1.37it/s]Extractor Predicting: 67it [00:48,  1.38it/s]Extractor Predicting: 68it [00:49,  1.41it/s]Extractor Predicting: 69it [00:49,  1.42it/s]Extractor Predicting: 70it [00:50,  1.41it/s]Extractor Predicting: 71it [00:51,  1.43it/s]Extractor Predicting: 72it [00:51,  1.47it/s]Extractor Predicting: 73it [00:52,  1.44it/s]Extractor Predicting: 74it [00:53,  1.46it/s]Extractor Predicting: 75it [00:53,  1.46it/s]Extractor Predicting: 76it [00:54,  1.42it/s]Extractor Predicting: 77it [00:55,  1.41it/s]Extractor Predicting: 78it [00:56,  1.43it/s]Extractor Predicting: 79it [00:56,  1.43it/s]Extractor Predicting: 80it [00:57,  1.43it/s]Extractor Predicting: 81it [00:58,  1.41it/s]Extractor Predicting: 82it [00:58,  1.40it/s]Extractor Predicting: 83it [00:59,  1.40it/s]Extractor Predicting: 84it [01:00,  1.43it/s]Extractor Predicting: 85it [01:00,  1.50it/s]Extractor Predicting: 86it [01:01,  1.49it/s]Extractor Predicting: 87it [01:02,  1.52it/s]Extractor Predicting: 88it [01:03,  1.43it/s]Extractor Predicting: 89it [01:03,  1.43it/s]Extractor Predicting: 90it [01:04,  1.41it/s]Extractor Predicting: 91it [01:05,  1.42it/s]Extractor Predicting: 92it [01:05,  1.44it/s]Extractor Predicting: 93it [01:06,  1.45it/s]Extractor Predicting: 94it [01:07,  1.46it/s]Extractor Predicting: 95it [01:07,  1.45it/s]Extractor Predicting: 96it [01:08,  1.46it/s]Extractor Predicting: 97it [01:09,  1.46it/s]Extractor Predicting: 98it [01:10,  1.41it/s]Extractor Predicting: 99it [01:10,  1.43it/s]Extractor Predicting: 100it [01:11,  1.40it/s]Extractor Predicting: 101it [01:12,  1.40it/s]Extractor Predicting: 102it [01:12,  1.42it/s]Extractor Predicting: 103it [01:13,  1.40it/s]Extractor Predicting: 104it [01:14,  1.43it/s]Extractor Predicting: 105it [01:14,  1.45it/s]Extractor Predicting: 106it [01:15,  1.40it/s]Extractor Predicting: 107it [01:16,  1.44it/s]Extractor Predicting: 108it [01:16,  1.46it/s]Extractor Predicting: 109it [01:17,  1.43it/s]Extractor Predicting: 110it [01:18,  1.43it/s]Extractor Predicting: 111it [01:19,  1.41it/s]Extractor Predicting: 112it [01:19,  1.41it/s]Extractor Predicting: 113it [01:20,  1.41it/s]Extractor Predicting: 114it [01:21,  1.42it/s]Extractor Predicting: 115it [01:21,  1.42it/s]Extractor Predicting: 116it [01:22,  1.42it/s]Extractor Predicting: 117it [01:23,  1.45it/s]Extractor Predicting: 118it [01:24,  1.45it/s]Extractor Predicting: 119it [01:24,  1.50it/s]Extractor Predicting: 120it [01:25,  1.49it/s]Extractor Predicting: 121it [01:26,  1.47it/s]Extractor Predicting: 122it [01:26,  1.45it/s]Extractor Predicting: 123it [01:27,  1.44it/s]Extractor Predicting: 124it [01:28,  1.46it/s]Extractor Predicting: 125it [01:28,  1.42it/s]Extractor Predicting: 126it [01:29,  1.45it/s]Extractor Predicting: 127it [01:30,  1.48it/s]Extractor Predicting: 128it [01:30,  1.45it/s]Extractor Predicting: 129it [01:31,  1.44it/s]Extractor Predicting: 130it [01:32,  1.40it/s]Extractor Predicting: 131it [01:33,  1.39it/s]Extractor Predicting: 132it [01:33,  1.30it/s]Extractor Predicting: 133it [01:34,  1.33it/s]Extractor Predicting: 134it [01:35,  1.34it/s]Extractor Predicting: 135it [01:36,  1.35it/s]Extractor Predicting: 136it [01:36,  1.37it/s]Extractor Predicting: 137it [01:37,  1.38it/s]Extractor Predicting: 138it [01:38,  1.38it/s]Extractor Predicting: 139it [01:39,  1.36it/s]Extractor Predicting: 140it [01:39,  1.35it/s]Extractor Predicting: 141it [01:40,  1.34it/s]Extractor Predicting: 142it [01:41,  1.33it/s]Extractor Predicting: 143it [01:42,  1.35it/s]Extractor Predicting: 144it [01:42,  1.34it/s]Extractor Predicting: 145it [01:43,  1.34it/s]Extractor Predicting: 146it [01:44,  1.30it/s]Extractor Predicting: 147it [01:45,  1.31it/s]Extractor Predicting: 148it [01:45,  1.34it/s]Extractor Predicting: 149it [01:46,  1.36it/s]Extractor Predicting: 150it [01:47,  1.35it/s]Extractor Predicting: 151it [01:47,  1.38it/s]Extractor Predicting: 152it [01:48,  1.36it/s]Extractor Predicting: 153it [01:49,  1.38it/s]Extractor Predicting: 154it [01:50,  1.37it/s]Extractor Predicting: 155it [01:50,  1.37it/s]Extractor Predicting: 156it [01:51,  1.35it/s]Extractor Predicting: 157it [01:52,  1.37it/s]Extractor Predicting: 158it [01:53,  1.37it/s]Extractor Predicting: 159it [01:53,  1.37it/s]Extractor Predicting: 160it [01:54,  1.37it/s]Extractor Predicting: 161it [01:55,  1.40it/s]Extractor Predicting: 162it [01:55,  1.39it/s]Extractor Predicting: 163it [01:56,  1.36it/s]Extractor Predicting: 164it [01:57,  1.35it/s]Extractor Predicting: 165it [01:58,  1.36it/s]Extractor Predicting: 166it [01:58,  1.34it/s]Extractor Predicting: 167it [01:59,  1.36it/s]Extractor Predicting: 168it [02:00,  1.36it/s]Extractor Predicting: 169it [02:01,  1.37it/s]Extractor Predicting: 170it [02:01,  1.37it/s]Extractor Predicting: 171it [02:02,  1.64it/s]Extractor Predicting: 171it [02:02,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:02,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:02,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:02,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:02,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:02,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:20:02,754 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:20:02,755 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:20:03,430 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:20:04,444 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:20:04,444 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:06,954 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:06,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:06,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:06,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:20:06,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:20:07,297 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:20:07,298 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:20:07,553 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:20:07,713 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:20:07,713 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:07,  1.37it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:10,  1.38it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.42it/s]Extractor Predicting: 27it [00:19,  1.44it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.41it/s]Extractor Predicting: 31it [00:22,  1.40it/s]Extractor Predicting: 32it [00:22,  1.41it/s]Extractor Predicting: 33it [00:23,  1.42it/s]Extractor Predicting: 34it [00:24,  1.41it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:25,  1.41it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:26,  1.42it/s]Extractor Predicting: 39it [00:27,  1.40it/s]Extractor Predicting: 40it [00:28,  1.42it/s]Extractor Predicting: 41it [00:29,  1.42it/s]Extractor Predicting: 42it [00:29,  1.43it/s]Extractor Predicting: 43it [00:30,  1.43it/s]Extractor Predicting: 44it [00:31,  1.44it/s]Extractor Predicting: 45it [00:31,  1.45it/s]Extractor Predicting: 46it [00:32,  1.46it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:33,  1.43it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.43it/s]Extractor Predicting: 51it [00:36,  1.43it/s]Extractor Predicting: 52it [00:36,  1.44it/s]Extractor Predicting: 53it [00:37,  1.43it/s]Extractor Predicting: 54it [00:38,  1.41it/s]Extractor Predicting: 55it [00:38,  1.40it/s]Extractor Predicting: 56it [00:39,  1.42it/s]Extractor Predicting: 57it [00:40,  1.43it/s]Extractor Predicting: 58it [00:40,  1.43it/s]Extractor Predicting: 59it [00:41,  1.43it/s]Extractor Predicting: 60it [00:42,  1.40it/s]Extractor Predicting: 61it [00:43,  1.40it/s]Extractor Predicting: 62it [00:43,  1.39it/s]Extractor Predicting: 63it [00:44,  1.42it/s]Extractor Predicting: 64it [00:45,  1.42it/s]Extractor Predicting: 65it [00:45,  1.43it/s]Extractor Predicting: 66it [00:46,  1.42it/s]Extractor Predicting: 67it [00:47,  1.44it/s]Extractor Predicting: 68it [00:47,  1.45it/s]Extractor Predicting: 69it [00:48,  1.47it/s]Extractor Predicting: 70it [00:49,  1.45it/s]Extractor Predicting: 71it [00:50,  1.39it/s]Extractor Predicting: 72it [00:50,  1.42it/s]Extractor Predicting: 73it [00:51,  1.40it/s]Extractor Predicting: 74it [00:52,  1.43it/s]Extractor Predicting: 75it [00:52,  1.43it/s]Extractor Predicting: 76it [00:53,  1.46it/s]Extractor Predicting: 77it [00:54,  1.49it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:55,  1.47it/s]Extractor Predicting: 80it [00:56,  1.48it/s]Extractor Predicting: 81it [00:56,  1.47it/s]Extractor Predicting: 82it [00:57,  1.48it/s]Extractor Predicting: 83it [00:58,  1.47it/s]Extractor Predicting: 84it [00:58,  1.44it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.46it/s]Extractor Predicting: 87it [01:01,  1.43it/s]Extractor Predicting: 88it [01:01,  1.45it/s]Extractor Predicting: 89it [01:02,  1.44it/s]Extractor Predicting: 90it [01:03,  1.42it/s]Extractor Predicting: 91it [01:03,  1.43it/s]Extractor Predicting: 92it [01:04,  1.42it/s]Extractor Predicting: 93it [01:05,  1.43it/s]Extractor Predicting: 94it [01:05,  1.42it/s]Extractor Predicting: 95it [01:06,  1.41it/s]Extractor Predicting: 96it [01:07,  1.37it/s]Extractor Predicting: 97it [01:08,  1.41it/s]Extractor Predicting: 98it [01:08,  1.42it/s]Extractor Predicting: 99it [01:09,  1.50it/s]Extractor Predicting: 100it [01:10,  1.49it/s]Extractor Predicting: 101it [01:10,  1.46it/s]Extractor Predicting: 102it [01:11,  1.47it/s]Extractor Predicting: 103it [01:12,  1.47it/s]Extractor Predicting: 104it [01:12,  1.47it/s]Extractor Predicting: 105it [01:13,  1.46it/s]Extractor Predicting: 106it [01:14,  1.47it/s]Extractor Predicting: 107it [01:14,  1.48it/s]Extractor Predicting: 108it [01:15,  1.47it/s]Extractor Predicting: 109it [01:16,  1.42it/s]Extractor Predicting: 110it [01:17,  1.26it/s]Extractor Predicting: 111it [01:18,  1.27it/s]Extractor Predicting: 112it [01:18,  1.29it/s]Extractor Predicting: 113it [01:19,  1.36it/s]Extractor Predicting: 114it [01:20,  1.39it/s]Extractor Predicting: 115it [01:20,  1.38it/s]Extractor Predicting: 116it [01:21,  1.39it/s]Extractor Predicting: 117it [01:22,  1.39it/s]Extractor Predicting: 118it [01:23,  1.35it/s]Extractor Predicting: 119it [01:23,  1.38it/s]Extractor Predicting: 120it [01:24,  1.35it/s]Extractor Predicting: 121it [01:25,  1.34it/s]Extractor Predicting: 122it [01:26,  1.36it/s]Extractor Predicting: 123it [01:26,  1.37it/s]Extractor Predicting: 124it [01:27,  1.37it/s]Extractor Predicting: 125it [01:27,  1.73it/s]Extractor Predicting: 125it [01:27,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:41,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:41,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:41,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:41,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:41,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:21:41,973 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:21:41,974 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:21:42,651 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:21:43,694 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:21:43,696 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:45,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:45,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:45,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:45,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:21:45,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:21:46,295 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:21:46,296 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:21:46,554 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:21:46,712 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:21:46,716 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.77it/s]Extractor Predicting: 6it [00:04,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:21:51,254 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:21:51,255 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:21:51,260 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:21:51,261 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:21:51,262 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:21:54,401 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:21:54,405 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:21:54,415 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:21:54,415 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:21:54,421 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:21:54,426 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:21:54,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:55,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:56,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:57,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:57,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:59,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:59,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:00,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:01,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:02,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:03,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:04,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:04,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:05,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:06,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:07,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:08,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:09,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:10,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:10,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:11,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:12,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:13,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:14,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:14,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:20<03:07, 20.82s/it][WARNING|generation_utils.py:914] 2023-08-28 05:22:15,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:16,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:16,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:17,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:18,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:19,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:20,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:21,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:21,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:22,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:23,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:24,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:25,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:25,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:26,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:27,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:28,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:28,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:29,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:30,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:31,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:26, 18.29s/it][WARNING|generation_utils.py:914] 2023-08-28 05:22:32,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:32,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:33,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:34,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:35,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:35,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:36,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:37,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:38,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:38,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:39,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:40,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:41,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:41,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:42,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:43,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:44,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:44,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:45,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:46,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:47,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:48,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:48,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:49,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:50,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:51,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:51,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:52,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:53,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:20, 20.02s/it][WARNING|generation_utils.py:914] 2023-08-28 05:22:54,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:54,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:55,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:56,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:57,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:58,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:58,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:59,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:00,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:01,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:02,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:02,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:03,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:04,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:05,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:06,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:06,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:07,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:08,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:09,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:10,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:11,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:12,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:13,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:14,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:14,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:15,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:21<02:05, 20.98s/it][WARNING|generation_utils.py:914] 2023-08-28 05:23:16,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:17,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:18,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:18,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:19,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:20,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:20,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:21,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:22,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:23,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:24,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:24,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:25,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:26,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:26,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:27,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:28,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:29,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:29,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:30,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:31,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:31,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:32,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:37, 19.53s/it][WARNING|generation_utils.py:914] 2023-08-28 05:23:33,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:34,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:35,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:35,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:36,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:37,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:38,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:38,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:39,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:40,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:41,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:43,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:43,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:44,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:45,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:46,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:46,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:47,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:48,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:49,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:50,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:56<01:15, 18.77s/it][WARNING|generation_utils.py:914] 2023-08-28 05:23:50,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:51,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:52,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:53,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:53,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:54,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:55,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:56,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:57,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:57,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:58,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:23:59,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:00,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:00,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:01,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:02,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:03,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:04,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:04,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:05,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:06,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:07,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:08,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:09,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:15<00:56, 18.90s/it][WARNING|generation_utils.py:914] 2023-08-28 05:24:09,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:10,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:11,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:12,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:13,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:13,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:14,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:15,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:16,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:16,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:17,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:18,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:19,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:19,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:20,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:21,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:22,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:23,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:23,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:24,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:25,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:31<00:36, 18.03s/it][WARNING|generation_utils.py:914] 2023-08-28 05:24:26,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:26,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:27,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:28,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:29,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:30,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:30,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:31,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:32,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:33,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:33,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:34,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:35,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:36,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:37,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:38,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:38,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:39,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:40,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:41,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:42,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:42,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:43,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:44,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:45,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:51<00:18, 18.54s/it][WARNING|generation_utils.py:914] 2023-08-28 05:24:45,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:46,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:47,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:47,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:48,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:49,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:50,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:51,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:52,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:52,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:53,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:54,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:55,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:55,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:56,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:57,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:57,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:58,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:24:59,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:00,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:01,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:01,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:02,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:03,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:03,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:04,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:05,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:06,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:07,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:07,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:08,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:09,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:25:10,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:16<00:00, 20.59s/it]Generating: 100%|██████████| 10/10 [03:16<00:00, 19.63s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:15,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:15,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:15,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:15,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:15,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:25:16,620 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:25:16,621 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:25:17,248 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:25:18,353 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:25:18,359 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:21,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:21,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:21,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:21,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:21,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:25:21,887 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:25:21,890 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:25:22,456 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:25:22,638 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:25:22,638 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.15it/s]Extractor Estimating: 2it [00:01,  1.20it/s]Extractor Estimating: 3it [00:02,  1.27it/s]Extractor Estimating: 4it [00:03,  1.28it/s]Extractor Estimating: 5it [00:04,  1.11it/s]Extractor Estimating: 6it [00:05,  1.19it/s]Extractor Estimating: 7it [00:05,  1.22it/s]Extractor Estimating: 8it [00:06,  1.26it/s]Extractor Estimating: 9it [00:07,  1.32it/s]Extractor Estimating: 10it [00:08,  1.30it/s]Extractor Estimating: 11it [00:08,  1.25it/s]Extractor Estimating: 12it [00:09,  1.28it/s]Extractor Estimating: 13it [00:10,  1.30it/s]Extractor Estimating: 14it [00:11,  1.30it/s]Extractor Estimating: 15it [00:11,  1.32it/s]Extractor Estimating: 16it [00:12,  1.32it/s]Extractor Estimating: 17it [00:13,  1.31it/s]Extractor Estimating: 18it [00:14,  1.30it/s]Extractor Estimating: 19it [00:14,  1.28it/s]Extractor Estimating: 20it [00:15,  1.27it/s]Extractor Estimating: 21it [00:16,  1.29it/s]Extractor Estimating: 22it [00:17,  1.28it/s]Extractor Estimating: 23it [00:18,  1.26it/s]Extractor Estimating: 24it [00:19,  1.23it/s]Extractor Estimating: 25it [00:19,  1.26it/s]Extractor Estimating: 26it [00:20,  1.23it/s]Extractor Estimating: 27it [00:21,  1.25it/s]Extractor Estimating: 28it [00:22,  1.28it/s]Extractor Estimating: 29it [00:22,  1.32it/s]Extractor Estimating: 30it [00:23,  1.31it/s]Extractor Estimating: 31it [00:24,  1.31it/s]Extractor Estimating: 32it [00:25,  1.32it/s]Extractor Estimating: 33it [00:25,  1.33it/s]Extractor Estimating: 34it [00:26,  1.35it/s]Extractor Estimating: 35it [00:27,  1.29it/s]Extractor Estimating: 36it [00:28,  1.33it/s]Extractor Estimating: 37it [00:28,  1.31it/s]Extractor Estimating: 38it [00:29,  1.32it/s]Extractor Estimating: 39it [00:30,  1.34it/s]Extractor Estimating: 40it [00:31,  1.30it/s]Extractor Estimating: 41it [00:32,  1.26it/s]Extractor Estimating: 42it [00:32,  1.29it/s]Extractor Estimating: 43it [00:33,  1.33it/s]Extractor Estimating: 44it [00:34,  1.35it/s]Extractor Estimating: 45it [00:34,  1.35it/s]Extractor Estimating: 46it [00:35,  1.35it/s]Extractor Estimating: 47it [00:36,  1.33it/s]Extractor Estimating: 48it [00:37,  1.34it/s]Extractor Estimating: 49it [00:37,  1.32it/s]Extractor Estimating: 50it [00:38,  1.34it/s]Extractor Estimating: 51it [00:39,  1.32it/s]Extractor Estimating: 52it [00:40,  1.30it/s]Extractor Estimating: 53it [00:41,  1.31it/s]Extractor Estimating: 54it [00:41,  1.30it/s]Extractor Estimating: 55it [00:42,  1.31it/s]Extractor Estimating: 56it [00:43,  1.27it/s]Extractor Estimating: 57it [00:44,  1.25it/s]Extractor Estimating: 58it [00:44,  1.31it/s]Extractor Estimating: 59it [00:45,  1.33it/s]Extractor Estimating: 60it [00:46,  1.32it/s]Extractor Estimating: 61it [00:47,  1.29it/s]Extractor Estimating: 62it [00:47,  1.31it/s]Extractor Estimating: 63it [00:48,  1.34it/s]Extractor Estimating: 64it [00:49,  1.32it/s]Extractor Estimating: 65it [00:50,  1.31it/s]Extractor Estimating: 66it [00:50,  1.30it/s]Extractor Estimating: 67it [00:51,  1.29it/s]Extractor Estimating: 68it [00:52,  1.30it/s]Extractor Estimating: 69it [00:53,  1.31it/s]Extractor Estimating: 70it [00:54,  1.30it/s]Extractor Estimating: 71it [00:54,  1.29it/s]Extractor Estimating: 72it [00:55,  1.29it/s]Extractor Estimating: 73it [00:56,  1.29it/s]Extractor Estimating: 74it [00:57,  1.31it/s]Extractor Estimating: 75it [00:57,  1.32it/s]Extractor Estimating: 76it [00:58,  1.36it/s]Extractor Estimating: 77it [00:59,  1.30it/s]Extractor Estimating: 78it [01:00,  1.28it/s]Extractor Estimating: 79it [01:00,  1.34it/s]Extractor Estimating: 80it [01:01,  1.35it/s]Extractor Estimating: 81it [01:02,  1.34it/s]Extractor Estimating: 82it [01:03,  1.31it/s]Extractor Estimating: 83it [01:03,  1.30it/s]Extractor Estimating: 84it [01:04,  1.26it/s]Extractor Estimating: 85it [01:05,  1.25it/s]Extractor Estimating: 86it [01:06,  1.26it/s]Extractor Estimating: 87it [01:07,  1.16it/s]Extractor Estimating: 88it [01:08,  1.22it/s]Extractor Estimating: 89it [01:08,  1.23it/s]Extractor Estimating: 90it [01:09,  1.24it/s]Extractor Estimating: 91it [01:10,  1.24it/s]Extractor Estimating: 92it [01:11,  1.26it/s]Extractor Estimating: 93it [01:12,  1.26it/s]Extractor Estimating: 94it [01:12,  1.24it/s]Extractor Estimating: 95it [01:13,  1.23it/s]Extractor Estimating: 96it [01:14,  1.23it/s]Extractor Estimating: 97it [01:15,  1.22it/s]Extractor Estimating: 98it [01:16,  1.23it/s]Extractor Estimating: 99it [01:17,  1.23it/s]Extractor Estimating: 100it [01:17,  1.28it/s]Extractor Estimating: 101it [01:18,  1.28it/s]Extractor Estimating: 102it [01:19,  1.25it/s]Extractor Estimating: 103it [01:20,  1.27it/s]Extractor Estimating: 104it [01:20,  1.28it/s]Extractor Estimating: 105it [01:21,  1.30it/s]Extractor Estimating: 106it [01:22,  1.29it/s]Extractor Estimating: 107it [01:23,  1.27it/s]Extractor Estimating: 108it [01:23,  1.30it/s]Extractor Estimating: 109it [01:24,  1.30it/s]Extractor Estimating: 110it [01:25,  1.31it/s]Extractor Estimating: 111it [01:26,  1.30it/s]Extractor Estimating: 112it [01:27,  1.29it/s]Extractor Estimating: 113it [01:27,  1.27it/s]Extractor Estimating: 114it [01:28,  1.30it/s]Extractor Estimating: 115it [01:29,  1.30it/s]Extractor Estimating: 116it [01:30,  1.31it/s]Extractor Estimating: 117it [01:30,  1.32it/s]Extractor Estimating: 118it [01:31,  1.33it/s]Extractor Estimating: 119it [01:32,  1.33it/s]Extractor Estimating: 120it [01:33,  1.33it/s]Extractor Estimating: 121it [01:33,  1.33it/s]Extractor Estimating: 122it [01:34,  1.33it/s]Extractor Estimating: 123it [01:35,  1.32it/s]Extractor Estimating: 124it [01:36,  1.30it/s]Extractor Estimating: 125it [01:36,  1.32it/s]Extractor Estimating: 126it [01:37,  1.32it/s]Extractor Estimating: 127it [01:38,  1.28it/s]Extractor Estimating: 128it [01:39,  1.32it/s]Extractor Estimating: 129it [01:40,  1.28it/s]Extractor Estimating: 130it [01:40,  1.25it/s]Extractor Estimating: 131it [01:41,  1.23it/s]Extractor Estimating: 132it [01:42,  1.26it/s]Extractor Estimating: 133it [01:43,  1.25it/s]Extractor Estimating: 134it [01:44,  1.25it/s]Extractor Estimating: 135it [01:44,  1.28it/s]Extractor Estimating: 136it [01:45,  1.30it/s]Extractor Estimating: 137it [01:46,  1.25it/s]Extractor Estimating: 138it [01:47,  1.25it/s]Extractor Estimating: 139it [01:48,  1.22it/s]Extractor Estimating: 140it [01:48,  1.26it/s]Extractor Estimating: 141it [01:49,  1.31it/s]Extractor Estimating: 142it [01:50,  1.32it/s]Extractor Estimating: 143it [01:51,  1.31it/s]Extractor Estimating: 144it [01:51,  1.28it/s]Extractor Estimating: 145it [01:52,  1.27it/s]Extractor Estimating: 146it [01:53,  1.24it/s]Extractor Estimating: 147it [01:54,  1.26it/s]Extractor Estimating: 148it [01:55,  1.27it/s]Extractor Estimating: 149it [01:55,  1.30it/s]Extractor Estimating: 150it [01:56,  1.30it/s]Extractor Estimating: 151it [01:57,  1.33it/s]Extractor Estimating: 152it [01:58,  1.31it/s]Extractor Estimating: 153it [01:58,  1.32it/s]Extractor Estimating: 154it [01:59,  1.31it/s]Extractor Estimating: 155it [02:00,  1.31it/s]Extractor Estimating: 156it [02:01,  1.31it/s]Extractor Estimating: 157it [02:01,  1.30it/s]Extractor Estimating: 158it [02:02,  1.31it/s]Extractor Estimating: 159it [02:03,  1.32it/s]Extractor Estimating: 160it [02:04,  1.30it/s]Extractor Estimating: 161it [02:04,  1.29it/s]Extractor Estimating: 162it [02:05,  1.21it/s]Extractor Estimating: 163it [02:06,  1.25it/s]Extractor Estimating: 164it [02:07,  1.24it/s]Extractor Estimating: 165it [02:08,  1.23it/s]Extractor Estimating: 166it [02:09,  1.26it/s]Extractor Estimating: 167it [02:09,  1.26it/s]Extractor Estimating: 168it [02:10,  1.28it/s]Extractor Estimating: 169it [02:11,  1.28it/s]Extractor Estimating: 170it [02:12,  1.25it/s]Extractor Estimating: 171it [02:12,  1.29it/s]Extractor Estimating: 172it [02:13,  1.27it/s]Extractor Estimating: 173it [02:14,  1.31it/s]Extractor Estimating: 174it [02:15,  1.32it/s]Extractor Estimating: 175it [02:16,  1.30it/s]Extractor Estimating: 176it [02:16,  1.28it/s]Extractor Estimating: 177it [02:17,  1.28it/s]Extractor Estimating: 178it [02:18,  1.27it/s]Extractor Estimating: 179it [02:19,  1.26it/s]Extractor Estimating: 180it [02:20,  1.24it/s]Extractor Estimating: 181it [02:20,  1.28it/s]Extractor Estimating: 182it [02:21,  1.29it/s]Extractor Estimating: 183it [02:22,  1.26it/s]Extractor Estimating: 184it [02:23,  1.27it/s]Extractor Estimating: 185it [02:23,  1.29it/s]Extractor Estimating: 186it [02:24,  1.33it/s]Extractor Estimating: 187it [02:25,  1.32it/s]Extractor Estimating: 188it [02:26,  1.33it/s]Extractor Estimating: 189it [02:26,  1.32it/s]Extractor Estimating: 190it [02:27,  1.27it/s]Extractor Estimating: 191it [02:28,  1.30it/s]Extractor Estimating: 192it [02:29,  1.28it/s]Extractor Estimating: 193it [02:30,  1.28it/s]Extractor Estimating: 194it [02:30,  1.30it/s]Extractor Estimating: 195it [02:31,  1.31it/s]Extractor Estimating: 196it [02:32,  1.32it/s]Extractor Estimating: 197it [02:33,  1.32it/s]Extractor Estimating: 198it [02:33,  1.30it/s]Extractor Estimating: 199it [02:34,  1.25it/s]Extractor Estimating: 200it [02:35,  1.27it/s]Extractor Estimating: 201it [02:36,  1.27it/s]Extractor Estimating: 202it [02:36,  1.29it/s]Extractor Estimating: 203it [02:37,  1.31it/s]Extractor Estimating: 204it [02:38,  1.31it/s]Extractor Estimating: 205it [02:39,  1.31it/s]Extractor Estimating: 206it [02:39,  1.34it/s]Extractor Estimating: 207it [02:40,  1.33it/s]Extractor Estimating: 208it [02:41,  1.33it/s]Extractor Estimating: 209it [02:42,  1.31it/s]Extractor Estimating: 210it [02:42,  1.32it/s]Extractor Estimating: 211it [02:43,  1.30it/s]Extractor Estimating: 212it [02:44,  1.30it/s]Extractor Estimating: 213it [02:45,  1.28it/s]Extractor Estimating: 214it [02:46,  1.30it/s]Extractor Estimating: 215it [02:46,  1.28it/s]Extractor Estimating: 216it [02:47,  1.27it/s]Extractor Estimating: 217it [02:48,  1.28it/s]Extractor Estimating: 218it [02:49,  1.31it/s]Extractor Estimating: 219it [02:49,  1.33it/s]Extractor Estimating: 220it [02:50,  1.33it/s]Extractor Estimating: 221it [02:51,  1.30it/s]Extractor Estimating: 222it [02:52,  1.32it/s]Extractor Estimating: 223it [02:52,  1.34it/s]Extractor Estimating: 224it [02:53,  1.33it/s]Extractor Estimating: 225it [02:54,  1.31it/s]Extractor Estimating: 226it [02:55,  1.30it/s]Extractor Estimating: 227it [02:56,  1.31it/s]Extractor Estimating: 228it [02:56,  1.26it/s]Extractor Estimating: 229it [02:57,  1.29it/s]Extractor Estimating: 230it [02:58,  1.28it/s]Extractor Estimating: 231it [02:59,  1.26it/s]Extractor Estimating: 232it [02:59,  1.28it/s]Extractor Estimating: 233it [03:00,  1.27it/s]Extractor Estimating: 234it [03:01,  1.30it/s]Extractor Estimating: 235it [03:02,  1.30it/s]Extractor Estimating: 236it [03:03,  1.31it/s]Extractor Estimating: 237it [03:03,  1.31it/s]Extractor Estimating: 238it [03:04,  1.31it/s]Extractor Estimating: 239it [03:05,  1.17it/s]Extractor Estimating: 240it [03:06,  1.18it/s]Extractor Estimating: 241it [03:07,  1.19it/s]Extractor Estimating: 242it [03:07,  1.26it/s]Extractor Estimating: 243it [03:08,  1.27it/s]Extractor Estimating: 244it [03:09,  1.31it/s]Extractor Estimating: 245it [03:10,  1.34it/s]Extractor Estimating: 246it [03:10,  1.34it/s]Extractor Estimating: 247it [03:11,  1.34it/s]Extractor Estimating: 248it [03:12,  1.30it/s]Extractor Estimating: 249it [03:13,  1.33it/s]Extractor Estimating: 250it [03:14,  1.24it/s]Extractor Estimating: 250it [03:14,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:53,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:53,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:53,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:53,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:53,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:28:54,106 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:28:54,107 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:28:54,784 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:28:55,861 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:28:55,861 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:58,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:28:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:28:58,716 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:28:58,717 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:28:59,387 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:28:59,564 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:28:59,564 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:24:45,563 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:24:45,590 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5234 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 21252
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21352, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21352, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.312, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.307, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.305, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.295, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.292, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.836, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.306, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.305, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.299, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.316, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.835, loss:nan
g_step 1200, step 105, avg_time 1.306, loss:nan
g_step 1300, step 205, avg_time 1.320, loss:nan
g_step 1400, step 86, avg_time 1.290, loss:nan
g_step 1500, step 186, avg_time 1.318, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.836, loss:nan
g_step 1700, step 167, avg_time 1.325, loss:nan
g_step 1800, step 48, avg_time 1.287, loss:nan
g_step 1900, step 148, avg_time 1.307, loss:nan
g_step 2000, step 29, avg_time 1.312, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.846, loss:nan
g_step 2200, step 10, avg_time 1.303, loss:nan
g_step 2300, step 110, avg_time 1.297, loss:nan
g_step 2400, step 210, avg_time 1.326, loss:nan
g_step 2500, step 91, avg_time 1.282, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.853, loss:nan
g_step 2700, step 72, avg_time 1.321, loss:nan
g_step 2800, step 172, avg_time 1.313, loss:nan
g_step 2900, step 53, avg_time 1.289, loss:nan
g_step 3000, step 153, avg_time 1.307, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.860, loss:nan
g_step 3200, step 134, avg_time 1.309, loss:nan
g_step 3300, step 15, avg_time 1.297, loss:nan
g_step 3400, step 115, avg_time 1.302, loss:nan
g_step 3500, step 215, avg_time 1.295, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.841, loss:nan
g_step 3700, step 196, avg_time 1.288, loss:nan
g_step 3800, step 77, avg_time 1.295, loss:nan
g_step 3900, step 177, avg_time 1.312, loss:nan
g_step 4000, step 58, avg_time 1.304, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.833, loss:nan
g_step 4200, step 39, avg_time 1.297, loss:nan
g_step 4300, step 139, avg_time 1.311, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:24:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:24:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-24-45_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:24:46 - WARNING - datasets.builder -   Using custom data configuration default-68dc60a4ff5ace3e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-68dc60a4ff5ace3e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:24:46,843 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:24:46,845 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:24:46,845 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:24:46,846 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:24:46,853 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:24:46,858 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:24:47,014 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:24:50,125 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:24:50,127 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-68dc60a4ff5ace3e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.15ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.06ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.63ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.95ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.17ba/s]100%|██████████| 6/6 [00:01<00:00,  4.42ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.03ba/s] 40%|████      | 2/5 [00:00<00:00,  4.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.39ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.45ba/s]100%|██████████| 5/5 [00:01<00:00,  5.17ba/s]100%|██████████| 5/5 [00:01<00:00,  4.76ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.95ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.76ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.27ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.17ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.35ba/s]100%|██████████| 6/6 [00:00<00:00, 10.48ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.64ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.30ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.44ba/s]100%|██████████| 5/5 [00:00<00:00, 10.14ba/s]
[INFO|trainer.py:414] 2023-08-28 07:24:53,988 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:24:54,001 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:24:54,001 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 07:24:54,001 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:24:54,001 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:24:54,001 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:24:54,001 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:24:54,001 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.49it/s]  0%|          | 2/410 [00:00<01:55,  3.54it/s]  1%|          | 3/410 [00:00<01:54,  3.56it/s]  1%|          | 4/410 [00:01<01:53,  3.57it/s]  1%|          | 5/410 [00:01<01:53,  3.57it/s]  1%|▏         | 6/410 [00:01<01:53,  3.57it/s]  2%|▏         | 7/410 [00:01<01:52,  3.57it/s]  2%|▏         | 8/410 [00:02<01:52,  3.57it/s]  2%|▏         | 9/410 [00:02<01:53,  3.54it/s]  2%|▏         | 10/410 [00:02<01:53,  3.53it/s]  3%|▎         | 11/410 [00:03<01:53,  3.52it/s]  3%|▎         | 12/410 [00:03<01:53,  3.52it/s]  3%|▎         | 13/410 [00:03<01:53,  3.51it/s]  3%|▎         | 14/410 [00:03<01:53,  3.50it/s]  4%|▎         | 15/410 [00:04<01:52,  3.51it/s]  4%|▍         | 16/410 [00:04<01:52,  3.50it/s]  4%|▍         | 17/410 [00:04<01:52,  3.50it/s]  4%|▍         | 18/410 [00:05<01:53,  3.46it/s]  5%|▍         | 19/410 [00:05<01:52,  3.47it/s]  5%|▍         | 20/410 [00:05<01:52,  3.48it/s]  5%|▌         | 21/410 [00:05<01:51,  3.48it/s]  5%|▌         | 22/410 [00:06<01:51,  3.48it/s]  6%|▌         | 23/410 [00:06<01:51,  3.49it/s]  6%|▌         | 24/410 [00:06<01:50,  3.49it/s]  6%|▌         | 25/410 [00:07<01:50,  3.49it/s]  6%|▋         | 26/410 [00:07<01:50,  3.49it/s]  7%|▋         | 27/410 [00:07<01:49,  3.49it/s]  7%|▋         | 28/410 [00:07<01:49,  3.49it/s]  7%|▋         | 29/410 [00:08<01:49,  3.49it/s]  7%|▋         | 30/410 [00:08<01:49,  3.49it/s]  8%|▊         | 31/410 [00:08<01:48,  3.49it/s]  8%|▊         | 32/410 [00:09<01:48,  3.49it/s]  8%|▊         | 33/410 [00:09<01:48,  3.49it/s]  8%|▊         | 34/410 [00:09<01:47,  3.49it/s]  9%|▊         | 35/410 [00:09<01:47,  3.49it/s]  9%|▉         | 36/410 [00:10<01:56,  3.20it/s]  9%|▉         | 37/410 [00:10<01:53,  3.28it/s]  9%|▉         | 38/410 [00:10<01:51,  3.34it/s] 10%|▉         | 39/410 [00:11<01:49,  3.38it/s] 10%|▉         | 40/410 [00:11<01:48,  3.42it/s] 10%|█         | 41/410 [00:11<01:47,  3.44it/s] 10%|█         | 42/410 [00:12<01:46,  3.45it/s] 10%|█         | 43/410 [00:12<01:45,  3.46it/s] 11%|█         | 44/410 [00:12<01:45,  3.47it/s] 11%|█         | 45/410 [00:12<01:45,  3.47it/s] 11%|█         | 46/410 [00:13<01:44,  3.48it/s] 11%|█▏        | 47/410 [00:13<01:44,  3.48it/s] 12%|█▏        | 48/410 [00:13<01:43,  3.49it/s] 12%|█▏        | 49/410 [00:14<01:43,  3.49it/s] 12%|█▏        | 50/410 [00:14<01:43,  3.49it/s] 12%|█▏        | 51/410 [00:14<01:42,  3.49it/s] 13%|█▎        | 52/410 [00:14<01:42,  3.49it/s] 13%|█▎        | 53/410 [00:15<01:43,  3.46it/s] 13%|█▎        | 54/410 [00:15<01:42,  3.47it/s] 13%|█▎        | 55/410 [00:15<01:42,  3.47it/s] 14%|█▎        | 56/410 [00:16<01:41,  3.48it/s] 14%|█▍        | 57/410 [00:16<01:41,  3.48it/s] 14%|█▍        | 58/410 [00:16<01:41,  3.48it/s] 14%|█▍        | 59/410 [00:16<01:40,  3.48it/s] 15%|█▍        | 60/410 [00:17<01:40,  3.48it/s] 15%|█▍        | 61/410 [00:17<01:40,  3.48it/s] 15%|█▌        | 62/410 [00:17<01:40,  3.48it/s] 15%|█▌        | 63/410 [00:18<01:39,  3.48it/s] 16%|█▌        | 64/410 [00:18<01:39,  3.48it/s] 16%|█▌        | 65/410 [00:18<01:39,  3.48it/s] 16%|█▌        | 66/410 [00:18<01:38,  3.48it/s] 16%|█▋        | 67/410 [00:19<01:38,  3.48it/s] 17%|█▋        | 68/410 [00:19<01:38,  3.48it/s] 17%|█▋        | 69/410 [00:19<01:37,  3.48it/s] 17%|█▋        | 70/410 [00:20<01:37,  3.49it/s] 17%|█▋        | 71/410 [00:20<01:37,  3.48it/s] 18%|█▊        | 72/410 [00:20<01:37,  3.48it/s] 18%|█▊        | 73/410 [00:20<01:36,  3.48it/s] 18%|█▊        | 74/410 [00:21<01:36,  3.48it/s] 18%|█▊        | 75/410 [00:21<01:36,  3.48it/s] 19%|█▊        | 76/410 [00:21<01:35,  3.48it/s] 19%|█▉        | 77/410 [00:22<01:35,  3.48it/s] 19%|█▉        | 78/410 [00:22<01:35,  3.48it/s] 19%|█▉        | 79/410 [00:22<01:35,  3.48it/s] 20%|█▉        | 80/410 [00:22<01:34,  3.48it/s] 20%|█▉        | 81/410 [00:23<01:34,  3.48it/s] 20%|██        | 82/410 [00:23<01:30,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 07:25:17,532 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:25:17,532 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:25:17,532 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.06it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.36it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.49it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.55it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.91it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.59it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.24it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.27it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.43it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.56it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.53it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.32it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.22it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.07it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.93it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.99it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.12it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.25it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.36it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.43it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.14it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.13it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.04it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.14it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.18it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.37it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.34it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.19it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.13it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.08it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.07it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.90it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.10it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.31it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.36it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.19it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.18it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.16it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.03it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.11it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.18it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.29it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.33it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.00it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.01it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.10it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.15it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.05it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.15it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.18it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.34it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.35it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.18it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.05it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.17it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.06it/s][A
 54%|█████▍    | 312/577 [00:07<00:05, 44.17it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.20it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.29it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.03it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.08it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.14it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.00it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.21it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.34it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.33it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.27it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.10it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.00it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.08it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.95it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.20it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.34it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.28it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.25it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.07it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.97it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.04it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.02it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.04it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.19it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.30it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.32it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.24it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.04it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.96it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.07it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.10it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.04it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.05it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.27it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.35it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.31it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.07it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.02it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.06it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.09it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.25it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.33it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.21it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.99it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.06it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.11it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A 20%|██        | 82/410 [00:36<01:30,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:25:30,613 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 07:25:30,635 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:25:32,216 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:25:32,231 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:25:32,244 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:38<26:03,  4.78s/it] 20%|██        | 84/410 [00:39<19:09,  3.53s/it] 21%|██        | 85/410 [00:39<13:50,  2.55s/it] 21%|██        | 86/410 [00:39<10:07,  1.87s/it] 21%|██        | 87/410 [00:40<07:31,  1.40s/it] 21%|██▏       | 88/410 [00:40<05:42,  1.07s/it] 22%|██▏       | 89/410 [00:40<04:26,  1.20it/s] 22%|██▏       | 90/410 [00:41<03:33,  1.50it/s] 22%|██▏       | 91/410 [00:41<02:56,  1.81it/s] 22%|██▏       | 92/410 [00:41<02:30,  2.11it/s] 23%|██▎       | 93/410 [00:42<02:12,  2.39it/s] 23%|██▎       | 94/410 [00:42<01:59,  2.64it/s] 23%|██▎       | 95/410 [00:42<01:50,  2.84it/s] 23%|██▎       | 96/410 [00:42<01:44,  3.00it/s] 24%|██▎       | 97/410 [00:43<01:39,  3.13it/s] 24%|██▍       | 98/410 [00:43<01:36,  3.23it/s] 24%|██▍       | 99/410 [00:43<01:34,  3.30it/s] 24%|██▍       | 100/410 [00:44<01:32,  3.35it/s] 25%|██▍       | 101/410 [00:44<01:31,  3.39it/s] 25%|██▍       | 102/410 [00:44<01:30,  3.42it/s] 25%|██▌       | 103/410 [00:44<01:29,  3.43it/s] 25%|██▌       | 104/410 [00:45<01:28,  3.45it/s] 26%|██▌       | 105/410 [00:45<01:28,  3.44it/s] 26%|██▌       | 106/410 [00:45<01:28,  3.45it/s] 26%|██▌       | 107/410 [00:46<01:27,  3.46it/s] 26%|██▋       | 108/410 [00:46<01:27,  3.46it/s] 27%|██▋       | 109/410 [00:46<01:26,  3.46it/s] 27%|██▋       | 110/410 [00:46<01:26,  3.47it/s] 27%|██▋       | 111/410 [00:47<01:26,  3.47it/s] 27%|██▋       | 112/410 [00:47<01:25,  3.47it/s] 28%|██▊       | 113/410 [00:47<01:25,  3.47it/s] 28%|██▊       | 114/410 [00:48<01:25,  3.47it/s] 28%|██▊       | 115/410 [00:48<01:24,  3.48it/s] 28%|██▊       | 116/410 [00:48<01:24,  3.47it/s] 29%|██▊       | 117/410 [00:48<01:24,  3.47it/s] 29%|██▉       | 118/410 [00:49<01:24,  3.47it/s] 29%|██▉       | 119/410 [00:49<01:23,  3.48it/s] 29%|██▉       | 120/410 [00:49<01:23,  3.48it/s] 30%|██▉       | 121/410 [00:50<01:23,  3.48it/s] 30%|██▉       | 122/410 [00:50<01:22,  3.48it/s] 30%|███       | 123/410 [00:50<01:22,  3.47it/s] 30%|███       | 124/410 [00:50<01:22,  3.47it/s] 30%|███       | 125/410 [00:51<01:21,  3.48it/s] 31%|███       | 126/410 [00:51<01:21,  3.48it/s] 31%|███       | 127/410 [00:51<01:23,  3.38it/s] 31%|███       | 128/410 [00:52<01:22,  3.40it/s] 31%|███▏      | 129/410 [00:52<01:22,  3.43it/s] 32%|███▏      | 130/410 [00:52<01:21,  3.44it/s] 32%|███▏      | 131/410 [00:52<01:20,  3.45it/s] 32%|███▏      | 132/410 [00:53<01:20,  3.46it/s] 32%|███▏      | 133/410 [00:53<01:19,  3.46it/s] 33%|███▎      | 134/410 [00:53<01:19,  3.47it/s] 33%|███▎      | 135/410 [00:54<01:19,  3.47it/s] 33%|███▎      | 136/410 [00:54<01:18,  3.48it/s] 33%|███▎      | 137/410 [00:54<01:18,  3.48it/s] 34%|███▎      | 138/410 [00:54<01:18,  3.46it/s] 34%|███▍      | 139/410 [00:55<01:18,  3.47it/s] 34%|███▍      | 140/410 [00:55<01:17,  3.47it/s] 34%|███▍      | 141/410 [00:55<01:17,  3.47it/s] 35%|███▍      | 142/410 [00:56<01:17,  3.47it/s] 35%|███▍      | 143/410 [00:56<01:16,  3.47it/s] 35%|███▌      | 144/410 [00:56<01:16,  3.47it/s] 35%|███▌      | 145/410 [00:57<01:16,  3.48it/s] 36%|███▌      | 146/410 [00:57<01:15,  3.48it/s] 36%|███▌      | 147/410 [00:57<01:15,  3.48it/s] 36%|███▌      | 148/410 [00:57<01:15,  3.48it/s] 36%|███▋      | 149/410 [00:58<01:16,  3.42it/s] 37%|███▋      | 150/410 [00:58<01:15,  3.44it/s] 37%|███▋      | 151/410 [00:58<01:15,  3.45it/s] 37%|███▋      | 152/410 [00:59<01:14,  3.46it/s] 37%|███▋      | 153/410 [00:59<01:14,  3.47it/s] 38%|███▊      | 154/410 [00:59<01:13,  3.47it/s] 38%|███▊      | 155/410 [00:59<01:13,  3.47it/s] 38%|███▊      | 156/410 [01:00<01:13,  3.47it/s] 38%|███▊      | 157/410 [01:00<01:12,  3.48it/s] 39%|███▊      | 158/410 [01:00<01:12,  3.48it/s] 39%|███▉      | 159/410 [01:01<01:12,  3.47it/s] 39%|███▉      | 160/410 [01:01<01:12,  3.47it/s] 39%|███▉      | 161/410 [01:01<01:11,  3.47it/s] 40%|███▉      | 162/410 [01:01<01:11,  3.47it/s] 40%|███▉      | 163/410 [01:02<01:11,  3.48it/s] 40%|████      | 164/410 [01:02<01:07,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 07:25:56,447 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:25:56,447 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:25:56,447 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0595, 'eval_samples_per_second': 352.922, 'eval_steps_per_second': 44.182, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.72it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.47it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.85it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.65it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.01it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.54it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.15it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.00it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.04it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.38it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.42it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.49it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.33it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.13it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.93it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.86it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.78it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.88it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.12it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.31it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.37it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.40it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.21it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.81it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.80it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.89it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.98it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.19it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.26it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.41it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.21it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.01it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.84it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.89it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.93it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.12it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.21it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.33it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.28it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.22it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.03it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.87it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.85it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.97it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.08it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.36it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.35it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.22it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.82it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.89it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.90it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.90it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.99it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.15it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.21it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.33it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.13it/s][A
 52%|█████▏    | 302/577 [00:06<00:08, 32.83it/s][A
 53%|█████▎    | 307/577 [00:07<00:07, 35.69it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 37.97it/s][A
 55%|█████▍    | 317/577 [00:07<00:06, 39.73it/s][A
 56%|█████▌    | 322/577 [00:07<00:06, 41.04it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 42.09it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 42.84it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.15it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.09it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.07it/s][A
 61%|██████    | 352/577 [00:08<00:05, 43.35it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.68it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.88it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.41it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.15it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.93it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.69it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.76it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.83it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.12it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.28it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.34it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.41it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.18it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.93it/s][A
 76%|███████▌  | 437/577 [00:10<00:03, 43.72it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.63it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.70it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.04it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.11it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.27it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.38it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.32it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.14it/s][A
 84%|████████▎ | 482/577 [00:11<00:02, 43.99it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.95it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.04it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.17it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.23it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.28it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.22it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.02it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 43.94it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.89it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.02it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.13it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.23it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.25it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.20it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.15it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 43.85it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.07it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:15<01:07,  3.63it/s]
100%|██████████| 577/577 [00:13<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:26:09,676 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 07:26:09,695 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:26:11,396 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:26:11,403 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:26:11,410 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:17<19:47,  4.85s/it] 40%|████      | 166/410 [01:18<14:09,  3.48s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:18<10:12,  2.52s/it] 41%|████      | 168/410 [01:18<07:28,  1.85s/it] 41%|████      | 169/410 [01:19<05:33,  1.38s/it] 41%|████▏     | 170/410 [01:19<04:13,  1.05s/it] 42%|████▏     | 171/410 [01:19<03:17,  1.21it/s] 42%|████▏     | 172/410 [01:19<02:37,  1.51it/s] 42%|████▏     | 173/410 [01:20<02:10,  1.82it/s] 42%|████▏     | 174/410 [01:20<01:51,  2.12it/s] 43%|████▎     | 175/410 [01:20<01:37,  2.40it/s] 43%|████▎     | 176/410 [01:21<01:28,  2.65it/s] 43%|████▎     | 177/410 [01:21<01:21,  2.85it/s] 43%|████▎     | 178/410 [01:21<01:16,  3.02it/s] 44%|████▎     | 179/410 [01:21<01:13,  3.13it/s] 44%|████▍     | 180/410 [01:22<01:11,  3.23it/s] 44%|████▍     | 181/410 [01:22<01:09,  3.30it/s] 44%|████▍     | 182/410 [01:22<01:08,  3.35it/s] 45%|████▍     | 183/410 [01:23<01:07,  3.39it/s] 45%|████▍     | 184/410 [01:23<01:06,  3.41it/s] 45%|████▌     | 185/410 [01:23<01:05,  3.43it/s] 45%|████▌     | 186/410 [01:24<01:04,  3.45it/s] 46%|████▌     | 187/410 [01:24<01:04,  3.46it/s] 46%|████▌     | 188/410 [01:24<01:04,  3.46it/s] 46%|████▌     | 189/410 [01:24<01:03,  3.47it/s] 46%|████▋     | 190/410 [01:25<01:03,  3.46it/s] 47%|████▋     | 191/410 [01:25<01:03,  3.47it/s] 47%|████▋     | 192/410 [01:25<01:02,  3.47it/s] 47%|████▋     | 193/410 [01:26<01:02,  3.47it/s] 47%|████▋     | 194/410 [01:26<01:02,  3.48it/s] 48%|████▊     | 195/410 [01:26<01:01,  3.48it/s] 48%|████▊     | 196/410 [01:26<01:01,  3.48it/s] 48%|████▊     | 197/410 [01:27<01:01,  3.48it/s] 48%|████▊     | 198/410 [01:27<01:00,  3.48it/s] 49%|████▊     | 199/410 [01:27<01:00,  3.48it/s] 49%|████▉     | 200/410 [01:28<01:00,  3.48it/s] 49%|████▉     | 201/410 [01:28<01:00,  3.46it/s] 49%|████▉     | 202/410 [01:28<00:59,  3.47it/s] 50%|████▉     | 203/410 [01:28<00:59,  3.47it/s] 50%|████▉     | 204/410 [01:29<00:59,  3.47it/s] 50%|█████     | 205/410 [01:29<00:59,  3.47it/s] 50%|█████     | 206/410 [01:29<00:58,  3.48it/s] 50%|█████     | 207/410 [01:30<00:58,  3.48it/s] 51%|█████     | 208/410 [01:30<00:58,  3.48it/s] 51%|█████     | 209/410 [01:30<00:57,  3.48it/s] 51%|█████     | 210/410 [01:30<00:57,  3.48it/s] 51%|█████▏    | 211/410 [01:31<00:57,  3.48it/s] 52%|█████▏    | 212/410 [01:31<00:57,  3.45it/s] 52%|█████▏    | 213/410 [01:31<00:56,  3.46it/s] 52%|█████▏    | 214/410 [01:32<00:56,  3.47it/s] 52%|█████▏    | 215/410 [01:32<00:56,  3.47it/s] 53%|█████▎    | 216/410 [01:32<00:55,  3.47it/s] 53%|█████▎    | 217/410 [01:32<00:55,  3.47it/s] 53%|█████▎    | 218/410 [01:33<00:55,  3.47it/s] 53%|█████▎    | 219/410 [01:33<00:54,  3.48it/s] 54%|█████▎    | 220/410 [01:33<00:54,  3.47it/s] 54%|█████▍    | 221/410 [01:34<00:54,  3.48it/s] 54%|█████▍    | 222/410 [01:34<00:53,  3.48it/s] 54%|█████▍    | 223/410 [01:34<00:53,  3.48it/s] 55%|█████▍    | 224/410 [01:34<00:53,  3.48it/s] 55%|█████▍    | 225/410 [01:35<00:53,  3.48it/s] 55%|█████▌    | 226/410 [01:35<00:52,  3.48it/s] 55%|█████▌    | 227/410 [01:35<00:52,  3.48it/s] 56%|█████▌    | 228/410 [01:36<00:52,  3.48it/s] 56%|█████▌    | 229/410 [01:36<00:52,  3.48it/s] 56%|█████▌    | 230/410 [01:36<00:51,  3.48it/s] 56%|█████▋    | 231/410 [01:36<00:51,  3.48it/s] 57%|█████▋    | 232/410 [01:37<01:01,  2.91it/s] 57%|█████▋    | 233/410 [01:37<00:57,  3.06it/s] 57%|█████▋    | 234/410 [01:38<00:55,  3.18it/s] 57%|█████▋    | 235/410 [01:38<00:53,  3.26it/s] 58%|█████▊    | 236/410 [01:38<00:52,  3.32it/s] 58%|█████▊    | 237/410 [01:38<00:51,  3.37it/s] 58%|█████▊    | 238/410 [01:39<00:50,  3.40it/s] 58%|█████▊    | 239/410 [01:39<00:49,  3.42it/s] 59%|█████▊    | 240/410 [01:39<00:49,  3.44it/s] 59%|█████▉    | 241/410 [01:40<00:48,  3.45it/s] 59%|█████▉    | 242/410 [01:40<00:48,  3.44it/s] 59%|█████▉    | 243/410 [01:40<00:48,  3.46it/s] 60%|█████▉    | 244/410 [01:40<00:47,  3.46it/s] 60%|█████▉    | 245/410 [01:41<00:47,  3.46it/s] 60%|██████    | 246/410 [01:41<00:45,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 07:26:35,420 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:26:35,420 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:26:35,420 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.2031, 'eval_samples_per_second': 349.085, 'eval_steps_per_second': 43.702, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.61it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.37it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.78it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.53it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.78it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.25it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.06it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.99it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.18it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.18it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.40it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.39it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.18it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.14it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.91it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.81it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.93it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.02it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.20it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.34it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.45it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.25it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.11it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.83it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.86it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.92it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.11it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.09it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.29it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.42it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.32it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.08it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.00it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.97it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.99it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.02it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.12it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.29it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.33it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.17it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.97it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.99it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.95it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.08it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.21it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.39it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.32it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.22it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.07it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.01it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.95it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.89it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.02it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.15it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.30it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.33it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.11it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.11it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.99it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.01it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.93it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.07it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.17it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.35it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.32it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.09it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.05it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.96it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.97it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.00it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.22it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.32it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.29it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.22it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.11it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.97it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.87it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.93it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.04it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.05it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.16it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.23it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.22it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.04it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.94it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 43.92it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.94it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.04it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.31it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.79it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.88it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.81it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.73it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.89it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.01it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.02it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.98it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.07it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.24it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.24it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.98it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.88it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.04it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.17it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.16it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.13it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.09it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.19it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.17it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.08it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.94it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.16it/s][A 60%|██████    | 246/410 [01:54<00:45,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:26:48,516 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 07:26:48,532 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:26:50,791 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:26:50,805 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:26:50,816 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:57<13:34,  5.00s/it] 60%|██████    | 248/410 [01:57<09:40,  3.58s/it] 61%|██████    | 249/410 [01:58<06:57,  2.60s/it] 61%|██████    | 250/410 [01:58<05:04,  1.90s/it] 61%|██████    | 251/410 [01:58<03:45,  1.42s/it] 61%|██████▏   | 252/410 [01:58<02:50,  1.08s/it] 62%|██████▏   | 253/410 [01:59<02:12,  1.19it/s] 62%|██████▏   | 254/410 [01:59<01:45,  1.48it/s] 62%|██████▏   | 255/410 [01:59<01:26,  1.79it/s] 62%|██████▏   | 256/410 [02:00<01:13,  2.09it/s] 63%|██████▎   | 257/410 [02:00<01:04,  2.37it/s] 63%|██████▎   | 258/410 [02:00<00:58,  2.62it/s] 63%|██████▎   | 259/410 [02:00<00:53,  2.83it/s] 63%|██████▎   | 260/410 [02:01<00:50,  3.00it/s] 64%|██████▎   | 261/410 [02:01<00:47,  3.13it/s] 64%|██████▍   | 262/410 [02:01<00:45,  3.23it/s] 64%|██████▍   | 263/410 [02:02<00:44,  3.30it/s] 64%|██████▍   | 264/410 [02:02<00:43,  3.35it/s] 65%|██████▍   | 265/410 [02:02<00:42,  3.39it/s] 65%|██████▍   | 266/410 [02:02<00:42,  3.41it/s] 65%|██████▌   | 267/410 [02:03<00:41,  3.44it/s] 65%|██████▌   | 268/410 [02:03<00:41,  3.44it/s] 66%|██████▌   | 269/410 [02:03<00:40,  3.45it/s] 66%|██████▌   | 270/410 [02:04<00:40,  3.46it/s] 66%|██████▌   | 271/410 [02:04<00:40,  3.46it/s] 66%|██████▋   | 272/410 [02:04<00:39,  3.46it/s] 67%|██████▋   | 273/410 [02:04<00:39,  3.47it/s] 67%|██████▋   | 274/410 [02:05<00:39,  3.47it/s] 67%|██████▋   | 275/410 [02:05<00:38,  3.47it/s] 67%|██████▋   | 276/410 [02:05<00:38,  3.47it/s] 68%|██████▊   | 277/410 [02:06<00:38,  3.48it/s] 68%|██████▊   | 278/410 [02:06<00:37,  3.48it/s] 68%|██████▊   | 279/410 [02:06<00:37,  3.50it/s] 68%|██████▊   | 280/410 [02:06<00:36,  3.52it/s] 69%|██████▊   | 281/410 [02:07<00:36,  3.53it/s] 69%|██████▉   | 282/410 [02:07<00:36,  3.53it/s] 69%|██████▉   | 283/410 [02:07<00:35,  3.54it/s] 69%|██████▉   | 284/410 [02:08<00:35,  3.53it/s] 70%|██████▉   | 285/410 [02:08<00:35,  3.54it/s] 70%|██████▉   | 286/410 [02:08<00:35,  3.54it/s] 70%|███████   | 287/410 [02:08<00:34,  3.54it/s] 70%|███████   | 288/410 [02:09<00:34,  3.54it/s] 70%|███████   | 289/410 [02:09<00:34,  3.54it/s] 71%|███████   | 290/410 [02:09<00:33,  3.54it/s] 71%|███████   | 291/410 [02:10<00:33,  3.55it/s] 71%|███████   | 292/410 [02:10<00:33,  3.55it/s] 71%|███████▏  | 293/410 [02:10<00:32,  3.55it/s] 72%|███████▏  | 294/410 [02:10<00:32,  3.55it/s] 72%|███████▏  | 295/410 [02:11<00:32,  3.55it/s] 72%|███████▏  | 296/410 [02:11<00:32,  3.55it/s] 72%|███████▏  | 297/410 [02:11<00:31,  3.55it/s] 73%|███████▎  | 298/410 [02:11<00:31,  3.55it/s] 73%|███████▎  | 299/410 [02:12<00:31,  3.55it/s] 73%|███████▎  | 300/410 [02:12<00:31,  3.55it/s] 73%|███████▎  | 301/410 [02:12<00:30,  3.55it/s] 74%|███████▎  | 302/410 [02:13<00:30,  3.55it/s] 74%|███████▍  | 303/410 [02:13<00:30,  3.55it/s] 74%|███████▍  | 304/410 [02:13<00:29,  3.55it/s] 74%|███████▍  | 305/410 [02:13<00:29,  3.55it/s] 75%|███████▍  | 306/410 [02:14<00:29,  3.53it/s] 75%|███████▍  | 307/410 [02:14<00:29,  3.53it/s] 75%|███████▌  | 308/410 [02:14<00:28,  3.54it/s] 75%|███████▌  | 309/410 [02:15<00:28,  3.54it/s] 76%|███████▌  | 310/410 [02:15<00:28,  3.54it/s] 76%|███████▌  | 311/410 [02:15<00:27,  3.55it/s] 76%|███████▌  | 312/410 [02:15<00:27,  3.55it/s] 76%|███████▋  | 313/410 [02:16<00:27,  3.55it/s] 77%|███████▋  | 314/410 [02:16<00:27,  3.55it/s] 77%|███████▋  | 315/410 [02:16<00:26,  3.56it/s] 77%|███████▋  | 316/410 [02:17<00:26,  3.55it/s] 77%|███████▋  | 317/410 [02:17<00:26,  3.54it/s] 78%|███████▊  | 318/410 [02:17<00:26,  3.54it/s] 78%|███████▊  | 319/410 [02:17<00:25,  3.54it/s] 78%|███████▊  | 320/410 [02:18<00:25,  3.54it/s] 78%|███████▊  | 321/410 [02:18<00:25,  3.54it/s] 79%|███████▊  | 322/410 [02:18<00:24,  3.55it/s] 79%|███████▉  | 323/410 [02:19<00:24,  3.55it/s] 79%|███████▉  | 324/410 [02:19<00:24,  3.55it/s] 79%|███████▉  | 325/410 [02:19<00:23,  3.55it/s] 80%|███████▉  | 326/410 [02:19<00:23,  3.54it/s] 80%|███████▉  | 327/410 [02:20<00:23,  3.54it/s] 80%|████████  | 328/410 [02:20<00:22,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 07:27:14,425 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:27:14,425 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:27:14,425 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0812, 'eval_samples_per_second': 352.338, 'eval_steps_per_second': 44.109, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.98it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.66it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.74it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.55it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.94it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.44it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.10it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.05it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.12it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.39it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.46it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.42it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.33it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.10it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.98it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.83it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.92it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.06it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.33it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.40it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.32it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.22it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.08it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.99it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.93it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.69it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.21it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.42it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.40it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.33it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.21it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.11it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.01it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.00it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.02it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.16it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.35it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.39it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.27it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.12it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.06it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.98it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.05it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.07it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.21it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.31it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.29it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.18it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.18it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.04it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.93it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.99it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.10it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.23it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.36it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.26it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.17it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.22it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.07it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.15it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.14it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.30it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.12it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.11it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.09it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.12it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.25it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.26it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.21it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.00it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.94it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.05it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.14it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.16it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.18it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.23it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.24it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.14it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.95it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.95it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.11it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.12it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.18it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.16it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.32it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.25it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.21it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.04it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.04it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.12it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.16it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.25it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.25it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.15it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.99it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.18it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.16it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.22it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.26it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.24it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.19it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.12it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.12it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.11it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.09it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.16it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.29it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.29it/s][A 80%|████████  | 328/410 [02:33<00:22,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:27:27,509 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 07:27:27,531 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:27:29,318 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:27:29,336 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:27:29,347 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:35<06:32,  4.84s/it] 80%|████████  | 330/410 [02:36<04:38,  3.48s/it] 81%|████████  | 331/410 [02:36<03:19,  2.52s/it] 81%|████████  | 332/410 [02:36<02:24,  1.85s/it] 81%|████████  | 333/410 [02:37<01:46,  1.38s/it] 81%|████████▏ | 334/410 [02:37<01:20,  1.05s/it] 82%|████████▏ | 335/410 [02:37<01:01,  1.21it/s] 82%|████████▏ | 336/410 [02:37<00:49,  1.51it/s] 82%|████████▏ | 337/410 [02:38<00:40,  1.82it/s] 82%|████████▏ | 338/410 [02:38<00:33,  2.12it/s] 83%|████████▎ | 339/410 [02:38<00:29,  2.40it/s] 83%|████████▎ | 340/410 [02:39<00:26,  2.64it/s] 83%|████████▎ | 341/410 [02:39<00:24,  2.85it/s] 83%|████████▎ | 342/410 [02:39<00:22,  3.01it/s] 84%|████████▎ | 343/410 [02:39<00:21,  3.14it/s] 84%|████████▍ | 344/410 [02:40<00:20,  3.23it/s] 84%|████████▍ | 345/410 [02:40<00:19,  3.30it/s] 84%|████████▍ | 346/410 [02:40<00:19,  3.36it/s] 85%|████████▍ | 347/410 [02:41<00:18,  3.39it/s] 85%|████████▍ | 348/410 [02:41<00:18,  3.42it/s] 85%|████████▌ | 349/410 [02:41<00:17,  3.43it/s] 85%|████████▌ | 350/410 [02:41<00:17,  3.44it/s] 86%|████████▌ | 351/410 [02:42<00:17,  3.45it/s] 86%|████████▌ | 352/410 [02:42<00:16,  3.46it/s] 86%|████████▌ | 353/410 [02:42<00:16,  3.46it/s] 86%|████████▋ | 354/410 [02:43<00:16,  3.47it/s] 87%|████████▋ | 355/410 [02:43<00:15,  3.47it/s] 87%|████████▋ | 356/410 [02:43<00:15,  3.47it/s] 87%|████████▋ | 357/410 [02:43<00:15,  3.48it/s] 87%|████████▋ | 358/410 [02:44<00:14,  3.48it/s] 88%|████████▊ | 359/410 [02:44<00:14,  3.48it/s] 88%|████████▊ | 360/410 [02:44<00:14,  3.46it/s] 88%|████████▊ | 361/410 [02:45<00:14,  3.47it/s] 88%|████████▊ | 362/410 [02:45<00:13,  3.47it/s] 89%|████████▊ | 363/410 [02:45<00:13,  3.47it/s] 89%|████████▉ | 364/410 [02:45<00:13,  3.48it/s] 89%|████████▉ | 365/410 [02:46<00:12,  3.48it/s] 89%|████████▉ | 366/410 [02:46<00:12,  3.47it/s] 90%|████████▉ | 367/410 [02:46<00:12,  3.48it/s] 90%|████████▉ | 368/410 [02:47<00:12,  3.48it/s] 90%|█████████ | 369/410 [02:47<00:11,  3.48it/s] 90%|█████████ | 370/410 [02:47<00:11,  3.48it/s] 90%|█████████ | 371/410 [02:48<00:11,  3.46it/s] 91%|█████████ | 372/410 [02:48<00:10,  3.47it/s] 91%|█████████ | 373/410 [02:48<00:10,  3.47it/s] 91%|█████████ | 374/410 [02:48<00:10,  3.47it/s] 91%|█████████▏| 375/410 [02:49<00:10,  3.48it/s] 92%|█████████▏| 376/410 [02:49<00:09,  3.47it/s] 92%|█████████▏| 377/410 [02:49<00:09,  3.48it/s] 92%|█████████▏| 378/410 [02:50<00:09,  3.48it/s] 92%|█████████▏| 379/410 [02:50<00:08,  3.48it/s] 93%|█████████▎| 380/410 [02:50<00:08,  3.48it/s] 93%|█████████▎| 381/410 [02:50<00:08,  3.46it/s] 93%|█████████▎| 382/410 [02:51<00:08,  3.45it/s] 93%|█████████▎| 383/410 [02:51<00:07,  3.46it/s] 94%|█████████▎| 384/410 [02:51<00:07,  3.47it/s] 94%|█████████▍| 385/410 [02:52<00:07,  3.40it/s] 94%|█████████▍| 386/410 [02:52<00:07,  3.42it/s] 94%|█████████▍| 387/410 [02:52<00:06,  3.44it/s] 95%|█████████▍| 388/410 [02:52<00:06,  3.45it/s] 95%|█████████▍| 389/410 [02:53<00:06,  3.46it/s] 95%|█████████▌| 390/410 [02:53<00:05,  3.46it/s] 95%|█████████▌| 391/410 [02:53<00:05,  3.47it/s] 96%|█████████▌| 392/410 [02:54<00:05,  3.47it/s] 96%|█████████▌| 393/410 [02:54<00:04,  3.46it/s] 96%|█████████▌| 394/410 [02:54<00:04,  3.47it/s] 96%|█████████▋| 395/410 [02:54<00:04,  3.47it/s] 97%|█████████▋| 396/410 [02:55<00:04,  3.47it/s] 97%|█████████▋| 397/410 [02:55<00:03,  3.47it/s] 97%|█████████▋| 398/410 [02:55<00:03,  3.47it/s] 97%|█████████▋| 399/410 [02:56<00:03,  3.47it/s] 98%|█████████▊| 400/410 [02:56<00:02,  3.47it/s] 98%|█████████▊| 401/410 [02:56<00:02,  3.47it/s] 98%|█████████▊| 402/410 [02:56<00:02,  3.47it/s] 98%|█████████▊| 403/410 [02:57<00:02,  3.47it/s] 99%|█████████▊| 404/410 [02:57<00:01,  3.47it/s] 99%|█████████▉| 405/410 [02:57<00:01,  3.47it/s] 99%|█████████▉| 406/410 [02:58<00:01,  3.47it/s] 99%|█████████▉| 407/410 [02:58<00:00,  3.47it/s]100%|█████████▉| 408/410 [02:58<00:00,  3.47it/s]100%|█████████▉| 409/410 [02:58<00:00,  3.48it/s]100%|██████████| 410/410 [02:59<00:00,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 07:27:53,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:27:53,227 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:27:53,227 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.059, 'eval_samples_per_second': 352.937, 'eval_steps_per_second': 44.184, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.41it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.39it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.72it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.63it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.83it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.41it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.15it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.97it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.00it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.16it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.32it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.43it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.46it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.23it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.96it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.90it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.86it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.94it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.29it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.42it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.46it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.19it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.09it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.91it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.95it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.87it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.07it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.26it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.34it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.28it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.16it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.98it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.94it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.85it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.94it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.30it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.31it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.26it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.11it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.93it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.93it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.00it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.10it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.34it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.27it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.06it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.95it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.90it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.99it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.96it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.10it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.18it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.29it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.30it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.09it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.03it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.87it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.91it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.95it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.09it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.35it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.20it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.17it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.03it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.02it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.00it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.01it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.02it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.11it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.27it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.20it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.20it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.10it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.99it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.02it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.15it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.22it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.21it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.27it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.19it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.02it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 43.96it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.98it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.02it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.12it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.26it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.20it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.24it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.12it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.08it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.97it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.04it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.97it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.14it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.25it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.25it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.25it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.02it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.02it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.97it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.97it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.04it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.18it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.26it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.30it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.10it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.97it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.05it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.05it/s][A100%|██████████| 410/410 [03:12<00:00,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:28:06,316 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 07:28:06,339 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:28:08,110 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:28:08,123 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:28:08,136 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 07:28:08,437 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 07:28:08,437 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82 (score: 1.12112557888031).
                                                 100%|██████████| 410/410 [03:16<00:00,  3.63it/s]100%|██████████| 410/410 [03:16<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 07:28:10,256 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 07:28:10,271 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:28:11,888 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:28:11,904 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:28:11,915 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:28:12,101 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   train_runtime            = 0:03:16.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   train_samples_per_second =     133.48
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:12,101 >>   train_steps_per_second   =      2.089
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0747, 'eval_samples_per_second': 352.513, 'eval_steps_per_second': 44.131, 'epoch': 5.0}
{'train_runtime': 196.2472, 'train_samples_per_second': 133.48, 'train_steps_per_second': 2.089, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 07:28:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 07:28:12,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:28:12,140 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 07:28:12,140 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.56it/s]  2%|▏         | 12/577 [00:00<00:11, 48.54it/s]  3%|▎         | 17/577 [00:00<00:11, 46.96it/s]  4%|▍         | 22/577 [00:00<00:12, 46.08it/s]  5%|▍         | 27/577 [00:00<00:12, 45.68it/s]  6%|▌         | 32/577 [00:00<00:11, 45.42it/s]  6%|▋         | 37/577 [00:00<00:11, 45.25it/s]  7%|▋         | 42/577 [00:00<00:11, 44.89it/s]  8%|▊         | 47/577 [00:01<00:11, 44.30it/s]  9%|▉         | 52/577 [00:01<00:11, 43.97it/s] 10%|▉         | 57/577 [00:01<00:11, 43.99it/s] 11%|█         | 62/577 [00:01<00:11, 44.16it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.31it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.30it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.46it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.60it/s] 15%|█▌        | 87/577 [00:01<00:10, 44.56it/s] 16%|█▌        | 92/577 [00:02<00:10, 44.24it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.98it/s] 18%|█▊        | 102/577 [00:02<00:10, 44.06it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.07it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s] 20%|██        | 117/577 [00:02<00:10, 44.35it/s] 21%|██        | 122/577 [00:02<00:10, 44.49it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.54it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.42it/s] 24%|██▎       | 137/577 [00:03<00:09, 44.11it/s] 25%|██▍       | 142/577 [00:03<00:09, 44.09it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.99it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.06it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.22it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.39it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.53it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.52it/s] 31%|███       | 177/577 [00:03<00:09, 44.27it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.12it/s] 32%|███▏      | 187/577 [00:04<00:08, 44.16it/s] 33%|███▎      | 192/577 [00:04<00:08, 44.09it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.12it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.09it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.22it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.35it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.45it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.23it/s] 39%|███▉      | 227/577 [00:05<00:07, 44.11it/s] 40%|████      | 232/577 [00:05<00:07, 44.10it/s] 41%|████      | 237/577 [00:05<00:07, 44.07it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.13it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.21it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.35it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.50it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.41it/s] 46%|████▋     | 267/577 [00:06<00:07, 44.24it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.21it/s] 48%|████▊     | 277/577 [00:06<00:06, 44.10it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.12it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.07it/s] 51%|█████     | 292/577 [00:06<00:06, 44.22it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.31it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.50it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.46it/s] 54%|█████▍    | 312/577 [00:07<00:05, 44.20it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.11it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.08it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.06it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.06it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.18it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.29it/s] 60%|██████    | 347/577 [00:07<00:05, 44.42it/s] 61%|██████    | 352/577 [00:07<00:05, 44.32it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.29it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.12it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.09it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.03it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.21it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.24it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.35it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.35it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.33it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.14it/s] 71%|███████   | 407/577 [00:09<00:03, 44.08it/s] 71%|███████▏  | 412/577 [00:09<00:03, 43.92it/s] 72%|███████▏  | 417/577 [00:09<00:03, 43.94it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.10it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.35it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.33it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.34it/s] 77%|███████▋  | 442/577 [00:09<00:03, 44.23it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.17it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.15it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.05it/s] 80%|████████  | 462/577 [00:10<00:02, 44.14it/s] 81%|████████  | 467/577 [00:10<00:02, 44.23it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.31it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.42it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.29it/s] 84%|████████▍ | 487/577 [00:10<00:02, 44.16it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.17it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.09it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.13it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.14it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.28it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.26it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.34it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.25it/s] 92%|█████████▏| 532/577 [00:11<00:01, 44.14it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.07it/s] 94%|█████████▍| 542/577 [00:12<00:00, 44.17it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.21it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.34it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.33it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.39it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.24it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.23it/s]100%|██████████| 577/577 [00:13<00:00, 44.26it/s]100%|██████████| 577/577 [00:13<00:00, 44.33it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:28:25,175 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   eval_loss               =     1.1211
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   eval_runtime            = 0:00:13.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   eval_samples_per_second =    353.601
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   eval_steps_per_second   =     44.267
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:28:25,175 >>   perplexity              =     3.0683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:31,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:31,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:31,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:31,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:31,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:28:32,451 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:28:32,452 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:28:33,017 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:28:34,048 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:28:34,050 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:36,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:36,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:36,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:36,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:28:36,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:28:37,593 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:28:37,595 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:28:38,164 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:28:38,334 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:28:38,334 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/checkpoint-82
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.42it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.33it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:08,  1.40it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.41it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.42it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:15,  1.40it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:17,  1.41it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.43it/s]Extractor Predicting: 31it [00:22,  1.40it/s]Extractor Predicting: 32it [00:22,  1.41it/s]Extractor Predicting: 33it [00:23,  1.41it/s]Extractor Predicting: 34it [00:24,  1.37it/s]Extractor Predicting: 35it [00:25,  1.41it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:27,  1.39it/s]Extractor Predicting: 39it [00:27,  1.37it/s]Extractor Predicting: 40it [00:28,  1.37it/s]Extractor Predicting: 41it [00:29,  1.33it/s]Extractor Predicting: 42it [00:30,  1.35it/s]Extractor Predicting: 43it [00:30,  1.35it/s]Extractor Predicting: 44it [00:31,  1.33it/s]Extractor Predicting: 45it [00:32,  1.32it/s]Extractor Predicting: 46it [00:33,  1.32it/s]Extractor Predicting: 47it [00:34,  1.32it/s]Extractor Predicting: 48it [00:34,  1.33it/s]Extractor Predicting: 49it [00:35,  1.38it/s]Extractor Predicting: 50it [00:36,  1.37it/s]Extractor Predicting: 51it [00:36,  1.39it/s]Extractor Predicting: 52it [00:37,  1.35it/s]Extractor Predicting: 53it [00:38,  1.34it/s]Extractor Predicting: 54it [00:39,  1.35it/s]Extractor Predicting: 55it [00:39,  1.35it/s]Extractor Predicting: 56it [00:40,  1.38it/s]Extractor Predicting: 57it [00:41,  1.36it/s]Extractor Predicting: 58it [00:42,  1.35it/s]Extractor Predicting: 59it [00:42,  1.38it/s]Extractor Predicting: 60it [00:43,  1.38it/s]Extractor Predicting: 61it [00:44,  1.35it/s]Extractor Predicting: 62it [00:45,  1.34it/s]Extractor Predicting: 63it [00:45,  1.32it/s]Extractor Predicting: 64it [00:46,  1.31it/s]Extractor Predicting: 65it [00:47,  1.32it/s]Extractor Predicting: 66it [00:48,  1.36it/s]Extractor Predicting: 67it [00:48,  1.37it/s]Extractor Predicting: 68it [00:49,  1.41it/s]Extractor Predicting: 69it [00:50,  1.41it/s]Extractor Predicting: 70it [00:50,  1.40it/s]Extractor Predicting: 71it [00:51,  1.43it/s]Extractor Predicting: 72it [00:52,  1.47it/s]Extractor Predicting: 73it [00:52,  1.44it/s]Extractor Predicting: 74it [00:53,  1.46it/s]Extractor Predicting: 75it [00:54,  1.46it/s]Extractor Predicting: 76it [00:54,  1.42it/s]Extractor Predicting: 77it [00:55,  1.42it/s]Extractor Predicting: 78it [00:56,  1.44it/s]Extractor Predicting: 79it [00:57,  1.43it/s]Extractor Predicting: 80it [00:57,  1.43it/s]Extractor Predicting: 81it [00:58,  1.41it/s]Extractor Predicting: 82it [00:59,  1.41it/s]Extractor Predicting: 83it [00:59,  1.40it/s]Extractor Predicting: 84it [01:00,  1.44it/s]Extractor Predicting: 85it [01:01,  1.50it/s]Extractor Predicting: 86it [01:01,  1.50it/s]Extractor Predicting: 87it [01:02,  1.53it/s]Extractor Predicting: 88it [01:03,  1.44it/s]Extractor Predicting: 89it [01:03,  1.44it/s]Extractor Predicting: 90it [01:04,  1.42it/s]Extractor Predicting: 91it [01:05,  1.43it/s]Extractor Predicting: 92it [01:06,  1.43it/s]Extractor Predicting: 93it [01:06,  1.43it/s]Extractor Predicting: 94it [01:07,  1.44it/s]Extractor Predicting: 95it [01:08,  1.43it/s]Extractor Predicting: 96it [01:08,  1.44it/s]Extractor Predicting: 97it [01:09,  1.45it/s]Extractor Predicting: 98it [01:10,  1.40it/s]Extractor Predicting: 99it [01:10,  1.42it/s]Extractor Predicting: 100it [01:11,  1.39it/s]Extractor Predicting: 101it [01:12,  1.38it/s]Extractor Predicting: 102it [01:13,  1.40it/s]Extractor Predicting: 103it [01:13,  1.39it/s]Extractor Predicting: 104it [01:14,  1.42it/s]Extractor Predicting: 105it [01:15,  1.45it/s]Extractor Predicting: 106it [01:15,  1.39it/s]Extractor Predicting: 107it [01:16,  1.44it/s]Extractor Predicting: 108it [01:17,  1.46it/s]Extractor Predicting: 109it [01:18,  1.43it/s]Extractor Predicting: 110it [01:18,  1.43it/s]Extractor Predicting: 111it [01:19,  1.40it/s]Extractor Predicting: 112it [01:20,  1.40it/s]Extractor Predicting: 113it [01:20,  1.40it/s]Extractor Predicting: 114it [01:21,  1.41it/s]Extractor Predicting: 115it [01:22,  1.41it/s]Extractor Predicting: 116it [01:22,  1.42it/s]Extractor Predicting: 117it [01:23,  1.45it/s]Extractor Predicting: 118it [01:24,  1.45it/s]Extractor Predicting: 119it [01:24,  1.50it/s]Extractor Predicting: 120it [01:25,  1.49it/s]Extractor Predicting: 121it [01:26,  1.34it/s]Extractor Predicting: 122it [01:27,  1.35it/s]Extractor Predicting: 123it [01:28,  1.36it/s]Extractor Predicting: 124it [01:28,  1.40it/s]Extractor Predicting: 125it [01:29,  1.37it/s]Extractor Predicting: 126it [01:30,  1.41it/s]Extractor Predicting: 127it [01:30,  1.44it/s]Extractor Predicting: 128it [01:31,  1.42it/s]Extractor Predicting: 129it [01:32,  1.41it/s]Extractor Predicting: 130it [01:32,  1.38it/s]Extractor Predicting: 131it [01:33,  1.37it/s]Extractor Predicting: 132it [01:34,  1.39it/s]Extractor Predicting: 133it [01:35,  1.40it/s]Extractor Predicting: 134it [01:35,  1.39it/s]Extractor Predicting: 135it [01:36,  1.38it/s]Extractor Predicting: 136it [01:37,  1.39it/s]Extractor Predicting: 137it [01:37,  1.40it/s]Extractor Predicting: 138it [01:38,  1.40it/s]Extractor Predicting: 139it [01:39,  1.37it/s]Extractor Predicting: 140it [01:40,  1.35it/s]Extractor Predicting: 141it [01:40,  1.35it/s]Extractor Predicting: 142it [01:41,  1.33it/s]Extractor Predicting: 143it [01:42,  1.35it/s]Extractor Predicting: 144it [01:43,  1.33it/s]Extractor Predicting: 145it [01:43,  1.34it/s]Extractor Predicting: 146it [01:44,  1.30it/s]Extractor Predicting: 147it [01:45,  1.31it/s]Extractor Predicting: 148it [01:46,  1.33it/s]Extractor Predicting: 149it [01:47,  1.35it/s]Extractor Predicting: 150it [01:47,  1.34it/s]Extractor Predicting: 151it [01:48,  1.37it/s]Extractor Predicting: 152it [01:49,  1.36it/s]Extractor Predicting: 153it [01:49,  1.38it/s]Extractor Predicting: 154it [01:50,  1.37it/s]Extractor Predicting: 155it [01:51,  1.37it/s]Extractor Predicting: 156it [01:52,  1.35it/s]Extractor Predicting: 157it [01:52,  1.37it/s]Extractor Predicting: 158it [01:53,  1.38it/s]Extractor Predicting: 159it [01:54,  1.37it/s]Extractor Predicting: 160it [01:55,  1.37it/s]Extractor Predicting: 161it [01:55,  1.40it/s]Extractor Predicting: 162it [01:56,  1.40it/s]Extractor Predicting: 163it [01:57,  1.36it/s]Extractor Predicting: 164it [01:57,  1.35it/s]Extractor Predicting: 165it [01:58,  1.36it/s]Extractor Predicting: 166it [01:59,  1.34it/s]Extractor Predicting: 167it [02:00,  1.36it/s]Extractor Predicting: 168it [02:00,  1.37it/s]Extractor Predicting: 169it [02:01,  1.37it/s]Extractor Predicting: 170it [02:02,  1.37it/s]Extractor Predicting: 171it [02:02,  1.65it/s]Extractor Predicting: 171it [02:02,  1.39it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:48,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:48,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:48,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:48,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:48,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:30:48,911 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:30:48,912 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:30:49,200 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:30:50,251 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:30:50,251 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:53,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:53,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:53,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:53,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:30:53,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:30:53,987 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:30:53,988 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:30:54,543 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:30:54,707 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:30:54,707 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.37it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:07,  1.36it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.34it/s]Extractor Predicting: 14it [00:10,  1.36it/s]Extractor Predicting: 15it [00:10,  1.35it/s]Extractor Predicting: 16it [00:11,  1.38it/s]Extractor Predicting: 17it [00:12,  1.40it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:15,  1.45it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:17,  1.40it/s]Extractor Predicting: 25it [00:17,  1.42it/s]Extractor Predicting: 26it [00:18,  1.45it/s]Extractor Predicting: 27it [00:19,  1.45it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.41it/s]Extractor Predicting: 31it [00:22,  1.40it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.42it/s]Extractor Predicting: 34it [00:24,  1.41it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:25,  1.40it/s]Extractor Predicting: 37it [00:26,  1.40it/s]Extractor Predicting: 38it [00:27,  1.41it/s]Extractor Predicting: 39it [00:27,  1.40it/s]Extractor Predicting: 40it [00:28,  1.42it/s]Extractor Predicting: 41it [00:29,  1.42it/s]Extractor Predicting: 42it [00:29,  1.43it/s]Extractor Predicting: 43it [00:30,  1.43it/s]Extractor Predicting: 44it [00:31,  1.44it/s]Extractor Predicting: 45it [00:31,  1.45it/s]Extractor Predicting: 46it [00:32,  1.46it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:34,  1.43it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.43it/s]Extractor Predicting: 51it [00:36,  1.44it/s]Extractor Predicting: 52it [00:36,  1.45it/s]Extractor Predicting: 53it [00:37,  1.44it/s]Extractor Predicting: 54it [00:38,  1.42it/s]Extractor Predicting: 55it [00:38,  1.41it/s]Extractor Predicting: 56it [00:39,  1.43it/s]Extractor Predicting: 57it [00:40,  1.44it/s]Extractor Predicting: 58it [00:41,  1.44it/s]Extractor Predicting: 59it [00:41,  1.44it/s]Extractor Predicting: 60it [00:42,  1.41it/s]Extractor Predicting: 61it [00:43,  1.41it/s]Extractor Predicting: 62it [00:43,  1.39it/s]Extractor Predicting: 63it [00:44,  1.43it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:45,  1.44it/s]Extractor Predicting: 66it [00:46,  1.43it/s]Extractor Predicting: 67it [00:47,  1.44it/s]Extractor Predicting: 68it [00:47,  1.45it/s]Extractor Predicting: 69it [00:48,  1.47it/s]Extractor Predicting: 70it [00:49,  1.44it/s]Extractor Predicting: 71it [00:50,  1.39it/s]Extractor Predicting: 72it [00:50,  1.41it/s]Extractor Predicting: 73it [00:51,  1.41it/s]Extractor Predicting: 74it [00:52,  1.43it/s]Extractor Predicting: 75it [00:52,  1.43it/s]Extractor Predicting: 76it [00:53,  1.45it/s]Extractor Predicting: 77it [00:54,  1.49it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:55,  1.47it/s]Extractor Predicting: 80it [00:56,  1.48it/s]Extractor Predicting: 81it [00:56,  1.47it/s]Extractor Predicting: 82it [00:57,  1.47it/s]Extractor Predicting: 83it [00:58,  1.47it/s]Extractor Predicting: 84it [00:59,  1.45it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.46it/s]Extractor Predicting: 87it [01:01,  1.43it/s]Extractor Predicting: 88it [01:02,  1.33it/s]Extractor Predicting: 89it [01:02,  1.36it/s]Extractor Predicting: 90it [01:03,  1.35it/s]Extractor Predicting: 91it [01:04,  1.36it/s]Extractor Predicting: 92it [01:04,  1.38it/s]Extractor Predicting: 93it [01:05,  1.39it/s]Extractor Predicting: 94it [01:06,  1.38it/s]Extractor Predicting: 95it [01:07,  1.38it/s]Extractor Predicting: 96it [01:07,  1.34it/s]Extractor Predicting: 97it [01:08,  1.38it/s]Extractor Predicting: 98it [01:09,  1.40it/s]Extractor Predicting: 99it [01:09,  1.47it/s]Extractor Predicting: 100it [01:10,  1.47it/s]Extractor Predicting: 101it [01:11,  1.43it/s]Extractor Predicting: 102it [01:11,  1.45it/s]Extractor Predicting: 103it [01:12,  1.44it/s]Extractor Predicting: 104it [01:13,  1.45it/s]Extractor Predicting: 105it [01:14,  1.43it/s]Extractor Predicting: 106it [01:14,  1.45it/s]Extractor Predicting: 107it [01:15,  1.46it/s]Extractor Predicting: 108it [01:16,  1.44it/s]Extractor Predicting: 109it [01:16,  1.40it/s]Extractor Predicting: 110it [01:17,  1.34it/s]Extractor Predicting: 111it [01:18,  1.32it/s]Extractor Predicting: 112it [01:19,  1.33it/s]Extractor Predicting: 113it [01:19,  1.38it/s]Extractor Predicting: 114it [01:20,  1.41it/s]Extractor Predicting: 115it [01:21,  1.39it/s]Extractor Predicting: 116it [01:21,  1.39it/s]Extractor Predicting: 117it [01:22,  1.39it/s]Extractor Predicting: 118it [01:23,  1.36it/s]Extractor Predicting: 119it [01:24,  1.37it/s]Extractor Predicting: 120it [01:24,  1.35it/s]Extractor Predicting: 121it [01:25,  1.33it/s]Extractor Predicting: 122it [01:26,  1.36it/s]Extractor Predicting: 123it [01:27,  1.36it/s]Extractor Predicting: 124it [01:27,  1.37it/s]Extractor Predicting: 125it [01:28,  1.72it/s]Extractor Predicting: 125it [01:28,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:28,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:28,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:28,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:28,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:28,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:32:29,199 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:32:29,200 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:32:29,777 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:32:30,798 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:32:30,798 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:33,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:33,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:33,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:33,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:32:33,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:32:34,288 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:32:34,289 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:32:34,856 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:32:35,024 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:32:35,024 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.78it/s]Extractor Predicting: 6it [00:04,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:32:39,554 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:32:39,555 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:32:39,559 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:32:39,560 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:32:39,563 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:32:42,511 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:32:42,513 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:32:42,528 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:32:42,529 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:32:42,535 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:32:42,539 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:32:42,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:43,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:44,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:45,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:45,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:47,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:48,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:48,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:49,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:50,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:51,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:52,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:53,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:53,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:54,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:56,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:57,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:58,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:59,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:32:59,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:00,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:01,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:02,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:02,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:20<03:07, 20.89s/it][WARNING|generation_utils.py:914] 2023-08-28 07:33:03,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:04,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:05,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:05,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:06,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:07,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:07,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:08,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:09,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:10,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:10,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:11,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:12,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:13,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:14,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:14,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:15,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:16,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:17,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:17,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:18,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:19,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:26, 18.31s/it][WARNING|generation_utils.py:914] 2023-08-28 07:33:20,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:21,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:21,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:22,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:23,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:24,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:24,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:25,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:26,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:27,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:27,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:28,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:29,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:30,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:30,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:31,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:32,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:33,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:34,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:34,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:35,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:36,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:37,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:37,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:38,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:39,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:40,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:40,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:20, 20.10s/it][WARNING|generation_utils.py:914] 2023-08-28 07:33:42,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:43,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:43,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:44,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:45,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:46,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:47,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:48,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:48,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:49,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:50,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:51,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:52,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:52,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:53,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:54,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:55,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:56,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:56,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:57,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:58,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:33:59,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:00,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:01,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:02,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:03,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:03,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:22<02:06, 21.01s/it][WARNING|generation_utils.py:914] 2023-08-28 07:34:04,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:05,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:06,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:07,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:07,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:08,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:09,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:09,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:10,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:11,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:12,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:13,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:13,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:14,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:15,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:15,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:16,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:17,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:18,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:18,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:19,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:20,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:20,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:37, 19.53s/it][WARNING|generation_utils.py:914] 2023-08-28 07:34:21,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:22,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:23,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:24,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:24,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:25,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:26,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:27,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:27,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:28,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:29,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:30,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:31,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:32,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:32,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:33,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:34,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:35,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:35,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:36,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:37,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:38,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:56<01:15, 18.76s/it][WARNING|generation_utils.py:914] 2023-08-28 07:34:38,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:39,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:40,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:41,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:41,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:42,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:43,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:44,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:45,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:46,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:47,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:47,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:48,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:49,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:50,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:51,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:51,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:52,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:53,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:54,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:55,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:56,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:57,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:15<00:56, 18.81s/it][WARNING|generation_utils.py:914] 2023-08-28 07:34:57,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:58,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:34:59,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:00,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:00,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:01,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:02,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:03,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:03,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:04,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:05,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:06,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:07,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:07,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:08,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:09,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:10,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:10,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:11,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:12,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:13,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:31<00:35, 17.94s/it][WARNING|generation_utils.py:914] 2023-08-28 07:35:13,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:14,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:15,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:16,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:17,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:17,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:18,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:19,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:20,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:21,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:21,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:22,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:23,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:24,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:25,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:26,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:27,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:27,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:28,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:29,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:30,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:30,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:31,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:32,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:33,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:51<00:18, 18.53s/it][WARNING|generation_utils.py:914] 2023-08-28 07:35:33,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:34,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:35,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:35,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:36,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:37,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:38,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:39,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:40,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:40,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:41,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:42,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:43,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:43,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:44,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:45,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:45,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:46,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:47,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:48,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:49,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:49,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:50,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:51,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:51,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:52,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:53,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:54,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:55,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:55,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:56,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:57,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:35:58,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:16<00:00, 20.56s/it]Generating: 100%|██████████| 10/10 [03:16<00:00, 19.61s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:03,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:03,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:03,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:03,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:03,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:36:04,505 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:36:04,507 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:36:05,076 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:36:06,195 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:36:06,195 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:09,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:09,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:09,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:09,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:36:09,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:36:09,791 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:36:09,792 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:36:10,382 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:36:10,564 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:36:10,564 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')"}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.16it/s]Extractor Estimating: 2it [00:01,  1.18it/s]Extractor Estimating: 3it [00:02,  1.25it/s]Extractor Estimating: 4it [00:03,  1.27it/s]Extractor Estimating: 5it [00:04,  1.09it/s]Extractor Estimating: 6it [00:05,  1.16it/s]Extractor Estimating: 7it [00:05,  1.20it/s]Extractor Estimating: 8it [00:06,  1.23it/s]Extractor Estimating: 9it [00:07,  1.29it/s]Extractor Estimating: 10it [00:08,  1.27it/s]Extractor Estimating: 11it [00:09,  1.23it/s]Extractor Estimating: 12it [00:09,  1.26it/s]Extractor Estimating: 13it [00:10,  1.27it/s]Extractor Estimating: 14it [00:11,  1.27it/s]Extractor Estimating: 15it [00:12,  1.29it/s]Extractor Estimating: 16it [00:12,  1.28it/s]Extractor Estimating: 17it [00:13,  1.29it/s]Extractor Estimating: 18it [00:14,  1.28it/s]Extractor Estimating: 19it [00:15,  1.28it/s]Extractor Estimating: 20it [00:16,  1.26it/s]Extractor Estimating: 21it [00:16,  1.28it/s]Extractor Estimating: 22it [00:17,  1.27it/s]Extractor Estimating: 23it [00:18,  1.25it/s]Extractor Estimating: 24it [00:19,  1.22it/s]Extractor Estimating: 25it [00:20,  1.25it/s]Extractor Estimating: 26it [00:20,  1.28it/s]Extractor Estimating: 27it [00:21,  1.27it/s]Extractor Estimating: 28it [00:22,  1.30it/s]Extractor Estimating: 29it [00:23,  1.34it/s]Extractor Estimating: 30it [00:23,  1.32it/s]Extractor Estimating: 31it [00:24,  1.32it/s]Extractor Estimating: 32it [00:25,  1.33it/s]Extractor Estimating: 33it [00:26,  1.34it/s]Extractor Estimating: 34it [00:26,  1.35it/s]Extractor Estimating: 35it [00:27,  1.23it/s]Extractor Estimating: 36it [00:28,  1.29it/s]Extractor Estimating: 37it [00:29,  1.27it/s]Extractor Estimating: 38it [00:29,  1.29it/s]Extractor Estimating: 39it [00:30,  1.32it/s]Extractor Estimating: 40it [00:31,  1.29it/s]Extractor Estimating: 41it [00:32,  1.25it/s]Extractor Estimating: 42it [00:33,  1.28it/s]Extractor Estimating: 43it [00:33,  1.32it/s]Extractor Estimating: 44it [00:34,  1.35it/s]Extractor Estimating: 45it [00:35,  1.34it/s]Extractor Estimating: 46it [00:36,  1.34it/s]Extractor Estimating: 47it [00:36,  1.32it/s]Extractor Estimating: 48it [00:37,  1.33it/s]Extractor Estimating: 49it [00:38,  1.31it/s]Extractor Estimating: 50it [00:39,  1.33it/s]Extractor Estimating: 51it [00:39,  1.31it/s]Extractor Estimating: 52it [00:40,  1.29it/s]Extractor Estimating: 53it [00:41,  1.29it/s]Extractor Estimating: 54it [00:42,  1.28it/s]Extractor Estimating: 55it [00:42,  1.29it/s]Extractor Estimating: 56it [00:43,  1.26it/s]Extractor Estimating: 57it [00:44,  1.24it/s]Extractor Estimating: 58it [00:45,  1.29it/s]Extractor Estimating: 59it [00:46,  1.31it/s]Extractor Estimating: 60it [00:46,  1.30it/s]Extractor Estimating: 61it [00:47,  1.27it/s]Extractor Estimating: 62it [00:48,  1.29it/s]Extractor Estimating: 63it [00:49,  1.32it/s]Extractor Estimating: 64it [00:49,  1.30it/s]Extractor Estimating: 65it [00:50,  1.28it/s]Extractor Estimating: 66it [00:51,  1.27it/s]Extractor Estimating: 67it [00:52,  1.27it/s]Extractor Estimating: 68it [00:53,  1.27it/s]Extractor Estimating: 69it [00:53,  1.28it/s]Extractor Estimating: 70it [00:54,  1.27it/s]Extractor Estimating: 71it [00:55,  1.26it/s]Extractor Estimating: 72it [00:56,  1.26it/s]Extractor Estimating: 73it [00:57,  1.27it/s]Extractor Estimating: 74it [00:57,  1.29it/s]Extractor Estimating: 75it [00:58,  1.30it/s]Extractor Estimating: 76it [00:59,  1.35it/s]Extractor Estimating: 77it [01:00,  1.28it/s]Extractor Estimating: 78it [01:00,  1.27it/s]Extractor Estimating: 79it [01:01,  1.33it/s]Extractor Estimating: 80it [01:02,  1.34it/s]Extractor Estimating: 81it [01:03,  1.33it/s]Extractor Estimating: 82it [01:03,  1.30it/s]Extractor Estimating: 83it [01:04,  1.28it/s]Extractor Estimating: 84it [01:05,  1.24it/s]Extractor Estimating: 85it [01:06,  1.25it/s]Extractor Estimating: 86it [01:07,  1.25it/s]Extractor Estimating: 87it [01:08,  1.24it/s]Extractor Estimating: 88it [01:08,  1.28it/s]Extractor Estimating: 89it [01:09,  1.28it/s]Extractor Estimating: 90it [01:10,  1.28it/s]Extractor Estimating: 91it [01:11,  1.26it/s]Extractor Estimating: 92it [01:11,  1.28it/s]Extractor Estimating: 93it [01:12,  1.28it/s]Extractor Estimating: 94it [01:13,  1.26it/s]Extractor Estimating: 95it [01:14,  1.26it/s]Extractor Estimating: 96it [01:15,  1.25it/s]Extractor Estimating: 97it [01:15,  1.24it/s]Extractor Estimating: 98it [01:16,  1.25it/s]Extractor Estimating: 99it [01:17,  1.25it/s]Extractor Estimating: 100it [01:18,  1.30it/s]Extractor Estimating: 101it [01:18,  1.30it/s]Extractor Estimating: 102it [01:19,  1.27it/s]Extractor Estimating: 103it [01:20,  1.29it/s]Extractor Estimating: 104it [01:21,  1.30it/s]Extractor Estimating: 105it [01:22,  1.32it/s]Extractor Estimating: 106it [01:22,  1.30it/s]Extractor Estimating: 107it [01:23,  1.30it/s]Extractor Estimating: 108it [01:24,  1.32it/s]Extractor Estimating: 109it [01:25,  1.31it/s]Extractor Estimating: 110it [01:25,  1.33it/s]Extractor Estimating: 111it [01:26,  1.31it/s]Extractor Estimating: 112it [01:27,  1.31it/s]Extractor Estimating: 113it [01:28,  1.20it/s]Extractor Estimating: 114it [01:29,  1.25it/s]Extractor Estimating: 115it [01:29,  1.27it/s]Extractor Estimating: 116it [01:30,  1.29it/s]Extractor Estimating: 117it [01:31,  1.30it/s]Extractor Estimating: 118it [01:32,  1.32it/s]Extractor Estimating: 119it [01:32,  1.32it/s]Extractor Estimating: 120it [01:33,  1.32it/s]Extractor Estimating: 121it [01:34,  1.31it/s]Extractor Estimating: 122it [01:35,  1.32it/s]Extractor Estimating: 123it [01:35,  1.30it/s]Extractor Estimating: 124it [01:36,  1.29it/s]Extractor Estimating: 125it [01:37,  1.31it/s]Extractor Estimating: 126it [01:38,  1.31it/s]Extractor Estimating: 127it [01:39,  1.27it/s]Extractor Estimating: 128it [01:39,  1.31it/s]Extractor Estimating: 129it [01:40,  1.27it/s]Extractor Estimating: 130it [01:41,  1.23it/s]Extractor Estimating: 131it [01:42,  1.21it/s]Extractor Estimating: 132it [01:43,  1.24it/s]Extractor Estimating: 133it [01:43,  1.23it/s]Extractor Estimating: 134it [01:44,  1.23it/s]Extractor Estimating: 135it [01:45,  1.25it/s]Extractor Estimating: 136it [01:46,  1.14it/s]Extractor Estimating: 137it [01:47,  1.15it/s]Extractor Estimating: 138it [01:48,  1.16it/s]Extractor Estimating: 139it [01:49,  1.15it/s]Extractor Estimating: 140it [01:49,  1.21it/s]Extractor Estimating: 141it [01:50,  1.26it/s]Extractor Estimating: 142it [01:51,  1.28it/s]Extractor Estimating: 143it [01:52,  1.27it/s]Extractor Estimating: 144it [01:52,  1.25it/s]Extractor Estimating: 145it [01:53,  1.24it/s]Extractor Estimating: 146it [01:54,  1.22it/s]Extractor Estimating: 147it [01:55,  1.24it/s]Extractor Estimating: 148it [01:56,  1.25it/s]Extractor Estimating: 149it [01:56,  1.28it/s]Extractor Estimating: 150it [01:57,  1.28it/s]Extractor Estimating: 151it [01:58,  1.31it/s]Extractor Estimating: 152it [01:59,  1.31it/s]Extractor Estimating: 153it [01:59,  1.31it/s]Extractor Estimating: 154it [02:00,  1.31it/s]Extractor Estimating: 155it [02:01,  1.31it/s]Extractor Estimating: 156it [02:02,  1.31it/s]Extractor Estimating: 157it [02:03,  1.29it/s]Extractor Estimating: 158it [02:03,  1.30it/s]Extractor Estimating: 159it [02:04,  1.31it/s]Extractor Estimating: 160it [02:05,  1.30it/s]Extractor Estimating: 161it [02:06,  1.29it/s]Extractor Estimating: 162it [02:06,  1.30it/s]Extractor Estimating: 163it [02:07,  1.32it/s]Extractor Estimating: 164it [02:08,  1.29it/s]Extractor Estimating: 165it [02:09,  1.27it/s]Extractor Estimating: 166it [02:10,  1.28it/s]Extractor Estimating: 167it [02:10,  1.29it/s]Extractor Estimating: 168it [02:11,  1.30it/s]Extractor Estimating: 169it [02:12,  1.29it/s]Extractor Estimating: 170it [02:13,  1.27it/s]Extractor Estimating: 171it [02:13,  1.31it/s]Extractor Estimating: 172it [02:14,  1.29it/s]Extractor Estimating: 173it [02:15,  1.33it/s]Extractor Estimating: 174it [02:16,  1.34it/s]Extractor Estimating: 175it [02:16,  1.32it/s]Extractor Estimating: 176it [02:17,  1.30it/s]Extractor Estimating: 177it [02:18,  1.30it/s]Extractor Estimating: 178it [02:19,  1.29it/s]Extractor Estimating: 179it [02:20,  1.28it/s]Extractor Estimating: 180it [02:20,  1.26it/s]Extractor Estimating: 181it [02:21,  1.30it/s]Extractor Estimating: 182it [02:22,  1.31it/s]Extractor Estimating: 183it [02:23,  1.28it/s]Extractor Estimating: 184it [02:23,  1.29it/s]Extractor Estimating: 185it [02:24,  1.31it/s]Extractor Estimating: 186it [02:25,  1.34it/s]Extractor Estimating: 187it [02:26,  1.33it/s]Extractor Estimating: 188it [02:26,  1.35it/s]Extractor Estimating: 189it [02:27,  1.23it/s]Extractor Estimating: 190it [02:28,  1.21it/s]Extractor Estimating: 191it [02:29,  1.26it/s]Extractor Estimating: 192it [02:30,  1.25it/s]Extractor Estimating: 193it [02:30,  1.25it/s]Extractor Estimating: 194it [02:31,  1.28it/s]Extractor Estimating: 195it [02:32,  1.29it/s]Extractor Estimating: 196it [02:33,  1.31it/s]Extractor Estimating: 197it [02:33,  1.31it/s]Extractor Estimating: 198it [02:34,  1.29it/s]Extractor Estimating: 199it [02:35,  1.24it/s]Extractor Estimating: 200it [02:36,  1.26it/s]Extractor Estimating: 201it [02:37,  1.26it/s]Extractor Estimating: 202it [02:37,  1.27it/s]Extractor Estimating: 203it [02:38,  1.29it/s]Extractor Estimating: 204it [02:39,  1.28it/s]Extractor Estimating: 205it [02:40,  1.29it/s]Extractor Estimating: 206it [02:40,  1.32it/s]Extractor Estimating: 207it [02:42,  1.17it/s]Extractor Estimating: 208it [02:42,  1.21it/s]Extractor Estimating: 209it [02:43,  1.22it/s]Extractor Estimating: 210it [02:44,  1.25it/s]Extractor Estimating: 211it [02:45,  1.25it/s]Extractor Estimating: 212it [02:45,  1.26it/s]Extractor Estimating: 213it [02:46,  1.25it/s]Extractor Estimating: 214it [02:47,  1.27it/s]Extractor Estimating: 215it [02:48,  1.25it/s]Extractor Estimating: 216it [02:49,  1.25it/s]Extractor Estimating: 217it [02:49,  1.26it/s]Extractor Estimating: 218it [02:50,  1.29it/s]Extractor Estimating: 219it [02:51,  1.31it/s]Extractor Estimating: 220it [02:52,  1.31it/s]Extractor Estimating: 221it [02:52,  1.29it/s]Extractor Estimating: 222it [02:53,  1.30it/s]Extractor Estimating: 223it [02:54,  1.32it/s]Extractor Estimating: 224it [02:55,  1.31it/s]Extractor Estimating: 225it [02:56,  1.30it/s]Extractor Estimating: 226it [02:56,  1.28it/s]Extractor Estimating: 227it [02:57,  1.30it/s]Extractor Estimating: 228it [02:58,  1.26it/s]Extractor Estimating: 229it [02:59,  1.29it/s]Extractor Estimating: 230it [02:59,  1.27it/s]Extractor Estimating: 231it [03:00,  1.25it/s]Extractor Estimating: 232it [03:01,  1.28it/s]Extractor Estimating: 233it [03:02,  1.27it/s]Extractor Estimating: 234it [03:03,  1.30it/s]Extractor Estimating: 235it [03:03,  1.30it/s]Extractor Estimating: 236it [03:04,  1.32it/s]Extractor Estimating: 237it [03:05,  1.31it/s]Extractor Estimating: 238it [03:06,  1.31it/s]Extractor Estimating: 239it [03:06,  1.26it/s]Extractor Estimating: 240it [03:07,  1.24it/s]Extractor Estimating: 241it [03:08,  1.24it/s]Extractor Estimating: 242it [03:09,  1.30it/s]Extractor Estimating: 243it [03:10,  1.30it/s]Extractor Estimating: 244it [03:10,  1.33it/s]Extractor Estimating: 245it [03:11,  1.36it/s]Extractor Estimating: 246it [03:12,  1.36it/s]Extractor Estimating: 247it [03:12,  1.36it/s]Extractor Estimating: 248it [03:13,  1.33it/s]Extractor Estimating: 249it [03:14,  1.35it/s]Extractor Estimating: 250it [03:15,  1.26it/s]Extractor Estimating: 250it [03:15,  1.28it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:42,562 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:42,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:42,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:42,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:42,567 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:39:43,275 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:39:43,276 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:39:43,945 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:39:45,017 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:39:45,017 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:47,534 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:47,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:47,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:47,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:39:47,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:39:47,874 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:39:47,875 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:39:48,122 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:39:48,301 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:39:48,301 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 09:34:17,300 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 09:34:17,321 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5099 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 19946
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20046, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20046, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.303, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.345, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.302, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.338, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.331, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.853, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.319, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.333, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.326, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.337, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.860, loss:nan
g_step 1200, step 135, avg_time 1.341, loss:nan
g_step 1300, step 22, avg_time 1.315, loss:nan
g_step 1400, step 122, avg_time 1.324, loss:nan
g_step 1500, step 9, avg_time 1.331, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.866, loss:nan
g_step 1700, step 209, avg_time 1.317, loss:nan
g_step 1800, step 96, avg_time 1.319, loss:nan
g_step 1900, step 196, avg_time 1.332, loss:nan
g_step 2000, step 83, avg_time 1.334, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.845, loss:nan
g_step 2200, step 70, avg_time 1.313, loss:nan
g_step 2300, step 170, avg_time 1.334, loss:nan
g_step 2400, step 57, avg_time 1.316, loss:nan
g_step 2500, step 157, avg_time 1.330, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.852, loss:nan
g_step 2700, step 144, avg_time 1.315, loss:nan
g_step 2800, step 31, avg_time 1.316, loss:nan
g_step 2900, step 131, avg_time 1.332, loss:nan
g_step 3000, step 18, avg_time 1.317, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.857, loss:nan
g_step 3200, step 5, avg_time 1.326, loss:nan
g_step 3300, step 105, avg_time 1.318, loss:nan
g_step 3400, step 205, avg_time 1.311, loss:nan
g_step 3500, step 92, avg_time 1.329, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.849, loss:nan
g_step 3700, step 79, avg_time 1.323, loss:nan
g_step 3800, step 179, avg_time 1.313, loss:nan
g_step 3900, step 66, avg_time 1.336, loss:nan
g_step 4000, step 166, avg_time 1.331, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.853, loss:nan
g_step 4200, step 153, avg_time 1.332, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:34:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:34:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-34-17_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:34:18 - WARNING - datasets.builder -   Using custom data configuration default-17b04cb97ae77f9f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-17b04cb97ae77f9f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:34:18,615 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:34:18,616 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:34:18,616 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:34:18,617 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:34:18,632 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:18,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:34:18,811 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:34:22,043 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:34:22,046 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-17b04cb97ae77f9f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.03ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.86ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.21ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.38ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.48ba/s]100%|██████████| 6/6 [00:01<00:00,  4.98ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.17ba/s] 40%|████      | 2/5 [00:00<00:00,  4.37ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.45ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.48ba/s]100%|██████████| 5/5 [00:01<00:00,  5.20ba/s]100%|██████████| 5/5 [00:01<00:00,  4.81ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.70ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.62ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.93ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.22ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.08ba/s]100%|██████████| 6/6 [00:00<00:00, 10.51ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.74ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.31ba/s]100%|██████████| 5/5 [00:00<00:00, 10.65ba/s]100%|██████████| 5/5 [00:00<00:00, 10.18ba/s]
[INFO|trainer.py:414] 2023-08-28 09:34:25,737 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:34:25,753 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:34:25,753 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 09:34:25,753 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:34:25,753 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:34:25,753 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:34:25,753 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:34:25,753 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<01:55,  3.45it/s]  0%|          | 2/400 [00:00<01:53,  3.52it/s]  1%|          | 3/400 [00:00<01:52,  3.54it/s]  1%|          | 4/400 [00:01<01:51,  3.55it/s]  1%|▏         | 5/400 [00:01<01:51,  3.56it/s]  2%|▏         | 6/400 [00:01<01:50,  3.56it/s]  2%|▏         | 7/400 [00:01<01:51,  3.53it/s]  2%|▏         | 8/400 [00:02<01:51,  3.52it/s]  2%|▏         | 9/400 [00:02<01:51,  3.51it/s]  2%|▎         | 10/400 [00:02<01:53,  3.44it/s]  3%|▎         | 11/400 [00:03<01:52,  3.46it/s]  3%|▎         | 12/400 [00:03<01:51,  3.47it/s]  3%|▎         | 13/400 [00:03<01:51,  3.48it/s]  4%|▎         | 14/400 [00:04<01:50,  3.48it/s]  4%|▍         | 15/400 [00:04<01:50,  3.49it/s]  4%|▍         | 16/400 [00:04<01:50,  3.49it/s]  4%|▍         | 17/400 [00:04<01:49,  3.49it/s]  4%|▍         | 18/400 [00:05<01:49,  3.49it/s]  5%|▍         | 19/400 [00:05<01:49,  3.49it/s]  5%|▌         | 20/400 [00:05<01:48,  3.49it/s]  5%|▌         | 21/400 [00:06<01:48,  3.48it/s]  6%|▌         | 22/400 [00:06<01:48,  3.49it/s]  6%|▌         | 23/400 [00:06<01:48,  3.49it/s]  6%|▌         | 24/400 [00:06<01:47,  3.49it/s]  6%|▋         | 25/400 [00:07<01:47,  3.49it/s]  6%|▋         | 26/400 [00:07<01:47,  3.49it/s]  7%|▋         | 27/400 [00:07<01:46,  3.49it/s]  7%|▋         | 28/400 [00:08<01:46,  3.49it/s]  7%|▋         | 29/400 [00:08<01:46,  3.49it/s]  8%|▊         | 30/400 [00:08<01:46,  3.49it/s]  8%|▊         | 31/400 [00:08<01:45,  3.49it/s]  8%|▊         | 32/400 [00:09<01:45,  3.48it/s]  8%|▊         | 33/400 [00:09<01:45,  3.49it/s]  8%|▊         | 34/400 [00:09<01:44,  3.49it/s]  9%|▉         | 35/400 [00:10<01:44,  3.48it/s]  9%|▉         | 36/400 [00:10<01:44,  3.49it/s]  9%|▉         | 37/400 [00:10<01:44,  3.49it/s] 10%|▉         | 38/400 [00:10<01:43,  3.48it/s] 10%|▉         | 39/400 [00:11<01:43,  3.49it/s] 10%|█         | 40/400 [00:11<01:43,  3.49it/s] 10%|█         | 41/400 [00:11<01:42,  3.49it/s] 10%|█         | 42/400 [00:12<01:42,  3.49it/s] 11%|█         | 43/400 [00:12<01:42,  3.48it/s] 11%|█         | 44/400 [00:12<01:42,  3.48it/s] 11%|█▏        | 45/400 [00:12<01:41,  3.49it/s] 12%|█▏        | 46/400 [00:13<01:41,  3.48it/s] 12%|█▏        | 47/400 [00:13<01:41,  3.49it/s] 12%|█▏        | 48/400 [00:13<01:40,  3.49it/s] 12%|█▏        | 49/400 [00:14<01:40,  3.49it/s] 12%|█▎        | 50/400 [00:14<01:40,  3.49it/s] 13%|█▎        | 51/400 [00:14<01:39,  3.49it/s] 13%|█▎        | 52/400 [00:14<01:39,  3.49it/s] 13%|█▎        | 53/400 [00:15<01:39,  3.49it/s] 14%|█▎        | 54/400 [00:15<01:39,  3.48it/s] 14%|█▍        | 55/400 [00:15<01:39,  3.48it/s] 14%|█▍        | 56/400 [00:16<01:38,  3.48it/s] 14%|█▍        | 57/400 [00:16<01:38,  3.48it/s] 14%|█▍        | 58/400 [00:16<01:38,  3.48it/s] 15%|█▍        | 59/400 [00:16<01:37,  3.48it/s] 15%|█▌        | 60/400 [00:17<01:37,  3.48it/s] 15%|█▌        | 61/400 [00:17<01:37,  3.48it/s] 16%|█▌        | 62/400 [00:17<01:37,  3.48it/s] 16%|█▌        | 63/400 [00:18<01:36,  3.48it/s] 16%|█▌        | 64/400 [00:18<01:36,  3.47it/s] 16%|█▋        | 65/400 [00:18<01:36,  3.47it/s] 16%|█▋        | 66/400 [00:18<01:36,  3.47it/s] 17%|█▋        | 67/400 [00:19<01:35,  3.47it/s] 17%|█▋        | 68/400 [00:19<01:35,  3.47it/s] 17%|█▋        | 69/400 [00:19<01:35,  3.48it/s] 18%|█▊        | 70/400 [00:20<01:34,  3.48it/s] 18%|█▊        | 71/400 [00:20<01:34,  3.48it/s] 18%|█▊        | 72/400 [00:20<01:34,  3.48it/s] 18%|█▊        | 73/400 [00:20<01:33,  3.48it/s] 18%|█▊        | 74/400 [00:21<01:33,  3.48it/s] 19%|█▉        | 75/400 [00:21<01:33,  3.49it/s] 19%|█▉        | 76/400 [00:21<01:33,  3.47it/s] 19%|█▉        | 77/400 [00:22<01:32,  3.47it/s] 20%|█▉        | 78/400 [00:22<01:32,  3.47it/s] 20%|█▉        | 79/400 [00:22<01:32,  3.48it/s] 20%|██        | 80/400 [00:22<01:24,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 09:34:48,623 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:34:48,623 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:34:48,623 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.72it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.46it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.85it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.48it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.78it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.49it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.95it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.12it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.21it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.33it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.45it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.37it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.14it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.07it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.80it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.77it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.95it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.15it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.26it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.37it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.14it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.99it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.90it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.84it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.80it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.95it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.28it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.43it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.37it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.20it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.04it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.91it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.87it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.97it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.18it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.29it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.35it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.17it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.12it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.95it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.88it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.94it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.05it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.17it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.25it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.36it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.37it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.22it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.06it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.98it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.93it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.07it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.15it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.26it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.33it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.16it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.24it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.07it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.95it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.92it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.16it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.25it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.28it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.24it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.16it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.03it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.97it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.99it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.13it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.30it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.28it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.41it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.27it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.01it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.95it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.11it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.17it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.33it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.25it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.25it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.09it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.06it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.09it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.05it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.03it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.16it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.16it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.30it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.22it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.04it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.06it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.07it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.90it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.86it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.01it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.16it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.32it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.21it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.04it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.04it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.10it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.98it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.23it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.36it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.39it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.24it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.03it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A                                                
                                                 [A 20%|██        | 80/400 [00:35<01:24,  3.79it/s]
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:35:01,712 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 09:35:01,731 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:35:03,737 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:35:03,763 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:35:03,774 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:38<26:06,  4.91s/it] 20%|██        | 82/400 [00:38<18:40,  3.52s/it] 21%|██        | 83/400 [00:39<13:29,  2.55s/it] 21%|██        | 84/400 [00:39<09:52,  1.87s/it] 21%|██▏       | 85/400 [00:39<07:20,  1.40s/it] 22%|██▏       | 86/400 [00:40<05:34,  1.06s/it] 22%|██▏       | 87/400 [00:40<04:20,  1.20it/s] 22%|██▏       | 88/400 [00:40<03:28,  1.50it/s] 22%|██▏       | 89/400 [00:40<02:52,  1.80it/s] 22%|██▎       | 90/400 [00:41<02:27,  2.11it/s] 23%|██▎       | 91/400 [00:41<02:09,  2.38it/s] 23%|██▎       | 92/400 [00:41<01:57,  2.63it/s] 23%|██▎       | 93/400 [00:42<01:48,  2.84it/s] 24%|██▎       | 94/400 [00:42<01:41,  3.00it/s] 24%|██▍       | 95/400 [00:42<01:37,  3.13it/s] 24%|██▍       | 96/400 [00:42<01:34,  3.23it/s] 24%|██▍       | 97/400 [00:43<01:31,  3.30it/s] 24%|██▍       | 98/400 [00:43<01:30,  3.35it/s] 25%|██▍       | 99/400 [00:43<01:28,  3.39it/s] 25%|██▌       | 100/400 [00:44<01:27,  3.42it/s] 25%|██▌       | 101/400 [00:44<01:27,  3.44it/s] 26%|██▌       | 102/400 [00:44<01:26,  3.44it/s] 26%|██▌       | 103/400 [00:44<01:26,  3.45it/s] 26%|██▌       | 104/400 [00:45<01:25,  3.46it/s] 26%|██▋       | 105/400 [00:45<01:25,  3.46it/s] 26%|██▋       | 106/400 [00:45<01:24,  3.47it/s] 27%|██▋       | 107/400 [00:46<01:24,  3.47it/s] 27%|██▋       | 108/400 [00:46<01:24,  3.47it/s] 27%|██▋       | 109/400 [00:46<01:23,  3.47it/s] 28%|██▊       | 110/400 [00:46<01:23,  3.47it/s] 28%|██▊       | 111/400 [00:47<01:23,  3.47it/s] 28%|██▊       | 112/400 [00:47<01:23,  3.47it/s] 28%|██▊       | 113/400 [00:47<01:22,  3.46it/s] 28%|██▊       | 114/400 [00:48<01:22,  3.46it/s] 29%|██▉       | 115/400 [00:48<01:22,  3.47it/s] 29%|██▉       | 116/400 [00:48<01:21,  3.47it/s] 29%|██▉       | 117/400 [00:48<01:21,  3.47it/s] 30%|██▉       | 118/400 [00:49<01:21,  3.47it/s] 30%|██▉       | 119/400 [00:49<01:20,  3.47it/s] 30%|███       | 120/400 [00:49<01:20,  3.47it/s] 30%|███       | 121/400 [00:50<01:20,  3.47it/s] 30%|███       | 122/400 [00:50<01:20,  3.47it/s] 31%|███       | 123/400 [00:50<01:20,  3.46it/s] 31%|███       | 124/400 [00:51<01:19,  3.45it/s] 31%|███▏      | 125/400 [00:51<01:19,  3.46it/s] 32%|███▏      | 126/400 [00:51<01:21,  3.37it/s] 32%|███▏      | 127/400 [00:51<01:20,  3.39it/s] 32%|███▏      | 128/400 [00:52<01:19,  3.42it/s] 32%|███▏      | 129/400 [00:52<01:18,  3.43it/s] 32%|███▎      | 130/400 [00:52<01:18,  3.45it/s] 33%|███▎      | 131/400 [00:53<01:17,  3.45it/s] 33%|███▎      | 132/400 [00:53<01:17,  3.46it/s] 33%|███▎      | 133/400 [00:53<01:17,  3.46it/s] 34%|███▎      | 134/400 [00:53<01:16,  3.47it/s] 34%|███▍      | 135/400 [00:54<01:16,  3.46it/s] 34%|███▍      | 136/400 [00:54<01:16,  3.47it/s] 34%|███▍      | 137/400 [00:54<01:15,  3.47it/s] 34%|███▍      | 138/400 [00:55<01:15,  3.47it/s] 35%|███▍      | 139/400 [00:55<01:15,  3.47it/s] 35%|███▌      | 140/400 [00:55<01:14,  3.48it/s] 35%|███▌      | 141/400 [00:55<01:14,  3.47it/s] 36%|███▌      | 142/400 [00:56<01:14,  3.47it/s] 36%|███▌      | 143/400 [00:56<01:13,  3.48it/s] 36%|███▌      | 144/400 [00:56<01:13,  3.47it/s] 36%|███▋      | 145/400 [00:57<01:13,  3.47it/s] 36%|███▋      | 146/400 [00:57<01:13,  3.48it/s] 37%|███▋      | 147/400 [00:57<01:12,  3.47it/s] 37%|███▋      | 148/400 [00:57<01:12,  3.48it/s] 37%|███▋      | 149/400 [00:58<01:12,  3.48it/s] 38%|███▊      | 150/400 [00:58<01:11,  3.50it/s] 38%|███▊      | 151/400 [00:58<01:10,  3.51it/s] 38%|███▊      | 152/400 [00:59<01:10,  3.52it/s] 38%|███▊      | 153/400 [00:59<01:09,  3.53it/s] 38%|███▊      | 154/400 [00:59<01:10,  3.51it/s] 39%|███▉      | 155/400 [00:59<01:09,  3.52it/s] 39%|███▉      | 156/400 [01:00<01:09,  3.50it/s] 39%|███▉      | 157/400 [01:00<01:09,  3.50it/s] 40%|███▉      | 158/400 [01:00<01:09,  3.49it/s] 40%|███▉      | 159/400 [01:01<01:09,  3.49it/s] 40%|████      | 160/400 [01:01<01:03,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 09:35:27,054 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:35:27,054 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:35:27,054 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0695, 'eval_samples_per_second': 352.652, 'eval_steps_per_second': 44.148, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.40it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.38it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.66it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.54it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.94it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.56it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.22it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.87it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.89it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.10it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.23it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.31it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.37it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.27it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.11it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.95it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.79it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.87it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.11it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.20it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.24it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.19it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.88it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.76it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.99it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.11it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.26it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.26it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.26it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.12it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.92it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.88it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.89it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.00it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.17it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.30it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.24it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.25it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.01it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.91it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.95it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.89it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.94it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.06it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.27it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.25it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.20it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.00it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.90it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.96it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.79it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.92it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.03it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.26it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.29it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.11it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.99it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.94it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.90it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.92it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.08it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.29it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.11it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.99it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.93it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.91it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.98it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.99it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.07it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.22it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.26it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.09it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.01it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.94it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.96it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.88it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.93it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.02it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.24it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.09it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.15it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.97it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.01it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.01it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.97it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.95it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.94it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.03it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.21it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.20it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.04it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.97it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.00it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.00it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.06it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.03it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.21it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.11it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.03it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.03it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.04it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.97it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.93it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.05it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.05it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.07it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.99it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A 40%|████      | 160/400 [01:14<01:03,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:35:40,164 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 09:35:40,185 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:35:42,165 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:35:42,182 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:35:42,190 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:16<19:29,  4.89s/it] 40%|████      | 162/400 [01:17<13:56,  3.51s/it] 41%|████      | 163/400 [01:17<10:03,  2.55s/it] 41%|████      | 164/400 [01:17<07:20,  1.87s/it] 41%|████▏     | 165/400 [01:18<05:27,  1.39s/it] 42%|████▏     | 166/400 [01:18<04:08,  1.06s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:18<03:13,  1.21it/s] 42%|████▏     | 168/400 [01:19<02:34,  1.50it/s] 42%|████▏     | 169/400 [01:19<02:07,  1.81it/s] 42%|████▎     | 170/400 [01:19<01:48,  2.11it/s] 43%|████▎     | 171/400 [01:19<01:35,  2.39it/s] 43%|████▎     | 172/400 [01:20<01:26,  2.64it/s] 43%|████▎     | 173/400 [01:20<01:19,  2.84it/s] 44%|████▎     | 174/400 [01:20<01:15,  3.01it/s] 44%|████▍     | 175/400 [01:21<01:11,  3.13it/s] 44%|████▍     | 176/400 [01:21<01:09,  3.23it/s] 44%|████▍     | 177/400 [01:21<01:07,  3.30it/s] 44%|████▍     | 178/400 [01:21<01:06,  3.35it/s] 45%|████▍     | 179/400 [01:22<01:05,  3.39it/s] 45%|████▌     | 180/400 [01:22<01:04,  3.42it/s] 45%|████▌     | 181/400 [01:22<01:03,  3.43it/s] 46%|████▌     | 182/400 [01:23<01:03,  3.44it/s] 46%|████▌     | 183/400 [01:23<01:02,  3.45it/s] 46%|████▌     | 184/400 [01:23<01:02,  3.46it/s] 46%|████▋     | 185/400 [01:23<01:01,  3.48it/s] 46%|████▋     | 186/400 [01:24<01:01,  3.50it/s] 47%|████▋     | 187/400 [01:24<01:00,  3.52it/s] 47%|████▋     | 188/400 [01:24<01:00,  3.52it/s] 47%|████▋     | 189/400 [01:25<00:59,  3.53it/s] 48%|████▊     | 190/400 [01:25<00:59,  3.53it/s] 48%|████▊     | 191/400 [01:25<00:59,  3.54it/s] 48%|████▊     | 192/400 [01:25<00:58,  3.54it/s] 48%|████▊     | 193/400 [01:26<00:58,  3.52it/s] 48%|████▊     | 194/400 [01:26<00:58,  3.53it/s] 49%|████▉     | 195/400 [01:26<00:58,  3.53it/s] 49%|████▉     | 196/400 [01:27<00:57,  3.53it/s] 49%|████▉     | 197/400 [01:27<00:57,  3.51it/s] 50%|████▉     | 198/400 [01:27<00:57,  3.52it/s] 50%|████▉     | 199/400 [01:27<00:56,  3.53it/s] 50%|█████     | 200/400 [01:28<00:56,  3.53it/s] 50%|█████     | 201/400 [01:28<00:56,  3.53it/s] 50%|█████     | 202/400 [01:28<00:56,  3.53it/s] 51%|█████     | 203/400 [01:28<00:55,  3.54it/s] 51%|█████     | 204/400 [01:29<00:55,  3.54it/s] 51%|█████▏    | 205/400 [01:29<00:54,  3.55it/s] 52%|█████▏    | 206/400 [01:29<00:54,  3.55it/s] 52%|█████▏    | 207/400 [01:30<00:54,  3.55it/s] 52%|█████▏    | 208/400 [01:30<00:54,  3.54it/s] 52%|█████▏    | 209/400 [01:30<00:53,  3.54it/s] 52%|█████▎    | 210/400 [01:30<00:53,  3.54it/s] 53%|█████▎    | 211/400 [01:31<00:53,  3.54it/s] 53%|█████▎    | 212/400 [01:31<00:53,  3.54it/s] 53%|█████▎    | 213/400 [01:31<00:52,  3.54it/s] 54%|█████▎    | 214/400 [01:32<00:52,  3.55it/s] 54%|█████▍    | 215/400 [01:32<00:52,  3.55it/s] 54%|█████▍    | 216/400 [01:32<00:51,  3.55it/s] 54%|█████▍    | 217/400 [01:32<00:51,  3.55it/s] 55%|█████▍    | 218/400 [01:33<00:51,  3.54it/s] 55%|█████▍    | 219/400 [01:33<00:51,  3.54it/s] 55%|█████▌    | 220/400 [01:33<00:50,  3.54it/s] 55%|█████▌    | 221/400 [01:34<00:50,  3.54it/s] 56%|█████▌    | 222/400 [01:34<00:50,  3.55it/s] 56%|█████▌    | 223/400 [01:34<00:49,  3.55it/s] 56%|█████▌    | 224/400 [01:34<00:49,  3.55it/s] 56%|█████▋    | 225/400 [01:35<00:49,  3.55it/s] 56%|█████▋    | 226/400 [01:35<00:49,  3.55it/s] 57%|█████▋    | 227/400 [01:35<00:48,  3.55it/s] 57%|█████▋    | 228/400 [01:36<00:48,  3.54it/s] 57%|█████▋    | 229/400 [01:36<00:48,  3.54it/s] 57%|█████▊    | 230/400 [01:36<00:48,  3.53it/s] 58%|█████▊    | 231/400 [01:36<00:47,  3.53it/s] 58%|█████▊    | 232/400 [01:37<00:47,  3.54it/s] 58%|█████▊    | 233/400 [01:37<00:47,  3.54it/s] 58%|█████▊    | 234/400 [01:37<00:46,  3.54it/s] 59%|█████▉    | 235/400 [01:38<00:46,  3.53it/s] 59%|█████▉    | 236/400 [01:38<00:46,  3.54it/s] 59%|█████▉    | 237/400 [01:38<00:46,  3.54it/s] 60%|█████▉    | 238/400 [01:38<00:45,  3.54it/s] 60%|█████▉    | 239/400 [01:39<00:45,  3.55it/s] 60%|██████    | 240/400 [01:39<00:41,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 09:36:05,119 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:36:05,119 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:36:05,119 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0885, 'eval_samples_per_second': 352.142, 'eval_steps_per_second': 44.085, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.80it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.48it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.56it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.59it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.89it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.55it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.19it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.99it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.11it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.20it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.36it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.32it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.08it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.15it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.02it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.00it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.93it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.99it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.16it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.23it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.31it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.17it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.09it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.91it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.91it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.97it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.08it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.13it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.22it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.24it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.11it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.78it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.89it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.90it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.84it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.10it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.11it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.26it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.21it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.13it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.87it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.84it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.99it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.13it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.20it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.15it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.19it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.90it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.84it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.84it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.99it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.12it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.21it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.11it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.96it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.87it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.89it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.06it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.18it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.23it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.05it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.06it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.02it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.82it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.06it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.14it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.16it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.27it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.20it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.09it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.95it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.96it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.90it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.00it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.13it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.11it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.21it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.12it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.03it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.96it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.77it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.89it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.93it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.14it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.24it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.22it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.07it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.00it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.92it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.89it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.98it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.19it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.23it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.20it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.07it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.00it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.81it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.99it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.99it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.14it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 42.45it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.25it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.47it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.48it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.68it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.75it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.85it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A 60%|██████    | 240/400 [01:52<00:41,  3.87it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:36:18,231 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 09:36:18,259 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:36:19,781 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:36:19,796 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:36:19,807 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [01:54<12:37,  4.76s/it] 60%|██████    | 242/400 [01:54<09:00,  3.42s/it] 61%|██████    | 243/400 [01:55<06:29,  2.48s/it] 61%|██████    | 244/400 [01:55<04:43,  1.82s/it] 61%|██████▏   | 245/400 [01:55<03:30,  1.36s/it] 62%|██████▏   | 246/400 [01:56<02:39,  1.04s/it] 62%|██████▏   | 247/400 [01:56<02:03,  1.24it/s] 62%|██████▏   | 248/400 [01:56<01:38,  1.54it/s] 62%|██████▏   | 249/400 [01:56<01:21,  1.85it/s] 62%|██████▎   | 250/400 [01:57<01:09,  2.16it/s] 63%|██████▎   | 251/400 [01:57<01:00,  2.45it/s] 63%|██████▎   | 252/400 [01:57<00:54,  2.70it/s] 63%|██████▎   | 253/400 [01:58<00:50,  2.91it/s] 64%|██████▎   | 254/400 [01:58<00:47,  3.07it/s] 64%|██████▍   | 255/400 [01:58<00:45,  3.20it/s] 64%|██████▍   | 256/400 [01:58<00:43,  3.30it/s] 64%|██████▍   | 257/400 [01:59<00:42,  3.37it/s] 64%|██████▍   | 258/400 [01:59<00:41,  3.43it/s] 65%|██████▍   | 259/400 [01:59<00:40,  3.47it/s] 65%|██████▌   | 260/400 [01:59<00:40,  3.49it/s] 65%|██████▌   | 261/400 [02:00<00:39,  3.51it/s] 66%|██████▌   | 262/400 [02:00<00:39,  3.53it/s] 66%|██████▌   | 263/400 [02:00<00:38,  3.53it/s] 66%|██████▌   | 264/400 [02:01<00:38,  3.53it/s] 66%|██████▋   | 265/400 [02:01<00:38,  3.53it/s] 66%|██████▋   | 266/400 [02:01<00:37,  3.54it/s] 67%|██████▋   | 267/400 [02:01<00:37,  3.54it/s] 67%|██████▋   | 268/400 [02:02<00:37,  3.55it/s] 67%|██████▋   | 269/400 [02:02<00:36,  3.55it/s] 68%|██████▊   | 270/400 [02:02<00:36,  3.55it/s] 68%|██████▊   | 271/400 [02:03<00:36,  3.55it/s] 68%|██████▊   | 272/400 [02:03<00:36,  3.55it/s] 68%|██████▊   | 273/400 [02:03<00:35,  3.55it/s] 68%|██████▊   | 274/400 [02:03<00:35,  3.55it/s] 69%|██████▉   | 275/400 [02:04<00:35,  3.53it/s] 69%|██████▉   | 276/400 [02:04<00:35,  3.54it/s] 69%|██████▉   | 277/400 [02:04<00:34,  3.54it/s] 70%|██████▉   | 278/400 [02:05<00:34,  3.55it/s] 70%|██████▉   | 279/400 [02:05<00:34,  3.55it/s] 70%|███████   | 280/400 [02:05<00:33,  3.55it/s] 70%|███████   | 281/400 [02:05<00:33,  3.55it/s] 70%|███████   | 282/400 [02:06<00:33,  3.55it/s] 71%|███████   | 283/400 [02:06<00:33,  3.54it/s] 71%|███████   | 284/400 [02:06<00:32,  3.55it/s] 71%|███████▏  | 285/400 [02:07<00:32,  3.54it/s] 72%|███████▏  | 286/400 [02:07<00:32,  3.54it/s] 72%|███████▏  | 287/400 [02:07<00:31,  3.54it/s] 72%|███████▏  | 288/400 [02:07<00:31,  3.55it/s] 72%|███████▏  | 289/400 [02:08<00:31,  3.55it/s] 72%|███████▎  | 290/400 [02:08<00:30,  3.55it/s] 73%|███████▎  | 291/400 [02:08<00:30,  3.55it/s] 73%|███████▎  | 292/400 [02:09<00:30,  3.55it/s] 73%|███████▎  | 293/400 [02:09<00:30,  3.55it/s] 74%|███████▎  | 294/400 [02:09<00:29,  3.55it/s] 74%|███████▍  | 295/400 [02:09<00:29,  3.54it/s] 74%|███████▍  | 296/400 [02:10<00:29,  3.54it/s] 74%|███████▍  | 297/400 [02:10<00:29,  3.53it/s] 74%|███████▍  | 298/400 [02:10<00:28,  3.54it/s] 75%|███████▍  | 299/400 [02:10<00:28,  3.54it/s] 75%|███████▌  | 300/400 [02:11<00:28,  3.54it/s] 75%|███████▌  | 301/400 [02:11<00:27,  3.54it/s] 76%|███████▌  | 302/400 [02:11<00:27,  3.54it/s] 76%|███████▌  | 303/400 [02:12<00:27,  3.54it/s] 76%|███████▌  | 304/400 [02:12<00:27,  3.54it/s] 76%|███████▋  | 305/400 [02:12<00:26,  3.55it/s] 76%|███████▋  | 306/400 [02:12<00:26,  3.55it/s] 77%|███████▋  | 307/400 [02:13<00:26,  3.55it/s] 77%|███████▋  | 308/400 [02:13<00:25,  3.54it/s] 77%|███████▋  | 309/400 [02:13<00:25,  3.55it/s] 78%|███████▊  | 310/400 [02:14<00:25,  3.54it/s] 78%|███████▊  | 311/400 [02:14<00:25,  3.54it/s] 78%|███████▊  | 312/400 [02:14<00:24,  3.54it/s] 78%|███████▊  | 313/400 [02:14<00:24,  3.54it/s] 78%|███████▊  | 314/400 [02:15<00:24,  3.55it/s] 79%|███████▉  | 315/400 [02:15<00:23,  3.55it/s] 79%|███████▉  | 316/400 [02:15<00:23,  3.55it/s] 79%|███████▉  | 317/400 [02:16<00:23,  3.55it/s] 80%|███████▉  | 318/400 [02:16<00:23,  3.55it/s] 80%|███████▉  | 319/400 [02:16<00:22,  3.54it/s] 80%|████████  | 320/400 [02:16<00:20,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 09:36:42,591 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:36:42,592 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:36:42,592 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0967, 'eval_samples_per_second': 351.921, 'eval_steps_per_second': 44.057, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.98it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.69it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.78it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.45it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.83it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.41it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.23it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.96it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.00it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.18it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.24it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.44it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.41it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.13it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.99it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.92it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.95it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.03it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.20it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.31it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.32it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.05it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.96it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.81it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.84it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.92it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.15it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.24it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.29it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.30it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.07it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.87it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.83it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.83it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.90it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.18it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.25it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.29it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.28it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.14it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.94it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.77it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.83it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.97it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.15it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.36it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.22it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.21it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.04it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.93it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.83it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.84it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.97it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.29it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.29it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.17it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.93it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.80it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.92it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.95it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.04it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.20it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.10it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.01it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.99it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.97it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.03it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.22it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.31it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.24it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.20it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.94it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.96it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.87it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.99it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.93it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.11it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.28it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.17it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.14it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.93it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.00it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.95it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.04it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.15it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.22it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.16it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.10it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.09it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.96it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.03it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.03it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.07it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.11it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.12it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.10it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.07it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.05it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.09it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.14it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.13it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.07it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.09it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.97it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.08it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A                                                 
                                                 [A 80%|████████  | 320/400 [02:29<00:20,  3.87it/s]
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:36:55,690 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 09:36:55,713 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:36:57,332 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:36:57,348 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:36:57,363 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [02:32<06:18,  4.79s/it] 80%|████████  | 322/400 [02:32<04:28,  3.44s/it] 81%|████████  | 323/400 [02:32<03:12,  2.49s/it] 81%|████████  | 324/400 [02:33<02:19,  1.83s/it] 81%|████████▏ | 325/400 [02:33<01:42,  1.37s/it] 82%|████████▏ | 326/400 [02:33<01:17,  1.04s/it] 82%|████████▏ | 327/400 [02:33<00:59,  1.22it/s] 82%|████████▏ | 328/400 [02:34<00:47,  1.52it/s] 82%|████████▏ | 329/400 [02:34<00:38,  1.83it/s] 82%|████████▎ | 330/400 [02:34<00:32,  2.13it/s] 83%|████████▎ | 331/400 [02:35<00:28,  2.41it/s] 83%|████████▎ | 332/400 [02:35<00:25,  2.65it/s] 83%|████████▎ | 333/400 [02:35<00:23,  2.85it/s] 84%|████████▎ | 334/400 [02:35<00:21,  3.02it/s] 84%|████████▍ | 335/400 [02:36<00:20,  3.14it/s] 84%|████████▍ | 336/400 [02:36<00:19,  3.23it/s] 84%|████████▍ | 337/400 [02:36<00:19,  3.31it/s] 84%|████████▍ | 338/400 [02:37<00:18,  3.36it/s] 85%|████████▍ | 339/400 [02:37<00:17,  3.39it/s] 85%|████████▌ | 340/400 [02:37<00:17,  3.42it/s] 85%|████████▌ | 341/400 [02:37<00:17,  3.43it/s] 86%|████████▌ | 342/400 [02:38<00:16,  3.44it/s] 86%|████████▌ | 343/400 [02:38<00:16,  3.45it/s] 86%|████████▌ | 344/400 [02:38<00:16,  3.46it/s] 86%|████████▋ | 345/400 [02:39<00:15,  3.47it/s] 86%|████████▋ | 346/400 [02:39<00:15,  3.47it/s] 87%|████████▋ | 347/400 [02:39<00:15,  3.47it/s] 87%|████████▋ | 348/400 [02:39<00:14,  3.48it/s] 87%|████████▋ | 349/400 [02:40<00:14,  3.47it/s] 88%|████████▊ | 350/400 [02:40<00:14,  3.47it/s] 88%|████████▊ | 351/400 [02:40<00:14,  3.47it/s] 88%|████████▊ | 352/400 [02:41<00:13,  3.48it/s] 88%|████████▊ | 353/400 [02:41<00:13,  3.49it/s] 88%|████████▊ | 354/400 [02:41<00:13,  3.50it/s] 89%|████████▉ | 355/400 [02:41<00:12,  3.52it/s] 89%|████████▉ | 356/400 [02:42<00:12,  3.53it/s] 89%|████████▉ | 357/400 [02:42<00:12,  3.53it/s] 90%|████████▉ | 358/400 [02:42<00:11,  3.54it/s] 90%|████████▉ | 359/400 [02:43<00:11,  3.54it/s] 90%|█████████ | 360/400 [02:43<00:11,  3.54it/s] 90%|█████████ | 361/400 [02:43<00:10,  3.55it/s] 90%|█████████ | 362/400 [02:43<00:10,  3.55it/s] 91%|█████████ | 363/400 [02:44<00:10,  3.54it/s] 91%|█████████ | 364/400 [02:44<00:10,  3.50it/s] 91%|█████████▏| 365/400 [02:44<00:10,  3.50it/s] 92%|█████████▏| 366/400 [02:45<00:09,  3.49it/s] 92%|█████████▏| 367/400 [02:45<00:09,  3.48it/s] 92%|█████████▏| 368/400 [02:45<00:09,  3.48it/s] 92%|█████████▏| 369/400 [02:45<00:08,  3.48it/s] 92%|█████████▎| 370/400 [02:46<00:08,  3.48it/s] 93%|█████████▎| 371/400 [02:46<00:08,  3.48it/s] 93%|█████████▎| 372/400 [02:46<00:08,  3.49it/s] 93%|█████████▎| 373/400 [02:47<00:07,  3.51it/s] 94%|█████████▎| 374/400 [02:47<00:07,  3.52it/s] 94%|█████████▍| 375/400 [02:47<00:07,  3.51it/s] 94%|█████████▍| 376/400 [02:47<00:06,  3.53it/s] 94%|█████████▍| 377/400 [02:48<00:06,  3.54it/s] 94%|█████████▍| 378/400 [02:48<00:06,  3.54it/s] 95%|█████████▍| 379/400 [02:48<00:05,  3.54it/s] 95%|█████████▌| 380/400 [02:49<00:05,  3.55it/s] 95%|█████████▌| 381/400 [02:49<00:05,  3.54it/s] 96%|█████████▌| 382/400 [02:49<00:05,  3.54it/s] 96%|█████████▌| 383/400 [02:49<00:04,  3.55it/s] 96%|█████████▌| 384/400 [02:50<00:04,  3.55it/s] 96%|█████████▋| 385/400 [02:50<00:04,  3.55it/s] 96%|█████████▋| 386/400 [02:50<00:03,  3.53it/s] 97%|█████████▋| 387/400 [02:51<00:03,  3.54it/s] 97%|█████████▋| 388/400 [02:51<00:03,  3.54it/s] 97%|█████████▋| 389/400 [02:51<00:03,  3.53it/s] 98%|█████████▊| 390/400 [02:51<00:02,  3.45it/s] 98%|█████████▊| 391/400 [02:52<00:02,  3.47it/s] 98%|█████████▊| 392/400 [02:52<00:02,  3.49it/s] 98%|█████████▊| 393/400 [02:52<00:01,  3.51it/s] 98%|█████████▊| 394/400 [02:53<00:01,  3.52it/s] 99%|█████████▉| 395/400 [02:53<00:01,  3.52it/s] 99%|█████████▉| 396/400 [02:53<00:01,  3.53it/s] 99%|█████████▉| 397/400 [02:53<00:00,  3.53it/s]100%|█████████▉| 398/400 [02:54<00:00,  3.53it/s]100%|█████████▉| 399/400 [02:54<00:00,  3.54it/s]100%|██████████| 400/400 [02:54<00:00,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 09:37:20,416 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:37:20,416 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:37:20,416 >>   Batch size = 8
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.081, 'eval_samples_per_second': 352.343, 'eval_steps_per_second': 44.11, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.92it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.47it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.51it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.51it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.85it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.51it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.20it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.02it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.16it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.17it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.28it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.43it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.18it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.22it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.01it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.96it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.08it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.22it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.32it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.30it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.16it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.19it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.89it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.99it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.99it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.28it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.34it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.17it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.13it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.11it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.04it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.99it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.17it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.27it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.21it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.23it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.16it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.08it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.01it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.00it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.09it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.14it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.11it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.25it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.27it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.25it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.13it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.03it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.02it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.12it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 43.86it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.09it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.10it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.96it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.03it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.00it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.03it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.17it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.19it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.28it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.17it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.08it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.03it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.05it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.13it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.19it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.19it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.20it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.13it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.07it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.13it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.11it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.10it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.17it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.18it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.14it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.07it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.15it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.00it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.17it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.14it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.10it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.15it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.12it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.19it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.09it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.19it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.20it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.16it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.18it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.10it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.07it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.10it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.04it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.18it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.17it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.18it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.10it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.16it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.23it/s][A                                                 
                                                 [A100%|██████████| 400/400 [03:07<00:00,  3.86it/s]
100%|██████████| 577/577 [00:13<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:37:33,491 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 09:37:33,512 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:37:35,332 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:37:35,348 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:37:35,361 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:37:35,630 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:37:35,630 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80 (score: 1.12112557888031).
                                                 100%|██████████| 400/400 [03:11<00:00,  3.86it/s]100%|██████████| 400/400 [03:11<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 09:37:37,261 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 09:37:37,282 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:37:39,105 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:37:39,118 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:37:39,127 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:37:39,318 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   train_runtime            = 0:03:11.50
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   train_samples_per_second =    133.159
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:39,318 >>   train_steps_per_second   =      2.089
{'eval_loss': 1.12112557888031, 'eval_runtime': 13.0663, 'eval_samples_per_second': 352.74, 'eval_steps_per_second': 44.16, 'epoch': 5.0}
{'train_runtime': 191.5008, 'train_samples_per_second': 133.159, 'train_steps_per_second': 2.089, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 09:37:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:37:39,360 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:37:39,360 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 09:37:39,360 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.21it/s]  2%|▏         | 12/577 [00:00<00:11, 48.45it/s]  3%|▎         | 17/577 [00:00<00:11, 46.80it/s]  4%|▍         | 22/577 [00:00<00:12, 46.17it/s]  5%|▍         | 27/577 [00:00<00:12, 45.64it/s]  6%|▌         | 32/577 [00:00<00:12, 45.40it/s]  6%|▋         | 37/577 [00:00<00:11, 45.24it/s]  7%|▋         | 42/577 [00:00<00:11, 44.77it/s]  8%|▊         | 47/577 [00:01<00:11, 44.28it/s]  9%|▉         | 52/577 [00:01<00:11, 43.92it/s] 10%|▉         | 57/577 [00:01<00:11, 44.04it/s] 11%|█         | 62/577 [00:01<00:11, 44.24it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.46it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.63it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.57it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.56it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.34it/s] 16%|█▌        | 92/577 [00:02<00:11, 44.05it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.86it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.98it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.09it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.35it/s] 20%|██        | 117/577 [00:02<00:10, 44.57it/s] 21%|██        | 122/577 [00:02<00:10, 44.61it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.49it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.23it/s] 24%|██▎       | 137/577 [00:03<00:09, 44.04it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.92it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.97it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.11it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.21it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.43it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.62it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.57it/s] 31%|███       | 177/577 [00:03<00:09, 44.30it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.01it/s] 32%|███▏      | 187/577 [00:04<00:08, 43.89it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.93it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.11it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.26it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.26it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.57it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.57it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.31it/s] 39%|███▉      | 227/577 [00:05<00:07, 44.09it/s] 40%|████      | 232/577 [00:05<00:07, 43.90it/s] 41%|████      | 237/577 [00:05<00:07, 43.99it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.14it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.29it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.47it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.53it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.45it/s] 46%|████▋     | 267/577 [00:06<00:07, 44.18it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.10it/s] 48%|████▊     | 277/577 [00:06<00:06, 44.04it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.10it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.11it/s] 51%|█████     | 292/577 [00:06<00:06, 44.33it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.38it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.53it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.36it/s] 54%|█████▍    | 312/577 [00:07<00:05, 44.26it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.03it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.05it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.19it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.32it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.38it/s] 60%|██████    | 347/577 [00:07<00:05, 44.52it/s] 61%|██████    | 352/577 [00:07<00:05, 44.46it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.27it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.01it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.07it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.06it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.20it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.36it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.31it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.41it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.36it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.21it/s] 71%|███████   | 407/577 [00:09<00:03, 44.05it/s] 71%|███████▏  | 412/577 [00:09<00:03, 43.94it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.03it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.17it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.32it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.40it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.50it/s] 77%|███████▋  | 442/577 [00:09<00:03, 44.34it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.21it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.12it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.00it/s] 80%|████████  | 462/577 [00:10<00:02, 43.97it/s] 81%|████████  | 467/577 [00:10<00:02, 44.13it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.30it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.40it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.45it/s] 84%|████████▍ | 487/577 [00:10<00:02, 44.37it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.15it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.12it/s] 87%|████████▋ | 502/577 [00:11<00:01, 43.92it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.00it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.22it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.33it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.35it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.45it/s] 92%|█████████▏| 532/577 [00:11<00:01, 44.29it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.21it/s] 94%|█████████▍| 542/577 [00:12<00:00, 43.93it/s] 95%|█████████▍| 547/577 [00:12<00:00, 43.93it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.00it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.09it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.30it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.31it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.34it/s]100%|██████████| 577/577 [00:13<00:00, 44.38it/s]100%|██████████| 577/577 [00:13<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:37:52,392 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,392 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,392 >>   eval_loss               =     1.1211
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,393 >>   eval_runtime            = 0:00:13.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,393 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,393 >>   eval_samples_per_second =    353.668
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,393 >>   eval_steps_per_second   =     44.276
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:37:52,393 >>   perplexity              =     3.0683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:59,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:59,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:59,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:59,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:59,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:37:59,672 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:37:59,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:38:00,235 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:38:01,266 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:38:01,266 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:04,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:04,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:04,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:04,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:04,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:38:04,817 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:38:04,819 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:38:05,385 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:38:05,581 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:38:05,581 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-160
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-80
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-320
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/generator/iter5/model/checkpoint-400
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.42it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.42it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:12,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.45it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:16,  1.39it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:19,  1.44it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.45it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.42it/s]Extractor Predicting: 34it [00:24,  1.38it/s]Extractor Predicting: 35it [00:24,  1.41it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:26,  1.40it/s]Extractor Predicting: 39it [00:27,  1.30it/s]Extractor Predicting: 40it [00:28,  1.33it/s]Extractor Predicting: 41it [00:29,  1.31it/s]Extractor Predicting: 42it [00:30,  1.33it/s]Extractor Predicting: 43it [00:30,  1.33it/s]Extractor Predicting: 44it [00:31,  1.33it/s]Extractor Predicting: 45it [00:32,  1.33it/s]Extractor Predicting: 46it [00:33,  1.33it/s]Extractor Predicting: 47it [00:33,  1.33it/s]Extractor Predicting: 48it [00:34,  1.34it/s]Extractor Predicting: 49it [00:35,  1.40it/s]Extractor Predicting: 50it [00:35,  1.39it/s]Extractor Predicting: 51it [00:36,  1.42it/s]Extractor Predicting: 52it [00:37,  1.38it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:38,  1.37it/s]Extractor Predicting: 55it [00:39,  1.37it/s]Extractor Predicting: 56it [00:40,  1.39it/s]Extractor Predicting: 57it [00:41,  1.38it/s]Extractor Predicting: 58it [00:41,  1.36it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:43,  1.40it/s]Extractor Predicting: 61it [00:43,  1.37it/s]Extractor Predicting: 62it [00:44,  1.35it/s]Extractor Predicting: 63it [00:45,  1.34it/s]Extractor Predicting: 64it [00:46,  1.32it/s]Extractor Predicting: 65it [00:46,  1.34it/s]Extractor Predicting: 66it [00:47,  1.38it/s]Extractor Predicting: 67it [00:48,  1.39it/s]Extractor Predicting: 68it [00:49,  1.42it/s]Extractor Predicting: 69it [00:49,  1.43it/s]Extractor Predicting: 70it [00:50,  1.41it/s]Extractor Predicting: 71it [00:51,  1.44it/s]Extractor Predicting: 72it [00:51,  1.48it/s]Extractor Predicting: 73it [00:52,  1.45it/s]Extractor Predicting: 74it [00:53,  1.47it/s]Extractor Predicting: 75it [00:53,  1.47it/s]Extractor Predicting: 76it [00:54,  1.43it/s]Extractor Predicting: 77it [00:55,  1.42it/s]Extractor Predicting: 78it [00:55,  1.44it/s]Extractor Predicting: 79it [00:56,  1.44it/s]Extractor Predicting: 80it [00:57,  1.44it/s]Extractor Predicting: 81it [00:58,  1.41it/s]Extractor Predicting: 82it [00:58,  1.41it/s]Extractor Predicting: 83it [00:59,  1.40it/s]Extractor Predicting: 84it [01:00,  1.44it/s]Extractor Predicting: 85it [01:00,  1.50it/s]Extractor Predicting: 86it [01:01,  1.49it/s]Extractor Predicting: 87it [01:02,  1.52it/s]Extractor Predicting: 88it [01:02,  1.44it/s]Extractor Predicting: 89it [01:03,  1.43it/s]Extractor Predicting: 90it [01:04,  1.41it/s]Extractor Predicting: 91it [01:04,  1.42it/s]Extractor Predicting: 92it [01:05,  1.43it/s]Extractor Predicting: 93it [01:06,  1.44it/s]Extractor Predicting: 94it [01:07,  1.44it/s]Extractor Predicting: 95it [01:07,  1.43it/s]Extractor Predicting: 96it [01:08,  1.45it/s]Extractor Predicting: 97it [01:09,  1.45it/s]Extractor Predicting: 98it [01:09,  1.41it/s]Extractor Predicting: 99it [01:10,  1.41it/s]Extractor Predicting: 100it [01:11,  1.38it/s]Extractor Predicting: 101it [01:12,  1.37it/s]Extractor Predicting: 102it [01:12,  1.40it/s]Extractor Predicting: 103it [01:13,  1.38it/s]Extractor Predicting: 104it [01:14,  1.42it/s]Extractor Predicting: 105it [01:14,  1.44it/s]Extractor Predicting: 106it [01:15,  1.39it/s]Extractor Predicting: 107it [01:16,  1.44it/s]Extractor Predicting: 108it [01:16,  1.45it/s]Extractor Predicting: 109it [01:17,  1.41it/s]Extractor Predicting: 110it [01:18,  1.41it/s]Extractor Predicting: 111it [01:19,  1.38it/s]Extractor Predicting: 112it [01:19,  1.39it/s]Extractor Predicting: 113it [01:20,  1.40it/s]Extractor Predicting: 114it [01:21,  1.42it/s]Extractor Predicting: 115it [01:21,  1.41it/s]Extractor Predicting: 116it [01:22,  1.42it/s]Extractor Predicting: 117it [01:23,  1.45it/s]Extractor Predicting: 118it [01:24,  1.45it/s]Extractor Predicting: 119it [01:24,  1.51it/s]Extractor Predicting: 120it [01:25,  1.49it/s]Extractor Predicting: 121it [01:26,  1.47it/s]Extractor Predicting: 122it [01:26,  1.45it/s]Extractor Predicting: 123it [01:27,  1.43it/s]Extractor Predicting: 124it [01:28,  1.45it/s]Extractor Predicting: 125it [01:28,  1.41it/s]Extractor Predicting: 126it [01:29,  1.46it/s]Extractor Predicting: 127it [01:30,  1.47it/s]Extractor Predicting: 128it [01:30,  1.45it/s]Extractor Predicting: 129it [01:31,  1.44it/s]Extractor Predicting: 130it [01:32,  1.41it/s]Extractor Predicting: 131it [01:33,  1.41it/s]Extractor Predicting: 132it [01:33,  1.31it/s]Extractor Predicting: 133it [01:34,  1.34it/s]Extractor Predicting: 134it [01:35,  1.35it/s]Extractor Predicting: 135it [01:36,  1.36it/s]Extractor Predicting: 136it [01:36,  1.37it/s]Extractor Predicting: 137it [01:37,  1.40it/s]Extractor Predicting: 138it [01:38,  1.39it/s]Extractor Predicting: 139it [01:38,  1.37it/s]Extractor Predicting: 140it [01:39,  1.36it/s]Extractor Predicting: 141it [01:40,  1.35it/s]Extractor Predicting: 142it [01:41,  1.34it/s]Extractor Predicting: 143it [01:41,  1.36it/s]Extractor Predicting: 144it [01:42,  1.34it/s]Extractor Predicting: 145it [01:43,  1.35it/s]Extractor Predicting: 146it [01:44,  1.31it/s]Extractor Predicting: 147it [01:44,  1.32it/s]Extractor Predicting: 148it [01:45,  1.34it/s]Extractor Predicting: 149it [01:46,  1.37it/s]Extractor Predicting: 150it [01:47,  1.36it/s]Extractor Predicting: 151it [01:47,  1.39it/s]Extractor Predicting: 152it [01:48,  1.38it/s]Extractor Predicting: 153it [01:49,  1.40it/s]Extractor Predicting: 154it [01:49,  1.39it/s]Extractor Predicting: 155it [01:50,  1.39it/s]Extractor Predicting: 156it [01:51,  1.36it/s]Extractor Predicting: 157it [01:52,  1.38it/s]Extractor Predicting: 158it [01:52,  1.39it/s]Extractor Predicting: 159it [01:53,  1.38it/s]Extractor Predicting: 160it [01:54,  1.38it/s]Extractor Predicting: 161it [01:55,  1.41it/s]Extractor Predicting: 162it [01:55,  1.40it/s]Extractor Predicting: 163it [01:56,  1.36it/s]Extractor Predicting: 164it [01:57,  1.36it/s]Extractor Predicting: 165it [01:57,  1.37it/s]Extractor Predicting: 166it [01:58,  1.35it/s]Extractor Predicting: 167it [01:59,  1.36it/s]Extractor Predicting: 168it [02:00,  1.37it/s]Extractor Predicting: 169it [02:00,  1.37it/s]Extractor Predicting: 170it [02:01,  1.38it/s]Extractor Predicting: 171it [02:01,  1.65it/s]Extractor Predicting: 171it [02:01,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:15,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:15,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:15,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:15,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:15,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:40:15,714 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:40:15,715 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:40:16,288 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:40:17,334 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:40:17,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:20,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:20,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:20,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:20,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:20,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:40:20,839 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:40:20,840 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:40:21,419 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:40:21,574 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:40:21,574 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:07,  1.37it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:12,  1.42it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.44it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.42it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.41it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:26,  1.42it/s]Extractor Predicting: 39it [00:27,  1.41it/s]Extractor Predicting: 40it [00:28,  1.43it/s]Extractor Predicting: 41it [00:28,  1.43it/s]Extractor Predicting: 42it [00:29,  1.43it/s]Extractor Predicting: 43it [00:30,  1.44it/s]Extractor Predicting: 44it [00:31,  1.45it/s]Extractor Predicting: 45it [00:31,  1.45it/s]Extractor Predicting: 46it [00:32,  1.46it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:33,  1.44it/s]Extractor Predicting: 49it [00:34,  1.44it/s]Extractor Predicting: 50it [00:35,  1.43it/s]Extractor Predicting: 51it [00:35,  1.44it/s]Extractor Predicting: 52it [00:36,  1.44it/s]Extractor Predicting: 53it [00:37,  1.44it/s]Extractor Predicting: 54it [00:38,  1.42it/s]Extractor Predicting: 55it [00:38,  1.40it/s]Extractor Predicting: 56it [00:39,  1.42it/s]Extractor Predicting: 57it [00:40,  1.43it/s]Extractor Predicting: 58it [00:40,  1.43it/s]Extractor Predicting: 59it [00:41,  1.44it/s]Extractor Predicting: 60it [00:42,  1.41it/s]Extractor Predicting: 61it [00:42,  1.41it/s]Extractor Predicting: 62it [00:43,  1.39it/s]Extractor Predicting: 63it [00:44,  1.43it/s]Extractor Predicting: 64it [00:45,  1.42it/s]Extractor Predicting: 65it [00:45,  1.44it/s]Extractor Predicting: 66it [00:46,  1.43it/s]Extractor Predicting: 67it [00:47,  1.44it/s]Extractor Predicting: 68it [00:47,  1.46it/s]Extractor Predicting: 69it [00:48,  1.47it/s]Extractor Predicting: 70it [00:49,  1.45it/s]Extractor Predicting: 71it [00:49,  1.39it/s]Extractor Predicting: 72it [00:50,  1.41it/s]Extractor Predicting: 73it [00:51,  1.40it/s]Extractor Predicting: 74it [00:52,  1.42it/s]Extractor Predicting: 75it [00:52,  1.42it/s]Extractor Predicting: 76it [00:53,  1.45it/s]Extractor Predicting: 77it [00:54,  1.49it/s]Extractor Predicting: 78it [00:54,  1.48it/s]Extractor Predicting: 79it [00:55,  1.46it/s]Extractor Predicting: 80it [00:56,  1.48it/s]Extractor Predicting: 81it [00:56,  1.47it/s]Extractor Predicting: 82it [00:57,  1.48it/s]Extractor Predicting: 83it [00:58,  1.47it/s]Extractor Predicting: 84it [00:58,  1.45it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.46it/s]Extractor Predicting: 87it [01:00,  1.43it/s]Extractor Predicting: 88it [01:01,  1.44it/s]Extractor Predicting: 89it [01:02,  1.44it/s]Extractor Predicting: 90it [01:03,  1.42it/s]Extractor Predicting: 91it [01:03,  1.43it/s]Extractor Predicting: 92it [01:04,  1.43it/s]Extractor Predicting: 93it [01:05,  1.44it/s]Extractor Predicting: 94it [01:05,  1.42it/s]Extractor Predicting: 95it [01:06,  1.41it/s]Extractor Predicting: 96it [01:07,  1.38it/s]Extractor Predicting: 97it [01:08,  1.42it/s]Extractor Predicting: 98it [01:08,  1.42it/s]Extractor Predicting: 99it [01:09,  1.50it/s]Extractor Predicting: 100it [01:09,  1.49it/s]Extractor Predicting: 101it [01:10,  1.47it/s]Extractor Predicting: 102it [01:11,  1.48it/s]Extractor Predicting: 103it [01:12,  1.48it/s]Extractor Predicting: 104it [01:12,  1.48it/s]Extractor Predicting: 105it [01:13,  1.47it/s]Extractor Predicting: 106it [01:14,  1.48it/s]Extractor Predicting: 107it [01:14,  1.50it/s]Extractor Predicting: 108it [01:15,  1.48it/s]Extractor Predicting: 109it [01:16,  1.43it/s]Extractor Predicting: 110it [01:17,  1.26it/s]Extractor Predicting: 111it [01:17,  1.28it/s]Extractor Predicting: 112it [01:18,  1.30it/s]Extractor Predicting: 113it [01:19,  1.36it/s]Extractor Predicting: 114it [01:20,  1.39it/s]Extractor Predicting: 115it [01:20,  1.39it/s]Extractor Predicting: 116it [01:21,  1.39it/s]Extractor Predicting: 117it [01:22,  1.39it/s]Extractor Predicting: 118it [01:22,  1.36it/s]Extractor Predicting: 119it [01:23,  1.38it/s]Extractor Predicting: 120it [01:24,  1.35it/s]Extractor Predicting: 121it [01:25,  1.34it/s]Extractor Predicting: 122it [01:25,  1.36it/s]Extractor Predicting: 123it [01:26,  1.38it/s]Extractor Predicting: 124it [01:27,  1.38it/s]Extractor Predicting: 125it [01:27,  1.73it/s]Extractor Predicting: 125it [01:27,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:54,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:54,628 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:54,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:54,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:54,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:41:54,936 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:41:54,937 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:41:55,207 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:41:56,255 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:41:56,255 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:57,650 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:57,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:57,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:57,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:41:57,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:41:58,394 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:41:58,395 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:41:58,653 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:41:58,812 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:41:58,813 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.78it/s]Extractor Predicting: 6it [00:04,  1.47it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_2', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:21<03:15, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:27, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:00<02:22, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:22<02:06, 21.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:39<01:38, 19.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:56<01:15, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:15<00:56, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:31<00:35, 17.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:51<00:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:16<00:00, 20.51s/it]Generating: 100%|██████████| 10/10 [03:16<00:00, 19.65s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')", "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_5_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.15s/it]Extractor Estimating: 2it [00:17,  7.46s/it]Extractor Estimating: 3it [00:18,  4.38s/it]Extractor Estimating: 4it [00:19,  3.17s/it]Extractor Estimating: 5it [00:25,  4.18s/it]Extractor Estimating: 6it [00:26,  3.00s/it]Extractor Estimating: 7it [00:26,  2.27s/it]Extractor Estimating: 8it [00:27,  1.78s/it]Extractor Estimating: 9it [00:28,  1.44s/it]Extractor Estimating: 10it [00:29,  1.24s/it]Extractor Estimating: 11it [00:30,  1.12s/it]Extractor Estimating: 12it [00:30,  1.00s/it]Extractor Estimating: 13it [00:31,  1.08it/s]Extractor Estimating: 14it [00:32,  1.14it/s]Extractor Estimating: 15it [00:33,  1.20it/s]Extractor Estimating: 16it [00:33,  1.22it/s]Extractor Estimating: 17it [00:34,  1.26it/s]Extractor Estimating: 18it [00:35,  1.27it/s]Extractor Estimating: 19it [00:36,  1.28it/s]Extractor Estimating: 20it [00:36,  1.27it/s]Extractor Estimating: 21it [00:37,  1.29it/s]Extractor Estimating: 22it [00:38,  1.29it/s]Extractor Estimating: 23it [00:39,  1.27it/s]Extractor Estimating: 24it [00:40,  1.24it/s]Extractor Estimating: 25it [00:40,  1.27it/s]Extractor Estimating: 26it [00:41,  1.30it/s]Extractor Estimating: 27it [00:42,  1.30it/s]Extractor Estimating: 28it [00:43,  1.32it/s]Extractor Estimating: 29it [00:43,  1.36it/s]Extractor Estimating: 30it [00:44,  1.35it/s]Extractor Estimating: 31it [00:45,  1.35it/s]Extractor Estimating: 32it [00:46,  1.29it/s]Extractor Estimating: 33it [00:46,  1.32it/s]Extractor Estimating: 34it [00:47,  1.34it/s]Extractor Estimating: 35it [00:48,  1.29it/s]Extractor Estimating: 36it [00:49,  1.35it/s]Extractor Estimating: 37it [00:49,  1.32it/s]Extractor Estimating: 38it [00:50,  1.34it/s]Extractor Estimating: 39it [00:51,  1.36it/s]Extractor Estimating: 40it [00:52,  1.32it/s]Extractor Estimating: 41it [00:52,  1.28it/s]Extractor Estimating: 42it [00:53,  1.31it/s]Extractor Estimating: 43it [00:54,  1.35it/s]Extractor Estimating: 44it [00:55,  1.37it/s]Extractor Estimating: 45it [00:55,  1.37it/s]Extractor Estimating: 46it [00:56,  1.37it/s]Extractor Estimating: 47it [00:57,  1.35it/s]Extractor Estimating: 48it [00:57,  1.36it/s]Extractor Estimating: 49it [00:58,  1.34it/s]Extractor Estimating: 50it [00:59,  1.36it/s]Extractor Estimating: 51it [01:00,  1.34it/s]Extractor Estimating: 52it [01:01,  1.32it/s]Extractor Estimating: 53it [01:01,  1.33it/s]Extractor Estimating: 54it [01:02,  1.32it/s]Extractor Estimating: 55it [01:03,  1.34it/s]Extractor Estimating: 56it [01:04,  1.30it/s]Extractor Estimating: 57it [01:04,  1.28it/s]Extractor Estimating: 58it [01:05,  1.34it/s]Extractor Estimating: 59it [01:06,  1.35it/s]Extractor Estimating: 60it [01:07,  1.34it/s]Extractor Estimating: 61it [01:07,  1.31it/s]Extractor Estimating: 62it [01:08,  1.33it/s]Extractor Estimating: 63it [01:09,  1.36it/s]Extractor Estimating: 64it [01:10,  1.33it/s]Extractor Estimating: 65it [01:10,  1.32it/s]Extractor Estimating: 66it [01:11,  1.30it/s]Extractor Estimating: 67it [01:12,  1.29it/s]Extractor Estimating: 68it [01:13,  1.31it/s]Extractor Estimating: 69it [01:13,  1.32it/s]Extractor Estimating: 70it [01:14,  1.30it/s]Extractor Estimating: 71it [01:15,  1.29it/s]Extractor Estimating: 72it [01:16,  1.29it/s]Extractor Estimating: 73it [01:16,  1.30it/s]Extractor Estimating: 74it [01:17,  1.32it/s]Extractor Estimating: 75it [01:18,  1.33it/s]Extractor Estimating: 76it [01:19,  1.37it/s]Extractor Estimating: 77it [01:19,  1.30it/s]Extractor Estimating: 78it [01:20,  1.29it/s]Extractor Estimating: 79it [01:21,  1.35it/s]Extractor Estimating: 80it [01:22,  1.36it/s]Extractor Estimating: 81it [01:22,  1.35it/s]Extractor Estimating: 82it [01:23,  1.31it/s]Extractor Estimating: 83it [01:24,  1.30it/s]Extractor Estimating: 84it [01:25,  1.26it/s]Extractor Estimating: 85it [01:26,  1.26it/s]Extractor Estimating: 86it [01:26,  1.26it/s]Extractor Estimating: 87it [01:27,  1.25it/s]Extractor Estimating: 88it [01:28,  1.29it/s]Extractor Estimating: 89it [01:29,  1.28it/s]Extractor Estimating: 90it [01:30,  1.28it/s]Extractor Estimating: 91it [01:30,  1.27it/s]Extractor Estimating: 92it [01:31,  1.29it/s]Extractor Estimating: 93it [01:32,  1.29it/s]Extractor Estimating: 94it [01:33,  1.27it/s]Extractor Estimating: 95it [01:33,  1.27it/s]Extractor Estimating: 96it [01:34,  1.26it/s]Extractor Estimating: 97it [01:35,  1.17it/s]Extractor Estimating: 98it [01:36,  1.20it/s]Extractor Estimating: 99it [01:37,  1.21it/s]Extractor Estimating: 100it [01:38,  1.27it/s]Extractor Estimating: 101it [01:38,  1.28it/s]Extractor Estimating: 102it [01:39,  1.26it/s]Extractor Estimating: 103it [01:40,  1.28it/s]Extractor Estimating: 104it [01:41,  1.30it/s]Extractor Estimating: 105it [01:41,  1.32it/s]Extractor Estimating: 106it [01:42,  1.31it/s]Extractor Estimating: 107it [01:43,  1.30it/s]Extractor Estimating: 108it [01:44,  1.32it/s]Extractor Estimating: 109it [01:44,  1.32it/s]Extractor Estimating: 110it [01:45,  1.33it/s]Extractor Estimating: 111it [01:46,  1.32it/s]Extractor Estimating: 112it [01:47,  1.31it/s]Extractor Estimating: 113it [01:48,  1.30it/s]Extractor Estimating: 114it [01:48,  1.33it/s]Extractor Estimating: 115it [01:49,  1.33it/s]Extractor Estimating: 116it [01:50,  1.34it/s]Extractor Estimating: 117it [01:50,  1.35it/s]Extractor Estimating: 118it [01:51,  1.36it/s]Extractor Estimating: 119it [01:52,  1.35it/s]Extractor Estimating: 120it [01:53,  1.35it/s]Extractor Estimating: 121it [01:53,  1.35it/s]Extractor Estimating: 122it [01:54,  1.35it/s]Extractor Estimating: 123it [01:55,  1.33it/s]Extractor Estimating: 124it [01:56,  1.32it/s]Extractor Estimating: 125it [01:56,  1.34it/s]Extractor Estimating: 126it [01:57,  1.34it/s]Extractor Estimating: 127it [01:58,  1.30it/s]Extractor Estimating: 128it [01:59,  1.34it/s]Extractor Estimating: 129it [02:00,  1.30it/s]Extractor Estimating: 130it [02:00,  1.26it/s]Extractor Estimating: 131it [02:01,  1.24it/s]Extractor Estimating: 132it [02:02,  1.27it/s]Extractor Estimating: 133it [02:03,  1.27it/s]Extractor Estimating: 134it [02:04,  1.26it/s]Extractor Estimating: 135it [02:04,  1.29it/s]Extractor Estimating: 136it [02:05,  1.31it/s]Extractor Estimating: 137it [02:06,  1.27it/s]Extractor Estimating: 138it [02:07,  1.26it/s]Extractor Estimating: 139it [02:08,  1.23it/s]Extractor Estimating: 140it [02:08,  1.27it/s]Extractor Estimating: 141it [02:09,  1.32it/s]Extractor Estimating: 142it [02:10,  1.33it/s]Extractor Estimating: 143it [02:10,  1.31it/s]Extractor Estimating: 144it [02:11,  1.29it/s]Extractor Estimating: 145it [02:12,  1.28it/s]Extractor Estimating: 146it [02:13,  1.25it/s]Extractor Estimating: 147it [02:14,  1.26it/s]Extractor Estimating: 148it [02:14,  1.28it/s]Extractor Estimating: 149it [02:15,  1.31it/s]Extractor Estimating: 150it [02:16,  1.30it/s]Extractor Estimating: 151it [02:17,  1.33it/s]Extractor Estimating: 152it [02:17,  1.33it/s]Extractor Estimating: 153it [02:18,  1.33it/s]Extractor Estimating: 154it [02:19,  1.33it/s]Extractor Estimating: 155it [02:20,  1.32it/s]Extractor Estimating: 156it [02:20,  1.33it/s]Extractor Estimating: 157it [02:21,  1.31it/s]Extractor Estimating: 158it [02:22,  1.32it/s]Extractor Estimating: 159it [02:23,  1.32it/s]Extractor Estimating: 160it [02:23,  1.31it/s]Extractor Estimating: 161it [02:24,  1.30it/s]Extractor Estimating: 162it [02:25,  1.32it/s]Extractor Estimating: 163it [02:26,  1.34it/s]Extractor Estimating: 164it [02:27,  1.30it/s]Extractor Estimating: 165it [02:27,  1.28it/s]Extractor Estimating: 166it [02:28,  1.29it/s]Extractor Estimating: 167it [02:29,  1.30it/s]Extractor Estimating: 168it [02:30,  1.22it/s]Extractor Estimating: 169it [02:31,  1.24it/s]Extractor Estimating: 170it [02:31,  1.24it/s]Extractor Estimating: 171it [02:32,  1.29it/s]Extractor Estimating: 172it [02:33,  1.28it/s]Extractor Estimating: 173it [02:34,  1.32it/s]Extractor Estimating: 174it [02:34,  1.33it/s]Extractor Estimating: 175it [02:35,  1.31it/s]Extractor Estimating: 176it [02:36,  1.30it/s]Extractor Estimating: 177it [02:37,  1.30it/s]Extractor Estimating: 178it [02:37,  1.28it/s]Extractor Estimating: 179it [02:38,  1.28it/s]Extractor Estimating: 180it [02:39,  1.26it/s]Extractor Estimating: 181it [02:40,  1.30it/s]Extractor Estimating: 182it [02:41,  1.31it/s]Extractor Estimating: 183it [02:41,  1.28it/s]Extractor Estimating: 184it [02:42,  1.29it/s]Extractor Estimating: 185it [02:43,  1.31it/s]Extractor Estimating: 186it [02:44,  1.35it/s]Extractor Estimating: 187it [02:44,  1.34it/s]Extractor Estimating: 188it [02:45,  1.35it/s]Extractor Estimating: 189it [02:46,  1.34it/s]Extractor Estimating: 190it [02:47,  1.29it/s]Extractor Estimating: 191it [02:47,  1.33it/s]Extractor Estimating: 192it [02:48,  1.30it/s]Extractor Estimating: 193it [02:49,  1.30it/s]Extractor Estimating: 194it [02:50,  1.33it/s]Extractor Estimating: 195it [02:50,  1.33it/s]Extractor Estimating: 196it [02:51,  1.35it/s]Extractor Estimating: 197it [02:52,  1.35it/s]Extractor Estimating: 198it [02:53,  1.33it/s]Extractor Estimating: 199it [02:53,  1.27it/s]Extractor Estimating: 200it [02:54,  1.30it/s]Extractor Estimating: 201it [02:55,  1.30it/s]Extractor Estimating: 202it [02:56,  1.31it/s]Extractor Estimating: 203it [02:56,  1.33it/s]Extractor Estimating: 204it [02:57,  1.33it/s]Extractor Estimating: 205it [02:58,  1.33it/s]Extractor Estimating: 206it [02:59,  1.36it/s]Extractor Estimating: 207it [02:59,  1.34it/s]Extractor Estimating: 208it [03:00,  1.35it/s]Extractor Estimating: 209it [03:01,  1.33it/s]Extractor Estimating: 210it [03:02,  1.34it/s]Extractor Estimating: 211it [03:02,  1.32it/s]Extractor Estimating: 212it [03:03,  1.31it/s]Extractor Estimating: 213it [03:04,  1.29it/s]Extractor Estimating: 214it [03:05,  1.32it/s]Extractor Estimating: 215it [03:06,  1.29it/s]Extractor Estimating: 216it [03:06,  1.28it/s]Extractor Estimating: 217it [03:07,  1.29it/s]Extractor Estimating: 218it [03:08,  1.32it/s]Extractor Estimating: 219it [03:09,  1.34it/s]Extractor Estimating: 220it [03:09,  1.34it/s]Extractor Estimating: 221it [03:10,  1.31it/s]Extractor Estimating: 222it [03:11,  1.33it/s]Extractor Estimating: 223it [03:12,  1.35it/s]Extractor Estimating: 224it [03:12,  1.34it/s]Extractor Estimating: 225it [03:13,  1.32it/s]Extractor Estimating: 226it [03:14,  1.31it/s]Extractor Estimating: 227it [03:15,  1.32it/s]Extractor Estimating: 228it [03:15,  1.28it/s]Extractor Estimating: 229it [03:16,  1.30it/s]Extractor Estimating: 230it [03:17,  1.28it/s]Extractor Estimating: 231it [03:18,  1.27it/s]Extractor Estimating: 232it [03:19,  1.30it/s]Extractor Estimating: 233it [03:19,  1.28it/s]Extractor Estimating: 234it [03:20,  1.23it/s]Extractor Estimating: 235it [03:21,  1.25it/s]Extractor Estimating: 236it [03:22,  1.28it/s]Extractor Estimating: 237it [03:23,  1.28it/s]Extractor Estimating: 238it [03:23,  1.29it/s]Extractor Estimating: 239it [03:24,  1.25it/s]Extractor Estimating: 240it [03:25,  1.24it/s]Extractor Estimating: 241it [03:26,  1.24it/s]Extractor Estimating: 242it [03:26,  1.31it/s]Extractor Estimating: 243it [03:27,  1.31it/s]Extractor Estimating: 244it [03:28,  1.34it/s]Extractor Estimating: 245it [03:29,  1.37it/s]Extractor Estimating: 246it [03:29,  1.37it/s]Extractor Estimating: 247it [03:30,  1.36it/s]Extractor Estimating: 248it [03:31,  1.33it/s]Extractor Estimating: 249it [03:32,  1.35it/s]Extractor Estimating: 250it [03:32,  1.28it/s]Extractor Estimating: 250it [03:32,  1.17it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5003 mean pseudo reward: 0.9051245405639752
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 24210
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24310, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_5_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24310, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.468, loss:1347.9843
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.219, loss:1273.7857
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.198, loss:1177.8043
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.207, loss:1225.7763
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.191, loss:1188.5646
>> valid entity prec:0.4121, rec:0.4181, f1:0.4151
>> valid relation prec:0.3356, rec:0.0107, f1:0.0207
>> valid relation with NER prec:0.3356, rec:0.0107, f1:0.0207
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.018, loss:1114.2662
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.199, loss:1122.3330
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.207, loss:1113.5956
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.194, loss:1042.3878
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.202, loss:1050.3765
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4600, rec:0.2757, f1:0.3448
>> valid relation prec:0.2154, rec:0.0347, f1:0.0597
>> valid relation with NER prec:0.2154, rec:0.0347, f1:0.0597
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 55, avg_time 2.984, loss:1035.9889
g_step 1200, step 155, avg_time 1.208, loss:1071.9607
g_step 1300, step 46, avg_time 1.202, loss:976.7206
g_step 1400, step 146, avg_time 1.190, loss:986.2064
g_step 1500, step 37, avg_time 1.213, loss:951.3438
>> valid entity prec:0.4873, rec:0.3580, f1:0.4127
>> valid relation prec:0.1723, rec:0.0133, f1:0.0247
>> valid relation with NER prec:0.1723, rec:0.0133, f1:0.0247
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 2.992, loss:944.7948
g_step 1700, step 28, avg_time 1.204, loss:909.5928
g_step 1800, step 128, avg_time 1.199, loss:896.2984
g_step 1900, step 19, avg_time 1.197, loss:886.9386
g_step 2000, step 119, avg_time 1.196, loss:853.8478
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4635, rec:0.3899, f1:0.4236
>> valid relation prec:0.2179, rec:0.0185, f1:0.0342
>> valid relation with NER prec:0.2179, rec:0.0185, f1:0.0342
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 10, avg_time 3.017, loss:872.2127
g_step 2200, step 110, avg_time 1.198, loss:824.6731
g_step 2300, step 1, avg_time 1.203, loss:838.7150
g_step 2400, step 101, avg_time 1.202, loss:761.9393
g_step 2500, step 201, avg_time 1.209, loss:839.3643
>> valid entity prec:0.4791, rec:0.3263, f1:0.3882
>> valid relation prec:0.1962, rec:0.0179, f1:0.0328
>> valid relation with NER prec:0.1962, rec:0.0179, f1:0.0328
g_step 2600, step 92, avg_time 3.015, loss:751.8472
g_step 2700, step 192, avg_time 1.210, loss:786.5788
g_step 2800, step 83, avg_time 1.208, loss:739.0221
g_step 2900, step 183, avg_time 1.206, loss:755.6703
g_step 3000, step 74, avg_time 1.186, loss:705.6545
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4431, rec:0.4683, f1:0.4554
>> valid relation prec:0.1581, rec:0.0148, f1:0.0271
>> valid relation with NER prec:0.1581, rec:0.0148, f1:0.0271
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 174, avg_time 3.041, loss:731.2683
g_step 3200, step 65, avg_time 1.204, loss:672.4489
g_step 3300, step 165, avg_time 1.199, loss:687.2384
g_step 3400, step 56, avg_time 1.211, loss:670.7210
g_step 3500, step 156, avg_time 1.207, loss:675.7450
>> valid entity prec:0.4578, rec:0.4584, f1:0.4581
>> valid relation prec:0.2157, rec:0.0384, f1:0.0651
>> valid relation with NER prec:0.2157, rec:0.0384, f1:0.0651
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 47, avg_time 3.024, loss:624.8817
g_step 3700, step 147, avg_time 1.208, loss:653.3457
g_step 3800, step 38, avg_time 1.202, loss:616.7425
g_step 3900, step 138, avg_time 1.197, loss:620.1733
g_step 4000, step 29, avg_time 1.203, loss:625.9729
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4847, rec:0.3018, f1:0.3720
>> valid relation prec:0.2470, rec:0.0270, f1:0.0487
>> valid relation with NER prec:0.2470, rec:0.0270, f1:0.0487
g_step 4100, step 129, avg_time 3.011, loss:591.9846
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 11:38:35 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 11:38:35 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_11-38-35_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 11:38:36 - WARNING - datasets.builder -   Using custom data configuration default-976e193af83b8d96
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-976e193af83b8d96/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 11:38:36,436 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:38:36,437 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:38:36,437 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:38:36,438 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:38:36,448 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:38:36,451 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 11:38:36,571 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:38:39,647 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 11:38:39,650 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-976e193af83b8d96/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 11:38:39 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x146a82ec8950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.96ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.77ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.10ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.27ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.38ba/s]100%|██████████| 6/6 [00:01<00:00,  4.89ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.10ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.37ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.41ba/s]100%|██████████| 5/5 [00:01<00:00,  5.10ba/s]100%|██████████| 5/5 [00:01<00:00,  4.73ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.34ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.91ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.30ba/s]100%|██████████| 6/6 [00:00<00:00, 12.03ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.33ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.44ba/s]100%|██████████| 5/5 [00:00<00:00, 11.70ba/s]100%|██████████| 5/5 [00:00<00:00, 11.30ba/s]
[INFO|trainer.py:414] 2023-08-28 11:38:43,463 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 11:38:43,475 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 11:38:43,475 >>   Num examples = 5052
[INFO|trainer.py:1149] 2023-08-28 11:38:43,475 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 11:38:43,475 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 11:38:43,475 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 11:38:43,475 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 11:38:43,475 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<02:47,  2.35it/s]  1%|          | 2/395 [00:00<02:16,  2.88it/s]  1%|          | 3/395 [00:01<02:06,  3.10it/s]  1%|          | 4/395 [00:01<02:01,  3.22it/s]  1%|▏         | 5/395 [00:01<01:58,  3.28it/s]  2%|▏         | 6/395 [00:01<01:56,  3.33it/s]  2%|▏         | 7/395 [00:02<01:55,  3.36it/s]  2%|▏         | 8/395 [00:02<01:55,  3.36it/s]  2%|▏         | 9/395 [00:02<01:54,  3.38it/s]  3%|▎         | 10/395 [00:03<01:53,  3.39it/s]  3%|▎         | 11/395 [00:03<01:53,  3.39it/s]  3%|▎         | 12/395 [00:03<01:52,  3.40it/s]  3%|▎         | 13/395 [00:03<01:52,  3.40it/s]  4%|▎         | 14/395 [00:04<01:51,  3.40it/s]  4%|▍         | 15/395 [00:04<01:51,  3.41it/s]  4%|▍         | 16/395 [00:04<01:51,  3.41it/s]  4%|▍         | 17/395 [00:05<01:50,  3.41it/s]  5%|▍         | 18/395 [00:05<01:50,  3.41it/s]  5%|▍         | 19/395 [00:05<01:50,  3.41it/s]  5%|▌         | 20/395 [00:05<01:49,  3.41it/s]  5%|▌         | 21/395 [00:06<01:49,  3.41it/s]  6%|▌         | 22/395 [00:06<01:49,  3.41it/s]  6%|▌         | 23/395 [00:06<01:49,  3.41it/s]  6%|▌         | 24/395 [00:07<01:48,  3.41it/s]  6%|▋         | 25/395 [00:07<01:48,  3.41it/s]  7%|▋         | 26/395 [00:07<01:48,  3.41it/s]  7%|▋         | 27/395 [00:08<01:48,  3.41it/s]  7%|▋         | 28/395 [00:08<01:47,  3.41it/s]  7%|▋         | 29/395 [00:08<01:47,  3.41it/s]  8%|▊         | 30/395 [00:08<01:47,  3.40it/s]  8%|▊         | 31/395 [00:09<01:46,  3.41it/s]  8%|▊         | 32/395 [00:09<01:46,  3.41it/s]  8%|▊         | 33/395 [00:09<01:46,  3.41it/s]  9%|▊         | 34/395 [00:10<01:45,  3.41it/s]  9%|▉         | 35/395 [00:10<01:45,  3.41it/s]  9%|▉         | 36/395 [00:10<01:45,  3.41it/s]  9%|▉         | 37/395 [00:10<01:45,  3.40it/s] 10%|▉         | 38/395 [00:11<01:44,  3.40it/s] 10%|▉         | 39/395 [00:11<01:44,  3.40it/s] 10%|█         | 40/395 [00:11<01:44,  3.40it/s] 10%|█         | 41/395 [00:12<01:44,  3.40it/s] 11%|█         | 42/395 [00:12<01:43,  3.40it/s] 11%|█         | 43/395 [00:12<01:43,  3.40it/s] 11%|█         | 44/395 [00:13<01:43,  3.40it/s] 11%|█▏        | 45/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 46/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 47/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 48/395 [00:14<01:41,  3.41it/s] 12%|█▏        | 49/395 [00:14<01:42,  3.39it/s] 13%|█▎        | 50/395 [00:14<01:41,  3.39it/s] 13%|█▎        | 51/395 [00:15<01:41,  3.40it/s] 13%|█▎        | 52/395 [00:15<01:40,  3.40it/s] 13%|█▎        | 53/395 [00:15<01:40,  3.40it/s] 14%|█▎        | 54/395 [00:15<01:40,  3.40it/s] 14%|█▍        | 55/395 [00:16<01:40,  3.40it/s] 14%|█▍        | 56/395 [00:16<01:39,  3.40it/s] 14%|█▍        | 57/395 [00:16<01:39,  3.40it/s] 15%|█▍        | 58/395 [00:17<01:39,  3.40it/s] 15%|█▍        | 59/395 [00:17<01:38,  3.40it/s] 15%|█▌        | 60/395 [00:17<01:38,  3.40it/s] 15%|█▌        | 61/395 [00:18<01:38,  3.40it/s] 16%|█▌        | 62/395 [00:18<01:37,  3.40it/s] 16%|█▌        | 63/395 [00:18<01:37,  3.40it/s] 16%|█▌        | 64/395 [00:18<01:37,  3.40it/s] 16%|█▋        | 65/395 [00:19<01:37,  3.40it/s] 17%|█▋        | 66/395 [00:19<01:36,  3.40it/s] 17%|█▋        | 67/395 [00:19<01:36,  3.40it/s] 17%|█▋        | 68/395 [00:20<01:36,  3.40it/s] 17%|█▋        | 69/395 [00:20<01:35,  3.40it/s] 18%|█▊        | 70/395 [00:20<01:35,  3.40it/s] 18%|█▊        | 71/395 [00:20<01:35,  3.40it/s] 18%|█▊        | 72/395 [00:21<01:34,  3.40it/s] 18%|█▊        | 73/395 [00:21<01:34,  3.40it/s] 19%|█▊        | 74/395 [00:21<01:34,  3.40it/s] 19%|█▉        | 75/395 [00:22<01:33,  3.41it/s] 19%|█▉        | 76/395 [00:22<01:33,  3.40it/s] 19%|█▉        | 77/395 [00:22<01:33,  3.40it/s] 20%|█▉        | 78/395 [00:23<01:33,  3.40it/s] 20%|██        | 79/395 [00:23<01:31,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 11:39:06,798 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:39:06,798 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:39:06,799 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 54.60it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.02it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.44it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.41it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.94it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.43it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.39it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.27it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.15it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.17it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.33it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.22it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.05it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.88it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.01it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.07it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.21it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.32it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.14it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.92it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.97it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.81it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.83it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.89it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.03it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.20it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.25it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.18it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.13it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.99it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.94it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.86it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.02it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.11it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.20it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.16it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.18it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.93it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.99it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.89it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.08it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.25it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.08it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.92it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.93it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.85it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.88it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.06it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.15it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.23it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.17it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.98it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.97it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.98it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.91it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.95it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.07it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.19it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.01it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.96it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.89it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.90it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.87it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.01it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.21it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.05it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.92it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.92it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.87it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.92it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.93it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.09it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.09it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.10it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.02it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.94it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.83it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.00it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.00it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.05it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.17it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.15it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.99it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.99it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.02it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.08it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.10it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.20it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.13it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.05it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.03it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.87it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.97it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.01it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.04it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.12it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.15it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.05it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.02it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.92it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.96it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:36<01:31,  3.45it/s]
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:39:19,912 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-28 11:39:19,935 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:39:21,657 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:39:21,677 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:39:21,686 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:42<31:11,  5.94s/it] 21%|██        | 81/395 [00:42<22:14,  4.25s/it] 21%|██        | 82/395 [00:43<15:59,  3.06s/it] 21%|██        | 83/395 [00:43<11:37,  2.24s/it] 21%|██▏       | 84/395 [00:43<08:34,  1.66s/it] 22%|██▏       | 85/395 [00:43<06:27,  1.25s/it] 22%|██▏       | 86/395 [00:44<04:58,  1.04it/s] 22%|██▏       | 87/395 [00:44<03:55,  1.31it/s] 22%|██▏       | 88/395 [00:44<03:12,  1.60it/s] 23%|██▎       | 89/395 [00:45<02:41,  1.89it/s] 23%|██▎       | 90/395 [00:45<02:20,  2.17it/s] 23%|██▎       | 91/395 [00:45<02:05,  2.43it/s] 23%|██▎       | 92/395 [00:46<01:54,  2.64it/s] 24%|██▎       | 93/395 [00:46<01:47,  2.81it/s] 24%|██▍       | 94/395 [00:46<01:42,  2.94it/s] 24%|██▍       | 95/395 [00:46<01:38,  3.05it/s] 24%|██▍       | 96/395 [00:47<01:35,  3.12it/s] 25%|██▍       | 97/395 [00:47<01:33,  3.18it/s] 25%|██▍       | 98/395 [00:47<01:32,  3.22it/s] 25%|██▌       | 99/395 [00:48<01:30,  3.25it/s] 25%|██▌       | 100/395 [00:48<01:30,  3.27it/s] 26%|██▌       | 101/395 [00:48<01:29,  3.29it/s] 26%|██▌       | 102/395 [00:49<01:28,  3.30it/s] 26%|██▌       | 103/395 [00:49<01:28,  3.30it/s] 26%|██▋       | 104/395 [00:49<01:28,  3.30it/s] 27%|██▋       | 105/395 [00:49<01:27,  3.31it/s] 27%|██▋       | 106/395 [00:50<01:27,  3.31it/s] 27%|██▋       | 107/395 [00:50<01:26,  3.31it/s] 27%|██▋       | 108/395 [00:50<01:26,  3.32it/s] 28%|██▊       | 109/395 [00:51<01:26,  3.32it/s] 28%|██▊       | 110/395 [00:51<01:25,  3.32it/s] 28%|██▊       | 111/395 [00:51<01:28,  3.20it/s] 28%|██▊       | 112/395 [00:52<01:27,  3.24it/s] 29%|██▊       | 113/395 [00:52<01:26,  3.26it/s] 29%|██▉       | 114/395 [00:52<01:25,  3.27it/s] 29%|██▉       | 115/395 [00:53<01:25,  3.28it/s] 29%|██▉       | 116/395 [00:53<01:24,  3.29it/s] 30%|██▉       | 117/395 [00:53<01:24,  3.30it/s] 30%|██▉       | 118/395 [00:53<01:23,  3.31it/s] 30%|███       | 119/395 [00:54<01:23,  3.31it/s] 30%|███       | 120/395 [00:54<01:22,  3.32it/s] 31%|███       | 121/395 [00:54<01:22,  3.32it/s] 31%|███       | 122/395 [00:55<01:22,  3.32it/s] 31%|███       | 123/395 [00:55<01:21,  3.32it/s] 31%|███▏      | 124/395 [00:55<01:21,  3.32it/s] 32%|███▏      | 125/395 [00:56<01:21,  3.32it/s] 32%|███▏      | 126/395 [00:56<01:21,  3.31it/s] 32%|███▏      | 127/395 [00:56<01:20,  3.31it/s] 32%|███▏      | 128/395 [00:56<01:20,  3.31it/s] 33%|███▎      | 129/395 [00:57<01:20,  3.32it/s] 33%|███▎      | 130/395 [00:57<01:19,  3.32it/s] 33%|███▎      | 131/395 [00:57<01:19,  3.32it/s] 33%|███▎      | 132/395 [00:58<01:19,  3.32it/s] 34%|███▎      | 133/395 [00:58<01:18,  3.32it/s] 34%|███▍      | 134/395 [00:58<01:18,  3.32it/s] 34%|███▍      | 135/395 [00:59<01:18,  3.32it/s] 34%|███▍      | 136/395 [00:59<01:18,  3.31it/s] 35%|███▍      | 137/395 [00:59<01:17,  3.31it/s] 35%|███▍      | 138/395 [00:59<01:17,  3.31it/s] 35%|███▌      | 139/395 [01:00<01:17,  3.31it/s] 35%|███▌      | 140/395 [01:00<01:16,  3.31it/s] 36%|███▌      | 141/395 [01:00<01:16,  3.31it/s] 36%|███▌      | 142/395 [01:01<01:16,  3.31it/s] 36%|███▌      | 143/395 [01:01<01:16,  3.32it/s] 36%|███▋      | 144/395 [01:01<01:15,  3.32it/s] 37%|███▋      | 145/395 [01:02<01:15,  3.32it/s] 37%|███▋      | 146/395 [01:02<01:15,  3.29it/s] 37%|███▋      | 147/395 [01:02<01:15,  3.30it/s] 37%|███▋      | 148/395 [01:02<01:14,  3.30it/s] 38%|███▊      | 149/395 [01:03<01:14,  3.31it/s] 38%|███▊      | 150/395 [01:03<01:14,  3.31it/s] 38%|███▊      | 151/395 [01:03<01:13,  3.31it/s] 38%|███▊      | 152/395 [01:04<01:13,  3.31it/s] 39%|███▊      | 153/395 [01:04<01:13,  3.31it/s] 39%|███▉      | 154/395 [01:04<01:12,  3.31it/s] 39%|███▉      | 155/395 [01:05<01:12,  3.32it/s] 39%|███▉      | 156/395 [01:05<01:12,  3.31it/s] 40%|███▉      | 157/395 [01:05<01:11,  3.31it/s] 40%|████      | 158/395 [01:05<01:10,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 11:39:49,463 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:39:49,464 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:39:49,464 >>   Batch size = 8
{'eval_loss': 0.9803475737571716, 'eval_runtime': 13.0904, 'eval_samples_per_second': 352.09, 'eval_steps_per_second': 44.078, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.32it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.11it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.33it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.19it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.74it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.20it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.07it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.14it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.29it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.29it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.12it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.09it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.94it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.85it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.78it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.86it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.86it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 43.97it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.05it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.15it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.05it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.93it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.77it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.92it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.98it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.07it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.13it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.12it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.10it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.96it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.99it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.80it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.67it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.06it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.10it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.18it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.05it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.01it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.92it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.96it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.76it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.88it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.11it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.19it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.06it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.02it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.94it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.86it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.87it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.94it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.04it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.23it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.17it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.01it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.00it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.81it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.88it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.83it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.93it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.12it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.18it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.21it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.94it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.83it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.85it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.98it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.09it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.16it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.14it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.03it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.96it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.82it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.82it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.94it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.01it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.09it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.00it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.99it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.86it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.82it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.87it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.94it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.99it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.15it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.09it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.06it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.90it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.79it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.84it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.85it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.95it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.05it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.13it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.19it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.88it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.88it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.97it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.94it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.93it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.00it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.12it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.23it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.13it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.00it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.93it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.90it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.98it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.86it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.85it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A                                                 
                                                 [A 40%|████      | 158/395 [01:19<01:10,  3.36it/s]
100%|██████████| 577/577 [00:13<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:40:02,594 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 11:40:02,615 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:40:04,596 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:40:04,609 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:40:04,618 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:25<23:32,  5.99s/it] 41%|████      | 160/395 [01:25<16:45,  4.28s/it] 41%|████      | 161/395 [01:25<12:02,  3.09s/it] 41%|████      | 162/395 [01:26<08:44,  2.25s/it] 41%|████▏     | 163/395 [01:26<06:26,  1.67s/it] 42%|████▏     | 164/395 [01:26<04:50,  1.26s/it] 42%|████▏     | 165/395 [01:27<03:43,  1.03it/s] 42%|████▏     | 166/395 [01:27<02:56,  1.30it/s] 42%|████▏     | 167/395 [01:27<02:23,  1.59it/s] 43%|████▎     | 168/395 [01:27<02:00,  1.88it/s] 43%|████▎     | 169/395 [01:28<01:44,  2.16it/s] 43%|████▎     | 170/395 [01:28<01:33,  2.41it/s] 43%|████▎     | 171/395 [01:28<01:25,  2.63it/s] 44%|████▎     | 172/395 [01:29<01:19,  2.80it/s] 44%|████▍     | 173/395 [01:29<01:15,  2.94it/s] 44%|████▍     | 174/395 [01:29<01:12,  3.04it/s] 44%|████▍     | 175/395 [01:30<01:10,  3.11it/s] 45%|████▍     | 176/395 [01:30<01:09,  3.17it/s] 45%|████▍     | 177/395 [01:30<01:07,  3.21it/s] 45%|████▌     | 178/395 [01:30<01:06,  3.25it/s] 45%|████▌     | 179/395 [01:31<01:06,  3.27it/s] 46%|████▌     | 180/395 [01:31<01:05,  3.29it/s] 46%|████▌     | 181/395 [01:31<01:04,  3.30it/s] 46%|████▌     | 182/395 [01:32<01:04,  3.30it/s] 46%|████▋     | 183/395 [01:32<01:04,  3.30it/s] 47%|████▋     | 184/395 [01:32<01:03,  3.31it/s] 47%|████▋     | 185/395 [01:33<01:03,  3.30it/s] 47%|████▋     | 186/395 [01:33<01:03,  3.30it/s] 47%|████▋     | 187/395 [01:33<01:02,  3.31it/s] 48%|████▊     | 188/395 [01:33<01:02,  3.31it/s] 48%|████▊     | 189/395 [01:34<01:02,  3.32it/s] 48%|████▊     | 190/395 [01:34<01:01,  3.32it/s] 48%|████▊     | 191/395 [01:34<01:01,  3.32it/s] 49%|████▊     | 192/395 [01:35<01:01,  3.32it/s] 49%|████▉     | 193/395 [01:35<01:00,  3.32it/s] 49%|████▉     | 194/395 [01:35<01:00,  3.32it/s] 49%|████▉     | 195/395 [01:36<01:00,  3.30it/s] 50%|████▉     | 196/395 [01:36<01:00,  3.30it/s] 50%|████▉     | 197/395 [01:36<00:59,  3.31it/s] 50%|█████     | 198/395 [01:37<00:59,  3.31it/s] 50%|█████     | 199/395 [01:37<00:59,  3.31it/s] 51%|█████     | 200/395 [01:37<00:58,  3.32it/s] 51%|█████     | 201/395 [01:37<00:58,  3.32it/s] 51%|█████     | 202/395 [01:38<00:58,  3.32it/s] 51%|█████▏    | 203/395 [01:38<00:57,  3.32it/s] 52%|█████▏    | 204/395 [01:38<00:57,  3.32it/s] 52%|█████▏    | 205/395 [01:39<00:57,  3.30it/s] 52%|█████▏    | 206/395 [01:39<00:57,  3.31it/s] 52%|█████▏    | 207/395 [01:39<00:56,  3.31it/s] 53%|█████▎    | 208/395 [01:40<00:56,  3.31it/s] 53%|█████▎    | 209/395 [01:40<00:56,  3.31it/s] 53%|█████▎    | 210/395 [01:40<00:55,  3.31it/s] 53%|█████▎    | 211/395 [01:40<00:55,  3.32it/s] 54%|█████▎    | 212/395 [01:41<00:55,  3.32it/s] 54%|█████▍    | 213/395 [01:41<00:54,  3.32it/s] 54%|█████▍    | 214/395 [01:41<00:54,  3.32it/s] 54%|█████▍    | 215/395 [01:42<00:54,  3.29it/s] 55%|█████▍    | 216/395 [01:42<00:54,  3.30it/s] 55%|█████▍    | 217/395 [01:42<00:53,  3.31it/s] 55%|█████▌    | 218/395 [01:43<00:53,  3.31it/s] 55%|█████▌    | 219/395 [01:43<00:53,  3.31it/s] 56%|█████▌    | 220/395 [01:43<00:52,  3.32it/s] 56%|█████▌    | 221/395 [01:43<00:52,  3.32it/s] 56%|█████▌    | 222/395 [01:44<00:52,  3.32it/s] 56%|█████▋    | 223/395 [01:44<00:51,  3.32it/s] 57%|█████▋    | 224/395 [01:44<00:51,  3.32it/s] 57%|█████▋    | 225/395 [01:45<00:51,  3.31it/s] 57%|█████▋    | 226/395 [01:45<00:50,  3.32it/s] 57%|█████▋    | 227/395 [01:45<00:50,  3.31it/s] 58%|█████▊    | 228/395 [01:46<00:50,  3.31it/s] 58%|█████▊    | 229/395 [01:46<00:50,  3.31it/s] 58%|█████▊    | 230/395 [01:46<00:49,  3.31it/s] 58%|█████▊    | 231/395 [01:46<00:49,  3.31it/s] 59%|█████▊    | 232/395 [01:47<00:49,  3.32it/s] 59%|█████▉    | 233/395 [01:47<00:48,  3.32it/s] 59%|█████▉    | 234/395 [01:47<00:48,  3.32it/s] 59%|█████▉    | 235/395 [01:48<00:48,  3.29it/s] 60%|█████▉    | 236/395 [01:48<00:48,  3.30it/s] 60%|██████    | 237/395 [01:48<00:47,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 11:40:32,258 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:40:32,259 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:40:32,259 >>   Batch size = 8
{'eval_loss': 0.967439591884613, 'eval_runtime': 13.1083, 'eval_samples_per_second': 351.609, 'eval_steps_per_second': 44.018, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.45it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.39it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.43it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.41it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.93it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.41it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.04it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.02it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.07it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.16it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.29it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.28it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.16it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.04it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.93it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.84it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.78it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.89it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.07it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.14it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.30it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.22it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.95it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.93it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.89it/s][A
 23%|██▎       | 132/577 [00:03<00:11, 40.20it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 42.28it/s][A
 25%|██▍       | 142/577 [00:03<00:10, 43.03it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 43.47it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 43.81it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.01it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.97it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.85it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.81it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.51it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.61it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.90it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.98it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.06it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.21it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.18it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.11it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.95it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.73it/s][A
 39%|███▉      | 227/577 [00:05<00:08, 43.73it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.94it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.12it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.12it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.16it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.94it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.72it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.71it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.80it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.96it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.09it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.20it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.24it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.92it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.79it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.82it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.92it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.96it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.03it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.90it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.77it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.81it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.76it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.01it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.09it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.26it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.10it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.88it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.87it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.85it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.79it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.93it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.05it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.24it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.03it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.98it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.81it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.82it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.82it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.92it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.11it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.17it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.13it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.03it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.85it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.88it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.05it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.07it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.96it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.03it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.97it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.89it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.93it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.02it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.01it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.10it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.01it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.97it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.94it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.95it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.98it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [02:01<00:47,  3.34it/s]
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:40:45,402 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-28 11:40:45,426 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:40:47,482 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:40:47,509 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:40:47,518 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:08<15:52,  6.07s/it] 61%|██████    | 239/395 [02:08<11:16,  4.34s/it] 61%|██████    | 240/395 [02:08<08:04,  3.13s/it] 61%|██████    | 241/395 [02:09<05:50,  2.28s/it] 61%|██████▏   | 242/395 [02:09<04:17,  1.69s/it] 62%|██████▏   | 243/395 [02:09<03:12,  1.27s/it] 62%|██████▏   | 244/395 [02:10<02:27,  1.02it/s] 62%|██████▏   | 245/395 [02:10<01:56,  1.29it/s] 62%|██████▏   | 246/395 [02:10<01:34,  1.58it/s] 63%|██████▎   | 247/395 [02:11<01:18,  1.88it/s] 63%|██████▎   | 248/395 [02:11<01:08,  2.16it/s] 63%|██████▎   | 249/395 [02:11<01:00,  2.41it/s] 63%|██████▎   | 250/395 [02:11<00:55,  2.62it/s] 64%|██████▎   | 251/395 [02:12<00:51,  2.80it/s] 64%|██████▍   | 252/395 [02:12<00:48,  2.94it/s] 64%|██████▍   | 253/395 [02:12<00:46,  3.04it/s] 64%|██████▍   | 254/395 [02:13<00:45,  3.12it/s] 65%|██████▍   | 255/395 [02:13<00:44,  3.18it/s] 65%|██████▍   | 256/395 [02:13<00:43,  3.22it/s] 65%|██████▌   | 257/395 [02:14<00:42,  3.25it/s] 65%|██████▌   | 258/395 [02:14<00:41,  3.27it/s] 66%|██████▌   | 259/395 [02:14<00:41,  3.28it/s] 66%|██████▌   | 260/395 [02:14<00:40,  3.30it/s] 66%|██████▌   | 261/395 [02:15<00:40,  3.30it/s] 66%|██████▋   | 262/395 [02:15<00:40,  3.31it/s] 67%|██████▋   | 263/395 [02:15<00:39,  3.32it/s] 67%|██████▋   | 264/395 [02:16<00:39,  3.32it/s] 67%|██████▋   | 265/395 [02:16<00:39,  3.32it/s] 67%|██████▋   | 266/395 [02:16<00:38,  3.32it/s] 68%|██████▊   | 267/395 [02:17<00:38,  3.33it/s] 68%|██████▊   | 268/395 [02:17<00:38,  3.32it/s] 68%|██████▊   | 269/395 [02:17<00:37,  3.32it/s] 68%|██████▊   | 270/395 [02:17<00:37,  3.32it/s] 69%|██████▊   | 271/395 [02:18<00:37,  3.32it/s] 69%|██████▉   | 272/395 [02:18<00:37,  3.32it/s] 69%|██████▉   | 273/395 [02:18<00:36,  3.32it/s] 69%|██████▉   | 274/395 [02:19<00:36,  3.32it/s] 70%|██████▉   | 275/395 [02:19<00:36,  3.32it/s] 70%|██████▉   | 276/395 [02:19<00:35,  3.32it/s] 70%|███████   | 277/395 [02:20<00:35,  3.32it/s] 70%|███████   | 278/395 [02:20<00:35,  3.32it/s] 71%|███████   | 279/395 [02:20<00:35,  3.30it/s] 71%|███████   | 280/395 [02:20<00:34,  3.30it/s] 71%|███████   | 281/395 [02:21<00:34,  3.30it/s] 71%|███████▏  | 282/395 [02:21<00:34,  3.30it/s] 72%|███████▏  | 283/395 [02:21<00:33,  3.31it/s] 72%|███████▏  | 284/395 [02:22<00:33,  3.31it/s] 72%|███████▏  | 285/395 [02:22<00:33,  3.32it/s] 72%|███████▏  | 286/395 [02:22<00:32,  3.32it/s] 73%|███████▎  | 287/395 [02:23<00:32,  3.32it/s] 73%|███████▎  | 288/395 [02:23<00:32,  3.32it/s] 73%|███████▎  | 289/395 [02:23<00:32,  3.31it/s] 73%|███████▎  | 290/395 [02:23<00:31,  3.31it/s] 74%|███████▎  | 291/395 [02:24<00:31,  3.31it/s] 74%|███████▍  | 292/395 [02:24<00:31,  3.31it/s] 74%|███████▍  | 293/395 [02:24<00:30,  3.31it/s] 74%|███████▍  | 294/395 [02:25<00:30,  3.32it/s] 75%|███████▍  | 295/395 [02:25<00:30,  3.32it/s] 75%|███████▍  | 296/395 [02:25<00:29,  3.32it/s] 75%|███████▌  | 297/395 [02:26<00:29,  3.32it/s] 75%|███████▌  | 298/395 [02:26<00:29,  3.32it/s] 76%|███████▌  | 299/395 [02:26<00:28,  3.32it/s] 76%|███████▌  | 300/395 [02:26<00:28,  3.32it/s] 76%|███████▌  | 301/395 [02:27<00:28,  3.32it/s] 76%|███████▋  | 302/395 [02:27<00:27,  3.32it/s] 77%|███████▋  | 303/395 [02:27<00:27,  3.32it/s] 77%|███████▋  | 304/395 [02:28<00:27,  3.32it/s] 77%|███████▋  | 305/395 [02:28<00:27,  3.30it/s] 77%|███████▋  | 306/395 [02:28<00:26,  3.30it/s] 78%|███████▊  | 307/395 [02:29<00:26,  3.31it/s] 78%|███████▊  | 308/395 [02:29<00:26,  3.31it/s] 78%|███████▊  | 309/395 [02:29<00:25,  3.31it/s] 78%|███████▊  | 310/395 [02:30<00:25,  3.32it/s] 79%|███████▊  | 311/395 [02:30<00:25,  3.32it/s] 79%|███████▉  | 312/395 [02:30<00:25,  3.32it/s] 79%|███████▉  | 313/395 [02:30<00:24,  3.32it/s] 79%|███████▉  | 314/395 [02:31<00:24,  3.32it/s] 80%|███████▉  | 315/395 [02:31<00:24,  3.29it/s] 80%|████████  | 316/395 [02:31<00:23,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 11:41:15,285 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:41:15,285 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:41:15,285 >>   Batch size = 8
{'eval_loss': 0.9696409702301025, 'eval_runtime': 13.129, 'eval_samples_per_second': 351.055, 'eval_steps_per_second': 43.949, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.77it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.28it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.19it/s][A
  4%|▍         | 22/577 [00:00<00:12, 44.99it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.58it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.32it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.13it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.15it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.10it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.35it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.27it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.98it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.97it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.92it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.84it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.88it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.09it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.19it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.23it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.06it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.94it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.90it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.99it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.93it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.93it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.10it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.27it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.17it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.11it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.00it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.86it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.92it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.00it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.94it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.11it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.21it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.16it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.07it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.94it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.87it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.91it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.96it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.96it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.92it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.00it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.16it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.08it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.90it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.93it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.02it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.93it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.98it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.06it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.08it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.05it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.95it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.87it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.94it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.94it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.99it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.88it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.01it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.16it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.95it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.88it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.95it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.95it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.96it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.97it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.13it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.11it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.04it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.95it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.01it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.06it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.96it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.10it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.12it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.06it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.97it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.96it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.94it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.00it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.04it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.92it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.01it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.06it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.02it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.00it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.89it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.04it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.03it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.00it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.09it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.04it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.02it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.95it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.92it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.94it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.99it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.02it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.05it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.06it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.94it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.93it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.97it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.00it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [02:44<00:23,  3.34it/s]
100%|██████████| 577/577 [00:13<00:00, 44.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:41:28,417 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 11:41:28,443 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:41:30,196 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:41:30,213 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:41:30,224 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [02:51<08:03,  6.20s/it] 81%|████████  | 318/395 [02:52<05:42,  4.44s/it] 81%|████████  | 319/395 [02:52<04:03,  3.20s/it] 81%|████████  | 320/395 [02:52<02:54,  2.33s/it] 81%|████████▏ | 321/395 [02:53<02:07,  1.72s/it] 82%|████████▏ | 322/395 [02:53<01:34,  1.30s/it] 82%|████████▏ | 323/395 [02:53<01:11,  1.00it/s] 82%|████████▏ | 324/395 [02:53<00:55,  1.27it/s] 82%|████████▏ | 325/395 [02:54<00:44,  1.56it/s] 83%|████████▎ | 326/395 [02:54<00:37,  1.85it/s] 83%|████████▎ | 327/395 [02:54<00:31,  2.14it/s] 83%|████████▎ | 328/395 [02:55<00:28,  2.39it/s] 83%|████████▎ | 329/395 [02:55<00:25,  2.61it/s] 84%|████████▎ | 330/395 [02:55<00:23,  2.79it/s] 84%|████████▍ | 331/395 [02:56<00:21,  2.93it/s] 84%|████████▍ | 332/395 [02:56<00:20,  3.04it/s] 84%|████████▍ | 333/395 [02:56<00:19,  3.12it/s] 85%|████████▍ | 334/395 [02:56<00:19,  3.18it/s] 85%|████████▍ | 335/395 [02:57<00:18,  3.22it/s] 85%|████████▌ | 336/395 [02:57<00:18,  3.25it/s] 85%|████████▌ | 337/395 [02:57<00:17,  3.27it/s] 86%|████████▌ | 338/395 [02:58<00:17,  3.29it/s] 86%|████████▌ | 339/395 [02:58<00:16,  3.29it/s] 86%|████████▌ | 340/395 [02:58<00:16,  3.30it/s] 86%|████████▋ | 341/395 [02:59<00:16,  3.31it/s] 87%|████████▋ | 342/395 [02:59<00:16,  3.30it/s] 87%|████████▋ | 343/395 [02:59<00:15,  3.31it/s] 87%|████████▋ | 344/395 [02:59<00:15,  3.31it/s] 87%|████████▋ | 345/395 [03:00<00:15,  3.32it/s] 88%|████████▊ | 346/395 [03:00<00:14,  3.32it/s] 88%|████████▊ | 347/395 [03:00<00:14,  3.32it/s] 88%|████████▊ | 348/395 [03:01<00:14,  3.32it/s] 88%|████████▊ | 349/395 [03:01<00:13,  3.32it/s] 89%|████████▊ | 350/395 [03:01<00:13,  3.32it/s] 89%|████████▉ | 351/395 [03:02<00:13,  3.32it/s] 89%|████████▉ | 352/395 [03:02<00:13,  3.28it/s] 89%|████████▉ | 353/395 [03:02<00:12,  3.29it/s] 90%|████████▉ | 354/395 [03:02<00:12,  3.30it/s] 90%|████████▉ | 355/395 [03:03<00:12,  3.31it/s] 90%|█████████ | 356/395 [03:03<00:11,  3.31it/s] 90%|█████████ | 357/395 [03:03<00:11,  3.31it/s] 91%|█████████ | 358/395 [03:04<00:11,  3.31it/s] 91%|█████████ | 359/395 [03:04<00:10,  3.31it/s] 91%|█████████ | 360/395 [03:04<00:10,  3.31it/s] 91%|█████████▏| 361/395 [03:05<00:10,  3.32it/s] 92%|█████████▏| 362/395 [03:05<00:09,  3.31it/s] 92%|█████████▏| 363/395 [03:05<00:09,  3.31it/s] 92%|█████████▏| 364/395 [03:05<00:09,  3.32it/s] 92%|█████████▏| 365/395 [03:06<00:09,  3.32it/s] 93%|█████████▎| 366/395 [03:06<00:08,  3.32it/s] 93%|█████████▎| 367/395 [03:06<00:08,  3.32it/s] 93%|█████████▎| 368/395 [03:07<00:08,  3.32it/s] 93%|█████████▎| 369/395 [03:07<00:07,  3.32it/s] 94%|█████████▎| 370/395 [03:07<00:07,  3.32it/s] 94%|█████████▍| 371/395 [03:08<00:07,  3.32it/s] 94%|█████████▍| 372/395 [03:08<00:06,  3.29it/s] 94%|█████████▍| 373/395 [03:08<00:06,  3.30it/s] 95%|█████████▍| 374/395 [03:09<00:06,  3.31it/s] 95%|█████████▍| 375/395 [03:09<00:06,  3.31it/s] 95%|█████████▌| 376/395 [03:09<00:05,  3.31it/s] 95%|█████████▌| 377/395 [03:09<00:05,  3.32it/s] 96%|█████████▌| 378/395 [03:10<00:05,  3.32it/s] 96%|█████████▌| 379/395 [03:10<00:04,  3.32it/s] 96%|█████████▌| 380/395 [03:10<00:04,  3.32it/s] 96%|█████████▋| 381/395 [03:11<00:04,  3.32it/s] 97%|█████████▋| 382/395 [03:11<00:03,  3.31it/s] 97%|█████████▋| 383/395 [03:11<00:03,  3.31it/s] 97%|█████████▋| 384/395 [03:12<00:03,  3.32it/s] 97%|█████████▋| 385/395 [03:12<00:03,  3.32it/s] 98%|█████████▊| 386/395 [03:12<00:02,  3.32it/s] 98%|█████████▊| 387/395 [03:12<00:02,  3.32it/s] 98%|█████████▊| 388/395 [03:13<00:02,  3.32it/s] 98%|█████████▊| 389/395 [03:13<00:01,  3.32it/s] 99%|█████████▊| 390/395 [03:13<00:01,  3.32it/s] 99%|█████████▉| 391/395 [03:14<00:01,  3.32it/s] 99%|█████████▉| 392/395 [03:14<00:00,  3.30it/s] 99%|█████████▉| 393/395 [03:14<00:00,  3.31it/s]100%|█████████▉| 394/395 [03:15<00:00,  3.31it/s]100%|██████████| 395/395 [03:15<00:00,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 11:41:58,809 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:41:58,809 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:41:58,809 >>   Batch size = 8
{'eval_loss': 0.9745411276817322, 'eval_runtime': 13.1044, 'eval_samples_per_second': 351.715, 'eval_steps_per_second': 44.031, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.41it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.27it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.25it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.30it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.65it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.37it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.09it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.00it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.08it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.26it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.34it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.19it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.20it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.05it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.95it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.88it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.89it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.94it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.06it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.19it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.19it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.02it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.96it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.85it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.96it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.92it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.93it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.14it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.16it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.21it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.99it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.92it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.96it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.94it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.97it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.02it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.11it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.11it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.20it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.00it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.02it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.98it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.95it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.98it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.97it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.11it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.14it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.04it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.05it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.05it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.03it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.09it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.05it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.06it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.14it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.16it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.13it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.03it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.92it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.07it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.95it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.01it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.93it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.91it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.03it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.02it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.85it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.00it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.97it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.11it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.00it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.94it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.04it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.02it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.03it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.99it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.01it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.08it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.98it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.92it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.90it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.02it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.01it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.98it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.12it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.10it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.00it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.95it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.88it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.99it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.99it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.00it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.17it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.11it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.83it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.95it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.01it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.01it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.91it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.07it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.12it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.09it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.96it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.90it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.07it/s][A                                                 
                                                 [A100%|██████████| 395/395 [03:28<00:00,  3.35it/s]
100%|██████████| 577/577 [00:13<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:42:11,922 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-28 11:42:11,937 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:42:13,733 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:42:13,748 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:42:13,759 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 11:42:17,238 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 11:42:17,240 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158 (score: 0.967439591884613).
                                                 100%|██████████| 395/395 [03:35<00:00,  3.35it/s]100%|██████████| 395/395 [03:35<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-28 11:42:18,972 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 11:42:18,989 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:42:20,944 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:42:20,966 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:42:20,973 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:42:21,164 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   train_loss               =      0.731
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   train_runtime            = 0:03:35.48
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   train_samples            =       5052
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   train_samples_per_second =    117.223
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:21,165 >>   train_steps_per_second   =      1.833
{'eval_loss': 0.9767600893974304, 'eval_runtime': 13.0986, 'eval_samples_per_second': 351.87, 'eval_steps_per_second': 44.051, 'epoch': 5.0}
{'train_runtime': 215.4868, 'train_samples_per_second': 117.223, 'train_steps_per_second': 1.833, 'train_loss': 0.7309595035601266, 'epoch': 5.0}
08/28/2023 11:42:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 11:42:21,212 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:42:21,212 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 11:42:21,212 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.39it/s]  2%|▏         | 12/577 [00:00<00:11, 48.15it/s]  3%|▎         | 17/577 [00:00<00:12, 46.49it/s]  4%|▍         | 22/577 [00:00<00:12, 45.87it/s]  5%|▍         | 27/577 [00:00<00:12, 45.56it/s]  6%|▌         | 32/577 [00:00<00:12, 44.97it/s]  6%|▋         | 37/577 [00:00<00:12, 44.86it/s]  7%|▋         | 42/577 [00:00<00:12, 44.44it/s]  8%|▊         | 47/577 [00:01<00:12, 43.96it/s]  9%|▉         | 52/577 [00:01<00:12, 43.69it/s] 10%|▉         | 57/577 [00:01<00:11, 43.72it/s] 11%|█         | 62/577 [00:01<00:11, 43.96it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.15it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.31it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.40it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.34it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.04it/s] 16%|█▌        | 92/577 [00:02<00:11, 43.81it/s] 17%|█▋        | 97/577 [00:02<00:11, 43.63it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.74it/s] 19%|█▊        | 107/577 [00:02<00:10, 43.79it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.05it/s] 20%|██        | 117/577 [00:02<00:10, 44.21it/s] 21%|██        | 122/577 [00:02<00:10, 44.34it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.23it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.03it/s] 24%|██▎       | 137/577 [00:03<00:10, 43.70it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.66it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.79it/s] 26%|██▋       | 152/577 [00:03<00:09, 43.76it/s] 27%|██▋       | 157/577 [00:03<00:09, 43.98it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.19it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.30it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.25it/s] 31%|███       | 177/577 [00:03<00:09, 43.93it/s] 32%|███▏      | 182/577 [00:04<00:09, 43.72it/s] 32%|███▏      | 187/577 [00:04<00:08, 43.72it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.85it/s] 34%|███▍      | 197/577 [00:04<00:08, 43.82it/s] 35%|███▌      | 202/577 [00:04<00:08, 43.96it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.13it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.29it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.18it/s] 38%|███▊      | 222/577 [00:05<00:08, 43.84it/s] 39%|███▉      | 227/577 [00:05<00:07, 43.79it/s] 40%|████      | 232/577 [00:05<00:07, 43.76it/s] 41%|████      | 237/577 [00:05<00:07, 43.90it/s] 42%|████▏     | 242/577 [00:05<00:07, 43.92it/s] 43%|████▎     | 247/577 [00:05<00:07, 43.93it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.06it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.14it/s] 45%|████▌     | 262/577 [00:05<00:07, 43.98it/s] 46%|████▋     | 267/577 [00:06<00:07, 43.77it/s] 47%|████▋     | 272/577 [00:06<00:06, 43.84it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.82it/s] 49%|████▉     | 282/577 [00:06<00:06, 43.85it/s] 50%|████▉     | 287/577 [00:06<00:06, 43.96it/s] 51%|█████     | 292/577 [00:06<00:06, 44.14it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.15it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.17it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.21it/s] 54%|█████▍    | 312/577 [00:07<00:06, 44.05it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.02it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.11it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.14it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.12it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.27it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.17it/s] 60%|██████    | 347/577 [00:07<00:05, 44.25it/s] 61%|██████    | 352/577 [00:07<00:05, 44.23it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.08it/s] 63%|██████▎   | 362/577 [00:08<00:04, 44.08it/s] 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.18it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.08it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.11it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.12it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.25it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.21it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.14it/s] 71%|███████   | 407/577 [00:09<00:03, 44.16it/s] 71%|███████▏  | 412/577 [00:09<00:03, 44.12it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.09it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.09it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.18it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.13it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.22it/s] 77%|███████▋  | 442/577 [00:10<00:03, 44.16it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.02it/s] 78%|███████▊  | 452/577 [00:10<00:02, 44.20it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.13it/s] 80%|████████  | 462/577 [00:10<00:02, 44.03it/s] 81%|████████  | 467/577 [00:10<00:02, 44.17it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.21it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.15it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.17it/s] 84%|████████▍ | 487/577 [00:11<00:02, 44.19it/s] 85%|████████▌ | 492/577 [00:11<00:01, 43.97it/s] 86%|████████▌ | 497/577 [00:11<00:01, 44.14it/s] 87%|████████▋ | 502/577 [00:11<00:01, 44.05it/s] 88%|████████▊ | 507/577 [00:11<00:01, 43.98it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.12it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.25it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.27it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.20it/s] 92%|█████████▏| 532/577 [00:12<00:01, 44.22it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.09it/s] 94%|█████████▍| 542/577 [00:12<00:00, 44.03it/s] 95%|█████████▍| 547/577 [00:12<00:00, 44.09it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.13it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.21it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.12it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.18it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.19it/s]100%|██████████| 577/577 [00:13<00:00, 44.23it/s]100%|██████████| 577/577 [00:13<00:00, 44.16it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:42:34,299 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   eval_loss               =     0.9674
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   eval_runtime            = 0:00:13.08
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   eval_samples_per_second =    352.191
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   eval_steps_per_second   =     44.091
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:42:34,299 >>   perplexity              =     2.6312
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:40,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:40,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:40,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:40,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:40,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:42:40,982 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:42:40,983 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:42:41,550 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:42:42,587 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:42:42,587 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:45,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:45,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:45,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:45,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:42:45,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:42:46,154 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:42:46,156 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:42:46,732 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:42:46,891 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:42:46,891 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-79
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/checkpoint-237
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.30it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.28it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.33it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.26it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.31it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.31it/s]Extractor Predicting: 14it [00:10,  1.31it/s]Extractor Predicting: 15it [00:11,  1.33it/s]Extractor Predicting: 16it [00:12,  1.34it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.34it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:16,  1.31it/s]Extractor Predicting: 23it [00:17,  1.29it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:19,  1.31it/s]Extractor Predicting: 27it [00:20,  1.27it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.30it/s]Extractor Predicting: 30it [00:22,  1.32it/s]Extractor Predicting: 31it [00:23,  1.30it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.30it/s]Extractor Predicting: 34it [00:26,  1.27it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.32it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.30it/s]Extractor Predicting: 39it [00:29,  1.28it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.26it/s]Extractor Predicting: 42it [00:32,  1.28it/s]Extractor Predicting: 43it [00:33,  1.26it/s]Extractor Predicting: 44it [00:33,  1.24it/s]Extractor Predicting: 45it [00:34,  1.24it/s]Extractor Predicting: 46it [00:35,  1.24it/s]Extractor Predicting: 47it [00:36,  1.24it/s]Extractor Predicting: 48it [00:37,  1.25it/s]Extractor Predicting: 49it [00:37,  1.30it/s]Extractor Predicting: 50it [00:38,  1.29it/s]Extractor Predicting: 51it [00:39,  1.31it/s]Extractor Predicting: 52it [00:40,  1.28it/s]Extractor Predicting: 53it [00:41,  1.27it/s]Extractor Predicting: 54it [00:41,  1.27it/s]Extractor Predicting: 55it [00:42,  1.28it/s]Extractor Predicting: 56it [00:43,  1.30it/s]Extractor Predicting: 57it [00:44,  1.29it/s]Extractor Predicting: 58it [00:44,  1.27it/s]Extractor Predicting: 59it [00:45,  1.31it/s]Extractor Predicting: 60it [00:46,  1.30it/s]Extractor Predicting: 61it [00:47,  1.28it/s]Extractor Predicting: 62it [00:48,  1.26it/s]Extractor Predicting: 63it [00:48,  1.25it/s]Extractor Predicting: 64it [00:49,  1.23it/s]Extractor Predicting: 65it [00:50,  1.24it/s]Extractor Predicting: 66it [00:51,  1.28it/s]Extractor Predicting: 67it [00:51,  1.29it/s]Extractor Predicting: 68it [00:52,  1.31it/s]Extractor Predicting: 69it [00:53,  1.32it/s]Extractor Predicting: 70it [00:54,  1.31it/s]Extractor Predicting: 71it [00:54,  1.34it/s]Extractor Predicting: 72it [00:55,  1.37it/s]Extractor Predicting: 73it [00:56,  1.34it/s]Extractor Predicting: 74it [00:57,  1.36it/s]Extractor Predicting: 75it [00:57,  1.36it/s]Extractor Predicting: 76it [00:58,  1.32it/s]Extractor Predicting: 77it [00:59,  1.32it/s]Extractor Predicting: 78it [01:00,  1.34it/s]Extractor Predicting: 79it [01:00,  1.33it/s]Extractor Predicting: 80it [01:01,  1.33it/s]Extractor Predicting: 81it [01:02,  1.31it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:03,  1.30it/s]Extractor Predicting: 84it [01:04,  1.34it/s]Extractor Predicting: 85it [01:05,  1.38it/s]Extractor Predicting: 86it [01:06,  1.38it/s]Extractor Predicting: 87it [01:06,  1.40it/s]Extractor Predicting: 88it [01:07,  1.33it/s]Extractor Predicting: 89it [01:08,  1.32it/s]Extractor Predicting: 90it [01:09,  1.30it/s]Extractor Predicting: 91it [01:09,  1.31it/s]Extractor Predicting: 92it [01:10,  1.32it/s]Extractor Predicting: 93it [01:11,  1.32it/s]Extractor Predicting: 94it [01:12,  1.33it/s]Extractor Predicting: 95it [01:12,  1.32it/s]Extractor Predicting: 96it [01:13,  1.32it/s]Extractor Predicting: 97it [01:14,  1.33it/s]Extractor Predicting: 98it [01:15,  1.29it/s]Extractor Predicting: 99it [01:16,  1.30it/s]Extractor Predicting: 100it [01:16,  1.27it/s]Extractor Predicting: 101it [01:17,  1.27it/s]Extractor Predicting: 102it [01:18,  1.29it/s]Extractor Predicting: 103it [01:19,  1.27it/s]Extractor Predicting: 104it [01:19,  1.30it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:21,  1.28it/s]Extractor Predicting: 107it [01:22,  1.32it/s]Extractor Predicting: 108it [01:22,  1.34it/s]Extractor Predicting: 109it [01:23,  1.32it/s]Extractor Predicting: 110it [01:24,  1.31it/s]Extractor Predicting: 111it [01:25,  1.29it/s]Extractor Predicting: 112it [01:26,  1.29it/s]Extractor Predicting: 113it [01:26,  1.30it/s]Extractor Predicting: 114it [01:27,  1.30it/s]Extractor Predicting: 115it [01:28,  1.21it/s]Extractor Predicting: 116it [01:29,  1.24it/s]Extractor Predicting: 117it [01:30,  1.28it/s]Extractor Predicting: 118it [01:30,  1.30it/s]Extractor Predicting: 119it [01:31,  1.35it/s]Extractor Predicting: 120it [01:32,  1.34it/s]Extractor Predicting: 121it [01:32,  1.33it/s]Extractor Predicting: 122it [01:33,  1.32it/s]Extractor Predicting: 123it [01:34,  1.30it/s]Extractor Predicting: 124it [01:35,  1.33it/s]Extractor Predicting: 125it [01:36,  1.29it/s]Extractor Predicting: 126it [01:36,  1.33it/s]Extractor Predicting: 127it [01:37,  1.35it/s]Extractor Predicting: 128it [01:38,  1.33it/s]Extractor Predicting: 129it [01:39,  1.32it/s]Extractor Predicting: 130it [01:39,  1.29it/s]Extractor Predicting: 131it [01:40,  1.28it/s]Extractor Predicting: 132it [01:41,  1.31it/s]Extractor Predicting: 133it [01:42,  1.31it/s]Extractor Predicting: 134it [01:42,  1.29it/s]Extractor Predicting: 135it [01:43,  1.29it/s]Extractor Predicting: 136it [01:44,  1.29it/s]Extractor Predicting: 137it [01:45,  1.30it/s]Extractor Predicting: 138it [01:45,  1.30it/s]Extractor Predicting: 139it [01:46,  1.27it/s]Extractor Predicting: 140it [01:47,  1.26it/s]Extractor Predicting: 141it [01:48,  1.25it/s]Extractor Predicting: 142it [01:49,  1.24it/s]Extractor Predicting: 143it [01:50,  1.26it/s]Extractor Predicting: 144it [01:50,  1.25it/s]Extractor Predicting: 145it [01:51,  1.25it/s]Extractor Predicting: 146it [01:52,  1.22it/s]Extractor Predicting: 147it [01:53,  1.23it/s]Extractor Predicting: 148it [01:54,  1.25it/s]Extractor Predicting: 149it [01:54,  1.26it/s]Extractor Predicting: 150it [01:55,  1.25it/s]Extractor Predicting: 151it [01:56,  1.28it/s]Extractor Predicting: 152it [01:57,  1.27it/s]Extractor Predicting: 153it [01:57,  1.29it/s]Extractor Predicting: 154it [01:58,  1.28it/s]Extractor Predicting: 155it [01:59,  1.27it/s]Extractor Predicting: 156it [02:00,  1.26it/s]Extractor Predicting: 157it [02:01,  1.27it/s]Extractor Predicting: 158it [02:01,  1.28it/s]Extractor Predicting: 159it [02:02,  1.27it/s]Extractor Predicting: 160it [02:03,  1.28it/s]Extractor Predicting: 161it [02:04,  1.30it/s]Extractor Predicting: 162it [02:05,  1.29it/s]Extractor Predicting: 163it [02:05,  1.25it/s]Extractor Predicting: 164it [02:06,  1.25it/s]Extractor Predicting: 165it [02:07,  1.26it/s]Extractor Predicting: 166it [02:08,  1.24it/s]Extractor Predicting: 167it [02:09,  1.25it/s]Extractor Predicting: 168it [02:09,  1.26it/s]Extractor Predicting: 169it [02:10,  1.26it/s]Extractor Predicting: 170it [02:11,  1.27it/s]Extractor Predicting: 171it [02:11,  1.50it/s]Extractor Predicting: 171it [02:11,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:06,281 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:06,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:06,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:06,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:06,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:45:06,924 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:45:06,925 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:45:07,505 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:45:08,564 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:45:08,564 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:11,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:11,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:11,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:11,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:45:11,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:45:12,048 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:45:12,049 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:45:12,614 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:45:12,790 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:45:12,791 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26142857142857145,
  "recall": 0.039704925146452595,
  "score": 0.06893953663590131,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.23it/s]Extractor Predicting: 2it [00:01,  1.30it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.33it/s]Extractor Predicting: 9it [00:06,  1.29it/s]Extractor Predicting: 10it [00:07,  1.24it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.32it/s]Extractor Predicting: 13it [00:10,  1.30it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.31it/s]Extractor Predicting: 17it [00:13,  1.33it/s]Extractor Predicting: 18it [00:13,  1.34it/s]Extractor Predicting: 19it [00:14,  1.35it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:15,  1.36it/s]Extractor Predicting: 22it [00:16,  1.33it/s]Extractor Predicting: 23it [00:17,  1.34it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.32it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:20,  1.35it/s]Extractor Predicting: 28it [00:21,  1.35it/s]Extractor Predicting: 29it [00:21,  1.34it/s]Extractor Predicting: 30it [00:22,  1.32it/s]Extractor Predicting: 31it [00:23,  1.31it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:25,  1.31it/s]Extractor Predicting: 35it [00:26,  1.32it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:28,  1.31it/s]Extractor Predicting: 39it [00:29,  1.30it/s]Extractor Predicting: 40it [00:30,  1.32it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:31,  1.33it/s]Extractor Predicting: 43it [00:32,  1.33it/s]Extractor Predicting: 44it [00:33,  1.34it/s]Extractor Predicting: 45it [00:34,  1.34it/s]Extractor Predicting: 46it [00:34,  1.35it/s]Extractor Predicting: 47it [00:35,  1.31it/s]Extractor Predicting: 48it [00:36,  1.31it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:37,  1.31it/s]Extractor Predicting: 51it [00:38,  1.32it/s]Extractor Predicting: 52it [00:39,  1.32it/s]Extractor Predicting: 53it [00:40,  1.32it/s]Extractor Predicting: 54it [00:41,  1.30it/s]Extractor Predicting: 55it [00:41,  1.29it/s]Extractor Predicting: 56it [00:42,  1.31it/s]Extractor Predicting: 57it [00:43,  1.32it/s]Extractor Predicting: 58it [00:44,  1.31it/s]Extractor Predicting: 59it [00:44,  1.32it/s]Extractor Predicting: 60it [00:45,  1.29it/s]Extractor Predicting: 61it [00:46,  1.29it/s]Extractor Predicting: 62it [00:47,  1.28it/s]Extractor Predicting: 63it [00:47,  1.31it/s]Extractor Predicting: 64it [00:48,  1.30it/s]Extractor Predicting: 65it [00:49,  1.32it/s]Extractor Predicting: 66it [00:50,  1.21it/s]Extractor Predicting: 67it [00:51,  1.25it/s]Extractor Predicting: 68it [00:51,  1.28it/s]Extractor Predicting: 69it [00:52,  1.31it/s]Extractor Predicting: 70it [00:53,  1.30it/s]Extractor Predicting: 71it [00:54,  1.26it/s]Extractor Predicting: 72it [00:55,  1.28it/s]Extractor Predicting: 73it [00:55,  1.28it/s]Extractor Predicting: 74it [00:56,  1.31it/s]Extractor Predicting: 75it [00:57,  1.31it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:58,  1.37it/s]Extractor Predicting: 78it [00:59,  1.37it/s]Extractor Predicting: 79it [01:00,  1.35it/s]Extractor Predicting: 80it [01:00,  1.36it/s]Extractor Predicting: 81it [01:01,  1.35it/s]Extractor Predicting: 82it [01:02,  1.35it/s]Extractor Predicting: 83it [01:03,  1.35it/s]Extractor Predicting: 84it [01:03,  1.33it/s]Extractor Predicting: 85it [01:04,  1.33it/s]Extractor Predicting: 86it [01:05,  1.34it/s]Extractor Predicting: 87it [01:06,  1.32it/s]Extractor Predicting: 88it [01:06,  1.33it/s]Extractor Predicting: 89it [01:07,  1.33it/s]Extractor Predicting: 90it [01:08,  1.30it/s]Extractor Predicting: 91it [01:09,  1.31it/s]Extractor Predicting: 92it [01:09,  1.31it/s]Extractor Predicting: 93it [01:10,  1.32it/s]Extractor Predicting: 94it [01:11,  1.31it/s]Extractor Predicting: 95it [01:12,  1.30it/s]Extractor Predicting: 96it [01:13,  1.27it/s]Extractor Predicting: 97it [01:13,  1.30it/s]Extractor Predicting: 98it [01:14,  1.31it/s]Extractor Predicting: 99it [01:15,  1.38it/s]Extractor Predicting: 100it [01:15,  1.38it/s]Extractor Predicting: 101it [01:16,  1.35it/s]Extractor Predicting: 102it [01:17,  1.36it/s]Extractor Predicting: 103it [01:18,  1.35it/s]Extractor Predicting: 104it [01:18,  1.36it/s]Extractor Predicting: 105it [01:19,  1.34it/s]Extractor Predicting: 106it [01:20,  1.36it/s]Extractor Predicting: 107it [01:21,  1.38it/s]Extractor Predicting: 108it [01:21,  1.36it/s]Extractor Predicting: 109it [01:22,  1.31it/s]Extractor Predicting: 110it [01:23,  1.26it/s]Extractor Predicting: 111it [01:24,  1.24it/s]Extractor Predicting: 112it [01:25,  1.25it/s]Extractor Predicting: 113it [01:25,  1.29it/s]Extractor Predicting: 114it [01:26,  1.31it/s]Extractor Predicting: 115it [01:27,  1.30it/s]Extractor Predicting: 116it [01:28,  1.30it/s]Extractor Predicting: 117it [01:28,  1.29it/s]Extractor Predicting: 118it [01:29,  1.26it/s]Extractor Predicting: 119it [01:30,  1.28it/s]Extractor Predicting: 120it [01:31,  1.25it/s]Extractor Predicting: 121it [01:32,  1.24it/s]Extractor Predicting: 122it [01:33,  1.26it/s]Extractor Predicting: 123it [01:33,  1.27it/s]Extractor Predicting: 124it [01:34,  1.27it/s]Extractor Predicting: 125it [01:34,  1.58it/s]Extractor Predicting: 125it [01:34,  1.32it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:53,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:53,978 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:53,978 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:46:54,286 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:46:54,287 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:46:54,545 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:46:55,615 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:46:55,615 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:58,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:58,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:58,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:58,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:46:58,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:46:59,496 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:46:59,497 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:47:00,075 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:47:00,250 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:47:00,251 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6098360655737705,
  "recall": 0.06243705941591138,
  "score": 0.11327649208282582,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.06it/s]Extractor Predicting: 2it [00:01,  1.15it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.22it/s]Extractor Predicting: 6it [00:04,  1.63it/s]Extractor Predicting: 6it [00:04,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-28 11:47:05,104 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:47:05,105 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:47:05,109 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:47:05,110 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 11:47:05,113 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:47:08,326 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 11:47:08,328 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 11:47:08,348 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:47:08,349 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:47:08,355 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:47:08,358 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.9090909090909091,
  "recall": 0.03937007874015748,
  "score": 0.07547169811320754,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 11:47:08,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:09,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:10,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:11,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:11,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:12,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:13,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:14,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:15,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:16,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:17,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:17,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:18,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:19,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:20,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:20,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:21,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:22,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:23,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:24,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:24,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:26,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:18<02:49, 18.81s/it][WARNING|generation_utils.py:914] 2023-08-28 11:47:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:28,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:28,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:29,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:30,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:31,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:32,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:33,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:33,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:34,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:35,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:36,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:37,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:37,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:38,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:39,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:39,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:40,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:41,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:42,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:43,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:35<02:18, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 11:47:43,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:44,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:45,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:46,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:47,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:47,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:48,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:49,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:50,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:51,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:52,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:52,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:53,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:54,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:55,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:56,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:56,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:57,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:58,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:59,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:47:59,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:00,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:02,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:54<02:07, 18.25s/it][WARNING|generation_utils.py:914] 2023-08-28 11:48:03,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:03,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:04,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:05,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:06,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:06,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:07,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:08,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:09,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:10,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:11,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:11,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:12,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:13,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:14,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:15,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:16,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:17,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:17,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:18,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:19,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:20,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:20,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:21,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:13<01:52, 18.75s/it][WARNING|generation_utils.py:914] 2023-08-28 11:48:22,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:23,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:24,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:24,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:25,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:26,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:27,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:27,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:28,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:29,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:30,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:30,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:31,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:32,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:33,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:33,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:34,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:35,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:36,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:36,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:29<01:27, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 11:48:37,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:38,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:39,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:40,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:41,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:41,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:42,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:43,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:44,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:45,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:45,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:46,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:47,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:48,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:48,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:49,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:50,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:51,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:52,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:52,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:53,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:45<01:08, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-28 11:48:54,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:55,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:55,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:56,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:57,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:57,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:58,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:48:59,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:00,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:01,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:02,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:03,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:04,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:04,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:05,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:06,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:08,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:08,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:09,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:10,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:11,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:03<00:52, 17.39s/it][WARNING|generation_utils.py:914] 2023-08-28 11:49:12,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:12,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:13,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:14,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:15,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:15,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:16,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:17,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:18,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:18,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:19,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:20,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:21,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:22,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:22,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:23,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:24,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:24,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:25,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:26,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:27,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:19<00:33, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 11:49:27,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:28,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:29,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:30,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:31,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:31,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:32,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:33,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:34,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:35,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:35,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:36,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:37,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:38,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:38,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:39,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:40,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:41,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:42,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:42,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:43,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:44,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:45,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:37<00:17, 17.29s/it][WARNING|generation_utils.py:914] 2023-08-28 11:49:46,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:46,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:47,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:48,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:48,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:49,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:50,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:51,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:51,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:52,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:53,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:54,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:54,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:55,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:56,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:57,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:58,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:58,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:49:59,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:00,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:01,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:01,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:02,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:03,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:03,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:04,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:05,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:06,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:50:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:59<00:00, 18.61s/it]Generating: 100%|██████████| 10/10 [02:59<00:00, 17.91s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:12,231 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:12,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:12,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:12,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:12,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:50:12,580 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:50:12,580 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:50:13,140 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:50:14,233 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:50:14,233 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:17,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:17,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:17,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:17,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:50:17,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:50:17,692 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:50:17,693 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:50:18,260 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:50:18,437 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:50:18,438 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : made from material .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('metal', 'manufacturer', '', 'It is available in both . 01 and . 02 metal versions .')", 'not enough values to unpack (expected 2, got 1)', "('Microsoft Word 2013', 'manufacturer', '', 'It runs Android , BlackBerry devices , Microsoft Windows Phone 8 , Windows Phone 8 , Mac OS 7 . 0 , Windows Central , Microsoft Word 2013 , Microsoft HoloLens and Windows Mixed Reality for developers .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8098958333333334, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : official language .', 'success_rate': 0.7903645833333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9136904761904762, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8206521739130435, 'errors': {'', "('French', 'use', '', 'In the United States , the term is derived from the English term , , , which originated in 1832 and the French term , from the name of Napoleon .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 192, 'raw': 288}
{'target': 600, 'success': 213, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 251, 'raw': 384}
{'target': 600, 'success': 270, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 317, 'raw': 480}
{'target': 600, 'success': 333, 'raw': 512}
{'target': 600, 'success': 353, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 399, 'raw': 608}
{'target': 600, 'success': 419, 'raw': 640}
{'target': 600, 'success': 443, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 488, 'raw': 736}
{'target': 600, 'success': 508, 'raw': 768}
{'target': 600, 'success': 534, 'raw': 800}
{'target': 600, 'success': 555, 'raw': 832}
{'target': 600, 'success': 574, 'raw': 864}
{'target': 600, 'success': 596, 'raw': 896}
{'target': 600, 'success': 616, 'raw': 928}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6637931034482759, 'errors': {'', "('All My Children', 'voice type', '', 'They have also done remixes on two other albums , All My Children and The Good Shepherd , that were released on UMD Records .')", "('Sivakumar', 'voice type', '', 'Kajj ( pronounced Jijj ) is a 1997 Bengali drama film directed by Suresh Raja and produced by Nandita Prasad for Sivakumar .')", "('John Cassaday', 'voice type', '', 'Muppet Babysitter is a 2005 Disney musical comedy television series created by John Cassaday and Tom Hanks .')", "('Doctor Who', 'voice type', '', 'In the 2013 drama Doctor Who , William Anderson appeared in the role of Doctor Daleks voice actor in the final two seasons .')", "('Rachel Weisz', 'voice type', '', 'In early 2011 , he performed in the show hosted by Amy Schumer , Rachel Weisz and Chris Rock , alongside fellow American actors such as Sam Shepard , Rachel Weisz , and Sarah Jessica Parker .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 11321
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11421, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.21it/s]Extractor Estimating: 2it [00:01,  1.16it/s]Extractor Estimating: 3it [00:02,  1.18it/s]Extractor Estimating: 4it [00:03,  1.22it/s]Extractor Estimating: 5it [00:04,  1.26it/s]Extractor Estimating: 6it [00:04,  1.29it/s]Extractor Estimating: 7it [00:05,  1.26it/s]Extractor Estimating: 8it [00:06,  1.28it/s]Extractor Estimating: 9it [00:07,  1.29it/s]Extractor Estimating: 10it [00:07,  1.29it/s]Extractor Estimating: 11it [00:08,  1.32it/s]Extractor Estimating: 12it [00:09,  1.34it/s]Extractor Estimating: 13it [00:10,  1.35it/s]Extractor Estimating: 14it [00:10,  1.32it/s]Extractor Estimating: 15it [00:11,  1.34it/s]Extractor Estimating: 16it [00:12,  1.34it/s]Extractor Estimating: 17it [00:13,  1.35it/s]Extractor Estimating: 18it [00:13,  1.35it/s]Extractor Estimating: 19it [00:14,  1.33it/s]Extractor Estimating: 20it [00:15,  1.30it/s]Extractor Estimating: 21it [00:16,  1.27it/s]Extractor Estimating: 22it [00:17,  1.28it/s]Extractor Estimating: 23it [00:17,  1.30it/s]Extractor Estimating: 24it [00:18,  1.28it/s]Extractor Estimating: 25it [00:19,  1.24it/s]Extractor Estimating: 26it [00:20,  1.29it/s]Extractor Estimating: 27it [00:20,  1.32it/s]Extractor Estimating: 28it [00:21,  1.33it/s]Extractor Estimating: 29it [00:22,  1.36it/s]Extractor Estimating: 30it [00:22,  1.37it/s]Extractor Estimating: 31it [00:23,  1.35it/s]Extractor Estimating: 32it [00:24,  1.32it/s]Extractor Estimating: 33it [00:25,  1.29it/s]Extractor Estimating: 34it [00:26,  1.34it/s]Extractor Estimating: 35it [00:26,  1.33it/s]Extractor Estimating: 36it [00:27,  1.39it/s]Extractor Estimating: 37it [00:28,  1.36it/s]Extractor Estimating: 38it [00:28,  1.35it/s]Extractor Estimating: 39it [00:29,  1.36it/s]Extractor Estimating: 40it [00:30,  1.38it/s]Extractor Estimating: 41it [00:31,  1.41it/s]Extractor Estimating: 42it [00:31,  1.42it/s]Extractor Estimating: 43it [00:32,  1.42it/s]Extractor Estimating: 44it [00:33,  1.43it/s]Extractor Estimating: 45it [00:33,  1.40it/s]Extractor Estimating: 46it [00:34,  1.32it/s]Extractor Estimating: 47it [00:35,  1.35it/s]Extractor Estimating: 48it [00:36,  1.32it/s]Extractor Estimating: 49it [00:37,  1.32it/s]Extractor Estimating: 50it [00:37,  1.37it/s]Extractor Estimating: 51it [00:38,  1.34it/s]Extractor Estimating: 52it [00:39,  1.35it/s]Extractor Estimating: 53it [00:39,  1.37it/s]Extractor Estimating: 54it [00:40,  1.34it/s]Extractor Estimating: 55it [00:41,  1.27it/s]Extractor Estimating: 56it [00:42,  1.31it/s]Extractor Estimating: 57it [00:43,  1.33it/s]Extractor Estimating: 58it [00:43,  1.32it/s]Extractor Estimating: 59it [00:44,  1.35it/s]Extractor Estimating: 60it [00:45,  1.28it/s]Extractor Estimating: 61it [00:46,  1.29it/s]Extractor Estimating: 62it [00:46,  1.30it/s]Extractor Estimating: 63it [00:47,  1.29it/s]Extractor Estimating: 64it [00:48,  1.29it/s]Extractor Estimating: 65it [00:49,  1.33it/s]Extractor Estimating: 66it [00:49,  1.33it/s]Extractor Estimating: 67it [00:50,  1.34it/s]Extractor Estimating: 68it [00:51,  1.31it/s]Extractor Estimating: 69it [00:52,  1.31it/s]Extractor Estimating: 70it [00:52,  1.33it/s]Extractor Estimating: 71it [00:53,  1.38it/s]Extractor Estimating: 72it [00:54,  1.37it/s]Extractor Estimating: 73it [00:55,  1.30it/s]Extractor Estimating: 74it [00:55,  1.31it/s]Extractor Estimating: 75it [00:56,  1.30it/s]Extractor Estimating: 76it [00:57,  1.32it/s]Extractor Estimating: 77it [00:58,  1.30it/s]Extractor Estimating: 78it [00:59,  1.27it/s]Extractor Estimating: 79it [00:59,  1.27it/s]Extractor Estimating: 80it [01:00,  1.30it/s]Extractor Estimating: 81it [01:01,  1.32it/s]Extractor Estimating: 82it [01:02,  1.33it/s]Extractor Estimating: 83it [01:02,  1.36it/s]Extractor Estimating: 84it [01:03,  1.30it/s]Extractor Estimating: 85it [01:04,  1.31it/s]Extractor Estimating: 86it [01:05,  1.30it/s]Extractor Estimating: 87it [01:05,  1.31it/s]Extractor Estimating: 88it [01:06,  1.33it/s]Extractor Estimating: 89it [01:07,  1.34it/s]Extractor Estimating: 90it [01:08,  1.33it/s]Extractor Estimating: 91it [01:08,  1.29it/s]Extractor Estimating: 92it [01:09,  1.33it/s]Extractor Estimating: 93it [01:10,  1.33it/s]Extractor Estimating: 94it [01:11,  1.33it/s]Extractor Estimating: 95it [01:11,  1.34it/s]Extractor Estimating: 96it [01:12,  1.35it/s]Extractor Estimating: 97it [01:13,  1.35it/s]Extractor Estimating: 98it [01:14,  1.36it/s]Extractor Estimating: 99it [01:14,  1.35it/s]Extractor Estimating: 100it [01:15,  1.32it/s]Extractor Estimating: 101it [01:16,  1.32it/s]Extractor Estimating: 102it [01:17,  1.31it/s]Extractor Estimating: 103it [01:17,  1.33it/s]Extractor Estimating: 104it [01:18,  1.33it/s]Extractor Estimating: 105it [01:19,  1.35it/s]Extractor Estimating: 106it [01:20,  1.30it/s]Extractor Estimating: 107it [01:20,  1.31it/s]Extractor Estimating: 108it [01:21,  1.31it/s]Extractor Estimating: 109it [01:22,  1.31it/s]Extractor Estimating: 110it [01:23,  1.30it/s]Extractor Estimating: 111it [01:23,  1.31it/s]Extractor Estimating: 112it [01:24,  1.35it/s]Extractor Estimating: 113it [01:25,  1.35it/s]Extractor Estimating: 114it [01:26,  1.32it/s]Extractor Estimating: 115it [01:26,  1.34it/s]Extractor Estimating: 116it [01:27,  1.33it/s]Extractor Estimating: 117it [01:28,  1.36it/s]Extractor Estimating: 118it [01:29,  1.36it/s]Extractor Estimating: 119it [01:29,  1.32it/s]Extractor Estimating: 120it [01:30,  1.30it/s]Extractor Estimating: 121it [01:31,  1.31it/s]Extractor Estimating: 122it [01:32,  1.21it/s]Extractor Estimating: 123it [01:33,  1.23it/s]Extractor Estimating: 124it [01:33,  1.26it/s]Extractor Estimating: 125it [01:34,  1.28it/s]Extractor Estimating: 126it [01:35,  1.27it/s]Extractor Estimating: 127it [01:36,  1.23it/s]Extractor Estimating: 128it [01:37,  1.26it/s]Extractor Estimating: 129it [01:38,  1.22it/s]Extractor Estimating: 130it [01:38,  1.28it/s]Extractor Estimating: 131it [01:39,  1.32it/s]Extractor Estimating: 132it [01:40,  1.29it/s]Extractor Estimating: 133it [01:41,  1.29it/s]Extractor Estimating: 134it [01:41,  1.26it/s]Extractor Estimating: 135it [01:42,  1.29it/s]Extractor Estimating: 136it [01:43,  1.28it/s]Extractor Estimating: 137it [01:44,  1.27it/s]Extractor Estimating: 138it [01:45,  1.25it/s]Extractor Estimating: 139it [01:45,  1.26it/s]Extractor Estimating: 140it [01:46,  1.25it/s]Extractor Estimating: 141it [01:47,  1.27it/s]Extractor Estimating: 142it [01:48,  1.28it/s]Extractor Estimating: 143it [01:48,  1.30it/s]Extractor Estimating: 144it [01:49,  1.30it/s]Extractor Estimating: 145it [01:50,  1.26it/s]Extractor Estimating: 146it [01:51,  1.25it/s]Extractor Estimating: 147it [01:52,  1.23it/s]Extractor Estimating: 148it [01:52,  1.25it/s]Extractor Estimating: 149it [01:53,  1.23it/s]Extractor Estimating: 150it [01:54,  1.25it/s]Extractor Estimating: 151it [01:55,  1.27it/s]Extractor Estimating: 152it [01:56,  1.28it/s]Extractor Estimating: 153it [01:56,  1.29it/s]Extractor Estimating: 154it [01:57,  1.28it/s]Extractor Estimating: 155it [01:58,  1.30it/s]Extractor Estimating: 156it [01:59,  1.27it/s]Extractor Estimating: 157it [02:00,  1.26it/s]Extractor Estimating: 158it [02:00,  1.30it/s]Extractor Estimating: 159it [02:01,  1.32it/s]Extractor Estimating: 160it [02:02,  1.30it/s]Extractor Estimating: 161it [02:03,  1.26it/s]Extractor Estimating: 162it [02:03,  1.31it/s]Extractor Estimating: 163it [02:04,  1.30it/s]Extractor Estimating: 164it [02:05,  1.30it/s]Extractor Estimating: 165it [02:06,  1.29it/s]Extractor Estimating: 166it [02:06,  1.30it/s]Extractor Estimating: 167it [02:07,  1.35it/s]Extractor Estimating: 168it [02:08,  1.34it/s]Extractor Estimating: 169it [02:09,  1.32it/s]Extractor Estimating: 170it [02:09,  1.33it/s]Extractor Estimating: 171it [02:10,  1.33it/s]Extractor Estimating: 172it [02:11,  1.32it/s]Extractor Estimating: 173it [02:12,  1.35it/s]Extractor Estimating: 174it [02:12,  1.33it/s]Extractor Estimating: 175it [02:13,  1.31it/s]Extractor Estimating: 176it [02:14,  1.30it/s]Extractor Estimating: 177it [02:15,  1.31it/s]Extractor Estimating: 178it [02:15,  1.30it/s]Extractor Estimating: 179it [02:16,  1.31it/s]Extractor Estimating: 180it [02:17,  1.32it/s]Extractor Estimating: 181it [02:18,  1.34it/s]Extractor Estimating: 182it [02:18,  1.32it/s]Extractor Estimating: 183it [02:19,  1.30it/s]Extractor Estimating: 184it [02:20,  1.33it/s]Extractor Estimating: 185it [02:21,  1.33it/s]Extractor Estimating: 186it [02:21,  1.32it/s]Extractor Estimating: 187it [02:22,  1.30it/s]Extractor Estimating: 188it [02:23,  1.30it/s]Extractor Estimating: 189it [02:24,  1.29it/s]Extractor Estimating: 190it [02:25,  1.34it/s]Extractor Estimating: 191it [02:25,  1.30it/s]Extractor Estimating: 192it [02:26,  1.32it/s]Extractor Estimating: 193it [02:27,  1.30it/s]Extractor Estimating: 194it [02:28,  1.31it/s]Extractor Estimating: 195it [02:29,  1.24it/s]Extractor Estimating: 196it [02:29,  1.27it/s]Extractor Estimating: 197it [02:30,  1.31it/s]Extractor Estimating: 198it [02:31,  1.30it/s]Extractor Estimating: 199it [02:32,  1.29it/s]Extractor Estimating: 200it [02:32,  1.31it/s]Extractor Estimating: 201it [02:33,  1.31it/s]Extractor Estimating: 202it [02:34,  1.33it/s]Extractor Estimating: 203it [02:34,  1.33it/s]Extractor Estimating: 204it [02:35,  1.34it/s]Extractor Estimating: 205it [02:36,  1.31it/s]Extractor Estimating: 206it [02:37,  1.27it/s]Extractor Estimating: 207it [02:38,  1.23it/s]Extractor Estimating: 208it [02:38,  1.27it/s]Extractor Estimating: 209it [02:39,  1.29it/s]Extractor Estimating: 210it [02:40,  1.30it/s]Extractor Estimating: 211it [02:41,  1.30it/s]Extractor Estimating: 212it [02:41,  1.32it/s]Extractor Estimating: 213it [02:42,  1.31it/s]Extractor Estimating: 214it [02:43,  1.28it/s]Extractor Estimating: 215it [02:44,  1.33it/s]Extractor Estimating: 216it [02:45,  1.32it/s]Extractor Estimating: 217it [02:45,  1.34it/s]Extractor Estimating: 218it [02:46,  1.36it/s]Extractor Estimating: 219it [02:47,  1.24it/s]Extractor Estimating: 220it [02:48,  1.29it/s]Extractor Estimating: 221it [02:48,  1.28it/s]Extractor Estimating: 222it [02:49,  1.30it/s]Extractor Estimating: 223it [02:50,  1.30it/s]Extractor Estimating: 224it [02:51,  1.32it/s]Extractor Estimating: 225it [02:51,  1.29it/s]Extractor Estimating: 226it [02:52,  1.32it/s]Extractor Estimating: 227it [02:53,  1.31it/s]Extractor Estimating: 228it [02:54,  1.36it/s]Extractor Estimating: 229it [02:54,  1.34it/s]Extractor Estimating: 230it [02:55,  1.33it/s]Extractor Estimating: 231it [02:56,  1.36it/s]Extractor Estimating: 232it [02:57,  1.34it/s]Extractor Estimating: 233it [02:57,  1.32it/s]Extractor Estimating: 234it [02:58,  1.29it/s]Extractor Estimating: 235it [02:59,  1.34it/s]Extractor Estimating: 236it [03:00,  1.32it/s]Extractor Estimating: 237it [03:00,  1.32it/s]Extractor Estimating: 238it [03:01,  1.34it/s]Extractor Estimating: 239it [03:02,  1.31it/s]Extractor Estimating: 240it [03:03,  1.32it/s]Extractor Estimating: 241it [03:03,  1.34it/s]Extractor Estimating: 242it [03:04,  1.32it/s]Extractor Estimating: 243it [03:05,  1.32it/s]Extractor Estimating: 244it [03:06,  1.36it/s]Extractor Estimating: 245it [03:06,  1.41it/s]Extractor Estimating: 246it [03:07,  1.39it/s]Extractor Estimating: 247it [03:08,  1.34it/s]Extractor Estimating: 248it [03:09,  1.29it/s]Extractor Estimating: 249it [03:09,  1.31it/s]Extractor Estimating: 250it [03:10,  1.34it/s]Extractor Estimating: 250it [03:10,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:50,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:50,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:50,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:50,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:50,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:53:50,723 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:53:50,724 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:53:51,311 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:53:52,394 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:53:52,394 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:55,397 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:55,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:55,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:55,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:53:55,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:53:56,056 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:53:56,057 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:53:56,760 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:53:56,942 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:53:56,942 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:43:18,851 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:43:18,890 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4992 mean pseudo reward: 0.9363313029065417
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 22699
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22799, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22799, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.207, loss:630.2463
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.230, loss:622.1992
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.234, loss:577.1577
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.213, loss:591.8501
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.221, loss:551.7606
>> valid entity prec:0.4855, rec:0.3994, f1:0.4383
>> valid relation prec:0.2105, rec:0.0183, f1:0.0337
>> valid relation with NER prec:0.2105, rec:0.0183, f1:0.0337
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.039, loss:587.5445
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.228, loss:556.9104
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.219, loss:581.6099
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.217, loss:549.3963
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.224, loss:587.8083
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4587, rec:0.4148, f1:0.4356
>> valid relation prec:0.1755, rec:0.0116, f1:0.0217
>> valid relation with NER prec:0.1755, rec:0.0116, f1:0.0217
g_step 1100, step 60, avg_time 3.051, loss:577.8810
g_step 1200, step 160, avg_time 1.244, loss:558.9671
g_step 1300, step 52, avg_time 1.218, loss:571.8414
g_step 1400, step 152, avg_time 1.227, loss:541.2353
g_step 1500, step 44, avg_time 1.225, loss:523.6407
>> valid entity prec:0.4794, rec:0.3590, f1:0.4106
>> valid relation prec:0.2500, rec:0.0235, f1:0.0430
>> valid relation with NER prec:0.2500, rec:0.0235, f1:0.0430
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 144, avg_time 3.053, loss:535.0462
g_step 1700, step 36, avg_time 1.234, loss:542.8563
g_step 1800, step 136, avg_time 1.225, loss:509.9485
g_step 1900, step 28, avg_time 1.240, loss:504.8731
g_step 2000, step 128, avg_time 1.227, loss:488.5941
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4966, rec:0.2997, f1:0.3738
>> valid relation prec:0.1842, rec:0.0107, f1:0.0202
>> valid relation with NER prec:0.1842, rec:0.0107, f1:0.0202
g_step 2100, step 20, avg_time 3.022, loss:497.3315
g_step 2200, step 120, avg_time 1.228, loss:466.4359
g_step 2300, step 12, avg_time 1.230, loss:474.5635
g_step 2400, step 112, avg_time 1.228, loss:436.6345
g_step 2500, step 4, avg_time 1.226, loss:482.3842
>> valid entity prec:0.4572, rec:0.3610, f1:0.4034
>> valid relation prec:0.2114, rec:0.0292, f1:0.0513
>> valid relation with NER prec:0.2114, rec:0.0292, f1:0.0513
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 3.050, loss:431.0090
g_step 2700, step 204, avg_time 1.222, loss:455.6929
g_step 2800, step 96, avg_time 1.233, loss:432.4485
g_step 2900, step 196, avg_time 1.227, loss:444.2781
g_step 3000, step 88, avg_time 1.225, loss:400.6676
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4729, rec:0.4265, f1:0.4485
>> valid relation prec:0.2164, rec:0.0453, f1:0.0750
>> valid relation with NER prec:0.2164, rec:0.0453, f1:0.0750
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 3.049, loss:433.3413
g_step 3200, step 80, avg_time 1.227, loss:400.1199
g_step 3300, step 180, avg_time 1.226, loss:406.0307
g_step 3400, step 72, avg_time 1.231, loss:385.7263
g_step 3500, step 172, avg_time 1.230, loss:397.8078
>> valid entity prec:0.4534, rec:0.4542, f1:0.4538
>> valid relation prec:0.1788, rec:0.0353, f1:0.0590
>> valid relation with NER prec:0.1788, rec:0.0353, f1:0.0590
new max entity f1 on valid!
g_step 3600, step 64, avg_time 3.049, loss:355.9684
g_step 3700, step 164, avg_time 1.230, loss:382.5145
g_step 3800, step 56, avg_time 1.225, loss:382.8725
g_step 3900, step 156, avg_time 1.224, loss:367.9497
g_step 4000, step 48, avg_time 1.234, loss:363.1534
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4257, rec:0.4283, f1:0.4270
>> valid relation prec:0.1337, rec:0.0336, f1:0.0537
>> valid relation with NER prec:0.1337, rec:0.0336, f1:0.0537
g_step 4100, step 148, avg_time 3.066, loss:361.8790
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:43:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:43:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-43-18_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:43:19 - WARNING - datasets.builder -   Using custom data configuration default-b9c65e3758d1645a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b9c65e3758d1645a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:43:20,139 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:43:20,140 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:43:20,141 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:43:20,142 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:43:20,156 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,162 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,162 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,162 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,162 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:43:20,162 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:43:20,303 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:43:23,432 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:43:23,437 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b9c65e3758d1645a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.07ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.89ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.16ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.54ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.84ba/s]100%|██████████| 6/6 [00:01<00:00,  4.50ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.13ba/s] 40%|████      | 2/5 [00:00<00:00,  4.33ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.40ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  5.12ba/s]100%|██████████| 5/5 [00:01<00:00,  4.74ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.27ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.34ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.71ba/s]100%|██████████| 6/6 [00:00<00:00, 12.56ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.80ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.06ba/s]100%|██████████| 5/5 [00:00<00:00, 11.61ba/s]100%|██████████| 5/5 [00:00<00:00, 11.11ba/s]
[INFO|trainer.py:414] 2023-08-28 13:43:27,177 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:43:27,196 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:43:27,196 >>   Num examples = 5019
[INFO|trainer.py:1149] 2023-08-28 13:43:27,196 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:43:27,196 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:43:27,196 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:43:27,196 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:43:27,197 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.28it/s]  1%|          | 2/390 [00:00<01:55,  3.36it/s]  1%|          | 3/390 [00:00<01:54,  3.38it/s]  1%|          | 4/390 [00:01<01:53,  3.40it/s]  1%|▏         | 5/390 [00:01<01:53,  3.41it/s]  2%|▏         | 6/390 [00:01<01:52,  3.41it/s]  2%|▏         | 7/390 [00:02<01:52,  3.41it/s]  2%|▏         | 8/390 [00:02<01:51,  3.41it/s]  2%|▏         | 9/390 [00:02<01:51,  3.41it/s]  3%|▎         | 10/390 [00:02<01:51,  3.41it/s]  3%|▎         | 11/390 [00:03<01:51,  3.41it/s]  3%|▎         | 12/390 [00:03<01:50,  3.41it/s]  3%|▎         | 13/390 [00:03<01:50,  3.41it/s]  4%|▎         | 14/390 [00:04<01:50,  3.41it/s]  4%|▍         | 15/390 [00:04<01:49,  3.41it/s]  4%|▍         | 16/390 [00:04<01:49,  3.42it/s]  4%|▍         | 17/390 [00:04<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:48,  3.41it/s]  5%|▍         | 19/390 [00:05<01:48,  3.41it/s]  5%|▌         | 20/390 [00:05<01:48,  3.41it/s]  5%|▌         | 21/390 [00:06<01:48,  3.41it/s]  6%|▌         | 22/390 [00:06<01:47,  3.41it/s]  6%|▌         | 23/390 [00:06<01:47,  3.41it/s]  6%|▌         | 24/390 [00:07<01:47,  3.41it/s]  6%|▋         | 25/390 [00:07<01:46,  3.41it/s]  7%|▋         | 26/390 [00:07<01:46,  3.41it/s]  7%|▋         | 27/390 [00:07<01:46,  3.41it/s]  7%|▋         | 28/390 [00:08<01:46,  3.41it/s]  7%|▋         | 29/390 [00:08<01:45,  3.41it/s]  8%|▊         | 30/390 [00:08<01:45,  3.41it/s]  8%|▊         | 31/390 [00:09<01:45,  3.40it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:45,  3.40it/s]  9%|▊         | 34/390 [00:09<01:44,  3.40it/s]  9%|▉         | 35/390 [00:10<01:44,  3.41it/s]  9%|▉         | 36/390 [00:10<01:43,  3.41it/s]  9%|▉         | 37/390 [00:10<01:43,  3.41it/s] 10%|▉         | 38/390 [00:11<01:43,  3.41it/s] 10%|█         | 39/390 [00:11<01:42,  3.41it/s] 10%|█         | 40/390 [00:11<01:42,  3.41it/s] 11%|█         | 41/390 [00:12<01:42,  3.41it/s] 11%|█         | 42/390 [00:12<01:42,  3.40it/s] 11%|█         | 43/390 [00:12<01:41,  3.40it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.41it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.41it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.41it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.41it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.41it/s] 13%|█▎        | 49/390 [00:14<01:40,  3.41it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.41it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.41it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.41it/s] 14%|█▎        | 53/390 [00:15<01:39,  3.39it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.40it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.40it/s] 14%|█▍        | 56/390 [00:16<01:38,  3.40it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.40it/s] 15%|█▍        | 58/390 [00:17<01:37,  3.41it/s] 15%|█▌        | 59/390 [00:17<01:37,  3.41it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 62/390 [00:18<01:36,  3.41it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.41it/s] 16%|█▋        | 64/390 [00:18<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.40it/s] 17%|█▋        | 67/390 [00:19<01:35,  3.40it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.40it/s] 18%|█▊        | 69/390 [00:20<01:34,  3.40it/s] 18%|█▊        | 70/390 [00:20<01:34,  3.40it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.40it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.40it/s] 19%|█▊        | 73/390 [00:21<01:33,  3.40it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.39it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.40it/s] 20%|█▉        | 77/390 [00:22<01:32,  3.40it/s] 20%|██        | 78/390 [00:22<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 13:43:50,226 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:43:50,226 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:43:50,226 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.19it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.41it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.85it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.12it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.67it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.45it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.24it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.26it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.27it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.30it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.17it/s][A
 11%|█         | 62/577 [00:01<00:11, 43.88it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.00it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.03it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.93it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.88it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.11it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.27it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.18it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.05it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.05it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.07it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.02it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.97it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.03it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.09it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.07it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.00it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.11it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.13it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.92it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.00it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.03it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.12it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.17it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.11it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.07it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.10it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.99it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.14it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.06it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.07it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.21it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.03it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.00it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.01it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.02it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.00it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.09it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.19it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.07it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.97it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.94it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 43.98it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.95it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.97it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.00it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.96it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.23it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.15it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.05it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.05it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.96it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.13it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.05it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.06it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.16it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.18it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.07it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.04it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.00it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.93it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.07it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.99it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.10it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.09it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.01it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.97it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.01it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.10it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.03it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.94it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.88it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.89it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.18it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.12it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.92it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.91it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.11it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.06it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.06it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.15it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.03it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.87it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.05it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.07it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.05it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.97it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.05it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.13it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.00it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.03it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.02it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.98it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.15it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.98it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.01it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.14it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:36<01:31,  3.40it/s]
100%|██████████| 577/577 [00:13<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:44:03,336 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 13:44:03,361 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:44:04,972 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:44:04,984 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:44:04,994 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:41<30:34,  5.90s/it] 21%|██        | 80/390 [00:42<21:48,  4.22s/it] 21%|██        | 81/390 [00:42<15:40,  3.04s/it] 21%|██        | 82/390 [00:42<11:24,  2.22s/it] 21%|██▏       | 83/390 [00:43<08:24,  1.64s/it] 22%|██▏       | 84/390 [00:43<06:19,  1.24s/it] 22%|██▏       | 85/390 [00:43<04:52,  1.04it/s] 22%|██▏       | 86/390 [00:43<03:51,  1.31it/s] 22%|██▏       | 87/390 [00:44<03:08,  1.61it/s] 23%|██▎       | 88/390 [00:44<02:38,  1.90it/s] 23%|██▎       | 89/390 [00:44<02:17,  2.18it/s] 23%|██▎       | 90/390 [00:45<02:03,  2.43it/s] 23%|██▎       | 91/390 [00:45<01:53,  2.64it/s] 24%|██▎       | 92/390 [00:45<01:45,  2.82it/s] 24%|██▍       | 93/390 [00:46<01:40,  2.95it/s] 24%|██▍       | 94/390 [00:46<01:37,  3.05it/s] 24%|██▍       | 95/390 [00:46<01:34,  3.12it/s] 25%|██▍       | 96/390 [00:47<01:32,  3.18it/s] 25%|██▍       | 97/390 [00:47<01:30,  3.22it/s] 25%|██▌       | 98/390 [00:47<01:29,  3.25it/s] 25%|██▌       | 99/390 [00:47<01:28,  3.27it/s] 26%|██▌       | 100/390 [00:48<01:28,  3.28it/s] 26%|██▌       | 101/390 [00:48<01:27,  3.29it/s] 26%|██▌       | 102/390 [00:48<01:27,  3.30it/s] 26%|██▋       | 103/390 [00:49<01:26,  3.31it/s] 27%|██▋       | 104/390 [00:49<01:26,  3.32it/s] 27%|██▋       | 105/390 [00:49<01:25,  3.32it/s] 27%|██▋       | 106/390 [00:50<01:25,  3.32it/s] 27%|██▋       | 107/390 [00:50<01:25,  3.32it/s] 28%|██▊       | 108/390 [00:50<01:24,  3.32it/s] 28%|██▊       | 109/390 [00:50<01:24,  3.32it/s] 28%|██▊       | 110/390 [00:51<01:24,  3.31it/s] 28%|██▊       | 111/390 [00:51<01:24,  3.31it/s] 29%|██▊       | 112/390 [00:51<01:26,  3.22it/s] 29%|██▉       | 113/390 [00:52<01:25,  3.25it/s] 29%|██▉       | 114/390 [00:52<01:24,  3.27it/s] 29%|██▉       | 115/390 [00:52<01:23,  3.29it/s] 30%|██▉       | 116/390 [00:53<01:23,  3.30it/s] 30%|███       | 117/390 [00:53<01:22,  3.31it/s] 30%|███       | 118/390 [00:53<01:22,  3.31it/s] 31%|███       | 119/390 [00:53<01:21,  3.32it/s] 31%|███       | 120/390 [00:54<01:21,  3.31it/s] 31%|███       | 121/390 [00:54<01:21,  3.32it/s] 31%|███▏      | 122/390 [00:54<01:20,  3.32it/s] 32%|███▏      | 123/390 [00:55<01:20,  3.32it/s] 32%|███▏      | 124/390 [00:55<01:20,  3.32it/s] 32%|███▏      | 125/390 [00:55<01:19,  3.32it/s] 32%|███▏      | 126/390 [00:56<01:19,  3.32it/s] 33%|███▎      | 127/390 [00:56<01:19,  3.32it/s] 33%|███▎      | 128/390 [00:56<01:18,  3.33it/s] 33%|███▎      | 129/390 [00:56<01:18,  3.32it/s] 33%|███▎      | 130/390 [00:57<01:18,  3.32it/s] 34%|███▎      | 131/390 [00:57<01:17,  3.33it/s] 34%|███▍      | 132/390 [00:57<01:17,  3.33it/s] 34%|███▍      | 133/390 [00:58<01:17,  3.33it/s] 34%|███▍      | 134/390 [00:58<01:16,  3.35it/s] 35%|███▍      | 135/390 [00:58<01:15,  3.36it/s] 35%|███▍      | 136/390 [00:59<01:15,  3.37it/s] 35%|███▌      | 137/390 [00:59<01:14,  3.39it/s] 35%|███▌      | 138/390 [00:59<01:14,  3.39it/s] 36%|███▌      | 139/390 [00:59<01:13,  3.40it/s] 36%|███▌      | 140/390 [01:00<01:13,  3.40it/s] 36%|███▌      | 141/390 [01:00<01:13,  3.40it/s] 36%|███▋      | 142/390 [01:00<01:12,  3.40it/s] 37%|███▋      | 143/390 [01:01<01:12,  3.40it/s] 37%|███▋      | 144/390 [01:01<01:12,  3.40it/s] 37%|███▋      | 145/390 [01:01<01:12,  3.40it/s] 37%|███▋      | 146/390 [01:01<01:11,  3.40it/s] 38%|███▊      | 147/390 [01:02<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:02<01:11,  3.40it/s] 38%|███▊      | 149/390 [01:02<01:10,  3.40it/s] 38%|███▊      | 150/390 [01:03<01:10,  3.41it/s] 39%|███▊      | 151/390 [01:03<01:10,  3.40it/s] 39%|███▉      | 152/390 [01:03<01:09,  3.40it/s] 39%|███▉      | 153/390 [01:04<01:09,  3.40it/s] 39%|███▉      | 154/390 [01:04<01:09,  3.41it/s] 40%|███▉      | 155/390 [01:04<01:09,  3.40it/s] 40%|████      | 156/390 [01:04<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 13:44:32,246 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:44:32,246 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:44:32,246 >>   Batch size = 8
{'eval_loss': 0.9724634289741516, 'eval_runtime': 13.093, 'eval_samples_per_second': 352.019, 'eval_steps_per_second': 44.069, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.59it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.53it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.67it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.14it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.68it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.29it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.14it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.22it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.16it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.28it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.24it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.10it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.04it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.00it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.59it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.71it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.83it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.00it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.01it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 43.88it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.01it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.88it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.97it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.89it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.04it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.14it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.01it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.00it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 43.99it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.85it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.87it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.97it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.02it/s][A
 31%|███       | 177/577 [00:04<00:09, 44.14it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.12it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.01it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.96it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.89it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.80it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.79it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.97it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.09it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.14it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.02it/s][A
 41%|████      | 237/577 [00:05<00:07, 43.90it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.84it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.79it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.90it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.85it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.04it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.06it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.01it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.07it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 43.96it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.88it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.99it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.95it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.00it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.04it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.94it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.89it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.90it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.80it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.97it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.03it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.10it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.08it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.10it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.02it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.87it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.87it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.84it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.90it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.99it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.95it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 44.05it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.95it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.96it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.88it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.00it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.06it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.02it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.05it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.96it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.00it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.94it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.89it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.98it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.05it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.95it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.01it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.00it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.86it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.94it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.98it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.57it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.81it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.81it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.80it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.82it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.90it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.92it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.92it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.84it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.08it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.01it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.03it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.03it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.99it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:18<01:08,  3.40it/s]
100%|██████████| 577/577 [00:13<00:00, 43.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:44:45,389 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 13:44:45,406 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:44:47,333 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:44:47,349 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:44:47,361 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:24<23:22,  6.02s/it] 41%|████      | 158/390 [01:24<16:38,  4.31s/it] 41%|████      | 159/390 [01:24<11:57,  3.10s/it] 41%|████      | 160/390 [01:25<08:40,  2.26s/it] 41%|████▏     | 161/390 [01:25<06:23,  1.67s/it] 42%|████▏     | 162/390 [01:25<04:47,  1.26s/it] 42%|████▏     | 163/390 [01:26<03:41,  1.03it/s] 42%|████▏     | 164/390 [01:26<02:54,  1.30it/s] 42%|████▏     | 165/390 [01:26<02:21,  1.59it/s] 43%|████▎     | 166/390 [01:27<01:59,  1.88it/s] 43%|████▎     | 167/390 [01:27<01:43,  2.16it/s] 43%|████▎     | 168/390 [01:27<01:31,  2.42it/s] 43%|████▎     | 169/390 [01:27<01:23,  2.63it/s] 44%|████▎     | 170/390 [01:28<01:18,  2.81it/s] 44%|████▍     | 171/390 [01:28<01:14,  2.95it/s] 44%|████▍     | 172/390 [01:28<01:11,  3.04it/s] 44%|████▍     | 173/390 [01:29<01:09,  3.12it/s] 45%|████▍     | 174/390 [01:29<01:08,  3.18it/s] 45%|████▍     | 175/390 [01:29<01:06,  3.22it/s] 45%|████▌     | 176/390 [01:30<01:05,  3.25it/s] 45%|████▌     | 177/390 [01:30<01:05,  3.27it/s] 46%|████▌     | 178/390 [01:30<01:04,  3.28it/s] 46%|████▌     | 179/390 [01:30<01:04,  3.30it/s] 46%|████▌     | 180/390 [01:31<01:03,  3.31it/s] 46%|████▋     | 181/390 [01:31<01:03,  3.31it/s] 47%|████▋     | 182/390 [01:31<01:02,  3.31it/s] 47%|████▋     | 183/390 [01:32<01:02,  3.31it/s] 47%|████▋     | 184/390 [01:32<01:02,  3.31it/s] 47%|████▋     | 185/390 [01:32<01:01,  3.31it/s] 48%|████▊     | 186/390 [01:33<01:01,  3.31it/s] 48%|████▊     | 187/390 [01:33<01:01,  3.32it/s] 48%|████▊     | 188/390 [01:33<01:00,  3.32it/s] 48%|████▊     | 189/390 [01:33<01:00,  3.31it/s] 49%|████▊     | 190/390 [01:34<01:00,  3.32it/s] 49%|████▉     | 191/390 [01:34<00:59,  3.32it/s] 49%|████▉     | 192/390 [01:34<00:59,  3.31it/s] 49%|████▉     | 193/390 [01:35<00:59,  3.31it/s] 50%|████▉     | 194/390 [01:35<00:59,  3.32it/s] 50%|█████     | 195/390 [01:35<00:58,  3.32it/s] 50%|█████     | 196/390 [01:36<00:58,  3.32it/s] 51%|█████     | 197/390 [01:36<00:58,  3.32it/s] 51%|█████     | 198/390 [01:36<00:57,  3.32it/s] 51%|█████     | 199/390 [01:36<00:57,  3.32it/s] 51%|█████▏    | 200/390 [01:37<00:57,  3.32it/s] 52%|█████▏    | 201/390 [01:37<00:56,  3.32it/s] 52%|█████▏    | 202/390 [01:37<00:56,  3.30it/s] 52%|█████▏    | 203/390 [01:38<00:56,  3.31it/s] 52%|█████▏    | 204/390 [01:38<00:56,  3.31it/s] 53%|█████▎    | 205/390 [01:38<00:55,  3.32it/s] 53%|█████▎    | 206/390 [01:39<00:55,  3.32it/s] 53%|█████▎    | 207/390 [01:39<00:55,  3.32it/s] 53%|█████▎    | 208/390 [01:39<00:54,  3.32it/s] 54%|█████▎    | 209/390 [01:39<00:54,  3.32it/s] 54%|█████▍    | 210/390 [01:40<00:54,  3.32it/s] 54%|█████▍    | 211/390 [01:40<00:53,  3.32it/s] 54%|█████▍    | 212/390 [01:40<00:53,  3.30it/s] 55%|█████▍    | 213/390 [01:41<00:53,  3.31it/s] 55%|█████▍    | 214/390 [01:41<00:53,  3.31it/s] 55%|█████▌    | 215/390 [01:41<00:52,  3.31it/s] 55%|█████▌    | 216/390 [01:42<00:52,  3.32it/s] 56%|█████▌    | 217/390 [01:42<00:52,  3.32it/s] 56%|█████▌    | 218/390 [01:42<00:51,  3.32it/s] 56%|█████▌    | 219/390 [01:42<00:51,  3.32it/s] 56%|█████▋    | 220/390 [01:43<00:51,  3.32it/s] 57%|█████▋    | 221/390 [01:43<00:50,  3.32it/s] 57%|█████▋    | 222/390 [01:43<00:50,  3.30it/s] 57%|█████▋    | 223/390 [01:44<00:50,  3.30it/s] 57%|█████▋    | 224/390 [01:44<00:50,  3.31it/s] 58%|█████▊    | 225/390 [01:44<00:49,  3.31it/s] 58%|█████▊    | 226/390 [01:45<00:49,  3.31it/s] 58%|█████▊    | 227/390 [01:45<00:49,  3.31it/s] 58%|█████▊    | 228/390 [01:45<00:48,  3.31it/s] 59%|█████▊    | 229/390 [01:46<00:48,  3.31it/s] 59%|█████▉    | 230/390 [01:46<00:48,  3.31it/s] 59%|█████▉    | 231/390 [01:46<00:47,  3.31it/s] 59%|█████▉    | 232/390 [01:46<00:47,  3.30it/s] 60%|█████▉    | 233/390 [01:47<00:47,  3.31it/s] 60%|██████    | 234/390 [01:47<00:47,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 13:45:14,849 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:45:14,849 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:45:14,849 >>   Batch size = 8
{'eval_loss': 0.9841283559799194, 'eval_runtime': 13.1181, 'eval_samples_per_second': 351.345, 'eval_steps_per_second': 43.985, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 54.50it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.46it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.89it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.10it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.59it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.38it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.10it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.99it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.03it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.27it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.28it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.09it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.94it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.94it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.84it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.87it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.91it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.97it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.09it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.08it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 43.99it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.88it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.70it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.89it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.83it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.95it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.15it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.12it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.09it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.01it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.86it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.92it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.88it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.83it/s][A
 31%|███       | 177/577 [00:04<00:09, 44.01it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.03it/s][A
 32%|███▏      | 187/577 [00:04<00:09, 42.75it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.26it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.42it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.52it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.62it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.70it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.71it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.91it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.85it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.01it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.10it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.93it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.89it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.93it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.96it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.96it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.96it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.01it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.08it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.95it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.85it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.61it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.89it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.06it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.07it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.04it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.97it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.95it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.06it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.82it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.85it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.04it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.05it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.03it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.07it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.99it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.94it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.98it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.87it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.69it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.89it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.07it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.03it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.96it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.98it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.95it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.94it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.83it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.78it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.11it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.18it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.08it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.95it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.97it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.89it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.76it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.90it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.86it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.06it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.10it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.04it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.02it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.00it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.78it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.80it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.92it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.94it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.05it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.07it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.95it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.04it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.91it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.61it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.87it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 43.91it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.16it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [02:00<00:47,  3.31it/s]
100%|██████████| 577/577 [00:13<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:45:27,994 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:45:28,009 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:45:30,037 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:45:30,056 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:45:30,069 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:06<15:15,  5.91s/it] 61%|██████    | 236/390 [02:06<10:50,  4.23s/it] 61%|██████    | 237/390 [02:07<07:46,  3.05s/it] 61%|██████    | 238/390 [02:07<05:38,  2.22s/it] 61%|██████▏   | 239/390 [02:07<04:08,  1.65s/it] 62%|██████▏   | 240/390 [02:08<03:06,  1.24s/it] 62%|██████▏   | 241/390 [02:08<02:23,  1.04it/s] 62%|██████▏   | 242/390 [02:08<01:52,  1.31it/s] 62%|██████▏   | 243/390 [02:08<01:31,  1.60it/s] 63%|██████▎   | 244/390 [02:09<01:16,  1.90it/s] 63%|██████▎   | 245/390 [02:09<01:06,  2.18it/s] 63%|██████▎   | 246/390 [02:09<00:59,  2.43it/s] 63%|██████▎   | 247/390 [02:10<00:54,  2.64it/s] 64%|██████▎   | 248/390 [02:10<00:50,  2.82it/s] 64%|██████▍   | 249/390 [02:10<00:47,  2.95it/s] 64%|██████▍   | 250/390 [02:11<00:45,  3.05it/s] 64%|██████▍   | 251/390 [02:11<00:44,  3.13it/s] 65%|██████▍   | 252/390 [02:11<00:43,  3.18it/s] 65%|██████▍   | 253/390 [02:11<00:42,  3.22it/s] 65%|██████▌   | 254/390 [02:12<00:41,  3.25it/s] 65%|██████▌   | 255/390 [02:12<00:41,  3.27it/s] 66%|██████▌   | 256/390 [02:12<00:40,  3.27it/s] 66%|██████▌   | 257/390 [02:13<00:40,  3.29it/s] 66%|██████▌   | 258/390 [02:13<00:39,  3.30it/s] 66%|██████▋   | 259/390 [02:13<00:39,  3.31it/s] 67%|██████▋   | 260/390 [02:14<00:39,  3.32it/s] 67%|██████▋   | 261/390 [02:14<00:38,  3.32it/s] 67%|██████▋   | 262/390 [02:14<00:38,  3.32it/s] 67%|██████▋   | 263/390 [02:14<00:38,  3.32it/s] 68%|██████▊   | 264/390 [02:15<00:37,  3.33it/s] 68%|██████▊   | 265/390 [02:15<00:37,  3.33it/s] 68%|██████▊   | 266/390 [02:15<00:37,  3.32it/s] 68%|██████▊   | 267/390 [02:16<00:37,  3.32it/s] 69%|██████▊   | 268/390 [02:16<00:36,  3.32it/s] 69%|██████▉   | 269/390 [02:16<00:36,  3.32it/s] 69%|██████▉   | 270/390 [02:17<00:36,  3.32it/s] 69%|██████▉   | 271/390 [02:17<00:35,  3.32it/s] 70%|██████▉   | 272/390 [02:17<00:35,  3.34it/s] 70%|███████   | 273/390 [02:17<00:34,  3.37it/s] 70%|███████   | 274/390 [02:18<00:34,  3.38it/s] 71%|███████   | 275/390 [02:18<00:33,  3.38it/s] 71%|███████   | 276/390 [02:18<00:33,  3.39it/s] 71%|███████   | 277/390 [02:19<00:33,  3.38it/s] 71%|███████▏  | 278/390 [02:19<00:33,  3.39it/s] 72%|███████▏  | 279/390 [02:19<00:32,  3.39it/s] 72%|███████▏  | 280/390 [02:20<00:32,  3.40it/s] 72%|███████▏  | 281/390 [02:20<00:32,  3.40it/s] 72%|███████▏  | 282/390 [02:20<00:31,  3.40it/s] 73%|███████▎  | 283/390 [02:20<00:31,  3.40it/s] 73%|███████▎  | 284/390 [02:21<00:31,  3.41it/s] 73%|███████▎  | 285/390 [02:21<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:21<00:30,  3.41it/s] 74%|███████▎  | 287/390 [02:22<00:30,  3.40it/s] 74%|███████▍  | 288/390 [02:22<00:30,  3.39it/s] 74%|███████▍  | 289/390 [02:22<00:29,  3.40it/s] 74%|███████▍  | 290/390 [02:22<00:29,  3.40it/s] 75%|███████▍  | 291/390 [02:23<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:23<00:28,  3.40it/s] 75%|███████▌  | 293/390 [02:23<00:28,  3.40it/s] 75%|███████▌  | 294/390 [02:24<00:28,  3.41it/s] 76%|███████▌  | 295/390 [02:24<00:27,  3.41it/s] 76%|███████▌  | 296/390 [02:24<00:27,  3.40it/s] 76%|███████▌  | 297/390 [02:24<00:27,  3.40it/s] 76%|███████▋  | 298/390 [02:25<00:27,  3.40it/s] 77%|███████▋  | 299/390 [02:25<00:26,  3.40it/s] 77%|███████▋  | 300/390 [02:25<00:26,  3.40it/s] 77%|███████▋  | 301/390 [02:26<00:26,  3.40it/s] 77%|███████▋  | 302/390 [02:26<00:25,  3.40it/s] 78%|███████▊  | 303/390 [02:26<00:25,  3.41it/s] 78%|███████▊  | 304/390 [02:27<00:25,  3.40it/s] 78%|███████▊  | 305/390 [02:27<00:24,  3.41it/s] 78%|███████▊  | 306/390 [02:27<00:24,  3.40it/s] 79%|███████▊  | 307/390 [02:27<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:28<00:24,  3.40it/s] 79%|███████▉  | 309/390 [02:28<00:23,  3.40it/s] 79%|███████▉  | 310/390 [02:28<00:23,  3.39it/s] 80%|███████▉  | 311/390 [02:29<00:23,  3.40it/s] 80%|████████  | 312/390 [02:29<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 13:45:56,723 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:45:56,724 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:45:56,724 >>   Batch size = 8
{'eval_loss': 0.9985993504524231, 'eval_runtime': 13.13, 'eval_samples_per_second': 351.027, 'eval_steps_per_second': 43.945, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.16it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.46it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.99it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.16it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.64it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.44it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.97it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.14it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.20it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.16it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.17it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.94it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.05it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.03it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.83it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.00it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 43.98it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.08it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 43.94it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.87it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.91it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.89it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.88it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.85it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.98it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.14it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.04it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.04it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.06it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.91it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.82it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.67it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.90it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.99it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.05it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.08it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.00it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.06it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.01it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.76it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.95it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.07it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.05it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.06it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.03it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.91it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.71it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.90it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.93it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.05it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.03it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.13it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.99it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.91it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.88it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.88it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.96it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.01it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.99it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.08it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.19it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.81it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.81it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.83it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.99it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.12it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.13it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.12it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.09it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.80it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.77it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.87it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.92it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.01it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.02it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.11it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.97it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.61it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.73it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.81it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.98it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.08it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.15it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.01it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.03it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.89it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.80it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.94it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.97it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.04it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.08it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.08it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.00it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.80it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.77it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.85it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 43.66it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.05it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.19it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.06it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.03it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.01it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.81it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.84it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.85it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.00it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:42<00:22,  3.40it/s]
100%|██████████| 577/577 [00:13<00:00, 44.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:46:09,860 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 13:46:09,883 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:46:11,523 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:46:11,537 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:46:11,545 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:48<07:40,  5.98s/it] 81%|████████  | 314/390 [02:48<05:25,  4.28s/it] 81%|████████  | 315/390 [02:49<03:51,  3.08s/it] 81%|████████  | 316/390 [02:49<02:46,  2.25s/it] 81%|████████▏ | 317/390 [02:49<02:01,  1.67s/it] 82%|████████▏ | 318/390 [02:50<01:30,  1.26s/it] 82%|████████▏ | 319/390 [02:50<01:08,  1.03it/s] 82%|████████▏ | 320/390 [02:50<00:53,  1.30it/s] 82%|████████▏ | 321/390 [02:51<00:43,  1.59it/s] 83%|████████▎ | 322/390 [02:51<00:36,  1.89it/s] 83%|████████▎ | 323/390 [02:51<00:30,  2.17it/s] 83%|████████▎ | 324/390 [02:52<00:27,  2.36it/s] 83%|████████▎ | 325/390 [02:52<00:25,  2.58it/s] 84%|████████▎ | 326/390 [02:52<00:23,  2.77it/s] 84%|████████▍ | 327/390 [02:52<00:21,  2.92it/s] 84%|████████▍ | 328/390 [02:53<00:20,  3.03it/s] 84%|████████▍ | 329/390 [02:53<00:19,  3.11it/s] 85%|████████▍ | 330/390 [02:53<00:18,  3.17it/s] 85%|████████▍ | 331/390 [02:54<00:18,  3.22it/s] 85%|████████▌ | 332/390 [02:54<00:17,  3.25it/s] 85%|████████▌ | 333/390 [02:54<00:17,  3.27it/s] 86%|████████▌ | 334/390 [02:55<00:17,  3.28it/s] 86%|████████▌ | 335/390 [02:55<00:16,  3.30it/s] 86%|████████▌ | 336/390 [02:55<00:16,  3.30it/s] 86%|████████▋ | 337/390 [02:55<00:16,  3.31it/s] 87%|████████▋ | 338/390 [02:56<00:15,  3.31it/s] 87%|████████▋ | 339/390 [02:56<00:15,  3.31it/s] 87%|████████▋ | 340/390 [02:56<00:15,  3.32it/s] 87%|████████▋ | 341/390 [02:57<00:14,  3.32it/s] 88%|████████▊ | 342/390 [02:57<00:14,  3.32it/s] 88%|████████▊ | 343/390 [02:57<00:14,  3.32it/s] 88%|████████▊ | 344/390 [02:58<00:13,  3.31it/s] 88%|████████▊ | 345/390 [02:58<00:13,  3.31it/s] 89%|████████▊ | 346/390 [02:58<00:13,  3.32it/s] 89%|████████▉ | 347/390 [02:58<00:12,  3.32it/s] 89%|████████▉ | 348/390 [02:59<00:12,  3.32it/s] 89%|████████▉ | 349/390 [02:59<00:12,  3.33it/s] 90%|████████▉ | 350/390 [02:59<00:12,  3.33it/s] 90%|█████████ | 351/390 [03:00<00:11,  3.33it/s] 90%|█████████ | 352/390 [03:00<00:11,  3.33it/s] 91%|█████████ | 353/390 [03:00<00:11,  3.33it/s] 91%|█████████ | 354/390 [03:01<00:10,  3.30it/s] 91%|█████████ | 355/390 [03:01<00:10,  3.31it/s] 91%|█████████▏| 356/390 [03:01<00:10,  3.31it/s] 92%|█████████▏| 357/390 [03:01<00:09,  3.31it/s] 92%|█████████▏| 358/390 [03:02<00:09,  3.31it/s] 92%|█████████▏| 359/390 [03:02<00:09,  3.32it/s] 92%|█████████▏| 360/390 [03:02<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:03<00:08,  3.32it/s] 93%|█████████▎| 362/390 [03:03<00:08,  3.32it/s] 93%|█████████▎| 363/390 [03:03<00:08,  3.32it/s] 93%|█████████▎| 364/390 [03:04<00:07,  3.32it/s] 94%|█████████▎| 365/390 [03:04<00:07,  3.33it/s] 94%|█████████▍| 366/390 [03:04<00:07,  3.33it/s] 94%|█████████▍| 367/390 [03:04<00:06,  3.32it/s] 94%|█████████▍| 368/390 [03:05<00:06,  3.32it/s] 95%|█████████▍| 369/390 [03:05<00:06,  3.32it/s] 95%|█████████▍| 370/390 [03:05<00:06,  3.32it/s] 95%|█████████▌| 371/390 [03:06<00:05,  3.31it/s] 95%|█████████▌| 372/390 [03:06<00:05,  3.31it/s] 96%|█████████▌| 373/390 [03:06<00:05,  3.32it/s] 96%|█████████▌| 374/390 [03:07<00:04,  3.32it/s] 96%|█████████▌| 375/390 [03:07<00:04,  3.32it/s] 96%|█████████▋| 376/390 [03:07<00:04,  3.32it/s] 97%|█████████▋| 377/390 [03:07<00:03,  3.32it/s] 97%|█████████▋| 378/390 [03:08<00:03,  3.33it/s] 97%|█████████▋| 379/390 [03:08<00:03,  3.32it/s] 97%|█████████▋| 380/390 [03:08<00:03,  3.32it/s] 98%|█████████▊| 381/390 [03:09<00:02,  3.31it/s] 98%|█████████▊| 382/390 [03:09<00:02,  3.31it/s] 98%|█████████▊| 383/390 [03:09<00:02,  3.31it/s] 98%|█████████▊| 384/390 [03:10<00:01,  3.31it/s] 99%|█████████▊| 385/390 [03:10<00:01,  3.31it/s] 99%|█████████▉| 386/390 [03:10<00:01,  3.31it/s] 99%|█████████▉| 387/390 [03:10<00:00,  3.32it/s] 99%|█████████▉| 388/390 [03:11<00:00,  3.32it/s]100%|█████████▉| 389/390 [03:11<00:00,  3.32it/s]100%|██████████| 390/390 [03:11<00:00,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 13:46:39,094 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:46:39,094 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:46:39,094 >>   Batch size = 8
{'eval_loss': 1.0088943243026733, 'eval_runtime': 13.1166, 'eval_samples_per_second': 351.386, 'eval_steps_per_second': 43.99, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.32it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.95it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.08it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.15it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.75it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.46it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.30it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.14it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.17it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.30it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.28it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.07it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.05it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.99it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.96it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.93it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.83it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.97it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.11it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.05it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.05it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.98it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.96it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.97it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.88it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.03it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.11it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.01it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.06it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.93it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.98it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.90it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.92it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.96it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.02it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.03it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.02it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.96it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.96it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.95it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.95it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.87it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.04it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.96it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.10it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.12it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.97it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.90it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.94it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.97it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.96it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.08it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.97it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.05it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.07it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.07it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.76it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.88it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.96it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.94it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.02it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.12it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.98it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.84it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.92it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.89it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.93it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.14it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.09it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.08it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.08it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.82it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.85it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.98it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.02it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.04it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.11it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.99it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.92it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.86it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.88it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.99it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.00it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.96it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.08it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.10it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.95it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.82it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.92it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.89it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.05it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.03it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.04it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.01it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.83it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.83it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.94it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.96it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.00it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.05it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.14it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.10it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.96it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.64it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.81it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.84it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.95it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.94it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.04it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:25<00:00,  3.32it/s]
100%|██████████| 577/577 [00:13<00:00, 44.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:46:52,223 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 13:46:52,242 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:46:54,073 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:46:54,084 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:46:54,094 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:46:58,179 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:46:58,182 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78 (score: 0.9724634289741516).
                                                 100%|██████████| 390/390 [03:32<00:00,  3.32it/s]100%|██████████| 390/390 [03:32<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-28 13:46:59,955 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 13:46:59,972 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:47:01,840 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:47:01,855 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:47:01,864 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:47:02,051 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   train_loss               =      0.653
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   train_runtime            = 0:03:32.75
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   train_samples            =       5019
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   train_samples_per_second =    117.953
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:02,052 >>   train_steps_per_second   =      1.833
{'eval_loss': 1.0122591257095337, 'eval_runtime': 13.1103, 'eval_samples_per_second': 351.555, 'eval_steps_per_second': 44.011, 'epoch': 4.99}
{'train_runtime': 212.7545, 'train_samples_per_second': 117.953, 'train_steps_per_second': 1.833, 'train_loss': 0.6530143150916466, 'epoch': 4.99}
08/28/2023 13:47:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:47:02,093 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:47:02,093 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 13:47:02,093 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.44it/s]  2%|▏         | 12/577 [00:00<00:11, 47.98it/s]  3%|▎         | 17/577 [00:00<00:12, 46.59it/s]  4%|▍         | 22/577 [00:00<00:12, 45.70it/s]  5%|▍         | 27/577 [00:00<00:12, 45.34it/s]  6%|▌         | 32/577 [00:00<00:12, 45.17it/s]  6%|▋         | 37/577 [00:00<00:12, 44.88it/s]  7%|▋         | 42/577 [00:00<00:12, 44.48it/s]  8%|▊         | 47/577 [00:01<00:12, 43.96it/s]  9%|▉         | 52/577 [00:01<00:12, 43.70it/s] 10%|▉         | 57/577 [00:01<00:11, 43.80it/s] 11%|█         | 62/577 [00:01<00:11, 43.95it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.16it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.25it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.37it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.25it/s] 15%|█▌        | 87/577 [00:01<00:11, 43.87it/s] 16%|█▌        | 92/577 [00:02<00:11, 43.78it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.67it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.66it/s] 19%|█▊        | 107/577 [00:02<00:10, 43.90it/s] 19%|█▉        | 112/577 [00:02<00:10, 43.95it/s] 20%|██        | 117/577 [00:02<00:10, 44.12it/s] 21%|██        | 122/577 [00:02<00:10, 44.15it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.13it/s] 23%|██▎       | 132/577 [00:02<00:10, 43.85it/s] 24%|██▎       | 137/577 [00:03<00:10, 43.70it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.61it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.80it/s] 26%|██▋       | 152/577 [00:03<00:09, 43.66it/s] 27%|██▋       | 157/577 [00:03<00:09, 43.93it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.08it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.02it/s] 30%|██▉       | 172/577 [00:03<00:09, 43.90it/s] 31%|███       | 177/577 [00:03<00:09, 43.76it/s] 32%|███▏      | 182/577 [00:04<00:09, 43.55it/s] 32%|███▏      | 187/577 [00:04<00:08, 43.50it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.61it/s] 34%|███▍      | 197/577 [00:04<00:08, 43.74it/s] 35%|███▌      | 202/577 [00:04<00:08, 43.89it/s] 36%|███▌      | 207/577 [00:04<00:08, 43.85it/s] 37%|███▋      | 212/577 [00:04<00:08, 43.84it/s] 38%|███▊      | 217/577 [00:04<00:08, 43.77it/s] 38%|███▊      | 222/577 [00:05<00:08, 43.68it/s] 39%|███▉      | 227/577 [00:05<00:08, 43.64it/s] 40%|████      | 232/577 [00:05<00:07, 43.71it/s] 41%|████      | 237/577 [00:05<00:07, 43.78it/s] 42%|████▏     | 242/577 [00:05<00:07, 43.91it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.08it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.17it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.13it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.17it/s] 46%|████▋     | 267/577 [00:06<00:07, 43.83it/s] 47%|████▋     | 272/577 [00:06<00:06, 43.76it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.85it/s] 49%|████▉     | 282/577 [00:06<00:06, 43.91it/s] 50%|████▉     | 287/577 [00:06<00:06, 43.94it/s] 51%|█████     | 292/577 [00:06<00:06, 44.01it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.09it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.03it/s] 54%|█████▍    | 312/577 [00:07<00:06, 43.86it/s] 55%|█████▍    | 317/577 [00:07<00:05, 43.73it/s] 56%|█████▌    | 322/577 [00:07<00:05, 43.91it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.00it/s] 58%|█████▊    | 332/577 [00:07<00:05, 43.93it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.03it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.04it/s] 60%|██████    | 347/577 [00:07<00:05, 44.04it/s] 61%|██████    | 352/577 [00:07<00:05, 43.96it/s] 62%|██████▏   | 357/577 [00:08<00:05, 43.86it/s] 63%|██████▎   | 362/577 [00:08<00:04, 43.85it/s] 64%|██████▎   | 367/577 [00:08<00:04, 43.94it/s] 64%|██████▍   | 372/577 [00:08<00:04, 43.91it/s] 65%|██████▌   | 377/577 [00:08<00:04, 43.99it/s] 66%|██████▌   | 382/577 [00:08<00:04, 43.91it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.10it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.13it/s] 69%|██████▉   | 397/577 [00:09<00:04, 43.78it/s] 70%|██████▉   | 402/577 [00:09<00:04, 43.70it/s] 71%|███████   | 407/577 [00:09<00:03, 43.82it/s] 71%|███████▏  | 412/577 [00:09<00:03, 43.86it/s] 72%|███████▏  | 417/577 [00:09<00:03, 43.87it/s] 73%|███████▎  | 422/577 [00:09<00:03, 43.95it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.09it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.12it/s] 76%|███████▌  | 437/577 [00:09<00:03, 43.95it/s] 77%|███████▋  | 442/577 [00:10<00:03, 43.95it/s] 77%|███████▋  | 447/577 [00:10<00:02, 43.84it/s] 78%|███████▊  | 452/577 [00:10<00:02, 43.87it/s] 79%|███████▉  | 457/577 [00:10<00:02, 43.93it/s] 80%|████████  | 462/577 [00:10<00:02, 43.88it/s] 81%|████████  | 467/577 [00:10<00:02, 43.99it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.09it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.03it/s] 84%|████████▎ | 482/577 [00:10<00:02, 43.89it/s] 84%|████████▍ | 487/577 [00:11<00:02, 43.81it/s] 85%|████████▌ | 492/577 [00:11<00:01, 43.83it/s] 86%|████████▌ | 497/577 [00:11<00:01, 43.80it/s] 87%|████████▋ | 502/577 [00:11<00:01, 43.86it/s] 88%|████████▊ | 507/577 [00:11<00:01, 44.00it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.16it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.06it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.06it/s] 91%|█████████▏| 527/577 [00:11<00:01, 43.69it/s] 92%|█████████▏| 532/577 [00:12<00:01, 43.84it/s] 93%|█████████▎| 537/577 [00:12<00:00, 43.81it/s] 94%|█████████▍| 542/577 [00:12<00:00, 43.80it/s] 95%|█████████▍| 547/577 [00:12<00:00, 43.85it/s] 96%|█████████▌| 552/577 [00:12<00:00, 44.02it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.11it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.01it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.01it/s] 99%|█████████▉| 572/577 [00:12<00:00, 43.92it/s]100%|██████████| 577/577 [00:13<00:00, 43.88it/s]100%|██████████| 577/577 [00:13<00:00, 44.02it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:47:15,221 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   eval_loss               =     0.9725
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   eval_runtime            = 0:00:13.12
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   eval_samples_per_second =    351.084
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   eval_steps_per_second   =     43.952
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:47:15,221 >>   perplexity              =     2.6445
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:21,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:21,981 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:21,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:21,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:21,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:47:22,584 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:47:22,585 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:47:23,153 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:47:24,183 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:47:24,183 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:27,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:27,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:27,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:27,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:27,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:47:27,858 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:47:27,859 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:47:28,455 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:47:28,630 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:47:28,630 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:06,  1.32it/s]Extractor Predicting: 9it [00:06,  1.26it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.31it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:09,  1.31it/s]Extractor Predicting: 14it [00:10,  1.32it/s]Extractor Predicting: 15it [00:11,  1.33it/s]Extractor Predicting: 16it [00:12,  1.35it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:15,  1.34it/s]Extractor Predicting: 21it [00:15,  1.31it/s]Extractor Predicting: 22it [00:16,  1.32it/s]Extractor Predicting: 23it [00:17,  1.30it/s]Extractor Predicting: 24it [00:18,  1.32it/s]Extractor Predicting: 25it [00:18,  1.31it/s]Extractor Predicting: 26it [00:19,  1.31it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:21,  1.32it/s]Extractor Predicting: 29it [00:21,  1.33it/s]Extractor Predicting: 30it [00:22,  1.35it/s]Extractor Predicting: 31it [00:23,  1.32it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:25,  1.28it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:28,  1.30it/s]Extractor Predicting: 39it [00:29,  1.28it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.26it/s]Extractor Predicting: 42it [00:32,  1.27it/s]Extractor Predicting: 43it [00:32,  1.27it/s]Extractor Predicting: 44it [00:33,  1.25it/s]Extractor Predicting: 45it [00:34,  1.25it/s]Extractor Predicting: 46it [00:35,  1.25it/s]Extractor Predicting: 47it [00:36,  1.24it/s]Extractor Predicting: 48it [00:36,  1.24it/s]Extractor Predicting: 49it [00:37,  1.29it/s]Extractor Predicting: 50it [00:38,  1.20it/s]Extractor Predicting: 51it [00:39,  1.24it/s]Extractor Predicting: 52it [00:40,  1.23it/s]Extractor Predicting: 53it [00:40,  1.23it/s]Extractor Predicting: 54it [00:41,  1.25it/s]Extractor Predicting: 55it [00:42,  1.25it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:44,  1.27it/s]Extractor Predicting: 58it [00:44,  1.26it/s]Extractor Predicting: 59it [00:45,  1.29it/s]Extractor Predicting: 60it [00:46,  1.29it/s]Extractor Predicting: 61it [00:47,  1.27it/s]Extractor Predicting: 62it [00:47,  1.25it/s]Extractor Predicting: 63it [00:48,  1.24it/s]Extractor Predicting: 64it [00:49,  1.23it/s]Extractor Predicting: 65it [00:50,  1.24it/s]Extractor Predicting: 66it [00:51,  1.27it/s]Extractor Predicting: 67it [00:51,  1.28it/s]Extractor Predicting: 68it [00:52,  1.31it/s]Extractor Predicting: 69it [00:53,  1.32it/s]Extractor Predicting: 70it [00:54,  1.31it/s]Extractor Predicting: 71it [00:54,  1.33it/s]Extractor Predicting: 72it [00:55,  1.37it/s]Extractor Predicting: 73it [00:56,  1.34it/s]Extractor Predicting: 74it [00:57,  1.36it/s]Extractor Predicting: 75it [00:57,  1.35it/s]Extractor Predicting: 76it [00:58,  1.32it/s]Extractor Predicting: 77it [00:59,  1.31it/s]Extractor Predicting: 78it [01:00,  1.33it/s]Extractor Predicting: 79it [01:00,  1.33it/s]Extractor Predicting: 80it [01:01,  1.33it/s]Extractor Predicting: 81it [01:02,  1.31it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:03,  1.30it/s]Extractor Predicting: 84it [01:04,  1.34it/s]Extractor Predicting: 85it [01:05,  1.38it/s]Extractor Predicting: 86it [01:06,  1.38it/s]Extractor Predicting: 87it [01:06,  1.39it/s]Extractor Predicting: 88it [01:07,  1.32it/s]Extractor Predicting: 89it [01:08,  1.32it/s]Extractor Predicting: 90it [01:09,  1.30it/s]Extractor Predicting: 91it [01:09,  1.31it/s]Extractor Predicting: 92it [01:10,  1.32it/s]Extractor Predicting: 93it [01:11,  1.32it/s]Extractor Predicting: 94it [01:12,  1.33it/s]Extractor Predicting: 95it [01:12,  1.32it/s]Extractor Predicting: 96it [01:13,  1.33it/s]Extractor Predicting: 97it [01:14,  1.33it/s]Extractor Predicting: 98it [01:15,  1.29it/s]Extractor Predicting: 99it [01:15,  1.30it/s]Extractor Predicting: 100it [01:16,  1.28it/s]Extractor Predicting: 101it [01:17,  1.27it/s]Extractor Predicting: 102it [01:18,  1.29it/s]Extractor Predicting: 103it [01:19,  1.28it/s]Extractor Predicting: 104it [01:19,  1.30it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:21,  1.27it/s]Extractor Predicting: 107it [01:22,  1.31it/s]Extractor Predicting: 108it [01:22,  1.33it/s]Extractor Predicting: 109it [01:23,  1.30it/s]Extractor Predicting: 110it [01:24,  1.30it/s]Extractor Predicting: 111it [01:25,  1.28it/s]Extractor Predicting: 112it [01:26,  1.28it/s]Extractor Predicting: 113it [01:26,  1.29it/s]Extractor Predicting: 114it [01:27,  1.30it/s]Extractor Predicting: 115it [01:28,  1.30it/s]Extractor Predicting: 116it [01:29,  1.30it/s]Extractor Predicting: 117it [01:29,  1.33it/s]Extractor Predicting: 118it [01:30,  1.33it/s]Extractor Predicting: 119it [01:31,  1.37it/s]Extractor Predicting: 120it [01:32,  1.36it/s]Extractor Predicting: 121it [01:32,  1.35it/s]Extractor Predicting: 122it [01:33,  1.33it/s]Extractor Predicting: 123it [01:34,  1.32it/s]Extractor Predicting: 124it [01:35,  1.34it/s]Extractor Predicting: 125it [01:35,  1.30it/s]Extractor Predicting: 126it [01:36,  1.33it/s]Extractor Predicting: 127it [01:37,  1.35it/s]Extractor Predicting: 128it [01:38,  1.33it/s]Extractor Predicting: 129it [01:38,  1.32it/s]Extractor Predicting: 130it [01:39,  1.29it/s]Extractor Predicting: 131it [01:40,  1.28it/s]Extractor Predicting: 132it [01:41,  1.30it/s]Extractor Predicting: 133it [01:41,  1.31it/s]Extractor Predicting: 134it [01:42,  1.29it/s]Extractor Predicting: 135it [01:43,  1.29it/s]Extractor Predicting: 136it [01:44,  1.20it/s]Extractor Predicting: 137it [01:45,  1.24it/s]Extractor Predicting: 138it [01:46,  1.25it/s]Extractor Predicting: 139it [01:46,  1.24it/s]Extractor Predicting: 140it [01:47,  1.24it/s]Extractor Predicting: 141it [01:48,  1.24it/s]Extractor Predicting: 142it [01:49,  1.23it/s]Extractor Predicting: 143it [01:50,  1.25it/s]Extractor Predicting: 144it [01:50,  1.24it/s]Extractor Predicting: 145it [01:51,  1.24it/s]Extractor Predicting: 146it [01:52,  1.21it/s]Extractor Predicting: 147it [01:53,  1.22it/s]Extractor Predicting: 148it [01:54,  1.24it/s]Extractor Predicting: 149it [01:54,  1.26it/s]Extractor Predicting: 150it [01:55,  1.25it/s]Extractor Predicting: 151it [01:56,  1.28it/s]Extractor Predicting: 152it [01:57,  1.27it/s]Extractor Predicting: 153it [01:58,  1.29it/s]Extractor Predicting: 154it [01:58,  1.28it/s]Extractor Predicting: 155it [01:59,  1.27it/s]Extractor Predicting: 156it [02:00,  1.26it/s]Extractor Predicting: 157it [02:01,  1.28it/s]Extractor Predicting: 158it [02:01,  1.28it/s]Extractor Predicting: 159it [02:02,  1.26it/s]Extractor Predicting: 160it [02:03,  1.27it/s]Extractor Predicting: 161it [02:04,  1.29it/s]Extractor Predicting: 162it [02:05,  1.29it/s]Extractor Predicting: 163it [02:05,  1.25it/s]Extractor Predicting: 164it [02:06,  1.25it/s]Extractor Predicting: 165it [02:07,  1.26it/s]Extractor Predicting: 166it [02:08,  1.24it/s]Extractor Predicting: 167it [02:09,  1.25it/s]Extractor Predicting: 168it [02:09,  1.26it/s]Extractor Predicting: 169it [02:10,  1.26it/s]Extractor Predicting: 170it [02:11,  1.27it/s]Extractor Predicting: 171it [02:11,  1.50it/s]Extractor Predicting: 171it [02:11,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:47,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:47,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:47,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:47,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:47,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:49:48,594 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:49:48,595 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:49:48,847 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:49:49,900 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:49:49,901 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:52,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:52,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:52,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:52,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:52,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:49:53,371 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:49:53,373 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:49:53,938 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:49:54,106 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:49:54,106 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2720306513409962,
  "recall": 0.04621392926882187,
  "score": 0.07900593471810088,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.27it/s]Extractor Predicting: 10it [00:07,  1.22it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.31it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.28it/s]Extractor Predicting: 16it [00:12,  1.30it/s]Extractor Predicting: 17it [00:13,  1.32it/s]Extractor Predicting: 18it [00:13,  1.33it/s]Extractor Predicting: 19it [00:14,  1.35it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:16,  1.36it/s]Extractor Predicting: 22it [00:16,  1.34it/s]Extractor Predicting: 23it [00:17,  1.34it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.31it/s]Extractor Predicting: 26it [00:19,  1.34it/s]Extractor Predicting: 27it [00:20,  1.35it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.29it/s]Extractor Predicting: 30it [00:22,  1.28it/s]Extractor Predicting: 31it [00:23,  1.29it/s]Extractor Predicting: 32it [00:24,  1.29it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.30it/s]Extractor Predicting: 35it [00:26,  1.31it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.31it/s]Extractor Predicting: 39it [00:29,  1.30it/s]Extractor Predicting: 40it [00:30,  1.31it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:32,  1.33it/s]Extractor Predicting: 44it [00:33,  1.33it/s]Extractor Predicting: 45it [00:34,  1.33it/s]Extractor Predicting: 46it [00:35,  1.35it/s]Extractor Predicting: 47it [00:35,  1.31it/s]Extractor Predicting: 48it [00:36,  1.31it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.31it/s]Extractor Predicting: 51it [00:38,  1.33it/s]Extractor Predicting: 52it [00:39,  1.33it/s]Extractor Predicting: 53it [00:40,  1.33it/s]Extractor Predicting: 54it [00:41,  1.31it/s]Extractor Predicting: 55it [00:41,  1.29it/s]Extractor Predicting: 56it [00:42,  1.31it/s]Extractor Predicting: 57it [00:43,  1.32it/s]Extractor Predicting: 58it [00:44,  1.32it/s]Extractor Predicting: 59it [00:44,  1.33it/s]Extractor Predicting: 60it [00:45,  1.30it/s]Extractor Predicting: 61it [00:46,  1.30it/s]Extractor Predicting: 62it [00:47,  1.28it/s]Extractor Predicting: 63it [00:48,  1.32it/s]Extractor Predicting: 64it [00:48,  1.31it/s]Extractor Predicting: 65it [00:49,  1.32it/s]Extractor Predicting: 66it [00:50,  1.31it/s]Extractor Predicting: 67it [00:51,  1.32it/s]Extractor Predicting: 68it [00:51,  1.33it/s]Extractor Predicting: 69it [00:52,  1.35it/s]Extractor Predicting: 70it [00:53,  1.32it/s]Extractor Predicting: 71it [00:54,  1.28it/s]Extractor Predicting: 72it [00:54,  1.30it/s]Extractor Predicting: 73it [00:55,  1.29it/s]Extractor Predicting: 74it [00:56,  1.31it/s]Extractor Predicting: 75it [00:57,  1.31it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:58,  1.36it/s]Extractor Predicting: 78it [00:59,  1.36it/s]Extractor Predicting: 79it [01:00,  1.34it/s]Extractor Predicting: 80it [01:00,  1.36it/s]Extractor Predicting: 81it [01:01,  1.35it/s]Extractor Predicting: 82it [01:02,  1.35it/s]Extractor Predicting: 83it [01:03,  1.35it/s]Extractor Predicting: 84it [01:03,  1.32it/s]Extractor Predicting: 85it [01:04,  1.33it/s]Extractor Predicting: 86it [01:05,  1.33it/s]Extractor Predicting: 87it [01:06,  1.30it/s]Extractor Predicting: 88it [01:06,  1.31it/s]Extractor Predicting: 89it [01:07,  1.31it/s]Extractor Predicting: 90it [01:08,  1.28it/s]Extractor Predicting: 91it [01:09,  1.29it/s]Extractor Predicting: 92it [01:10,  1.30it/s]Extractor Predicting: 93it [01:10,  1.30it/s]Extractor Predicting: 94it [01:11,  1.29it/s]Extractor Predicting: 95it [01:12,  1.28it/s]Extractor Predicting: 96it [01:13,  1.17it/s]Extractor Predicting: 97it [01:14,  1.23it/s]Extractor Predicting: 98it [01:14,  1.25it/s]Extractor Predicting: 99it [01:15,  1.33it/s]Extractor Predicting: 100it [01:16,  1.34it/s]Extractor Predicting: 101it [01:17,  1.32it/s]Extractor Predicting: 102it [01:17,  1.34it/s]Extractor Predicting: 103it [01:18,  1.33it/s]Extractor Predicting: 104it [01:19,  1.34it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:20,  1.35it/s]Extractor Predicting: 107it [01:21,  1.36it/s]Extractor Predicting: 108it [01:22,  1.35it/s]Extractor Predicting: 109it [01:23,  1.31it/s]Extractor Predicting: 110it [01:23,  1.26it/s]Extractor Predicting: 111it [01:24,  1.23it/s]Extractor Predicting: 112it [01:25,  1.24it/s]Extractor Predicting: 113it [01:26,  1.28it/s]Extractor Predicting: 114it [01:27,  1.30it/s]Extractor Predicting: 115it [01:27,  1.29it/s]Extractor Predicting: 116it [01:28,  1.29it/s]Extractor Predicting: 117it [01:29,  1.29it/s]Extractor Predicting: 118it [01:30,  1.26it/s]Extractor Predicting: 119it [01:30,  1.28it/s]Extractor Predicting: 120it [01:31,  1.25it/s]Extractor Predicting: 121it [01:32,  1.24it/s]Extractor Predicting: 122it [01:33,  1.26it/s]Extractor Predicting: 123it [01:34,  1.27it/s]Extractor Predicting: 124it [01:34,  1.27it/s]Extractor Predicting: 125it [01:35,  1.58it/s]Extractor Predicting: 125it [01:35,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:36,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:36,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:36,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:36,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:36,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:51:36,703 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:51:36,704 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:51:37,263 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:51:38,307 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:51:38,307 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:41,119 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:41,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:41,128 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:41,128 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:51:41,128 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:51:41,753 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:51:41,754 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:51:42,335 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:51:42,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:51:42,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5239786856127886,
  "recall": 0.099026518966096,
  "score": 0.1665725578769057,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.21it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:04,  1.23it/s]Extractor Predicting: 6it [00:04,  1.64it/s]Extractor Predicting: 6it [00:04,  1.39it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:51:47,274 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:51:47,275 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:51:47,284 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:51:47,284 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:51:47,286 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:51:50,438 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:51:50,440 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:51:50,453 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:51:50,453 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:51:50,459 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:51:50,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.05511811023622047,
  "score": 0.10181818181818182,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:51:50,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:51,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:53,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:54,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:54,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:55,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:56,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:57,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:57,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:58,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:51:59,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:00,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:01,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:02,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:03,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:04,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:04,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:05,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:06,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:07,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:08,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:18<02:48, 18.70s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:09,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:10,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:10,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:11,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:12,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:13,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:14,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:15,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:15,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:16,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:17,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:18,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:19,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:20,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:20,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:21,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:22,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:23,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:23,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:24,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:25,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:35<02:19, 17.41s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:25,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:26,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:27,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:28,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:28,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:29,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:30,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:31,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:31,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:32,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:33,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:34,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:34,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:35,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:36,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:36,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:37,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:38,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:39,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:40,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:40,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:50<01:56, 16.61s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:41,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:42,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:43,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:44,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:44,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:45,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:46,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:47,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:48,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:49,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:50,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:51,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:52,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:52,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:53,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:55,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:56,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:57,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:57,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:58,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:59,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:44, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:00,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:01,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:01,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:02,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:03,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:04,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:05,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:05,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:07,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:08,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:09,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:10,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:10,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:11,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:12,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:12,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:13,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:14,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:15,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:25<01:24, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:15,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:16,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:17,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:18,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:19,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:19,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:20,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:21,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:22,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:22,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:23,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:24,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:25,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:25,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:26,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:27,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:28,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:28,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:29,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:30,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:31,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:32,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:42<01:07, 16.78s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:32,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:33,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:34,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:35,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:35,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:36,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:37,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:38,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:39,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:39,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:40,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:41,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:42,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:42,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:43,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:44,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:45,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:46,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:46,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:47,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:48,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:49,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:59<00:50, 16.91s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:49,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:50,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:51,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:52,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:52,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:53,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:54,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:55,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:55,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:56,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:57,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:58,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:58,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:59,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:00,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:00,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:01,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:02,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:03,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:04,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:04,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:14<00:32, 16.50s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:05,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:06,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:07,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:07,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:08,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:09,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:10,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:11,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:11,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:12,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:13,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:14,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:15,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:16,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:16,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:17,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:18,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:19,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:20,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:20,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:21,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:22,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:23,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:33<00:17, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:24,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:24,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:25,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:26,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:27,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:27,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:28,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:29,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:30,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:30,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:31,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:32,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:33,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:33,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:34,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:35,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:36,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:37,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:37,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:38,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:39,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:39,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:40,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:41,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:42,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:43,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:44,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:45,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:55<00:00, 18.61s/it]Generating: 100%|██████████| 10/10 [02:55<00:00, 17.53s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:52,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:52,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:52,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:52,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:52,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:54:52,618 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:54:52,619 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:54:53,345 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:54:54,422 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:54:54,422 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:56,647 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:56,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:56,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:56,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:56,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:54:56,977 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:54:56,979 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:54:57,238 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:54:57,408 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:54:57,409 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : made from material .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : official language .', 'success_rate': 0.875, 'errors': {'', "('Yamanashi Miyano', 'official language', '', 'In 2012 , she played the title role in the anime adaptation of Yamanashi Miyano s manga series as a young girl from Nohrura .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8920454545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8505434782608695, 'errors': {'', "('utilities', 'use', '', 'It uses the following utilities in order to access the .')"}}
['Relation : voice type . Context : Later in 2008 , he played the title role of a young musician in the Broadway production of the same name , directed by Michael Ian Black , based on a play by the same name by Paul Simon . Head Entity : Paul Simon , Tail Entity : song .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 149, 'raw': 224}
{'target': 600, 'success': 168, 'raw': 256}
{'target': 600, 'success': 190, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 257, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 343, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 382, 'raw': 576}
{'target': 600, 'success': 407, 'raw': 608}
{'target': 600, 'success': 427, 'raw': 640}
{'target': 600, 'success': 445, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 509, 'raw': 768}
{'target': 600, 'success': 530, 'raw': 800}
{'target': 600, 'success': 553, 'raw': 832}
{'target': 600, 'success': 573, 'raw': 864}
{'target': 600, 'success': 590, 'raw': 896}
{'target': 600, 'success': 611, 'raw': 928}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6584051724137931, 'errors': {'', "('Saoirse Ronan', 'voice type', '', 'It is a remake of the 1990 drama film of the same name , directed by Adam Rodriguez and starring Colin Farrell , Elizabeth Taylor and Saoirse Ronan .')", "('Lisa Anderson', 'voice type', '', 'She is known for portraying the role of the main character Lisa Anderson as a professional football player from the NFL s New York Giants .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 9840
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9940, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.24it/s]Extractor Estimating: 2it [00:01,  1.16it/s]Extractor Estimating: 3it [00:02,  1.23it/s]Extractor Estimating: 4it [00:03,  1.29it/s]Extractor Estimating: 5it [00:03,  1.31it/s]Extractor Estimating: 6it [00:04,  1.36it/s]Extractor Estimating: 7it [00:05,  1.35it/s]Extractor Estimating: 8it [00:06,  1.33it/s]Extractor Estimating: 9it [00:06,  1.35it/s]Extractor Estimating: 10it [00:07,  1.39it/s]Extractor Estimating: 11it [00:08,  1.33it/s]Extractor Estimating: 12it [00:09,  1.29it/s]Extractor Estimating: 13it [00:09,  1.29it/s]Extractor Estimating: 14it [00:10,  1.32it/s]Extractor Estimating: 15it [00:11,  1.35it/s]Extractor Estimating: 16it [00:12,  1.39it/s]Extractor Estimating: 17it [00:12,  1.37it/s]Extractor Estimating: 18it [00:13,  1.39it/s]Extractor Estimating: 19it [00:14,  1.34it/s]Extractor Estimating: 20it [00:14,  1.36it/s]Extractor Estimating: 21it [00:15,  1.36it/s]Extractor Estimating: 22it [00:16,  1.35it/s]Extractor Estimating: 23it [00:17,  1.27it/s]Extractor Estimating: 24it [00:18,  1.25it/s]Extractor Estimating: 25it [00:18,  1.27it/s]Extractor Estimating: 26it [00:19,  1.25it/s]Extractor Estimating: 27it [00:20,  1.29it/s]Extractor Estimating: 28it [00:21,  1.30it/s]Extractor Estimating: 29it [00:21,  1.36it/s]Extractor Estimating: 30it [00:22,  1.39it/s]Extractor Estimating: 31it [00:23,  1.43it/s]Extractor Estimating: 32it [00:23,  1.42it/s]Extractor Estimating: 33it [00:24,  1.45it/s]Extractor Estimating: 34it [00:25,  1.50it/s]Extractor Estimating: 35it [00:25,  1.57it/s]Extractor Estimating: 36it [00:26,  1.38it/s]Extractor Estimating: 37it [00:27,  1.43it/s]Extractor Estimating: 38it [00:28,  1.47it/s]Extractor Estimating: 39it [00:28,  1.43it/s]Extractor Estimating: 40it [00:29,  1.38it/s]Extractor Estimating: 41it [00:30,  1.41it/s]Extractor Estimating: 42it [00:31,  1.33it/s]Extractor Estimating: 43it [00:31,  1.37it/s]Extractor Estimating: 44it [00:32,  1.40it/s]Extractor Estimating: 45it [00:33,  1.41it/s]Extractor Estimating: 46it [00:33,  1.36it/s]Extractor Estimating: 47it [00:34,  1.43it/s]Extractor Estimating: 48it [00:35,  1.48it/s]Extractor Estimating: 49it [00:35,  1.50it/s]Extractor Estimating: 50it [00:36,  1.49it/s]Extractor Estimating: 51it [00:37,  1.45it/s]Extractor Estimating: 52it [00:37,  1.41it/s]Extractor Estimating: 53it [00:38,  1.40it/s]Extractor Estimating: 54it [00:39,  1.40it/s]Extractor Estimating: 55it [00:40,  1.37it/s]Extractor Estimating: 56it [00:40,  1.37it/s]Extractor Estimating: 57it [00:41,  1.39it/s]Extractor Estimating: 58it [00:42,  1.38it/s]Extractor Estimating: 59it [00:43,  1.40it/s]Extractor Estimating: 60it [00:43,  1.39it/s]Extractor Estimating: 61it [00:44,  1.39it/s]Extractor Estimating: 62it [00:45,  1.40it/s]Extractor Estimating: 63it [00:45,  1.39it/s]Extractor Estimating: 64it [00:46,  1.42it/s]Extractor Estimating: 65it [00:47,  1.41it/s]Extractor Estimating: 66it [00:47,  1.43it/s]Extractor Estimating: 67it [00:48,  1.45it/s]Extractor Estimating: 68it [00:49,  1.42it/s]Extractor Estimating: 69it [00:50,  1.39it/s]Extractor Estimating: 70it [00:50,  1.38it/s]Extractor Estimating: 71it [00:51,  1.35it/s]Extractor Estimating: 72it [00:52,  1.35it/s]Extractor Estimating: 73it [00:53,  1.37it/s]Extractor Estimating: 74it [00:53,  1.39it/s]Extractor Estimating: 75it [00:54,  1.39it/s]Extractor Estimating: 76it [00:55,  1.39it/s]Extractor Estimating: 77it [00:56,  1.34it/s]Extractor Estimating: 78it [00:56,  1.35it/s]Extractor Estimating: 79it [00:57,  1.31it/s]Extractor Estimating: 80it [00:58,  1.32it/s]Extractor Estimating: 81it [00:59,  1.35it/s]Extractor Estimating: 82it [00:59,  1.34it/s]Extractor Estimating: 83it [01:00,  1.32it/s]Extractor Estimating: 84it [01:01,  1.36it/s]Extractor Estimating: 85it [01:01,  1.41it/s]Extractor Estimating: 86it [01:02,  1.39it/s]Extractor Estimating: 87it [01:03,  1.37it/s]Extractor Estimating: 88it [01:04,  1.37it/s]Extractor Estimating: 89it [01:04,  1.33it/s]Extractor Estimating: 90it [01:05,  1.33it/s]Extractor Estimating: 91it [01:06,  1.38it/s]Extractor Estimating: 92it [01:07,  1.32it/s]Extractor Estimating: 93it [01:07,  1.38it/s]Extractor Estimating: 94it [01:08,  1.38it/s]Extractor Estimating: 95it [01:09,  1.39it/s]Extractor Estimating: 96it [01:10,  1.37it/s]Extractor Estimating: 97it [01:10,  1.38it/s]Extractor Estimating: 98it [01:11,  1.37it/s]Extractor Estimating: 99it [01:12,  1.36it/s]Extractor Estimating: 100it [01:13,  1.33it/s]Extractor Estimating: 101it [01:13,  1.32it/s]Extractor Estimating: 102it [01:14,  1.31it/s]Extractor Estimating: 103it [01:15,  1.28it/s]Extractor Estimating: 104it [01:16,  1.25it/s]Extractor Estimating: 105it [01:16,  1.26it/s]Extractor Estimating: 106it [01:17,  1.26it/s]Extractor Estimating: 107it [01:18,  1.28it/s]Extractor Estimating: 108it [01:19,  1.28it/s]Extractor Estimating: 109it [01:20,  1.28it/s]Extractor Estimating: 110it [01:20,  1.29it/s]Extractor Estimating: 111it [01:21,  1.29it/s]Extractor Estimating: 112it [01:22,  1.28it/s]Extractor Estimating: 113it [01:23,  1.26it/s]Extractor Estimating: 114it [01:23,  1.29it/s]Extractor Estimating: 115it [01:24,  1.29it/s]Extractor Estimating: 116it [01:25,  1.27it/s]Extractor Estimating: 117it [01:26,  1.27it/s]Extractor Estimating: 118it [01:27,  1.26it/s]Extractor Estimating: 119it [01:27,  1.25it/s]Extractor Estimating: 120it [01:28,  1.26it/s]Extractor Estimating: 121it [01:29,  1.26it/s]Extractor Estimating: 122it [01:30,  1.26it/s]Extractor Estimating: 123it [01:31,  1.26it/s]Extractor Estimating: 124it [01:31,  1.24it/s]Extractor Estimating: 125it [01:32,  1.15it/s]Extractor Estimating: 126it [01:33,  1.19it/s]Extractor Estimating: 127it [01:34,  1.18it/s]Extractor Estimating: 128it [01:35,  1.22it/s]Extractor Estimating: 129it [01:36,  1.21it/s]Extractor Estimating: 130it [01:36,  1.24it/s]Extractor Estimating: 131it [01:37,  1.27it/s]Extractor Estimating: 132it [01:38,  1.28it/s]Extractor Estimating: 133it [01:39,  1.29it/s]Extractor Estimating: 134it [01:39,  1.32it/s]Extractor Estimating: 135it [01:40,  1.30it/s]Extractor Estimating: 136it [01:41,  1.26it/s]Extractor Estimating: 137it [01:42,  1.25it/s]Extractor Estimating: 138it [01:43,  1.27it/s]Extractor Estimating: 139it [01:43,  1.26it/s]Extractor Estimating: 140it [01:44,  1.29it/s]Extractor Estimating: 141it [01:45,  1.29it/s]Extractor Estimating: 142it [01:46,  1.29it/s]Extractor Estimating: 143it [01:46,  1.32it/s]Extractor Estimating: 144it [01:47,  1.32it/s]Extractor Estimating: 145it [01:48,  1.30it/s]Extractor Estimating: 146it [01:49,  1.30it/s]Extractor Estimating: 147it [01:50,  1.29it/s]Extractor Estimating: 148it [01:50,  1.30it/s]Extractor Estimating: 149it [01:51,  1.29it/s]Extractor Estimating: 150it [01:52,  1.32it/s]Extractor Estimating: 151it [01:53,  1.32it/s]Extractor Estimating: 152it [01:53,  1.34it/s]Extractor Estimating: 153it [01:54,  1.31it/s]Extractor Estimating: 154it [01:55,  1.33it/s]Extractor Estimating: 155it [01:56,  1.33it/s]Extractor Estimating: 156it [01:56,  1.36it/s]Extractor Estimating: 157it [01:57,  1.39it/s]Extractor Estimating: 158it [01:58,  1.42it/s]Extractor Estimating: 159it [01:58,  1.37it/s]Extractor Estimating: 160it [01:59,  1.36it/s]Extractor Estimating: 161it [02:00,  1.34it/s]Extractor Estimating: 162it [02:01,  1.33it/s]Extractor Estimating: 163it [02:01,  1.34it/s]Extractor Estimating: 164it [02:02,  1.36it/s]Extractor Estimating: 165it [02:03,  1.31it/s]Extractor Estimating: 166it [02:04,  1.32it/s]Extractor Estimating: 167it [02:05,  1.31it/s]Extractor Estimating: 168it [02:05,  1.29it/s]Extractor Estimating: 169it [02:06,  1.28it/s]Extractor Estimating: 170it [02:07,  1.28it/s]Extractor Estimating: 171it [02:08,  1.29it/s]Extractor Estimating: 172it [02:08,  1.29it/s]Extractor Estimating: 173it [02:09,  1.28it/s]Extractor Estimating: 174it [02:10,  1.34it/s]Extractor Estimating: 175it [02:11,  1.35it/s]Extractor Estimating: 176it [02:11,  1.33it/s]Extractor Estimating: 177it [02:12,  1.30it/s]Extractor Estimating: 178it [02:13,  1.29it/s]Extractor Estimating: 179it [02:14,  1.30it/s]Extractor Estimating: 180it [02:15,  1.31it/s]Extractor Estimating: 181it [02:15,  1.30it/s]Extractor Estimating: 182it [02:16,  1.32it/s]Extractor Estimating: 183it [02:17,  1.33it/s]Extractor Estimating: 184it [02:18,  1.32it/s]Extractor Estimating: 185it [02:18,  1.36it/s]Extractor Estimating: 186it [02:19,  1.36it/s]Extractor Estimating: 187it [02:20,  1.38it/s]Extractor Estimating: 188it [02:20,  1.36it/s]Extractor Estimating: 189it [02:21,  1.37it/s]Extractor Estimating: 190it [02:22,  1.28it/s]Extractor Estimating: 191it [02:23,  1.29it/s]Extractor Estimating: 192it [02:24,  1.31it/s]Extractor Estimating: 193it [02:24,  1.33it/s]Extractor Estimating: 194it [02:25,  1.28it/s]Extractor Estimating: 195it [02:26,  1.31it/s]Extractor Estimating: 196it [02:27,  1.31it/s]Extractor Estimating: 197it [02:27,  1.33it/s]Extractor Estimating: 198it [02:28,  1.29it/s]Extractor Estimating: 199it [02:29,  1.32it/s]Extractor Estimating: 200it [02:29,  1.39it/s]Extractor Estimating: 201it [02:30,  1.37it/s]Extractor Estimating: 202it [02:31,  1.40it/s]Extractor Estimating: 203it [02:32,  1.42it/s]Extractor Estimating: 204it [02:32,  1.48it/s]Extractor Estimating: 205it [02:33,  1.37it/s]Extractor Estimating: 206it [02:34,  1.36it/s]Extractor Estimating: 207it [02:35,  1.37it/s]Extractor Estimating: 208it [02:35,  1.40it/s]Extractor Estimating: 209it [02:36,  1.40it/s]Extractor Estimating: 210it [02:37,  1.40it/s]Extractor Estimating: 211it [02:37,  1.37it/s]Extractor Estimating: 212it [02:38,  1.36it/s]Extractor Estimating: 213it [02:39,  1.37it/s]Extractor Estimating: 214it [02:40,  1.34it/s]Extractor Estimating: 215it [02:40,  1.37it/s]Extractor Estimating: 216it [02:41,  1.36it/s]Extractor Estimating: 217it [02:42,  1.35it/s]Extractor Estimating: 218it [02:43,  1.38it/s]Extractor Estimating: 219it [02:43,  1.38it/s]Extractor Estimating: 220it [02:44,  1.42it/s]Extractor Estimating: 221it [02:45,  1.42it/s]Extractor Estimating: 222it [02:45,  1.40it/s]Extractor Estimating: 223it [02:46,  1.40it/s]Extractor Estimating: 224it [02:47,  1.42it/s]Extractor Estimating: 225it [02:47,  1.40it/s]Extractor Estimating: 226it [02:48,  1.38it/s]Extractor Estimating: 227it [02:49,  1.36it/s]Extractor Estimating: 228it [02:50,  1.34it/s]Extractor Estimating: 229it [02:50,  1.35it/s]Extractor Estimating: 230it [02:51,  1.33it/s]Extractor Estimating: 231it [02:52,  1.33it/s]Extractor Estimating: 232it [02:53,  1.34it/s]Extractor Estimating: 233it [02:54,  1.33it/s]Extractor Estimating: 234it [02:54,  1.33it/s]Extractor Estimating: 235it [02:55,  1.29it/s]Extractor Estimating: 236it [02:56,  1.29it/s]Extractor Estimating: 237it [02:57,  1.32it/s]Extractor Estimating: 238it [02:57,  1.35it/s]Extractor Estimating: 239it [02:58,  1.33it/s]Extractor Estimating: 240it [02:59,  1.36it/s]Extractor Estimating: 241it [03:00,  1.36it/s]Extractor Estimating: 242it [03:00,  1.33it/s]Extractor Estimating: 243it [03:01,  1.31it/s]Extractor Estimating: 244it [03:02,  1.37it/s]Extractor Estimating: 245it [03:02,  1.38it/s]Extractor Estimating: 246it [03:03,  1.38it/s]Extractor Estimating: 247it [03:04,  1.34it/s]Extractor Estimating: 248it [03:05,  1.39it/s]Extractor Estimating: 249it [03:05,  1.37it/s]Extractor Estimating: 250it [03:06,  1.32it/s]Extractor Estimating: 250it [03:06,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:21,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:21,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:21,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:21,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:21,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:58:22,521 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:58:22,521 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:58:23,095 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:58:24,172 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:58:24,172 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:27,147 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:27,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:27,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:27,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:58:27,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:58:27,791 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:58:27,792 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:58:28,382 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:58:28,563 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:58:28,563 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:48:40,784 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:48:40,836 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4983 mean pseudo reward: 0.953413882199872
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 20856
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20956, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20956, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.268, loss:547.9468
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.230, loss:499.6092
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.238, loss:461.3437
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.243, loss:457.4479
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.243, loss:420.3353
>> valid entity prec:0.4597, rec:0.4810, f1:0.4701
>> valid relation prec:0.1595, rec:0.0218, f1:0.0384
>> valid relation with NER prec:0.1595, rec:0.0218, f1:0.0384
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.072, loss:441.5807
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.231, loss:415.6281
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.249, loss:451.3141
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.240, loss:443.0723
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.233, loss:454.5962
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4728, rec:0.3406, f1:0.3960
>> valid relation prec:0.1041, rec:0.0100, f1:0.0183
>> valid relation with NER prec:0.1041, rec:0.0100, f1:0.0183
g_step 1100, step 60, avg_time 3.068, loss:442.3182
g_step 1200, step 160, avg_time 1.231, loss:454.0498
g_step 1300, step 52, avg_time 1.230, loss:429.8297
g_step 1400, step 152, avg_time 1.248, loss:436.8021
g_step 1500, step 44, avg_time 1.238, loss:422.7404
>> valid entity prec:0.4711, rec:0.3778, f1:0.4193
>> valid relation prec:0.1132, rec:0.0181, f1:0.0312
>> valid relation with NER prec:0.1132, rec:0.0181, f1:0.0312
g_step 1600, step 144, avg_time 3.078, loss:397.6297
g_step 1700, step 36, avg_time 1.249, loss:395.0057
g_step 1800, step 136, avg_time 1.233, loss:388.8488
g_step 1900, step 28, avg_time 1.228, loss:415.1473
g_step 2000, step 128, avg_time 1.229, loss:364.3943
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4501, rec:0.3918, f1:0.4189
>> valid relation prec:0.0782, rec:0.0116, f1:0.0201
>> valid relation with NER prec:0.0782, rec:0.0116, f1:0.0201
g_step 2100, step 20, avg_time 3.081, loss:370.0243
g_step 2200, step 120, avg_time 1.238, loss:340.9329
g_step 2300, step 12, avg_time 1.231, loss:374.3486
g_step 2400, step 112, avg_time 1.233, loss:341.8640
g_step 2500, step 4, avg_time 1.244, loss:347.4366
>> valid entity prec:0.4489, rec:0.4222, f1:0.4352
>> valid relation prec:0.0917, rec:0.0203, f1:0.0332
>> valid relation with NER prec:0.0917, rec:0.0203, f1:0.0332
g_step 2600, step 104, avg_time 3.066, loss:321.0151
g_step 2700, step 204, avg_time 1.243, loss:346.9560
g_step 2800, step 96, avg_time 1.242, loss:304.1394
g_step 2900, step 196, avg_time 1.239, loss:353.0509
g_step 3000, step 88, avg_time 1.226, loss:308.4990
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4653, rec:0.3991, f1:0.4297
>> valid relation prec:0.0920, rec:0.0131, f1:0.0229
>> valid relation with NER prec:0.0920, rec:0.0131, f1:0.0229
g_step 3100, step 188, avg_time 3.062, loss:315.9881
g_step 3200, step 80, avg_time 1.240, loss:287.4913
g_step 3300, step 180, avg_time 1.240, loss:309.1068
g_step 3400, step 72, avg_time 1.236, loss:288.8507
g_step 3500, step 172, avg_time 1.240, loss:297.8201
>> valid entity prec:0.4527, rec:0.3810, f1:0.4138
>> valid relation prec:0.1089, rec:0.0277, f1:0.0441
>> valid relation with NER prec:0.1089, rec:0.0277, f1:0.0441
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 64, avg_time 3.060, loss:286.4373
g_step 3700, step 164, avg_time 1.214, loss:280.0378
g_step 3800, step 56, avg_time 1.245, loss:275.2605
g_step 3900, step 156, avg_time 1.221, loss:283.7869
g_step 4000, step 48, avg_time 1.216, loss:271.3857
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4564, rec:0.4165, f1:0.4355
>> valid relation prec:0.1061, rec:0.0290, f1:0.0455
>> valid relation with NER prec:0.1061, rec:0.0290, f1:0.0455
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 148, avg_time 3.057, loss:264.1613
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:48:40 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:48:40 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-48-40_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:48:41 - WARNING - datasets.builder -   Using custom data configuration default-1d62bcefb845844c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1d62bcefb845844c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  7.47 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:48:42,277 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:48:42,278 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:48:42,278 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:48:42,279 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:48:42,289 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:48:42,294 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:48:42,442 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:48:45,661 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:48:45,666 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1d62bcefb845844c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.92ba/s] 40%|████      | 2/5 [00:00<00:00,  3.78ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.36ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]100%|██████████| 5/5 [00:01<00:00,  4.20ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.09ba/s] 40%|████      | 2/5 [00:00<00:00,  3.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.75ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.00ba/s]100%|██████████| 5/5 [00:01<00:00,  4.76ba/s]100%|██████████| 5/5 [00:01<00:00,  4.25ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.88ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.86ba/s]100%|██████████| 5/5 [00:00<00:00, 10.22ba/s]100%|██████████| 5/5 [00:00<00:00, 10.00ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.86ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.23ba/s]100%|██████████| 5/5 [00:00<00:00, 11.60ba/s]100%|██████████| 5/5 [00:00<00:00, 11.15ba/s]
[INFO|trainer.py:414] 2023-08-28 15:48:49,415 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:48:49,425 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:48:49,425 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 15:48:49,425 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:48:49,426 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:48:49,426 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:48:49,426 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:48:49,426 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<03:46,  1.72it/s]  1%|          | 2/390 [00:00<02:39,  2.43it/s]  1%|          | 3/390 [00:01<02:18,  2.80it/s]  1%|          | 4/390 [00:01<02:08,  3.01it/s]  1%|▏         | 5/390 [00:01<02:02,  3.14it/s]  2%|▏         | 6/390 [00:02<01:58,  3.23it/s]  2%|▏         | 7/390 [00:02<01:56,  3.29it/s]  2%|▏         | 8/390 [00:02<01:55,  3.31it/s]  2%|▏         | 9/390 [00:02<01:53,  3.34it/s]  3%|▎         | 10/390 [00:03<01:52,  3.37it/s]  3%|▎         | 11/390 [00:03<01:52,  3.38it/s]  3%|▎         | 12/390 [00:03<01:51,  3.39it/s]  3%|▎         | 13/390 [00:04<01:50,  3.40it/s]  4%|▎         | 14/390 [00:04<01:50,  3.40it/s]  4%|▍         | 15/390 [00:04<01:50,  3.41it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:05<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:49,  3.41it/s]  5%|▍         | 19/390 [00:05<01:49,  3.40it/s]  5%|▌         | 20/390 [00:06<01:48,  3.40it/s]  5%|▌         | 21/390 [00:06<01:48,  3.41it/s]  6%|▌         | 22/390 [00:06<01:47,  3.41it/s]  6%|▌         | 23/390 [00:07<01:47,  3.40it/s]  6%|▌         | 24/390 [00:07<01:48,  3.38it/s]  6%|▋         | 25/390 [00:07<01:48,  3.36it/s]  7%|▋         | 26/390 [00:07<01:48,  3.35it/s]  7%|▋         | 27/390 [00:08<01:48,  3.34it/s]  7%|▋         | 28/390 [00:08<01:48,  3.34it/s]  7%|▋         | 29/390 [00:08<01:48,  3.34it/s]  8%|▊         | 30/390 [00:09<01:48,  3.33it/s]  8%|▊         | 31/390 [00:09<01:47,  3.33it/s]  8%|▊         | 32/390 [00:09<01:47,  3.33it/s]  8%|▊         | 33/390 [00:10<01:47,  3.33it/s]  9%|▊         | 34/390 [00:10<01:46,  3.33it/s]  9%|▉         | 35/390 [00:10<01:46,  3.33it/s]  9%|▉         | 36/390 [00:10<01:46,  3.33it/s]  9%|▉         | 37/390 [00:11<01:45,  3.33it/s] 10%|▉         | 38/390 [00:11<01:45,  3.33it/s] 10%|█         | 39/390 [00:11<01:45,  3.33it/s] 10%|█         | 40/390 [00:12<01:45,  3.31it/s] 11%|█         | 41/390 [00:12<01:45,  3.32it/s] 11%|█         | 42/390 [00:12<01:44,  3.32it/s] 11%|█         | 43/390 [00:13<01:44,  3.33it/s] 11%|█▏        | 44/390 [00:13<01:44,  3.33it/s] 12%|█▏        | 45/390 [00:13<01:43,  3.33it/s] 12%|█▏        | 46/390 [00:13<01:43,  3.33it/s] 12%|█▏        | 47/390 [00:14<01:43,  3.33it/s] 12%|█▏        | 48/390 [00:14<01:43,  3.32it/s] 13%|█▎        | 49/390 [00:14<01:42,  3.32it/s] 13%|█▎        | 50/390 [00:15<01:42,  3.30it/s] 13%|█▎        | 51/390 [00:15<01:42,  3.31it/s] 13%|█▎        | 52/390 [00:15<01:41,  3.32it/s] 14%|█▎        | 53/390 [00:16<01:41,  3.32it/s] 14%|█▍        | 54/390 [00:16<01:41,  3.33it/s] 14%|█▍        | 55/390 [00:16<01:40,  3.33it/s] 14%|█▍        | 56/390 [00:16<01:40,  3.33it/s] 15%|█▍        | 57/390 [00:17<01:40,  3.33it/s] 15%|█▍        | 58/390 [00:17<01:39,  3.33it/s] 15%|█▌        | 59/390 [00:17<01:39,  3.33it/s] 15%|█▌        | 60/390 [00:18<01:39,  3.32it/s] 16%|█▌        | 61/390 [00:18<01:39,  3.32it/s] 16%|█▌        | 62/390 [00:18<01:38,  3.32it/s] 16%|█▌        | 63/390 [00:19<01:38,  3.32it/s] 16%|█▋        | 64/390 [00:19<01:38,  3.32it/s] 17%|█▋        | 65/390 [00:19<01:37,  3.32it/s] 17%|█▋        | 66/390 [00:19<01:37,  3.32it/s] 17%|█▋        | 67/390 [00:20<01:37,  3.32it/s] 17%|█▋        | 68/390 [00:20<01:36,  3.32it/s] 18%|█▊        | 69/390 [00:20<01:36,  3.32it/s] 18%|█▊        | 70/390 [00:21<01:36,  3.31it/s] 18%|█▊        | 71/390 [00:21<01:36,  3.32it/s] 18%|█▊        | 72/390 [00:21<01:35,  3.32it/s] 19%|█▊        | 73/390 [00:22<01:35,  3.32it/s] 19%|█▉        | 74/390 [00:22<01:35,  3.33it/s] 19%|█▉        | 75/390 [00:22<01:34,  3.33it/s] 19%|█▉        | 76/390 [00:22<01:34,  3.32it/s] 20%|█▉        | 77/390 [00:23<01:34,  3.32it/s] 20%|██        | 78/390 [00:23<01:33,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 15:49:13,082 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:49:13,082 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:49:13,082 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.84it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.72it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.85it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.99it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.20it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.73it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.67it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.08it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.99it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.26it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.39it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.47it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.36it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.14it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.11it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.97it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.77it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.86it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.05it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.32it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.36it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.30it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.15it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.87it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.84it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.91it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.07it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.29it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.35it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.31it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.25it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.98it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.93it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.94it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.83it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.11it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.20it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.45it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.35it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.23it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.03it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.93it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.95it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.00it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.09it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.17it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.33it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.34it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.15it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.04it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.90it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.97it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.96it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.15it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.25it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.23it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.19it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.02it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.02it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.85it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.12it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.28it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.18it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.11it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.04it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.84it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.01it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.14it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.22it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.33it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.92it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.17it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.07it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.91it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.01it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.15it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.12it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.23it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.20it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.24it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.14it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.08it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 43.95it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.15it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.09it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.15it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.14it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.11it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.06it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.96it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.11it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.08it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.90it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.08it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.23it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.21it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.20it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.04it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.05it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.17it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.05it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.12it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.22it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.19it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.10it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.08it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.07it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.18it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:13<00:00, 44.18it/s][A 20%|██        | 78/390 [00:36<01:33,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:49:26,176 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 15:49:26,192 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:49:28,461 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:49:28,476 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:49:28,492 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:44<33:24,  6.44s/it] 21%|██        | 80/390 [00:44<23:46,  4.60s/it] 21%|██        | 81/390 [00:44<17:03,  3.31s/it] 21%|██        | 82/390 [00:45<12:21,  2.41s/it] 21%|██▏       | 83/390 [00:45<09:05,  1.78s/it] 22%|██▏       | 84/390 [00:45<06:48,  1.33s/it] 22%|██▏       | 85/390 [00:46<05:12,  1.02s/it] 22%|██▏       | 86/390 [00:46<04:05,  1.24it/s] 22%|██▏       | 87/390 [00:46<03:18,  1.53it/s] 23%|██▎       | 88/390 [00:47<02:45,  1.82it/s] 23%|██▎       | 89/390 [00:47<02:22,  2.11it/s] 23%|██▎       | 90/390 [00:47<02:06,  2.36it/s] 23%|██▎       | 91/390 [00:47<01:55,  2.59it/s] 24%|██▎       | 92/390 [00:48<01:47,  2.77it/s] 24%|██▍       | 93/390 [00:48<01:41,  2.92it/s] 24%|██▍       | 94/390 [00:48<01:37,  3.03it/s] 24%|██▍       | 95/390 [00:49<01:34,  3.12it/s] 25%|██▍       | 96/390 [00:49<01:32,  3.18it/s] 25%|██▍       | 97/390 [00:49<01:30,  3.22it/s] 25%|██▌       | 98/390 [00:50<01:29,  3.25it/s] 25%|██▌       | 99/390 [00:50<01:29,  3.26it/s] 26%|██▌       | 100/390 [00:50<01:28,  3.27it/s] 26%|██▌       | 101/390 [00:50<01:28,  3.28it/s] 26%|██▌       | 102/390 [00:51<01:27,  3.30it/s] 26%|██▋       | 103/390 [00:51<01:28,  3.23it/s] 27%|██▋       | 104/390 [00:51<01:27,  3.26it/s] 27%|██▋       | 105/390 [00:52<01:26,  3.29it/s] 27%|██▋       | 106/390 [00:52<01:26,  3.30it/s] 27%|██▋       | 107/390 [00:52<01:25,  3.31it/s] 28%|██▊       | 108/390 [00:53<01:24,  3.32it/s] 28%|██▊       | 109/390 [00:53<01:24,  3.32it/s] 28%|██▊       | 110/390 [00:53<01:24,  3.32it/s] 28%|██▊       | 111/390 [00:54<01:23,  3.33it/s] 29%|██▊       | 112/390 [00:54<01:23,  3.33it/s] 29%|██▉       | 113/390 [00:54<01:23,  3.33it/s] 29%|██▉       | 114/390 [00:54<01:22,  3.33it/s] 29%|██▉       | 115/390 [00:55<01:22,  3.33it/s] 30%|██▉       | 116/390 [00:55<01:22,  3.33it/s] 30%|███       | 117/390 [00:55<01:21,  3.33it/s] 30%|███       | 118/390 [00:56<01:22,  3.31it/s] 31%|███       | 119/390 [00:56<01:21,  3.31it/s] 31%|███       | 120/390 [00:56<01:21,  3.32it/s] 31%|███       | 121/390 [00:57<01:21,  3.32it/s] 31%|███▏      | 122/390 [00:57<01:20,  3.32it/s] 32%|███▏      | 123/390 [00:57<01:20,  3.32it/s] 32%|███▏      | 124/390 [00:57<01:19,  3.33it/s] 32%|███▏      | 125/390 [00:58<01:19,  3.35it/s] 32%|███▏      | 126/390 [00:58<01:18,  3.37it/s] 33%|███▎      | 127/390 [00:58<01:17,  3.38it/s] 33%|███▎      | 128/390 [00:59<01:17,  3.39it/s] 33%|███▎      | 129/390 [00:59<01:17,  3.38it/s] 33%|███▎      | 130/390 [00:59<01:16,  3.39it/s] 34%|███▎      | 131/390 [00:59<01:16,  3.40it/s] 34%|███▍      | 132/390 [01:00<01:15,  3.40it/s] 34%|███▍      | 133/390 [01:00<01:15,  3.40it/s] 34%|███▍      | 134/390 [01:00<01:15,  3.40it/s] 35%|███▍      | 135/390 [01:01<01:14,  3.41it/s] 35%|███▍      | 136/390 [01:01<01:14,  3.41it/s] 35%|███▌      | 137/390 [01:01<01:14,  3.41it/s] 35%|███▌      | 138/390 [01:02<01:13,  3.41it/s] 36%|███▌      | 139/390 [01:02<01:13,  3.41it/s] 36%|███▌      | 140/390 [01:02<01:13,  3.40it/s] 36%|███▌      | 141/390 [01:02<01:13,  3.40it/s] 36%|███▋      | 142/390 [01:03<01:12,  3.40it/s] 37%|███▋      | 143/390 [01:03<01:12,  3.40it/s] 37%|███▋      | 144/390 [01:03<01:12,  3.41it/s] 37%|███▋      | 145/390 [01:04<01:11,  3.41it/s] 37%|███▋      | 146/390 [01:04<01:11,  3.41it/s] 38%|███▊      | 147/390 [01:04<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:04<01:10,  3.41it/s] 38%|███▊      | 149/390 [01:05<01:10,  3.41it/s] 38%|███▊      | 150/390 [01:05<01:10,  3.41it/s] 39%|███▊      | 151/390 [01:05<01:10,  3.40it/s] 39%|███▉      | 152/390 [01:06<01:09,  3.40it/s] 39%|███▉      | 153/390 [01:06<01:09,  3.40it/s] 39%|███▉      | 154/390 [01:06<01:09,  3.41it/s] 40%|███▉      | 155/390 [01:07<01:08,  3.41it/s] 40%|████      | 156/390 [01:07<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 15:49:56,789 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:49:56,789 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:49:56,789 >>   Batch size = 8
{'eval_loss': 0.9890071749687195, 'eval_runtime': 13.0804, 'eval_samples_per_second': 352.361, 'eval_steps_per_second': 44.112, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.21it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.70it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.93it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.32it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.70it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.55it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.37it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.26it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.29it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.32it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.34it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.24it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.07it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.02it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.00it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.00it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.00it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.10it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.15it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.26it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.10it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.02it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.00it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.00it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.09it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.10it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.24it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.18it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.08it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.00it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.08it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.02it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.04it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.13it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.16it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.17it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.97it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.07it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.98it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.02it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.09it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.15it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.20it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.21it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.08it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.03it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.96it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.13it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.20it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.13it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.11it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.14it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.02it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.05it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.91it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.11it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.05it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.13it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.14it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.00it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.91it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.08it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.14it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.18it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.16it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.17it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.23it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.08it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.00it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.92it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.85it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.08it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.17it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.14it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.13it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.17it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.14it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.06it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.81it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.01it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.18it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.22it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.14it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.00it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.16it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.02it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.97it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.94it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.03it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.13it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.24it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.21it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.17it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.11it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.00it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.91it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.91it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.11it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.22it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.13it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.19it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.07it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.03it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.97it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.90it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.98it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.11it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.18it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.24it/s][A 40%|████      | 156/390 [01:20<01:08,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:50:09,898 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 15:50:09,925 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:50:12,482 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:50:12,509 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:50:12,526 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:27<24:53,  6.41s/it] 41%|████      | 158/390 [01:28<17:42,  4.58s/it] 41%|████      | 159/390 [01:28<12:41,  3.30s/it] 41%|████      | 160/390 [01:28<09:11,  2.40s/it] 41%|████▏     | 161/390 [01:29<06:44,  1.77s/it] 42%|████▏     | 162/390 [01:29<05:02,  1.33s/it] 42%|████▏     | 163/390 [01:29<03:51,  1.02s/it] 42%|████▏     | 164/390 [01:30<03:01,  1.24it/s] 42%|████▏     | 165/390 [01:30<02:26,  1.53it/s] 43%|████▎     | 166/390 [01:30<02:02,  1.83it/s] 43%|████▎     | 167/390 [01:31<01:45,  2.11it/s] 43%|████▎     | 168/390 [01:31<01:33,  2.36it/s] 43%|████▎     | 169/390 [01:31<01:25,  2.59it/s] 44%|████▎     | 170/390 [01:31<01:19,  2.77it/s] 44%|████▍     | 171/390 [01:32<01:15,  2.91it/s] 44%|████▍     | 172/390 [01:32<01:11,  3.04it/s] 44%|████▍     | 173/390 [01:32<01:09,  3.14it/s] 45%|████▍     | 174/390 [01:33<01:07,  3.22it/s] 45%|████▍     | 175/390 [01:33<01:05,  3.27it/s] 45%|████▌     | 176/390 [01:33<01:04,  3.31it/s] 45%|████▌     | 177/390 [01:33<01:03,  3.34it/s] 46%|████▌     | 178/390 [01:34<01:03,  3.36it/s] 46%|████▌     | 179/390 [01:34<01:02,  3.37it/s] 46%|████▌     | 180/390 [01:34<01:02,  3.38it/s] 46%|████▋     | 181/390 [01:35<01:01,  3.39it/s] 47%|████▋     | 182/390 [01:35<01:01,  3.39it/s] 47%|████▋     | 183/390 [01:35<01:00,  3.39it/s] 47%|████▋     | 184/390 [01:36<01:00,  3.40it/s] 47%|████▋     | 185/390 [01:36<01:00,  3.40it/s] 48%|████▊     | 186/390 [01:36<00:59,  3.40it/s] 48%|████▊     | 187/390 [01:36<00:59,  3.41it/s] 48%|████▊     | 188/390 [01:37<00:59,  3.41it/s] 48%|████▊     | 189/390 [01:37<00:59,  3.41it/s] 49%|████▊     | 190/390 [01:37<00:59,  3.39it/s] 49%|████▉     | 191/390 [01:38<00:58,  3.40it/s] 49%|████▉     | 192/390 [01:38<00:58,  3.40it/s] 49%|████▉     | 193/390 [01:38<00:57,  3.40it/s] 50%|████▉     | 194/390 [01:38<00:57,  3.40it/s] 50%|█████     | 195/390 [01:39<00:57,  3.40it/s] 50%|█████     | 196/390 [01:39<00:57,  3.40it/s] 51%|█████     | 197/390 [01:39<00:56,  3.41it/s] 51%|█████     | 198/390 [01:40<00:56,  3.41it/s] 51%|█████     | 199/390 [01:40<00:56,  3.41it/s] 51%|█████▏    | 200/390 [01:40<00:55,  3.41it/s] 52%|█████▏    | 201/390 [01:41<00:55,  3.39it/s] 52%|█████▏    | 202/390 [01:41<00:55,  3.40it/s] 52%|█████▏    | 203/390 [01:41<00:55,  3.40it/s] 52%|█████▏    | 204/390 [01:41<00:54,  3.40it/s] 53%|█████▎    | 205/390 [01:42<00:54,  3.40it/s] 53%|█████▎    | 206/390 [01:42<00:54,  3.40it/s] 53%|█████▎    | 207/390 [01:42<00:53,  3.41it/s] 53%|█████▎    | 208/390 [01:43<00:53,  3.41it/s] 54%|█████▎    | 209/390 [01:43<00:53,  3.41it/s] 54%|█████▍    | 210/390 [01:43<00:52,  3.41it/s] 54%|█████▍    | 211/390 [01:43<00:52,  3.41it/s] 54%|█████▍    | 212/390 [01:44<00:52,  3.40it/s] 55%|█████▍    | 213/390 [01:44<00:52,  3.40it/s] 55%|█████▍    | 214/390 [01:44<00:51,  3.40it/s] 55%|█████▌    | 215/390 [01:45<00:51,  3.40it/s] 55%|█████▌    | 216/390 [01:45<00:51,  3.40it/s] 56%|█████▌    | 217/390 [01:45<00:50,  3.41it/s] 56%|█████▌    | 218/390 [01:46<00:50,  3.41it/s] 56%|█████▌    | 219/390 [01:46<00:50,  3.41it/s] 56%|█████▋    | 220/390 [01:46<00:49,  3.41it/s] 57%|█████▋    | 221/390 [01:46<00:49,  3.41it/s] 57%|█████▋    | 222/390 [01:47<00:49,  3.41it/s] 57%|█████▋    | 223/390 [01:47<00:49,  3.40it/s] 57%|█████▋    | 224/390 [01:47<00:48,  3.40it/s] 58%|█████▊    | 225/390 [01:48<00:48,  3.40it/s] 58%|█████▊    | 226/390 [01:48<00:48,  3.40it/s] 58%|█████▊    | 227/390 [01:48<00:47,  3.40it/s] 58%|█████▊    | 228/390 [01:48<00:47,  3.40it/s] 59%|█████▊    | 229/390 [01:49<00:47,  3.41it/s] 59%|█████▉    | 230/390 [01:49<00:46,  3.41it/s] 59%|█████▉    | 231/390 [01:49<00:46,  3.41it/s] 59%|█████▉    | 232/390 [01:50<00:46,  3.41it/s] 60%|█████▉    | 233/390 [01:50<00:46,  3.36it/s] 60%|██████    | 234/390 [01:50<00:46,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 15:50:40,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:50:40,228 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:50:40,228 >>   Batch size = 8
{'eval_loss': 1.0068217515945435, 'eval_runtime': 13.079, 'eval_samples_per_second': 352.397, 'eval_steps_per_second': 44.116, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.80it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.04it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.33it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.46it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.84it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.34it/s][A
  6%|▋         | 37/577 [00:00<00:13, 40.71it/s][A
  7%|▋         | 42/577 [00:00<00:12, 42.06it/s][A
  8%|▊         | 47/577 [00:01<00:12, 42.88it/s][A
  9%|▉         | 52/577 [00:01<00:12, 43.31it/s][A
 10%|▉         | 57/577 [00:01<00:11, 43.65it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.01it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.05it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.91it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.65it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.57it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.69it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.88it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.20it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.22it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.24it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.27it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.08it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.83it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.78it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.81it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.94it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.07it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.19it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.33it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.35it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.12it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.65it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.80it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.84it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.04it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.18it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.25it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.25it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.80it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.74it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.97it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.13it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.11it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.31it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.17it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.01it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.80it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.84it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.08it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.24it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.26it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.23it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.13it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.89it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.89it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.95it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.08it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.28it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.89it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.92it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.92it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.00it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.07it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.28it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.27it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.27it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.19it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.95it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.03it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.97it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.03it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.17it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.18it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.20it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.11it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.06it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.04it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.93it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.89it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.06it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.07it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.21it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.17it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.19it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.99it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.99it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.05it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.12it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.14it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.94it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.04it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.07it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.03it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.08it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.90it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.15it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.09it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.07it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.06it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.03it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.07it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.14it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.20it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.20it/s][A 60%|██████    | 234/390 [02:03<00:46,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:50:53,354 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:50:53,377 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:50:55,726 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:50:55,742 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:50:55,754 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:11<16:14,  6.29s/it] 61%|██████    | 236/390 [02:11<11:31,  4.49s/it] 61%|██████    | 237/390 [02:11<08:15,  3.24s/it] 61%|██████    | 238/390 [02:11<05:57,  2.36s/it] 61%|██████▏   | 239/390 [02:12<04:22,  1.74s/it] 62%|██████▏   | 240/390 [02:12<03:16,  1.31s/it] 62%|██████▏   | 241/390 [02:12<02:29,  1.01s/it] 62%|██████▏   | 242/390 [02:13<01:57,  1.26it/s] 62%|██████▏   | 243/390 [02:13<01:34,  1.55it/s] 63%|██████▎   | 244/390 [02:13<01:19,  1.85it/s] 63%|██████▎   | 245/390 [02:14<01:08,  2.13it/s] 63%|██████▎   | 246/390 [02:14<01:00,  2.38it/s] 63%|██████▎   | 247/390 [02:14<00:54,  2.61it/s] 64%|██████▎   | 248/390 [02:14<00:50,  2.79it/s] 64%|██████▍   | 249/390 [02:15<00:48,  2.93it/s] 64%|██████▍   | 250/390 [02:15<00:46,  3.04it/s] 64%|██████▍   | 251/390 [02:15<00:44,  3.12it/s] 65%|██████▍   | 252/390 [02:16<00:43,  3.18it/s] 65%|██████▍   | 253/390 [02:16<00:42,  3.23it/s] 65%|██████▌   | 254/390 [02:16<00:41,  3.26it/s] 65%|██████▌   | 255/390 [02:17<00:41,  3.28it/s] 66%|██████▌   | 256/390 [02:17<00:40,  3.28it/s] 66%|██████▌   | 257/390 [02:17<00:40,  3.30it/s] 66%|██████▌   | 258/390 [02:17<00:39,  3.31it/s] 66%|██████▋   | 259/390 [02:18<00:39,  3.32it/s] 67%|██████▋   | 260/390 [02:18<00:39,  3.32it/s] 67%|██████▋   | 261/390 [02:18<00:38,  3.33it/s] 67%|██████▋   | 262/390 [02:19<00:38,  3.33it/s] 67%|██████▋   | 263/390 [02:19<00:38,  3.33it/s] 68%|██████▊   | 264/390 [02:19<00:37,  3.33it/s] 68%|██████▊   | 265/390 [02:20<00:37,  3.33it/s] 68%|██████▊   | 266/390 [02:20<00:37,  3.28it/s] 68%|██████▊   | 267/390 [02:20<00:37,  3.29it/s] 69%|██████▊   | 268/390 [02:20<00:36,  3.31it/s] 69%|██████▉   | 269/390 [02:21<00:36,  3.31it/s] 69%|██████▉   | 270/390 [02:21<00:36,  3.32it/s] 69%|██████▉   | 271/390 [02:21<00:35,  3.33it/s] 70%|██████▉   | 272/390 [02:22<00:35,  3.33it/s] 70%|███████   | 273/390 [02:22<00:35,  3.33it/s] 70%|███████   | 274/390 [02:22<00:34,  3.33it/s] 71%|███████   | 275/390 [02:23<00:34,  3.33it/s] 71%|███████   | 276/390 [02:23<00:34,  3.31it/s] 71%|███████   | 277/390 [02:23<00:34,  3.32it/s] 71%|███████▏  | 278/390 [02:23<00:33,  3.33it/s] 72%|███████▏  | 279/390 [02:24<00:33,  3.36it/s] 72%|███████▏  | 280/390 [02:24<00:32,  3.37it/s] 72%|███████▏  | 281/390 [02:24<00:32,  3.38it/s] 72%|███████▏  | 282/390 [02:25<00:31,  3.39it/s] 73%|███████▎  | 283/390 [02:25<00:31,  3.40it/s] 73%|███████▎  | 284/390 [02:25<00:31,  3.40it/s] 73%|███████▎  | 285/390 [02:25<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:26<00:30,  3.41it/s] 74%|███████▎  | 287/390 [02:26<00:30,  3.38it/s] 74%|███████▍  | 288/390 [02:26<00:30,  3.38it/s] 74%|███████▍  | 289/390 [02:27<00:29,  3.39it/s] 74%|███████▍  | 290/390 [02:27<00:29,  3.40it/s] 75%|███████▍  | 291/390 [02:27<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:28<00:28,  3.41it/s] 75%|███████▌  | 293/390 [02:28<00:28,  3.41it/s] 75%|███████▌  | 294/390 [02:28<00:28,  3.41it/s] 76%|███████▌  | 295/390 [02:28<00:27,  3.41it/s] 76%|███████▌  | 296/390 [02:29<00:27,  3.41it/s] 76%|███████▌  | 297/390 [02:29<00:27,  3.41it/s] 76%|███████▋  | 298/390 [02:29<00:27,  3.40it/s] 77%|███████▋  | 299/390 [02:30<00:26,  3.40it/s] 77%|███████▋  | 300/390 [02:30<00:26,  3.40it/s] 77%|███████▋  | 301/390 [02:30<00:26,  3.40it/s] 77%|███████▋  | 302/390 [02:30<00:25,  3.40it/s] 78%|███████▊  | 303/390 [02:31<00:25,  3.40it/s] 78%|███████▊  | 304/390 [02:31<00:25,  3.40it/s] 78%|███████▊  | 305/390 [02:31<00:24,  3.41it/s] 78%|███████▊  | 306/390 [02:32<00:24,  3.41it/s] 79%|███████▊  | 307/390 [02:32<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:32<00:24,  3.41it/s] 79%|███████▉  | 309/390 [02:33<00:23,  3.40it/s] 79%|███████▉  | 310/390 [02:33<00:23,  3.40it/s] 80%|███████▉  | 311/390 [02:33<00:23,  3.41it/s] 80%|████████  | 312/390 [02:33<00:22,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 15:51:23,406 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:51:23,406 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:51:23,406 >>   Batch size = 8
{'eval_loss': 1.0209136009216309, 'eval_runtime': 13.1103, 'eval_samples_per_second': 351.556, 'eval_steps_per_second': 44.011, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.06it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.74it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.04it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.15it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.64it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.46it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.52it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.15it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.26it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.42it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.27it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.20it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.05it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.88it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.01it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.10it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.10it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.29it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.23it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 43.97it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.03it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.01it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.92it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.12it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.02it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.18it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.26it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.18it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.13it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.09it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.06it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.89it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.09it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.19it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.23it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.26it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.81it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.91it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.92it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.90it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.11it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.14it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.15it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.11it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.11it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.96it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.02it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.06it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.19it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.08it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.25it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.17it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.01it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.97it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.92it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.04it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.02it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.14it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.21it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.04it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.99it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.98it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.97it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.99it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.05it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.18it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.09it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.06it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.99it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.09it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.02it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.06it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.06it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.13it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.01it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.96it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.00it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.92it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.98it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.08it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.24it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.11it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.13it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.15it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.02it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.07it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.06it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.15it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.11it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.15it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.10it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.12it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.97it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.14it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.14it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.14it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.18it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.12it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.14it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.01it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.02it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A 80%|████████  | 312/390 [02:47<00:22,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:51:36,524 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 15:51:36,554 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:51:39,439 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:51:39,467 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:51:39,488 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:56<08:49,  6.88s/it] 81%|████████  | 314/390 [02:56<06:12,  4.91s/it] 81%|████████  | 315/390 [02:56<04:24,  3.53s/it] 81%|████████  | 316/390 [02:57<03:09,  2.56s/it] 81%|████████▏ | 317/390 [02:57<02:17,  1.88s/it] 82%|████████▏ | 318/390 [02:57<01:41,  1.41s/it] 82%|████████▏ | 319/390 [02:57<01:16,  1.07s/it] 82%|████████▏ | 320/390 [02:58<00:58,  1.19it/s] 82%|████████▏ | 321/390 [02:58<00:46,  1.47it/s] 83%|████████▎ | 322/390 [02:58<00:38,  1.77it/s] 83%|████████▎ | 323/390 [02:59<00:32,  2.06it/s] 83%|████████▎ | 324/390 [02:59<00:28,  2.32it/s] 83%|████████▎ | 325/390 [02:59<00:25,  2.55it/s] 84%|████████▎ | 326/390 [03:00<00:23,  2.74it/s] 84%|████████▍ | 327/390 [03:00<00:21,  2.89it/s] 84%|████████▍ | 328/390 [03:00<00:20,  3.01it/s] 84%|████████▍ | 329/390 [03:01<00:19,  3.10it/s] 85%|████████▍ | 330/390 [03:01<00:18,  3.16it/s] 85%|████████▍ | 331/390 [03:01<00:18,  3.21it/s] 85%|████████▌ | 332/390 [03:01<00:17,  3.24it/s] 85%|████████▌ | 333/390 [03:02<00:17,  3.26it/s] 86%|████████▌ | 334/390 [03:02<00:17,  3.28it/s] 86%|████████▌ | 335/390 [03:02<00:16,  3.29it/s] 86%|████████▌ | 336/390 [03:03<00:16,  3.30it/s] 86%|████████▋ | 337/390 [03:03<00:16,  3.31it/s] 87%|████████▋ | 338/390 [03:03<00:15,  3.32it/s] 87%|████████▋ | 339/390 [03:04<00:15,  3.32it/s] 87%|████████▋ | 340/390 [03:04<00:15,  3.33it/s] 87%|████████▋ | 341/390 [03:04<00:14,  3.33it/s] 88%|████████▊ | 342/390 [03:04<00:14,  3.33it/s] 88%|████████▊ | 343/390 [03:05<00:14,  3.33it/s] 88%|████████▊ | 344/390 [03:05<00:13,  3.32it/s] 88%|████████▊ | 345/390 [03:05<00:13,  3.32it/s] 89%|████████▊ | 346/390 [03:06<00:13,  3.32it/s] 89%|████████▉ | 347/390 [03:06<00:12,  3.32it/s] 89%|████████▉ | 348/390 [03:06<00:12,  3.32it/s] 89%|████████▉ | 349/390 [03:07<00:12,  3.32it/s] 90%|████████▉ | 350/390 [03:07<00:12,  3.32it/s] 90%|█████████ | 351/390 [03:07<00:11,  3.32it/s] 90%|█████████ | 352/390 [03:07<00:11,  3.32it/s] 91%|█████████ | 353/390 [03:08<00:11,  3.32it/s] 91%|█████████ | 354/390 [03:08<00:10,  3.32it/s] 91%|█████████ | 355/390 [03:08<00:10,  3.32it/s] 91%|█████████▏| 356/390 [03:09<00:10,  3.32it/s] 92%|█████████▏| 357/390 [03:09<00:09,  3.31it/s] 92%|█████████▏| 358/390 [03:09<00:09,  3.31it/s] 92%|█████████▏| 359/390 [03:10<00:09,  3.32it/s] 92%|█████████▏| 360/390 [03:10<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:10<00:08,  3.32it/s] 93%|█████████▎| 362/390 [03:10<00:08,  3.34it/s] 93%|█████████▎| 363/390 [03:11<00:08,  3.36it/s] 93%|█████████▎| 364/390 [03:11<00:07,  3.37it/s] 94%|█████████▎| 365/390 [03:11<00:07,  3.38it/s] 94%|█████████▍| 366/390 [03:12<00:07,  3.39it/s] 94%|█████████▍| 367/390 [03:12<00:06,  3.40it/s] 94%|█████████▍| 368/390 [03:12<00:06,  3.39it/s] 95%|█████████▍| 369/390 [03:12<00:06,  3.39it/s] 95%|█████████▍| 370/390 [03:13<00:05,  3.40it/s] 95%|█████████▌| 371/390 [03:13<00:05,  3.40it/s] 95%|█████████▌| 372/390 [03:13<00:05,  3.41it/s] 96%|█████████▌| 373/390 [03:14<00:04,  3.41it/s] 96%|█████████▌| 374/390 [03:14<00:04,  3.41it/s] 96%|█████████▌| 375/390 [03:14<00:04,  3.41it/s] 96%|█████████▋| 376/390 [03:15<00:04,  3.41it/s] 97%|█████████▋| 377/390 [03:15<00:03,  3.41it/s] 97%|█████████▋| 378/390 [03:15<00:03,  3.41it/s] 97%|█████████▋| 379/390 [03:15<00:03,  3.39it/s] 97%|█████████▋| 380/390 [03:16<00:02,  3.39it/s] 98%|█████████▊| 381/390 [03:16<00:02,  3.40it/s] 98%|█████████▊| 382/390 [03:16<00:02,  3.40it/s] 98%|█████████▊| 383/390 [03:17<00:02,  3.40it/s] 98%|█████████▊| 384/390 [03:17<00:01,  3.40it/s] 99%|█████████▊| 385/390 [03:17<00:01,  3.40it/s] 99%|█████████▉| 386/390 [03:17<00:01,  3.40it/s] 99%|█████████▉| 387/390 [03:18<00:00,  3.40it/s] 99%|█████████▉| 388/390 [03:18<00:00,  3.40it/s]100%|█████████▉| 389/390 [03:18<00:00,  3.40it/s]100%|██████████| 390/390 [03:19<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 15:52:08,591 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:52:08,591 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:52:08,591 >>   Batch size = 8
{'eval_loss': 1.031615972518921, 'eval_runtime': 13.0827, 'eval_samples_per_second': 352.296, 'eval_steps_per_second': 44.104, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 54.99it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.52it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.91it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.20it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.79it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.50it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.30it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.26it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.30it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.28it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.22it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.12it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.92it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.07it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.09it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.03it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.18it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.22it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.16it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.16it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.08it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.95it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.05it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.11it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.21it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.25it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.15it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.05it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.96it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.04it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.02it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.16it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.19it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.96it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.01it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.04it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.17it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.08it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.12it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.20it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.07it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.08it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.07it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.04it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.16it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.22it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.13it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.08it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.13it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.98it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.00it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.03it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.05it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.17it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.09it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.12it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.08it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.97it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.00it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.09it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.16it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.10it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.02it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.03it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.07it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.05it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.95it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.09it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.12it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.16it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.10it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.10it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.09it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.00it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.07it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.10it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.18it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.21it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.06it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.03it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.04it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.94it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.07it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.13it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.19it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.17it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.08it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.11it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.12it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.07it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.92it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.07it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.08it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.10it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.10it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.02it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.00it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.18it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.21it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.14it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.98it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.09it/s][A100%|██████████| 390/390 [03:32<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:52:21,684 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 15:52:21,702 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:52:23,752 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:52:23,773 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:52:23,795 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:52:27,676 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:52:27,676 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78 (score: 0.9890071749687195).
                                                 100%|██████████| 390/390 [03:40<00:00,  3.39it/s]100%|██████████| 390/390 [03:40<00:00,  1.77it/s]
[INFO|trainer.py:1894] 2023-08-28 15:52:29,528 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 15:52:29,543 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:52:31,532 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:52:31,553 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:52:31,562 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:52:31,762 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   train_loss               =     0.5798
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   train_runtime            = 0:03:40.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   train_samples_per_second =    113.586
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:31,762 >>   train_steps_per_second   =      1.772
{'eval_loss': 1.03514564037323, 'eval_runtime': 13.0798, 'eval_samples_per_second': 352.376, 'eval_steps_per_second': 44.114, 'epoch': 4.99}
{'train_runtime': 220.0968, 'train_samples_per_second': 113.586, 'train_steps_per_second': 1.772, 'train_loss': 0.5798254551031651, 'epoch': 4.99}
08/28/2023 15:52:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:52:31,804 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:52:31,804 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 15:52:31,804 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.52it/s]  2%|▏         | 12/577 [00:00<00:11, 48.11it/s]  3%|▎         | 17/577 [00:00<00:12, 46.53it/s]  4%|▍         | 22/577 [00:00<00:12, 45.90it/s]  5%|▍         | 27/577 [00:00<00:12, 45.44it/s]  6%|▌         | 32/577 [00:00<00:12, 45.17it/s]  6%|▋         | 37/577 [00:00<00:11, 45.03it/s]  7%|▋         | 42/577 [00:00<00:11, 44.65it/s]  8%|▊         | 47/577 [00:01<00:12, 44.00it/s]  9%|▉         | 52/577 [00:01<00:11, 43.84it/s] 10%|▉         | 57/577 [00:01<00:11, 43.81it/s] 11%|█         | 62/577 [00:01<00:11, 44.03it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.31it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.38it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.49it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.42it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.09it/s] 16%|█▌        | 92/577 [00:02<00:11, 43.89it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.69it/s] 18%|█▊        | 102/577 [00:02<00:10, 43.77it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.01it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.13it/s] 20%|██        | 117/577 [00:02<00:10, 44.33it/s] 21%|██        | 122/577 [00:02<00:10, 44.34it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.33it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.09it/s] 24%|██▎       | 137/577 [00:03<00:10, 43.85it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.73it/s] 25%|██▌       | 147/577 [00:03<00:09, 43.77it/s] 26%|██▋       | 152/577 [00:03<00:09, 43.89it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.11it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.24it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.30it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.26it/s] 31%|███       | 177/577 [00:03<00:09, 44.11it/s] 32%|███▏      | 182/577 [00:04<00:09, 43.87it/s] 32%|███▏      | 187/577 [00:04<00:08, 43.68it/s] 33%|███▎      | 192/577 [00:04<00:08, 43.76it/s] 34%|███▍      | 197/577 [00:04<00:08, 43.88it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.07it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.21it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.20it/s] 38%|███▊      | 217/577 [00:04<00:08, 44.20it/s] 38%|███▊      | 222/577 [00:05<00:08, 43.97it/s] 39%|███▉      | 227/577 [00:05<00:07, 43.80it/s] 40%|████      | 232/577 [00:05<00:07, 43.78it/s] 41%|████      | 237/577 [00:05<00:07, 43.89it/s] 42%|████▏     | 242/577 [00:05<00:07, 43.90it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.09it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.11it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.23it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.19it/s] 46%|████▋     | 267/577 [00:06<00:07, 44.05it/s] 47%|████▋     | 272/577 [00:06<00:06, 43.96it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.84it/s] 49%|████▉     | 282/577 [00:06<00:06, 43.85it/s] 50%|████▉     | 287/577 [00:06<00:06, 43.93it/s] 51%|█████     | 292/577 [00:06<00:06, 44.12it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.16it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.30it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.25it/s] 54%|█████▍    | 312/577 [00:07<00:06, 44.03it/s] 55%|█████▍    | 317/577 [00:07<00:05, 44.01it/s] 56%|█████▌    | 322/577 [00:07<00:05, 43.79it/s] 57%|█████▋    | 327/577 [00:07<00:05, 43.78it/s] 58%|█████▊    | 332/577 [00:07<00:05, 43.90it/s] 58%|█████▊    | 337/577 [00:07<00:05, 43.91it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.16it/s] 60%|██████    | 347/577 [00:07<00:05, 44.13it/s] 61%|██████    | 352/577 [00:07<00:05, 44.16it/s] 62%|██████▏   | 357/577 [00:08<00:04, 44.01it/s] 63%|██████▎   | 362/577 [00:08<00:04, 43.87it/s] 64%|██████▎   | 367/577 [00:08<00:04, 43.87it/s] 64%|██████▍   | 372/577 [00:08<00:04, 43.81it/s] 65%|██████▌   | 377/577 [00:08<00:04, 43.96it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.01it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.12it/s] 68%|██████▊   | 392/577 [00:08<00:04, 44.19it/s] 69%|██████▉   | 397/577 [00:08<00:04, 44.12it/s] 70%|██████▉   | 402/577 [00:09<00:03, 44.06it/s] 71%|███████   | 407/577 [00:09<00:03, 43.90it/s] 71%|███████▏  | 412/577 [00:09<00:03, 43.84it/s] 72%|███████▏  | 417/577 [00:09<00:03, 43.84it/s] 73%|███████▎  | 422/577 [00:09<00:03, 43.96it/s] 74%|███████▍  | 427/577 [00:09<00:03, 43.99it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.12it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.11it/s] 77%|███████▋  | 442/577 [00:10<00:03, 44.04it/s] 77%|███████▋  | 447/577 [00:10<00:02, 44.06it/s] 78%|███████▊  | 452/577 [00:10<00:02, 43.87it/s] 79%|███████▉  | 457/577 [00:10<00:02, 43.87it/s] 80%|████████  | 462/577 [00:10<00:02, 43.85it/s] 81%|████████  | 467/577 [00:10<00:02, 43.95it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.13it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.16it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.08it/s] 84%|████████▍ | 487/577 [00:11<00:02, 44.04it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.00it/s] 86%|████████▌ | 497/577 [00:11<00:01, 43.88it/s] 87%|████████▋ | 502/577 [00:11<00:01, 43.75it/s] 88%|████████▊ | 507/577 [00:11<00:01, 43.85it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.01it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.11it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.19it/s] 91%|█████████▏| 527/577 [00:11<00:01, 44.06it/s] 92%|█████████▏| 532/577 [00:12<00:01, 44.04it/s] 93%|█████████▎| 537/577 [00:12<00:00, 44.05it/s] 94%|█████████▍| 542/577 [00:12<00:00, 43.81it/s] 95%|█████████▍| 547/577 [00:12<00:00, 43.73it/s] 96%|█████████▌| 552/577 [00:12<00:00, 43.88it/s] 97%|█████████▋| 557/577 [00:12<00:00, 43.94it/s] 97%|█████████▋| 562/577 [00:12<00:00, 43.96it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.14it/s] 99%|█████████▉| 572/577 [00:12<00:00, 44.16it/s]100%|██████████| 577/577 [00:13<00:00, 44.16it/s]100%|██████████| 577/577 [00:13<00:00, 44.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:52:44,904 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   eval_loss               =      0.989
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   eval_runtime            = 0:00:13.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   eval_samples_per_second =    351.835
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   eval_steps_per_second   =     44.046
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:52:44,904 >>   perplexity              =     2.6886
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:50,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:50,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:50,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:50,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:50,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:52:50,999 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:52:51,000 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:52:51,273 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:52:52,354 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:52:52,354 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:53,714 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:53,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:53,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:53,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:53,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:52:54,054 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:52:54,055 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:52:54,315 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:52:54,467 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:52:54,467 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:07,  1.24it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.30it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.31it/s]Extractor Predicting: 16it [00:12,  1.33it/s]Extractor Predicting: 17it [00:13,  1.32it/s]Extractor Predicting: 18it [00:13,  1.34it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:16,  1.30it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:17,  1.29it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:20,  1.31it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:21,  1.32it/s]Extractor Predicting: 29it [00:22,  1.33it/s]Extractor Predicting: 30it [00:23,  1.35it/s]Extractor Predicting: 31it [00:23,  1.32it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.30it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.26it/s]Extractor Predicting: 42it [00:32,  1.27it/s]Extractor Predicting: 43it [00:33,  1.26it/s]Extractor Predicting: 44it [00:34,  1.25it/s]Extractor Predicting: 45it [00:34,  1.24it/s]Extractor Predicting: 46it [00:35,  1.24it/s]Extractor Predicting: 47it [00:36,  1.23it/s]Extractor Predicting: 48it [00:37,  1.24it/s]Extractor Predicting: 49it [00:37,  1.29it/s]Extractor Predicting: 50it [00:38,  1.27it/s]Extractor Predicting: 51it [00:39,  1.29it/s]Extractor Predicting: 52it [00:40,  1.26it/s]Extractor Predicting: 53it [00:41,  1.25it/s]Extractor Predicting: 54it [00:41,  1.25it/s]Extractor Predicting: 55it [00:42,  1.26it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:44,  1.27it/s]Extractor Predicting: 58it [00:45,  1.26it/s]Extractor Predicting: 59it [00:45,  1.29it/s]Extractor Predicting: 60it [00:46,  1.28it/s]Extractor Predicting: 61it [00:47,  1.26it/s]Extractor Predicting: 62it [00:48,  1.13it/s]Extractor Predicting: 63it [00:49,  1.15it/s]Extractor Predicting: 64it [00:50,  1.17it/s]Extractor Predicting: 65it [00:50,  1.19it/s]Extractor Predicting: 66it [00:51,  1.24it/s]Extractor Predicting: 67it [00:52,  1.26it/s]Extractor Predicting: 68it [00:53,  1.29it/s]Extractor Predicting: 69it [00:53,  1.30it/s]Extractor Predicting: 70it [00:54,  1.29it/s]Extractor Predicting: 71it [00:55,  1.32it/s]Extractor Predicting: 72it [00:56,  1.35it/s]Extractor Predicting: 73it [00:56,  1.32it/s]Extractor Predicting: 74it [00:57,  1.34it/s]Extractor Predicting: 75it [00:58,  1.34it/s]Extractor Predicting: 76it [00:59,  1.31it/s]Extractor Predicting: 77it [01:00,  1.31it/s]Extractor Predicting: 78it [01:00,  1.33it/s]Extractor Predicting: 79it [01:01,  1.32it/s]Extractor Predicting: 80it [01:02,  1.33it/s]Extractor Predicting: 81it [01:03,  1.31it/s]Extractor Predicting: 82it [01:03,  1.30it/s]Extractor Predicting: 83it [01:04,  1.30it/s]Extractor Predicting: 84it [01:05,  1.33it/s]Extractor Predicting: 85it [01:05,  1.38it/s]Extractor Predicting: 86it [01:06,  1.38it/s]Extractor Predicting: 87it [01:07,  1.40it/s]Extractor Predicting: 88it [01:08,  1.33it/s]Extractor Predicting: 89it [01:08,  1.32it/s]Extractor Predicting: 90it [01:09,  1.30it/s]Extractor Predicting: 91it [01:10,  1.32it/s]Extractor Predicting: 92it [01:11,  1.31it/s]Extractor Predicting: 93it [01:12,  1.32it/s]Extractor Predicting: 94it [01:13,  1.22it/s]Extractor Predicting: 95it [01:13,  1.24it/s]Extractor Predicting: 96it [01:14,  1.26it/s]Extractor Predicting: 97it [01:15,  1.28it/s]Extractor Predicting: 98it [01:16,  1.26it/s]Extractor Predicting: 99it [01:16,  1.27it/s]Extractor Predicting: 100it [01:17,  1.25it/s]Extractor Predicting: 101it [01:18,  1.25it/s]Extractor Predicting: 102it [01:19,  1.27it/s]Extractor Predicting: 103it [01:20,  1.26it/s]Extractor Predicting: 104it [01:20,  1.29it/s]Extractor Predicting: 105it [01:21,  1.31it/s]Extractor Predicting: 106it [01:22,  1.27it/s]Extractor Predicting: 107it [01:23,  1.31it/s]Extractor Predicting: 108it [01:23,  1.33it/s]Extractor Predicting: 109it [01:24,  1.30it/s]Extractor Predicting: 110it [01:25,  1.30it/s]Extractor Predicting: 111it [01:26,  1.28it/s]Extractor Predicting: 112it [01:26,  1.28it/s]Extractor Predicting: 113it [01:27,  1.28it/s]Extractor Predicting: 114it [01:28,  1.29it/s]Extractor Predicting: 115it [01:29,  1.29it/s]Extractor Predicting: 116it [01:30,  1.30it/s]Extractor Predicting: 117it [01:30,  1.32it/s]Extractor Predicting: 118it [01:31,  1.32it/s]Extractor Predicting: 119it [01:32,  1.36it/s]Extractor Predicting: 120it [01:32,  1.35it/s]Extractor Predicting: 121it [01:33,  1.34it/s]Extractor Predicting: 122it [01:34,  1.32it/s]Extractor Predicting: 123it [01:35,  1.31it/s]Extractor Predicting: 124it [01:36,  1.32it/s]Extractor Predicting: 125it [01:36,  1.29it/s]Extractor Predicting: 126it [01:37,  1.32it/s]Extractor Predicting: 127it [01:38,  1.34it/s]Extractor Predicting: 128it [01:39,  1.32it/s]Extractor Predicting: 129it [01:39,  1.31it/s]Extractor Predicting: 130it [01:40,  1.28it/s]Extractor Predicting: 131it [01:41,  1.28it/s]Extractor Predicting: 132it [01:42,  1.30it/s]Extractor Predicting: 133it [01:42,  1.30it/s]Extractor Predicting: 134it [01:43,  1.29it/s]Extractor Predicting: 135it [01:44,  1.28it/s]Extractor Predicting: 136it [01:45,  1.29it/s]Extractor Predicting: 137it [01:46,  1.30it/s]Extractor Predicting: 138it [01:46,  1.30it/s]Extractor Predicting: 139it [01:47,  1.27it/s]Extractor Predicting: 140it [01:48,  1.26it/s]Extractor Predicting: 141it [01:49,  1.25it/s]Extractor Predicting: 142it [01:50,  1.24it/s]Extractor Predicting: 143it [01:50,  1.26it/s]Extractor Predicting: 144it [01:51,  1.24it/s]Extractor Predicting: 145it [01:52,  1.25it/s]Extractor Predicting: 146it [01:53,  1.22it/s]Extractor Predicting: 147it [01:54,  1.23it/s]Extractor Predicting: 148it [01:54,  1.24it/s]Extractor Predicting: 149it [01:55,  1.26it/s]Extractor Predicting: 150it [01:56,  1.26it/s]Extractor Predicting: 151it [01:57,  1.28it/s]Extractor Predicting: 152it [01:58,  1.27it/s]Extractor Predicting: 153it [01:58,  1.29it/s]Extractor Predicting: 154it [01:59,  1.28it/s]Extractor Predicting: 155it [02:00,  1.28it/s]Extractor Predicting: 156it [02:01,  1.26it/s]Extractor Predicting: 157it [02:01,  1.28it/s]Extractor Predicting: 158it [02:02,  1.28it/s]Extractor Predicting: 159it [02:03,  1.27it/s]Extractor Predicting: 160it [02:04,  1.27it/s]Extractor Predicting: 161it [02:05,  1.30it/s]Extractor Predicting: 162it [02:05,  1.29it/s]Extractor Predicting: 163it [02:06,  1.26it/s]Extractor Predicting: 164it [02:07,  1.25it/s]Extractor Predicting: 165it [02:08,  1.26it/s]Extractor Predicting: 166it [02:09,  1.25it/s]Extractor Predicting: 167it [02:09,  1.26it/s]Extractor Predicting: 168it [02:10,  1.27it/s]Extractor Predicting: 169it [02:11,  1.27it/s]Extractor Predicting: 170it [02:12,  1.27it/s]Extractor Predicting: 171it [02:12,  1.50it/s]Extractor Predicting: 171it [02:12,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:13,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:13,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:13,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:13,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:13,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:55:13,603 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:55:13,603 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:55:13,865 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:55:14,880 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:55:14,880 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:16,180 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:16,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:16,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:16,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:55:16,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:55:16,507 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:55:16,510 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:55:16,777 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:55:16,941 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:55:16,941 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2751842751842752,
  "recall": 0.024300282056845302,
  "score": 0.044657097288676235,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.27it/s]Extractor Predicting: 10it [00:07,  1.23it/s]Extractor Predicting: 11it [00:08,  1.27it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:10,  1.28it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:13,  1.33it/s]Extractor Predicting: 19it [00:14,  1.34it/s]Extractor Predicting: 20it [00:15,  1.32it/s]Extractor Predicting: 21it [00:16,  1.34it/s]Extractor Predicting: 22it [00:16,  1.33it/s]Extractor Predicting: 23it [00:17,  1.33it/s]Extractor Predicting: 24it [00:18,  1.28it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:19,  1.32it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.33it/s]Extractor Predicting: 29it [00:22,  1.33it/s]Extractor Predicting: 30it [00:22,  1.31it/s]Extractor Predicting: 31it [00:23,  1.30it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.31it/s]Extractor Predicting: 35it [00:26,  1.31it/s]Extractor Predicting: 36it [00:27,  1.29it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.31it/s]Extractor Predicting: 39it [00:29,  1.30it/s]Extractor Predicting: 40it [00:30,  1.31it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:32,  1.33it/s]Extractor Predicting: 44it [00:33,  1.33it/s]Extractor Predicting: 45it [00:34,  1.33it/s]Extractor Predicting: 46it [00:35,  1.35it/s]Extractor Predicting: 47it [00:35,  1.31it/s]Extractor Predicting: 48it [00:36,  1.32it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.32it/s]Extractor Predicting: 51it [00:38,  1.33it/s]Extractor Predicting: 52it [00:39,  1.33it/s]Extractor Predicting: 53it [00:40,  1.24it/s]Extractor Predicting: 54it [00:41,  1.24it/s]Extractor Predicting: 55it [00:42,  1.24it/s]Extractor Predicting: 56it [00:42,  1.27it/s]Extractor Predicting: 57it [00:43,  1.28it/s]Extractor Predicting: 58it [00:44,  1.29it/s]Extractor Predicting: 59it [00:45,  1.30it/s]Extractor Predicting: 60it [00:46,  1.28it/s]Extractor Predicting: 61it [00:46,  1.28it/s]Extractor Predicting: 62it [00:47,  1.27it/s]Extractor Predicting: 63it [00:48,  1.30it/s]Extractor Predicting: 64it [00:49,  1.30it/s]Extractor Predicting: 65it [00:49,  1.31it/s]Extractor Predicting: 66it [00:50,  1.30it/s]Extractor Predicting: 67it [00:51,  1.32it/s]Extractor Predicting: 68it [00:52,  1.33it/s]Extractor Predicting: 69it [00:52,  1.34it/s]Extractor Predicting: 70it [00:53,  1.32it/s]Extractor Predicting: 71it [00:54,  1.27it/s]Extractor Predicting: 72it [00:55,  1.30it/s]Extractor Predicting: 73it [00:55,  1.29it/s]Extractor Predicting: 74it [00:56,  1.31it/s]Extractor Predicting: 75it [00:57,  1.31it/s]Extractor Predicting: 76it [00:58,  1.34it/s]Extractor Predicting: 77it [00:58,  1.37it/s]Extractor Predicting: 78it [00:59,  1.37it/s]Extractor Predicting: 79it [01:00,  1.34it/s]Extractor Predicting: 80it [01:01,  1.35it/s]Extractor Predicting: 81it [01:01,  1.34it/s]Extractor Predicting: 82it [01:02,  1.35it/s]Extractor Predicting: 83it [01:03,  1.35it/s]Extractor Predicting: 84it [01:04,  1.32it/s]Extractor Predicting: 85it [01:04,  1.32it/s]Extractor Predicting: 86it [01:05,  1.33it/s]Extractor Predicting: 87it [01:06,  1.31it/s]Extractor Predicting: 88it [01:07,  1.31it/s]Extractor Predicting: 89it [01:07,  1.31it/s]Extractor Predicting: 90it [01:08,  1.29it/s]Extractor Predicting: 91it [01:09,  1.29it/s]Extractor Predicting: 92it [01:10,  1.30it/s]Extractor Predicting: 93it [01:11,  1.30it/s]Extractor Predicting: 94it [01:11,  1.29it/s]Extractor Predicting: 95it [01:12,  1.28it/s]Extractor Predicting: 96it [01:13,  1.26it/s]Extractor Predicting: 97it [01:14,  1.29it/s]Extractor Predicting: 98it [01:14,  1.30it/s]Extractor Predicting: 99it [01:15,  1.37it/s]Extractor Predicting: 100it [01:16,  1.36it/s]Extractor Predicting: 101it [01:17,  1.34it/s]Extractor Predicting: 102it [01:17,  1.33it/s]Extractor Predicting: 103it [01:18,  1.33it/s]Extractor Predicting: 104it [01:19,  1.33it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:20,  1.34it/s]Extractor Predicting: 107it [01:21,  1.37it/s]Extractor Predicting: 108it [01:22,  1.34it/s]Extractor Predicting: 109it [01:23,  1.31it/s]Extractor Predicting: 110it [01:24,  1.25it/s]Extractor Predicting: 111it [01:24,  1.24it/s]Extractor Predicting: 112it [01:25,  1.24it/s]Extractor Predicting: 113it [01:26,  1.29it/s]Extractor Predicting: 114it [01:27,  1.30it/s]Extractor Predicting: 115it [01:27,  1.29it/s]Extractor Predicting: 116it [01:28,  1.30it/s]Extractor Predicting: 117it [01:29,  1.29it/s]Extractor Predicting: 118it [01:30,  1.26it/s]Extractor Predicting: 119it [01:31,  1.28it/s]Extractor Predicting: 120it [01:31,  1.26it/s]Extractor Predicting: 121it [01:32,  1.25it/s]Extractor Predicting: 122it [01:33,  1.26it/s]Extractor Predicting: 123it [01:34,  1.27it/s]Extractor Predicting: 124it [01:35,  1.27it/s]Extractor Predicting: 125it [01:35,  1.58it/s]Extractor Predicting: 125it [01:35,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:56:59,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:56:59,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:56:59,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:56:59,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:56:59,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:56:59,672 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:56:59,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:57:00,228 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:57:01,242 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:57:01,242 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:57:04,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:57:04,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:57:04,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:57:04,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:57:04,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:57:04,768 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:57:04,769 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:57:05,344 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:57:05,514 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:57:05,514 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5966029723991507,
  "recall": 0.09432695535414569,
  "score": 0.1628985507246377,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.14it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.62it/s]Extractor Predicting: 6it [00:04,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-28 15:57:10,373 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:57:10,373 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:57:10,377 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:57:10,378 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 15:57:10,379 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:57:14,084 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 15:57:14,089 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 15:57:14,109 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:57:14,110 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:57:14,121 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:57:14,128 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7333333333333333,
  "recall": 0.04330708661417323,
  "score": 0.08178438661710037,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 15:57:14,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:15,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:15,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:16,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:17,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:18,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:19,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:19,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:20,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:21,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:21,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:22,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:23,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:24,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:25,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:26,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:27,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:28,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:28,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:29,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:25, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-28 15:57:30,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:31,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:31,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:32,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:33,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:34,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:34,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:35,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:36,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:37,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:38,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:39,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:40,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:41,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:42,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:44,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:45,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:46,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:46,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:47,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:16, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 15:57:48,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:48,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:49,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:50,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:51,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:52,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:52,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:53,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:54,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:54,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:55,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:56,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:57,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:57,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:58,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:59,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:57:59,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:00,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:01,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:02,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:48<01:52, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-28 15:58:02,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:03,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:04,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:05,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:06,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:07,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:08,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:09,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:10,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:10,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:11,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:12,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:13,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:14,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:14,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:15,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:16,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:17,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:17,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:18,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:19,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:05<01:38, 16.46s/it][WARNING|generation_utils.py:914] 2023-08-28 15:58:20,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:20,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:21,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:22,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:23,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:23,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:24,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:25,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:26,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:26,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:27,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:28,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:29,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:30,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:30,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:31,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:32,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:33,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:34,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:20<01:20, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-28 15:58:35,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:36,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:36,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:37,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:38,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:39,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:39,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:40,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:41,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:41,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:42,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:43,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:44,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:45,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:46,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:47,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:47,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:48,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:49,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:35<01:02, 15.60s/it][WARNING|generation_utils.py:914] 2023-08-28 15:58:50,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:50,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:51,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:52,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:53,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:53,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:55,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:55,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:56,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:57,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:58,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:58,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:58:59,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:00,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:01,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:01,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:02,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:03,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:04,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:04,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:05,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:52<00:47, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-28 15:59:06,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:07,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:08,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:09,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:10,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:10,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:11,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:12,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:13,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:13,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:14,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:15,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:16,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:16,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:17,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:18,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:19,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:19,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:20,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:30, 15.43s/it][WARNING|generation_utils.py:914] 2023-08-28 15:59:21,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:21,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:22,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:23,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:24,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:24,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:25,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:26,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:26,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:27,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:28,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:28,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:29,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:30,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:30,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:31,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:32,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:32,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:33,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:34,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:34,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:22<00:15, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-28 15:59:36,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:37,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:37,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:38,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:39,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:40,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:40,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:41,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:42,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:42,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:43,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:45,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:45,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:46,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:47,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:48,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:48,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:49,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:50,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:50,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:51,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:52,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:53,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:53,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:59:54,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:40<00:00, 16.49s/it]Generating: 100%|██████████| 10/10 [02:40<00:00, 16.10s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:01,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:01,582 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:01,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:01,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:01,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:00:02,209 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:00:02,210 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:02,768 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:03,865 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:03,865 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:06,754 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:06,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:06,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:06,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:06,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:00:07,408 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:00:07,409 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:07,975 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:08,163 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:08,163 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('LG G8', 'manufacturer', '', 'He is best known for his work on the LG G2 , LG G3 , LG G4 , LG G5 , LG G6 , LG G7 , LG G8 , LG G9 , LG G10 , LG G11 , LG G12 , LG G13 , LG G14 , LG G15 , LG G16 , LG G17 , LG G18 , LG G19 , LG G20 , LG G21 , LG G22 , LG G23 , LG G24 , LG G25 , and LG G26 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : official language .', 'success_rate': 0.8622159090909091, 'errors': {'', "('Mika Mizuki', 'official language', '', 'In 2015 , he also worked with the director Gautama Buddha to create a musical adaptation of the novel by Japanese writer Mika Mizuki .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8863636363636364, 'errors': {'', "('World Food Programme', 'use', '', 'In May 2004 , I was invited to visit St Martin to attend the World Food Programme meeting in Copenhagen .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : voice type . Context : Later in 2008 , he played the title role of a character on the BBC soap opera EastEnders , starring Emma Thompson and Ian Fleming . Head Entity : Ian Fleming , Tail Entity : voice type .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 424, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 577, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : voice type .', 'success_rate': 0.7525, 'errors': {'', "('David Bowie', 'voice type', '', 'His first release , in 1994 , featured vocals by David Bowie on the single Heart of Darkness .')", 'not enough values to unpack (expected 2, got 1)', "('Richard Tudor', 'voice type', '', 'In May 2009 , she appeared in the BBC series Downton Abbey where she played the role of Susan Tudor , wife of the actor and comedian Richard Tudor .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 8671
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8771, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.30it/s]Extractor Estimating: 3it [00:02,  1.34it/s]Extractor Estimating: 4it [00:03,  1.32it/s]Extractor Estimating: 5it [00:03,  1.30it/s]Extractor Estimating: 6it [00:04,  1.37it/s]Extractor Estimating: 7it [00:05,  1.31it/s]Extractor Estimating: 8it [00:06,  1.33it/s]Extractor Estimating: 9it [00:06,  1.38it/s]Extractor Estimating: 10it [00:07,  1.38it/s]Extractor Estimating: 11it [00:08,  1.34it/s]Extractor Estimating: 12it [00:08,  1.36it/s]Extractor Estimating: 13it [00:09,  1.33it/s]Extractor Estimating: 14it [00:10,  1.35it/s]Extractor Estimating: 15it [00:11,  1.39it/s]Extractor Estimating: 16it [00:11,  1.39it/s]Extractor Estimating: 17it [00:12,  1.35it/s]Extractor Estimating: 18it [00:13,  1.38it/s]Extractor Estimating: 19it [00:14,  1.35it/s]Extractor Estimating: 20it [00:14,  1.37it/s]Extractor Estimating: 21it [00:15,  1.36it/s]Extractor Estimating: 22it [00:16,  1.36it/s]Extractor Estimating: 23it [00:17,  1.33it/s]Extractor Estimating: 24it [00:17,  1.34it/s]Extractor Estimating: 25it [00:18,  1.34it/s]Extractor Estimating: 26it [00:19,  1.35it/s]Extractor Estimating: 27it [00:19,  1.37it/s]Extractor Estimating: 28it [00:20,  1.43it/s]Extractor Estimating: 29it [00:21,  1.48it/s]Extractor Estimating: 30it [00:21,  1.56it/s]Extractor Estimating: 31it [00:22,  1.39it/s]Extractor Estimating: 32it [00:23,  1.43it/s]Extractor Estimating: 33it [00:24,  1.43it/s]Extractor Estimating: 34it [00:24,  1.46it/s]Extractor Estimating: 35it [00:25,  1.42it/s]Extractor Estimating: 36it [00:26,  1.30it/s]Extractor Estimating: 37it [00:26,  1.36it/s]Extractor Estimating: 38it [00:27,  1.32it/s]Extractor Estimating: 39it [00:28,  1.40it/s]Extractor Estimating: 40it [00:29,  1.26it/s]Extractor Estimating: 41it [00:30,  1.31it/s]Extractor Estimating: 42it [00:30,  1.37it/s]Extractor Estimating: 43it [00:31,  1.41it/s]Extractor Estimating: 44it [00:32,  1.43it/s]Extractor Estimating: 45it [00:32,  1.47it/s]Extractor Estimating: 46it [00:33,  1.40it/s]Extractor Estimating: 47it [00:34,  1.46it/s]Extractor Estimating: 48it [00:34,  1.46it/s]Extractor Estimating: 49it [00:35,  1.46it/s]Extractor Estimating: 50it [00:36,  1.47it/s]Extractor Estimating: 51it [00:36,  1.44it/s]Extractor Estimating: 52it [00:37,  1.43it/s]Extractor Estimating: 53it [00:38,  1.40it/s]Extractor Estimating: 54it [00:39,  1.32it/s]Extractor Estimating: 55it [00:39,  1.35it/s]Extractor Estimating: 56it [00:40,  1.37it/s]Extractor Estimating: 57it [00:41,  1.39it/s]Extractor Estimating: 58it [00:42,  1.38it/s]Extractor Estimating: 59it [00:42,  1.39it/s]Extractor Estimating: 60it [00:43,  1.42it/s]Extractor Estimating: 61it [00:44,  1.40it/s]Extractor Estimating: 62it [00:44,  1.41it/s]Extractor Estimating: 63it [00:45,  1.40it/s]Extractor Estimating: 64it [00:46,  1.41it/s]Extractor Estimating: 65it [00:46,  1.41it/s]Extractor Estimating: 66it [00:47,  1.41it/s]Extractor Estimating: 67it [00:48,  1.42it/s]Extractor Estimating: 68it [00:49,  1.45it/s]Extractor Estimating: 69it [00:49,  1.42it/s]Extractor Estimating: 70it [00:50,  1.48it/s]Extractor Estimating: 71it [00:51,  1.45it/s]Extractor Estimating: 72it [00:51,  1.41it/s]Extractor Estimating: 73it [00:52,  1.44it/s]Extractor Estimating: 74it [00:53,  1.39it/s]Extractor Estimating: 75it [00:54,  1.37it/s]Extractor Estimating: 76it [00:54,  1.39it/s]Extractor Estimating: 77it [00:55,  1.41it/s]Extractor Estimating: 78it [00:56,  1.40it/s]Extractor Estimating: 79it [00:56,  1.41it/s]Extractor Estimating: 80it [00:57,  1.37it/s]Extractor Estimating: 81it [00:58,  1.42it/s]Extractor Estimating: 82it [00:59,  1.34it/s]Extractor Estimating: 83it [00:59,  1.36it/s]Extractor Estimating: 84it [01:00,  1.42it/s]Extractor Estimating: 85it [01:01,  1.45it/s]Extractor Estimating: 86it [01:01,  1.46it/s]Extractor Estimating: 87it [01:02,  1.43it/s]Extractor Estimating: 88it [01:03,  1.45it/s]Extractor Estimating: 89it [01:03,  1.45it/s]Extractor Estimating: 90it [01:04,  1.42it/s]Extractor Estimating: 91it [01:05,  1.46it/s]Extractor Estimating: 92it [01:05,  1.45it/s]Extractor Estimating: 93it [01:06,  1.45it/s]Extractor Estimating: 94it [01:07,  1.43it/s]Extractor Estimating: 95it [01:08,  1.44it/s]Extractor Estimating: 96it [01:08,  1.45it/s]Extractor Estimating: 97it [01:09,  1.45it/s]Extractor Estimating: 98it [01:10,  1.47it/s]Extractor Estimating: 99it [01:10,  1.47it/s]Extractor Estimating: 100it [01:11,  1.47it/s]Extractor Estimating: 101it [01:12,  1.40it/s]Extractor Estimating: 102it [01:13,  1.35it/s]Extractor Estimating: 103it [01:13,  1.31it/s]Extractor Estimating: 104it [01:14,  1.30it/s]Extractor Estimating: 105it [01:15,  1.29it/s]Extractor Estimating: 106it [01:16,  1.28it/s]Extractor Estimating: 107it [01:17,  1.26it/s]Extractor Estimating: 108it [01:17,  1.24it/s]Extractor Estimating: 109it [01:18,  1.24it/s]Extractor Estimating: 110it [01:19,  1.24it/s]Extractor Estimating: 111it [01:20,  1.24it/s]Extractor Estimating: 112it [01:21,  1.24it/s]Extractor Estimating: 113it [01:21,  1.26it/s]Extractor Estimating: 114it [01:22,  1.27it/s]Extractor Estimating: 115it [01:23,  1.24it/s]Extractor Estimating: 116it [01:24,  1.23it/s]Extractor Estimating: 117it [01:25,  1.25it/s]Extractor Estimating: 118it [01:25,  1.24it/s]Extractor Estimating: 119it [01:26,  1.22it/s]Extractor Estimating: 120it [01:27,  1.23it/s]Extractor Estimating: 121it [01:28,  1.22it/s]Extractor Estimating: 122it [01:29,  1.23it/s]Extractor Estimating: 123it [01:29,  1.23it/s]Extractor Estimating: 124it [01:30,  1.25it/s]Extractor Estimating: 125it [01:31,  1.24it/s]Extractor Estimating: 126it [01:32,  1.25it/s]Extractor Estimating: 127it [01:33,  1.16it/s]Extractor Estimating: 128it [01:34,  1.21it/s]Extractor Estimating: 129it [01:34,  1.27it/s]Extractor Estimating: 130it [01:35,  1.28it/s]Extractor Estimating: 131it [01:36,  1.29it/s]Extractor Estimating: 132it [01:37,  1.32it/s]Extractor Estimating: 133it [01:37,  1.39it/s]Extractor Estimating: 134it [01:38,  1.40it/s]Extractor Estimating: 135it [01:39,  1.39it/s]Extractor Estimating: 136it [01:39,  1.39it/s]Extractor Estimating: 137it [01:40,  1.38it/s]Extractor Estimating: 138it [01:41,  1.35it/s]Extractor Estimating: 139it [01:42,  1.36it/s]Extractor Estimating: 140it [01:42,  1.33it/s]Extractor Estimating: 141it [01:43,  1.30it/s]Extractor Estimating: 142it [01:44,  1.35it/s]Extractor Estimating: 143it [01:45,  1.35it/s]Extractor Estimating: 144it [01:45,  1.33it/s]Extractor Estimating: 145it [01:46,  1.36it/s]Extractor Estimating: 146it [01:47,  1.35it/s]Extractor Estimating: 147it [01:48,  1.31it/s]Extractor Estimating: 148it [01:48,  1.33it/s]Extractor Estimating: 149it [01:49,  1.33it/s]Extractor Estimating: 150it [01:50,  1.37it/s]Extractor Estimating: 151it [01:50,  1.41it/s]Extractor Estimating: 152it [01:51,  1.36it/s]Extractor Estimating: 153it [01:52,  1.34it/s]Extractor Estimating: 154it [01:53,  1.36it/s]Extractor Estimating: 155it [01:53,  1.36it/s]Extractor Estimating: 156it [01:54,  1.38it/s]Extractor Estimating: 157it [01:55,  1.40it/s]Extractor Estimating: 158it [01:56,  1.41it/s]Extractor Estimating: 159it [01:56,  1.39it/s]Extractor Estimating: 160it [01:57,  1.40it/s]Extractor Estimating: 161it [01:58,  1.43it/s]Extractor Estimating: 162it [01:58,  1.37it/s]Extractor Estimating: 163it [01:59,  1.39it/s]Extractor Estimating: 164it [02:00,  1.39it/s]Extractor Estimating: 165it [02:01,  1.33it/s]Extractor Estimating: 166it [02:01,  1.38it/s]Extractor Estimating: 167it [02:02,  1.38it/s]Extractor Estimating: 168it [02:03,  1.38it/s]Extractor Estimating: 169it [02:03,  1.41it/s]Extractor Estimating: 170it [02:04,  1.38it/s]Extractor Estimating: 171it [02:05,  1.34it/s]Extractor Estimating: 172it [02:06,  1.37it/s]Extractor Estimating: 173it [02:06,  1.37it/s]Extractor Estimating: 174it [02:07,  1.41it/s]Extractor Estimating: 175it [02:08,  1.38it/s]Extractor Estimating: 176it [02:09,  1.43it/s]Extractor Estimating: 177it [02:09,  1.45it/s]Extractor Estimating: 178it [02:10,  1.48it/s]Extractor Estimating: 179it [02:11,  1.47it/s]Extractor Estimating: 180it [02:11,  1.46it/s]Extractor Estimating: 181it [02:12,  1.43it/s]Extractor Estimating: 182it [02:13,  1.46it/s]Extractor Estimating: 183it [02:13,  1.42it/s]Extractor Estimating: 184it [02:14,  1.45it/s]Extractor Estimating: 185it [02:15,  1.40it/s]Extractor Estimating: 186it [02:16,  1.37it/s]Extractor Estimating: 187it [02:16,  1.41it/s]Extractor Estimating: 188it [02:17,  1.41it/s]Extractor Estimating: 189it [02:18,  1.27it/s]Extractor Estimating: 190it [02:19,  1.31it/s]Extractor Estimating: 191it [02:19,  1.34it/s]Extractor Estimating: 192it [02:20,  1.38it/s]Extractor Estimating: 193it [02:21,  1.37it/s]Extractor Estimating: 194it [02:21,  1.40it/s]Extractor Estimating: 195it [02:22,  1.39it/s]Extractor Estimating: 196it [02:23,  1.40it/s]Extractor Estimating: 197it [02:24,  1.38it/s]Extractor Estimating: 198it [02:24,  1.40it/s]Extractor Estimating: 199it [02:25,  1.41it/s]Extractor Estimating: 200it [02:26,  1.45it/s]Extractor Estimating: 201it [02:26,  1.44it/s]Extractor Estimating: 202it [02:27,  1.36it/s]Extractor Estimating: 203it [02:28,  1.43it/s]Extractor Estimating: 204it [02:28,  1.41it/s]Extractor Estimating: 205it [02:29,  1.38it/s]Extractor Estimating: 206it [02:30,  1.42it/s]Extractor Estimating: 207it [02:31,  1.42it/s]Extractor Estimating: 208it [02:31,  1.49it/s]Extractor Estimating: 209it [02:32,  1.47it/s]Extractor Estimating: 210it [02:33,  1.51it/s]Extractor Estimating: 211it [02:33,  1.51it/s]Extractor Estimating: 212it [02:34,  1.53it/s]Extractor Estimating: 213it [02:35,  1.51it/s]Extractor Estimating: 214it [02:35,  1.50it/s]Extractor Estimating: 215it [02:36,  1.45it/s]Extractor Estimating: 216it [02:37,  1.46it/s]Extractor Estimating: 217it [02:37,  1.46it/s]Extractor Estimating: 218it [02:38,  1.45it/s]Extractor Estimating: 219it [02:39,  1.48it/s]Extractor Estimating: 220it [02:39,  1.40it/s]Extractor Estimating: 221it [02:40,  1.45it/s]Extractor Estimating: 222it [02:41,  1.49it/s]Extractor Estimating: 223it [02:41,  1.52it/s]Extractor Estimating: 224it [02:42,  1.50it/s]Extractor Estimating: 225it [02:43,  1.43it/s]Extractor Estimating: 226it [02:43,  1.44it/s]Extractor Estimating: 227it [02:44,  1.47it/s]Extractor Estimating: 228it [02:45,  1.44it/s]Extractor Estimating: 229it [02:46,  1.40it/s]Extractor Estimating: 230it [02:46,  1.34it/s]Extractor Estimating: 231it [02:47,  1.37it/s]Extractor Estimating: 232it [02:48,  1.41it/s]Extractor Estimating: 233it [02:48,  1.41it/s]Extractor Estimating: 234it [02:49,  1.40it/s]Extractor Estimating: 235it [02:50,  1.37it/s]Extractor Estimating: 236it [02:51,  1.36it/s]Extractor Estimating: 237it [02:51,  1.38it/s]Extractor Estimating: 238it [02:52,  1.38it/s]Extractor Estimating: 239it [02:53,  1.42it/s]Extractor Estimating: 240it [02:54,  1.37it/s]Extractor Estimating: 241it [02:54,  1.36it/s]Extractor Estimating: 242it [02:55,  1.39it/s]Extractor Estimating: 243it [02:56,  1.43it/s]Extractor Estimating: 244it [02:56,  1.37it/s]Extractor Estimating: 245it [02:57,  1.37it/s]Extractor Estimating: 246it [02:58,  1.34it/s]Extractor Estimating: 247it [02:59,  1.39it/s]Extractor Estimating: 248it [02:59,  1.42it/s]Extractor Estimating: 249it [03:00,  1.40it/s]Extractor Estimating: 250it [03:01,  1.37it/s]Extractor Estimating: 250it [03:01,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:25,856 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:25,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:25,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:25,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:25,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:03:26,162 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:03:26,163 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:03:26,427 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:03:27,496 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:03:27,496 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:28,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:28,783 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:28,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:28,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:28,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:03:29,111 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:03:29,115 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:03:29,771 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:03:29,942 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:03:29,942 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:52:28,039 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:52:28,174 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4994 mean pseudo reward: 0.9352189321343135
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 18965
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19065, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19065, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.253, loss:506.3508
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.207, loss:477.8959
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.210, loss:451.6210
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.205, loss:458.4088
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.203, loss:417.1790
>> valid entity prec:0.4384, rec:0.4622, f1:0.4500
>> valid relation prec:0.1276, rec:0.0244, f1:0.0410
>> valid relation with NER prec:0.1276, rec:0.0244, f1:0.0410
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.049, loss:439.8049
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.191, loss:413.3820
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.216, loss:450.4659
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.212, loss:425.3245
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.210, loss:427.6014
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4336, rec:0.4544, f1:0.4437
>> valid relation prec:0.0971, rec:0.0209, f1:0.0344
>> valid relation with NER prec:0.0971, rec:0.0209, f1:0.0344
g_step 1100, step 55, avg_time 3.041, loss:426.0145
g_step 1200, step 155, avg_time 1.207, loss:412.8334
g_step 1300, step 46, avg_time 1.203, loss:404.7772
g_step 1400, step 146, avg_time 1.213, loss:383.1191
g_step 1500, step 37, avg_time 1.212, loss:402.8334
>> valid entity prec:0.4635, rec:0.4451, f1:0.4541
>> valid relation prec:0.1313, rec:0.0257, f1:0.0430
>> valid relation with NER prec:0.1313, rec:0.0257, f1:0.0430
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 3.041, loss:380.3561
g_step 1700, step 28, avg_time 1.223, loss:368.3239
g_step 1800, step 128, avg_time 1.219, loss:363.2587
g_step 1900, step 19, avg_time 1.193, loss:359.7311
g_step 2000, step 119, avg_time 1.212, loss:345.7104
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4661, rec:0.4691, f1:0.4676
>> valid relation prec:0.1582, rec:0.0224, f1:0.0393
>> valid relation with NER prec:0.1582, rec:0.0224, f1:0.0393
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 10, avg_time 3.048, loss:348.8358
g_step 2200, step 110, avg_time 1.219, loss:335.8755
g_step 2300, step 1, avg_time 1.200, loss:339.6956
g_step 2400, step 101, avg_time 1.214, loss:318.0551
g_step 2500, step 201, avg_time 1.207, loss:327.8174
>> valid entity prec:0.4360, rec:0.4825, f1:0.4581
>> valid relation prec:0.2148, rec:0.0519, f1:0.0836
>> valid relation with NER prec:0.2148, rec:0.0519, f1:0.0836
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.030, loss:278.2480
g_step 2700, step 192, avg_time 1.228, loss:307.7579
g_step 2800, step 83, avg_time 1.211, loss:286.7846
g_step 2900, step 183, avg_time 1.203, loss:299.9713
g_step 3000, step 74, avg_time 1.202, loss:284.2379
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4910, rec:0.4593, f1:0.4746
>> valid relation prec:0.2089, rec:0.0521, f1:0.0834
>> valid relation with NER prec:0.2089, rec:0.0521, f1:0.0834
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 174, avg_time 3.056, loss:284.0692
g_step 3200, step 65, avg_time 1.199, loss:266.4933
g_step 3300, step 165, avg_time 1.214, loss:266.7328
g_step 3400, step 56, avg_time 1.214, loss:248.9323
g_step 3500, step 156, avg_time 1.222, loss:265.3974
>> valid entity prec:0.4534, rec:0.4300, f1:0.4414
>> valid relation prec:0.1902, rec:0.0355, f1:0.0599
>> valid relation with NER prec:0.1902, rec:0.0355, f1:0.0599
g_step 3600, step 47, avg_time 3.052, loss:261.9413
g_step 3700, step 147, avg_time 1.228, loss:245.7103
g_step 3800, step 38, avg_time 1.218, loss:262.4521
g_step 3900, step 138, avg_time 1.207, loss:241.2004
g_step 4000, step 29, avg_time 1.217, loss:255.3838
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4672, rec:0.4258, f1:0.4455
>> valid relation prec:0.1590, rec:0.0375, f1:0.0607
>> valid relation with NER prec:0.1590, rec:0.0375, f1:0.0607
g_step 4100, step 129, avg_time 3.065, loss:239.5424
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:52:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:52:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-52-28_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:52:29 - WARNING - datasets.builder -   Using custom data configuration default-0cf73728a5deb5e6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0cf73728a5deb5e6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:52:30,302 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:52:30,304 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:52:30,304 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:52:30,305 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:52:30,335 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:52:30,343 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:52:30,544 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:52:33,757 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:52:33,764 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0cf73728a5deb5e6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.91ba/s] 40%|████      | 2/5 [00:00<00:00,  3.83ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.26ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.59ba/s]100%|██████████| 5/5 [00:01<00:00,  3.95ba/s]100%|██████████| 5/5 [00:01<00:00,  3.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.30ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.39ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:01<00:00,  5.13ba/s]100%|██████████| 5/5 [00:01<00:00,  4.74ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.65ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.28ba/s]100%|██████████| 5/5 [00:00<00:00, 10.10ba/s]100%|██████████| 5/5 [00:00<00:00,  9.10ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.78ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.21ba/s]100%|██████████| 5/5 [00:00<00:00, 11.05ba/s]100%|██████████| 5/5 [00:00<00:00, 10.32ba/s]
[INFO|trainer.py:414] 2023-08-28 17:52:37,997 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:52:38,031 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:52:38,031 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 17:52:38,031 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:52:38,031 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:52:38,031 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:52:38,031 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:52:38,031 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:00,  3.22it/s]  1%|          | 2/390 [00:00<01:56,  3.33it/s]  1%|          | 3/390 [00:00<01:54,  3.37it/s]  1%|          | 4/390 [00:01<01:53,  3.39it/s]  1%|▏         | 5/390 [00:01<01:53,  3.39it/s]  2%|▏         | 6/390 [00:01<01:52,  3.40it/s]  2%|▏         | 7/390 [00:02<01:52,  3.40it/s]  2%|▏         | 8/390 [00:02<01:55,  3.32it/s]  2%|▏         | 9/390 [00:02<01:53,  3.35it/s]  3%|▎         | 10/390 [00:02<01:52,  3.37it/s]  3%|▎         | 11/390 [00:03<01:52,  3.38it/s]  3%|▎         | 12/390 [00:03<01:51,  3.38it/s]  3%|▎         | 13/390 [00:03<01:50,  3.40it/s]  4%|▎         | 14/390 [00:04<01:50,  3.40it/s]  4%|▍         | 15/390 [00:04<01:50,  3.40it/s]  4%|▍         | 16/390 [00:04<01:49,  3.40it/s]  4%|▍         | 17/390 [00:05<01:49,  3.40it/s]  5%|▍         | 18/390 [00:05<01:49,  3.40it/s]  5%|▍         | 19/390 [00:05<01:55,  3.21it/s]  5%|▌         | 20/390 [00:05<01:53,  3.25it/s]  5%|▌         | 21/390 [00:06<01:52,  3.27it/s]  6%|▌         | 22/390 [00:06<01:51,  3.29it/s]  6%|▌         | 23/390 [00:06<01:51,  3.30it/s]  6%|▌         | 24/390 [00:07<01:50,  3.31it/s]  6%|▋         | 25/390 [00:07<01:50,  3.31it/s]  7%|▋         | 26/390 [00:07<01:49,  3.32it/s]  7%|▋         | 27/390 [00:08<01:49,  3.32it/s]  7%|▋         | 28/390 [00:08<01:49,  3.32it/s]  7%|▋         | 29/390 [00:08<01:49,  3.31it/s]  8%|▊         | 30/390 [00:08<01:48,  3.31it/s]  8%|▊         | 31/390 [00:09<01:48,  3.31it/s]  8%|▊         | 32/390 [00:09<01:48,  3.31it/s]  8%|▊         | 33/390 [00:09<01:47,  3.32it/s]  9%|▊         | 34/390 [00:10<01:46,  3.34it/s]  9%|▉         | 35/390 [00:10<01:45,  3.36it/s]  9%|▉         | 36/390 [00:10<01:44,  3.37it/s]  9%|▉         | 37/390 [00:11<01:44,  3.38it/s] 10%|▉         | 38/390 [00:11<01:43,  3.39it/s] 10%|█         | 39/390 [00:11<01:43,  3.39it/s] 10%|█         | 40/390 [00:11<01:43,  3.38it/s] 11%|█         | 41/390 [00:12<01:43,  3.39it/s] 11%|█         | 42/390 [00:12<01:42,  3.39it/s] 11%|█         | 43/390 [00:12<01:42,  3.39it/s] 11%|█▏        | 44/390 [00:13<01:41,  3.39it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 47/390 [00:14<01:40,  3.40it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.40it/s] 13%|█▎        | 51/390 [00:15<01:40,  3.38it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.39it/s] 14%|█▎        | 53/390 [00:15<01:39,  3.39it/s] 14%|█▍        | 54/390 [00:16<01:39,  3.39it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.40it/s] 14%|█▍        | 56/390 [00:16<01:38,  3.40it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.40it/s] 15%|█▍        | 58/390 [00:17<01:37,  3.40it/s] 15%|█▌        | 59/390 [00:17<01:37,  3.40it/s] 15%|█▌        | 60/390 [00:17<01:37,  3.40it/s] 16%|█▌        | 61/390 [00:18<01:36,  3.40it/s] 16%|█▌        | 62/390 [00:18<01:37,  3.36it/s] 16%|█▌        | 63/390 [00:18<01:36,  3.38it/s] 16%|█▋        | 64/390 [00:19<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 67/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 68/390 [00:20<01:34,  3.39it/s] 18%|█▊        | 69/390 [00:20<01:34,  3.40it/s] 18%|█▊        | 70/390 [00:20<01:34,  3.40it/s] 18%|█▊        | 71/390 [00:21<01:33,  3.40it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.40it/s] 19%|█▊        | 73/390 [00:21<01:33,  3.40it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.40it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.40it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.40it/s] 20%|█▉        | 77/390 [00:22<01:32,  3.40it/s] 20%|██        | 78/390 [00:23<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 17:53:01,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:53:01,224 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:53:01,224 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.15it/s][A
  2%|▏         | 12/577 [00:00<00:11, 47.99it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.92it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.19it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.88it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.54it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.43it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.11it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.17it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.33it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.25it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.12it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.25it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.13it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.01it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.01it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.13it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.17it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.22it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.28it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.04it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.04it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.09it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.09it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.09it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.10it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.31it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.31it/s][A
 27%|██▋       | 157/577 [00:03<00:10, 40.07it/s][A
 28%|██▊       | 162/577 [00:03<00:10, 41.25it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 42.27it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 42.84it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.14it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.54it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.78it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.90it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.63it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.62it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.82it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.03it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.04it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.03it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.07it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.29it/s][A
 41%|████      | 237/577 [00:05<00:07, 43.96it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.79it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.78it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.92it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.03it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.20it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.14it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.19it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.24it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.11it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.74it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.60it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.87it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.90it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.06it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.08it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.19it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.13it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.93it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.69it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.89it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.02it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.12it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.15it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.20it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.19it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.95it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.90it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.79it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.81it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.06it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.05it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 44.06it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.22it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.20it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.03it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 43.76it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.78it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 40.75it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 41.82it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 42.69it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.14it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.59it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.89it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.78it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.63it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.44it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.52it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.78it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.85it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.22it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.31it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.23it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.10it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.85it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.68it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.78it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.95it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 44.01it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.32it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.20it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.03it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.93it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.67it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.71it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.92it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 44.02it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.26it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:13<00:00, 44.26it/s][A 20%|██        | 78/390 [00:36<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:53:14,544 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 17:53:14,632 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:53:19,288 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:53:19,317 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:53:19,331 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:51<45:07,  8.71s/it] 21%|██        | 80/390 [00:51<31:57,  6.18s/it] 21%|██        | 81/390 [00:52<22:45,  4.42s/it] 21%|██        | 82/390 [00:52<16:20,  3.18s/it] 21%|██▏       | 83/390 [00:52<11:51,  2.32s/it] 22%|██▏       | 84/390 [00:52<08:44,  1.71s/it] 22%|██▏       | 85/390 [00:53<06:33,  1.29s/it] 22%|██▏       | 86/390 [00:53<05:01,  1.01it/s] 22%|██▏       | 87/390 [00:53<03:57,  1.27it/s] 23%|██▎       | 88/390 [00:54<03:13,  1.56it/s] 23%|██▎       | 89/390 [00:54<02:41,  1.86it/s] 23%|██▎       | 90/390 [00:54<02:20,  2.14it/s] 23%|██▎       | 91/390 [00:55<02:04,  2.39it/s] 24%|██▎       | 92/390 [00:55<01:54,  2.61it/s] 24%|██▍       | 93/390 [00:55<01:46,  2.79it/s] 24%|██▍       | 94/390 [00:55<01:40,  2.93it/s] 24%|██▍       | 95/390 [00:56<01:36,  3.04it/s] 25%|██▍       | 96/390 [00:56<01:34,  3.12it/s] 25%|██▍       | 97/390 [00:56<01:32,  3.18it/s] 25%|██▌       | 98/390 [00:57<01:30,  3.22it/s] 25%|██▌       | 99/390 [00:57<01:29,  3.25it/s] 26%|██▌       | 100/390 [00:57<01:29,  3.25it/s] 26%|██▌       | 101/390 [00:58<01:28,  3.27it/s] 26%|██▌       | 102/390 [00:58<01:27,  3.29it/s] 26%|██▋       | 103/390 [00:58<01:26,  3.30it/s] 27%|██▋       | 104/390 [00:58<01:25,  3.33it/s] 27%|██▋       | 105/390 [00:59<01:25,  3.35it/s] 27%|██▋       | 106/390 [00:59<01:24,  3.37it/s] 27%|██▋       | 107/390 [00:59<01:23,  3.38it/s] 28%|██▊       | 108/390 [01:00<01:23,  3.39it/s] 28%|██▊       | 109/390 [01:00<01:22,  3.39it/s] 28%|██▊       | 110/390 [01:00<01:22,  3.40it/s] 28%|██▊       | 111/390 [01:01<01:22,  3.38it/s] 29%|██▊       | 112/390 [01:01<01:22,  3.39it/s] 29%|██▉       | 113/390 [01:01<01:21,  3.39it/s] 29%|██▉       | 114/390 [01:01<01:21,  3.40it/s] 29%|██▉       | 115/390 [01:02<01:20,  3.40it/s] 30%|██▉       | 116/390 [01:02<01:20,  3.40it/s] 30%|███       | 117/390 [01:02<01:20,  3.40it/s] 30%|███       | 118/390 [01:03<01:19,  3.40it/s] 31%|███       | 119/390 [01:03<01:19,  3.40it/s] 31%|███       | 120/390 [01:03<01:19,  3.40it/s] 31%|███       | 121/390 [01:03<01:19,  3.40it/s] 31%|███▏      | 122/390 [01:04<01:20,  3.34it/s] 32%|███▏      | 123/390 [01:04<01:19,  3.36it/s] 32%|███▏      | 124/390 [01:04<01:18,  3.37it/s] 32%|███▏      | 125/390 [01:05<01:18,  3.38it/s] 32%|███▏      | 126/390 [01:05<01:17,  3.39it/s] 33%|███▎      | 127/390 [01:05<01:17,  3.39it/s] 33%|███▎      | 128/390 [01:06<01:17,  3.40it/s] 33%|███▎      | 129/390 [01:06<01:16,  3.40it/s] 33%|███▎      | 130/390 [01:06<01:16,  3.40it/s] 34%|███▎      | 131/390 [01:06<01:16,  3.40it/s] 34%|███▍      | 132/390 [01:07<01:15,  3.40it/s] 34%|███▍      | 133/390 [01:07<01:18,  3.27it/s] 34%|███▍      | 134/390 [01:07<01:17,  3.31it/s] 35%|███▍      | 135/390 [01:08<01:16,  3.33it/s] 35%|███▍      | 136/390 [01:08<01:15,  3.35it/s] 35%|███▌      | 137/390 [01:08<01:15,  3.37it/s] 35%|███▌      | 138/390 [01:09<01:14,  3.38it/s] 36%|███▌      | 139/390 [01:09<01:14,  3.38it/s] 36%|███▌      | 140/390 [01:09<01:13,  3.39it/s] 36%|███▌      | 141/390 [01:09<01:13,  3.39it/s] 36%|███▋      | 142/390 [01:10<01:13,  3.39it/s] 37%|███▋      | 143/390 [01:10<01:12,  3.40it/s] 37%|███▋      | 144/390 [01:10<01:15,  3.24it/s] 37%|███▋      | 145/390 [01:11<01:14,  3.28it/s] 37%|███▋      | 146/390 [01:11<01:13,  3.32it/s] 38%|███▊      | 147/390 [01:11<01:12,  3.34it/s] 38%|███▊      | 148/390 [01:12<01:12,  3.36it/s] 38%|███▊      | 149/390 [01:12<01:11,  3.37it/s] 38%|███▊      | 150/390 [01:12<01:10,  3.38it/s] 39%|███▊      | 151/390 [01:12<01:10,  3.38it/s] 39%|███▉      | 152/390 [01:13<01:10,  3.39it/s] 39%|███▉      | 153/390 [01:13<01:09,  3.39it/s] 39%|███▉      | 154/390 [01:13<01:09,  3.39it/s] 40%|███▉      | 155/390 [01:14<01:09,  3.38it/s] 40%|████      | 156/390 [01:14<01:09,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 17:53:52,475 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:53:52,476 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:53:52,476 >>   Batch size = 8
{'eval_loss': 1.0160781145095825, 'eval_runtime': 13.1543, 'eval_samples_per_second': 350.38, 'eval_steps_per_second': 43.864, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.59it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.03it/s][A
  3%|▎         | 17/577 [00:00<00:12, 45.92it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.14it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.70it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.47it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.41it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.26it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.31it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.48it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.27it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.24it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.14it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.12it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.98it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.99it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 44.08it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.23it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.26it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.24it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.12it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.96it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.93it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.16it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.23it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.17it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.13it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 43.95it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.21it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.08it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.02it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.98it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.11it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.25it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.00it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.07it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.13it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.11it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.08it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.98it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.03it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.14it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.93it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.18it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.17it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.08it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.01it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.04it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.15it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.08it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.03it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.14it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.11it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.09it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.02it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.06it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.08it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.10it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 337/577 [00:07<00:07, 32.80it/s][A
 59%|█████▉    | 342/577 [00:07<00:06, 35.68it/s][A
 60%|██████    | 347/577 [00:07<00:06, 37.89it/s][A
 61%|██████    | 352/577 [00:08<00:05, 39.72it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 41.04it/s][A
 63%|██████▎   | 362/577 [00:08<00:05, 42.04it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 42.67it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.10it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.01it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 42.97it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.33it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.62it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.98it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.08it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.18it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.29it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.12it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.82it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.60it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.76it/s][A
 76%|███████▌  | 437/577 [00:10<00:03, 43.96it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.18it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.17it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.32it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.24it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.04it/s][A
 81%|████████  | 467/577 [00:10<00:02, 41.19it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 42.06it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 42.69it/s][A
 84%|████████▎ | 482/577 [00:11<00:02, 43.16it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.49it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.82it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.88it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.65it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.68it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.71it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.87it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 44.06it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.18it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.28it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.26it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.98it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.77it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.83it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.83it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.87it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 44.16it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.19it/s][A 40%|████      | 156/390 [01:27<01:09,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:54:05,735 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:54:05,761 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:54:11,757 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:54:11,782 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:54:11,791 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:45<37:29,  9.65s/it] 41%|████      | 158/390 [01:46<26:29,  6.85s/it] 41%|████      | 159/390 [01:46<18:48,  4.89s/it] 41%|████      | 160/390 [01:46<13:27,  3.51s/it] 41%|████▏     | 161/390 [01:47<09:43,  2.55s/it] 42%|████▏     | 162/390 [01:47<07:07,  1.87s/it] 42%|████▏     | 163/390 [01:47<05:18,  1.40s/it] 42%|████▏     | 164/390 [01:48<04:02,  1.07s/it] 42%|████▏     | 165/390 [01:48<03:09,  1.19it/s] 43%|████▎     | 166/390 [01:48<02:31,  1.47it/s] 43%|████▎     | 167/390 [01:48<02:05,  1.77it/s] 43%|████▎     | 168/390 [01:49<01:49,  2.03it/s] 43%|████▎     | 169/390 [01:49<01:36,  2.30it/s] 44%|████▎     | 170/390 [01:49<01:26,  2.53it/s] 44%|████▍     | 171/390 [01:50<01:20,  2.73it/s] 44%|████▍     | 172/390 [01:50<01:15,  2.88it/s] 44%|████▍     | 173/390 [01:50<01:12,  3.00it/s] 45%|████▍     | 174/390 [01:51<01:11,  3.02it/s] 45%|████▍     | 175/390 [01:51<01:19,  2.71it/s] 45%|████▌     | 176/390 [01:51<01:14,  2.89it/s] 45%|████▌     | 177/390 [01:52<01:10,  3.03it/s] 46%|████▌     | 178/390 [01:52<01:08,  3.11it/s] 46%|████▌     | 179/390 [01:52<01:06,  3.20it/s] 46%|████▌     | 180/390 [01:53<01:06,  3.16it/s] 46%|████▋     | 181/390 [01:53<01:04,  3.23it/s] 47%|████▋     | 182/390 [01:53<01:03,  3.28it/s] 47%|████▋     | 183/390 [01:53<01:02,  3.32it/s] 47%|████▋     | 184/390 [01:54<01:01,  3.34it/s] 47%|████▋     | 185/390 [01:54<01:01,  3.36it/s] 48%|████▊     | 186/390 [01:54<01:00,  3.38it/s] 48%|████▊     | 187/390 [01:55<00:59,  3.39it/s] 48%|████▊     | 188/390 [01:55<00:59,  3.39it/s] 48%|████▊     | 189/390 [01:55<00:59,  3.40it/s] 49%|████▊     | 190/390 [01:55<00:58,  3.40it/s] 49%|████▉     | 191/390 [01:56<01:00,  3.31it/s] 49%|████▉     | 192/390 [01:56<00:59,  3.34it/s] 49%|████▉     | 193/390 [01:56<00:58,  3.36it/s] 50%|████▉     | 194/390 [01:57<00:58,  3.37it/s] 50%|█████     | 195/390 [01:57<00:57,  3.38it/s] 50%|█████     | 196/390 [01:57<00:57,  3.39it/s] 51%|█████     | 197/390 [01:58<00:56,  3.39it/s] 51%|█████     | 198/390 [01:58<00:56,  3.40it/s] 51%|█████     | 199/390 [01:58<00:56,  3.40it/s] 51%|█████▏    | 200/390 [01:58<00:55,  3.40it/s] 52%|█████▏    | 201/390 [01:59<00:55,  3.40it/s] 52%|█████▏    | 202/390 [01:59<01:06,  2.81it/s] 52%|█████▏    | 203/390 [02:00<01:03,  2.96it/s] 52%|█████▏    | 204/390 [02:00<01:00,  3.08it/s] 53%|█████▎    | 205/390 [02:00<00:58,  3.17it/s] 53%|█████▎    | 206/390 [02:00<00:56,  3.24it/s] 53%|█████▎    | 207/390 [02:01<00:55,  3.28it/s] 53%|█████▎    | 208/390 [02:01<00:54,  3.32it/s] 54%|█████▎    | 209/390 [02:01<00:54,  3.34it/s] 54%|█████▍    | 210/390 [02:02<00:53,  3.36it/s] 54%|█████▍    | 211/390 [02:02<00:53,  3.37it/s] 54%|█████▍    | 212/390 [02:02<00:52,  3.36it/s] 55%|█████▍    | 213/390 [02:02<00:52,  3.37it/s] 55%|█████▍    | 214/390 [02:03<00:52,  3.38it/s] 55%|█████▌    | 215/390 [02:03<00:51,  3.39it/s] 55%|█████▌    | 216/390 [02:03<00:51,  3.39it/s] 56%|█████▌    | 217/390 [02:04<00:50,  3.40it/s] 56%|█████▌    | 218/390 [02:04<00:50,  3.40it/s] 56%|█████▌    | 219/390 [02:04<00:50,  3.40it/s] 56%|█████▋    | 220/390 [02:05<00:49,  3.40it/s] 57%|█████▋    | 221/390 [02:05<00:49,  3.40it/s] 57%|█████▋    | 222/390 [02:05<00:49,  3.40it/s] 57%|█████▋    | 223/390 [02:05<00:49,  3.38it/s] 57%|█████▋    | 224/390 [02:06<00:49,  3.39it/s] 58%|█████▊    | 225/390 [02:06<00:48,  3.39it/s] 58%|█████▊    | 226/390 [02:06<00:48,  3.39it/s] 58%|█████▊    | 227/390 [02:07<00:47,  3.40it/s] 58%|█████▊    | 228/390 [02:07<00:47,  3.40it/s] 59%|█████▊    | 229/390 [02:07<00:47,  3.40it/s] 59%|█████▉    | 230/390 [02:07<00:47,  3.40it/s] 59%|█████▉    | 231/390 [02:08<00:46,  3.40it/s] 59%|█████▉    | 232/390 [02:08<00:46,  3.40it/s] 60%|█████▉    | 233/390 [02:08<00:46,  3.40it/s] 60%|██████    | 234/390 [02:09<00:46,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 17:54:47,226 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:54:47,227 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:54:47,227 >>   Batch size = 8
{'eval_loss': 1.034571647644043, 'eval_runtime': 13.2317, 'eval_samples_per_second': 348.331, 'eval_steps_per_second': 43.607, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.59it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.71it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.78it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.65it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.03it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.61it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.23it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.03it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.18it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.36it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.46it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.47it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.27it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.20it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.08it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.85it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.88it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.99it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.18it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.38it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.42it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.30it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.16it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.84it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.82it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.01it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.20it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.41it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.41it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.25it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.12it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.87it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.85it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.82it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.02it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.18it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.31it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.23it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.25it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.12it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.80it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.89it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.77it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.05it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.30it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.41it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.35it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.25it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.09it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.99it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.86it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.87it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.10it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.27it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.38it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.33it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.19it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.99it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.83it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.90it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.10it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.25it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.32it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.27it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.21it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.09it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.93it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.01it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.11it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.22it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.23it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.19it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.24it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.10it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.01it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.74it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.87it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.07it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.07it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.13it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.14it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.05it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.99it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 43.95it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.02it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.05it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.14it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.11it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.21it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.29it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.10it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.93it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.95it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.96it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.13it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.17it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.24it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.31it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.19it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.00it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 42.75it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 43.26it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 43.65it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.80it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.82it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.10it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.04it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.94it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.88it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 43.98it/s][A 60%|██████    | 234/390 [02:22<00:46,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:55:00,331 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 17:55:00,372 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:55:06,294 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:55:06,418 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:55:06,453 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:41<25:23,  9.83s/it] 61%|██████    | 236/390 [02:41<17:56,  6.99s/it] 61%|██████    | 237/390 [02:41<12:42,  4.98s/it] 61%|██████    | 238/390 [02:42<09:04,  3.58s/it] 61%|██████▏   | 239/390 [02:42<06:31,  2.60s/it] 62%|██████▏   | 240/390 [02:42<04:46,  1.91s/it] 62%|██████▏   | 241/390 [02:43<03:32,  1.42s/it] 62%|██████▏   | 242/390 [02:43<02:40,  1.09s/it] 62%|██████▏   | 243/390 [02:43<02:05,  1.18it/s] 63%|██████▎   | 244/390 [02:43<01:40,  1.46it/s] 63%|██████▎   | 245/390 [02:44<01:22,  1.75it/s] 63%|██████▎   | 246/390 [02:44<01:11,  2.01it/s] 63%|██████▎   | 247/390 [02:44<01:02,  2.28it/s] 64%|██████▎   | 248/390 [02:45<00:56,  2.52it/s] 64%|██████▍   | 249/390 [02:45<00:51,  2.72it/s] 64%|██████▍   | 250/390 [02:45<00:48,  2.87it/s] 64%|██████▍   | 251/390 [02:46<00:46,  3.00it/s] 65%|██████▍   | 252/390 [02:46<00:44,  3.09it/s] 65%|██████▍   | 253/390 [02:46<00:43,  3.15it/s] 65%|██████▌   | 254/390 [02:47<00:42,  3.21it/s] 65%|██████▌   | 255/390 [02:47<00:41,  3.24it/s] 66%|██████▌   | 256/390 [02:47<00:41,  3.26it/s] 66%|██████▌   | 257/390 [02:47<00:40,  3.28it/s] 66%|██████▌   | 258/390 [02:48<00:40,  3.30it/s] 66%|██████▋   | 259/390 [02:48<00:39,  3.31it/s] 67%|██████▋   | 260/390 [02:48<00:39,  3.32it/s] 67%|██████▋   | 261/390 [02:49<00:38,  3.32it/s] 67%|██████▋   | 262/390 [02:49<00:38,  3.32it/s] 67%|██████▋   | 263/390 [02:49<00:38,  3.33it/s] 68%|██████▊   | 264/390 [02:50<00:37,  3.33it/s] 68%|██████▊   | 265/390 [02:50<00:37,  3.33it/s] 68%|██████▊   | 266/390 [02:50<00:37,  3.30it/s] 68%|██████▊   | 267/390 [02:50<00:37,  3.31it/s] 69%|██████▊   | 268/390 [02:51<00:36,  3.30it/s] 69%|██████▉   | 269/390 [02:51<00:40,  3.02it/s] 69%|██████▉   | 270/390 [02:51<00:38,  3.11it/s] 69%|██████▉   | 271/390 [02:52<00:37,  3.17it/s] 70%|██████▉   | 272/390 [02:52<00:36,  3.21it/s] 70%|███████   | 273/390 [02:52<00:36,  3.24it/s] 70%|███████   | 274/390 [02:53<00:35,  3.27it/s] 71%|███████   | 275/390 [02:53<00:34,  3.29it/s] 71%|███████   | 276/390 [02:53<00:36,  3.10it/s] 71%|███████   | 277/390 [02:54<00:35,  3.16it/s] 71%|███████▏  | 278/390 [02:54<00:34,  3.21it/s] 72%|███████▏  | 279/390 [02:54<00:34,  3.24it/s] 72%|███████▏  | 280/390 [02:55<00:33,  3.27it/s] 72%|███████▏  | 281/390 [02:55<00:33,  3.28it/s] 72%|███████▏  | 282/390 [02:55<00:32,  3.29it/s] 73%|███████▎  | 283/390 [02:55<00:32,  3.30it/s] 73%|███████▎  | 284/390 [02:56<00:32,  3.31it/s] 73%|███████▎  | 285/390 [02:56<00:31,  3.31it/s] 73%|███████▎  | 286/390 [02:56<00:31,  3.31it/s] 74%|███████▎  | 287/390 [02:57<00:31,  3.31it/s] 74%|███████▍  | 288/390 [02:57<00:31,  3.23it/s] 74%|███████▍  | 289/390 [02:57<00:31,  3.26it/s] 74%|███████▍  | 290/390 [02:58<00:30,  3.28it/s] 75%|███████▍  | 291/390 [02:58<00:30,  3.29it/s] 75%|███████▍  | 292/390 [02:58<00:29,  3.29it/s] 75%|███████▌  | 293/390 [02:58<00:29,  3.30it/s] 75%|███████▌  | 294/390 [02:59<00:29,  3.30it/s] 76%|███████▌  | 295/390 [02:59<00:28,  3.31it/s] 76%|███████▌  | 296/390 [02:59<00:28,  3.31it/s] 76%|███████▌  | 297/390 [03:00<00:28,  3.31it/s] 76%|███████▋  | 298/390 [03:00<00:28,  3.25it/s] 77%|███████▋  | 299/390 [03:00<00:27,  3.27it/s] 77%|███████▋  | 300/390 [03:01<00:27,  3.29it/s] 77%|███████▋  | 301/390 [03:01<00:26,  3.30it/s] 77%|███████▋  | 302/390 [03:01<00:26,  3.31it/s] 78%|███████▊  | 303/390 [03:01<00:26,  3.31it/s] 78%|███████▊  | 304/390 [03:02<00:25,  3.31it/s] 78%|███████▊  | 305/390 [03:02<00:25,  3.31it/s] 78%|███████▊  | 306/390 [03:02<00:25,  3.31it/s] 79%|███████▊  | 307/390 [03:03<00:25,  3.31it/s] 79%|███████▉  | 308/390 [03:03<00:26,  3.13it/s] 79%|███████▉  | 309/390 [03:03<00:25,  3.18it/s] 79%|███████▉  | 310/390 [03:04<00:24,  3.22it/s] 80%|███████▉  | 311/390 [03:04<00:24,  3.25it/s] 80%|████████  | 312/390 [03:04<00:23,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 17:55:42,846 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:55:42,846 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:55:42,846 >>   Batch size = 8
{'eval_loss': 1.0523054599761963, 'eval_runtime': 13.0822, 'eval_samples_per_second': 352.311, 'eval_steps_per_second': 44.106, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.54it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.19it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.39it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.64it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.78it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.50it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.30it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.95it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.16it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.25it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.39it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.49it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.15it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.14it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.04it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.92it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.87it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.97it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.27it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.39it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.23it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.07it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.05it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.86it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.94it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.06it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.12it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.30it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.31it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.14it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.91it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.83it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.85it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.09it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.19it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.28it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.27it/s][A
 35%|███▌      | 202/577 [00:04<00:11, 32.51it/s][A
 36%|███▌      | 207/577 [00:04<00:10, 35.44it/s][A
 37%|███▋      | 212/577 [00:04<00:09, 37.70it/s][A
 38%|███▊      | 217/577 [00:05<00:09, 39.64it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 41.05it/s][A
 39%|███▉      | 227/577 [00:05<00:08, 42.02it/s][A
 40%|████      | 232/577 [00:05<00:08, 42.84it/s][A
 41%|████      | 237/577 [00:05<00:07, 43.11it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.07it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.00it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.19it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.52it/s][A
 45%|████▌     | 262/577 [00:06<00:07, 43.87it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.13it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.28it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.31it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.17it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.89it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.66it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.68it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.93it/s][A
 53%|█████▎    | 307/577 [00:07<00:06, 44.14it/s][A
 54%|█████▍    | 312/577 [00:07<00:05, 44.33it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.32it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.35it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.29it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 43.87it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.75it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.77it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.94it/s][A
 61%|██████    | 352/577 [00:08<00:05, 44.12it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.32it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.26it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.11it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.02it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 43.94it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.66it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.75it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.97it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 44.10it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.33it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.16it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.10it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.06it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 43.93it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.90it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.84it/s][A
 76%|███████▌  | 437/577 [00:10<00:03, 44.12it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.31it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.30it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.22it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.11it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.96it/s][A
 81%|████████  | 467/577 [00:10<00:02, 41.21it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 42.22it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 42.74it/s][A
 84%|████████▎ | 482/577 [00:11<00:02, 43.30it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 43.70it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.76it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.72it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.88it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.61it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.68it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.92it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.13it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 44.24it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.29it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.18it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.15it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 43.94it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.79it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.75it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.04it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.24it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 44.32it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.26it/s][A 80%|████████  | 312/390 [03:18<00:23,  3.27it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:55:56,220 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 17:55:56,376 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:00,779 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:00,827 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:00,840 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:31<10:38,  8.29s/it] 81%|████████  | 314/390 [03:32<07:28,  5.90s/it] 81%|████████  | 315/390 [03:32<05:16,  4.22s/it] 81%|████████  | 316/390 [03:32<03:45,  3.05s/it] 81%|████████▏ | 317/390 [03:32<02:42,  2.22s/it] 82%|████████▏ | 318/390 [03:33<01:58,  1.65s/it] 82%|████████▏ | 319/390 [03:33<01:28,  1.24s/it] 82%|████████▏ | 320/390 [03:33<01:07,  1.04it/s] 82%|████████▏ | 321/390 [03:34<00:52,  1.31it/s] 83%|████████▎ | 322/390 [03:34<00:42,  1.60it/s] 83%|████████▎ | 323/390 [03:34<00:35,  1.90it/s] 83%|████████▎ | 324/390 [03:35<00:31,  2.10it/s] 83%|████████▎ | 325/390 [03:35<00:27,  2.36it/s] 84%|████████▎ | 326/390 [03:35<00:24,  2.59it/s] 84%|████████▍ | 327/390 [03:35<00:22,  2.77it/s] 84%|████████▍ | 328/390 [03:36<00:21,  2.92it/s] 84%|████████▍ | 329/390 [03:36<00:20,  3.03it/s] 85%|████████▍ | 330/390 [03:36<00:19,  3.11it/s] 85%|████████▍ | 331/390 [03:37<00:18,  3.17it/s] 85%|████████▌ | 332/390 [03:37<00:18,  3.22it/s] 85%|████████▌ | 333/390 [03:37<00:17,  3.25it/s] 86%|████████▌ | 334/390 [03:38<00:17,  3.24it/s] 86%|████████▌ | 335/390 [03:38<00:16,  3.26it/s] 86%|████████▌ | 336/390 [03:38<00:16,  3.28it/s] 86%|████████▋ | 337/390 [03:39<00:16,  3.29it/s] 87%|████████▋ | 338/390 [03:39<00:15,  3.30it/s] 87%|████████▋ | 339/390 [03:39<00:15,  3.31it/s] 87%|████████▋ | 340/390 [03:39<00:15,  3.31it/s] 87%|████████▋ | 341/390 [03:40<00:14,  3.32it/s] 88%|████████▊ | 342/390 [03:40<00:14,  3.31it/s] 88%|████████▊ | 343/390 [03:40<00:14,  3.32it/s] 88%|████████▊ | 344/390 [03:41<00:13,  3.30it/s] 88%|████████▊ | 345/390 [03:41<00:13,  3.30it/s] 89%|████████▊ | 346/390 [03:41<00:13,  3.31it/s] 89%|████████▉ | 347/390 [03:42<00:13,  3.31it/s] 89%|████████▉ | 348/390 [03:42<00:12,  3.31it/s] 89%|████████▉ | 349/390 [03:42<00:12,  3.32it/s] 90%|████████▉ | 350/390 [03:42<00:12,  3.32it/s] 90%|█████████ | 351/390 [03:43<00:11,  3.32it/s] 90%|█████████ | 352/390 [03:43<00:11,  3.32it/s] 91%|█████████ | 353/390 [03:43<00:11,  3.32it/s] 91%|█████████ | 354/390 [03:44<00:10,  3.28it/s] 91%|█████████ | 355/390 [03:44<00:10,  3.29it/s] 91%|█████████▏| 356/390 [03:44<00:10,  3.30it/s] 92%|█████████▏| 357/390 [03:45<00:09,  3.31it/s] 92%|█████████▏| 358/390 [03:45<00:09,  3.31it/s] 92%|█████████▏| 359/390 [03:45<00:09,  3.32it/s] 92%|█████████▏| 360/390 [03:45<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:46<00:08,  3.32it/s] 93%|█████████▎| 362/390 [03:46<00:08,  3.32it/s] 93%|█████████▎| 363/390 [03:46<00:08,  3.32it/s] 93%|█████████▎| 364/390 [03:47<00:08,  3.08it/s] 94%|█████████▎| 365/390 [03:47<00:07,  3.15it/s] 94%|█████████▍| 366/390 [03:47<00:07,  3.20it/s] 94%|█████████▍| 367/390 [03:48<00:07,  3.23it/s] 94%|█████████▍| 368/390 [03:48<00:06,  3.25it/s] 95%|█████████▍| 369/390 [03:48<00:06,  3.27it/s] 95%|█████████▍| 370/390 [03:49<00:06,  3.28it/s] 95%|█████████▌| 371/390 [03:49<00:05,  3.30it/s] 95%|█████████▌| 372/390 [03:49<00:05,  3.30it/s] 96%|█████████▌| 373/390 [03:49<00:05,  3.31it/s] 96%|█████████▌| 374/390 [03:50<00:04,  3.25it/s] 96%|█████████▌| 375/390 [03:50<00:04,  3.27it/s] 96%|█████████▋| 376/390 [03:50<00:04,  3.29it/s] 97%|█████████▋| 377/390 [03:51<00:03,  3.30it/s] 97%|█████████▋| 378/390 [03:51<00:03,  3.24it/s] 97%|█████████▋| 379/390 [03:51<00:03,  3.14it/s] 97%|█████████▋| 380/390 [03:52<00:03,  3.19it/s] 98%|█████████▊| 381/390 [03:52<00:02,  3.23it/s] 98%|█████████▊| 382/390 [03:52<00:02,  3.25it/s] 98%|█████████▊| 383/390 [03:53<00:02,  3.27it/s] 98%|█████████▊| 384/390 [03:53<00:01,  3.27it/s] 99%|█████████▊| 385/390 [03:53<00:01,  3.28it/s] 99%|█████████▉| 386/390 [03:53<00:01,  3.30it/s] 99%|█████████▉| 387/390 [03:54<00:00,  3.30it/s] 99%|█████████▉| 388/390 [03:54<00:00,  3.31it/s]100%|█████████▉| 389/390 [03:54<00:00,  3.31it/s]100%|██████████| 390/390 [03:55<00:00,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 17:56:33,189 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:56:33,190 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:56:33,190 >>   Batch size = 8
{'eval_loss': 1.0604016780853271, 'eval_runtime': 13.2349, 'eval_samples_per_second': 348.246, 'eval_steps_per_second': 43.597, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.33it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.06it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.22it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.26it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.88it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.51it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.33it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.14it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.27it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.37it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.26it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.25it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.22it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.98it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.94it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.96it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.99it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.08it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.18it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.23it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.25it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.09it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.01it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.91it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.03it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.92it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.13it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.21it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.29it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 43.99it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 43.95it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.75it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.86it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.90it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.02it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.14it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.22it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.26it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.09it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.95it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.97it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.02it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.08it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.99it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.26it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.20it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.15it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.00it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.93it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.86it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.03it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.18it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.16it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.29it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.11it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.11it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.91it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.86it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.80it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.91it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.09it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.24it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.24it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.16it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.85it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.91it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.07it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.21it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.15it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.12it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.20it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.10it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.99it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.88it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.85it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.11it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.10it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.14it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.19it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.06it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.09it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.99it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.92it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.95it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.08it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.13it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.08it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.17it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.96it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.85it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.89it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.03it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.04it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.09it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.18it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.19it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.15it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.00it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.90it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.96it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.00it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.06it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.24it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.26it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.06it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.98it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.99it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.92it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.85it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.96it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.13it/s][A100%|██████████| 390/390 [04:08<00:00,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:56:46,289 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 17:56:46,311 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:50,213 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:50,231 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:50,260 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:56:59,142 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:56:59,155 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78 (score: 1.0160781145095825).
                                                 100%|██████████| 390/390 [04:24<00:00,  3.31it/s]100%|██████████| 390/390 [04:24<00:00,  1.48it/s]
[INFO|trainer.py:1894] 2023-08-28 17:57:02,479 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 17:57:02,505 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:05,095 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:05,114 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:05,125 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:57:05,333 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   train_loss               =     0.4901
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   train_runtime            = 0:04:24.37
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   train_samples_per_second =     94.544
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:05,333 >>   train_steps_per_second   =      1.475
{'eval_loss': 1.0636523962020874, 'eval_runtime': 13.0888, 'eval_samples_per_second': 352.133, 'eval_steps_per_second': 44.083, 'epoch': 4.99}
{'train_runtime': 264.3747, 'train_samples_per_second': 94.544, 'train_steps_per_second': 1.475, 'train_loss': 0.4901228293394431, 'epoch': 4.99}
08/28/2023 17:57:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:57:05,374 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:05,374 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 17:57:05,374 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.73it/s]  2%|▏         | 12/577 [00:00<00:11, 48.19it/s]  3%|▎         | 17/577 [00:00<00:11, 46.78it/s]  4%|▍         | 22/577 [00:00<00:12, 46.00it/s]  5%|▍         | 27/577 [00:00<00:12, 45.56it/s]  6%|▌         | 32/577 [00:00<00:12, 45.15it/s]  6%|▋         | 37/577 [00:00<00:11, 45.10it/s]  7%|▋         | 42/577 [00:00<00:12, 44.55it/s]  8%|▊         | 47/577 [00:01<00:12, 44.07it/s]  9%|▉         | 52/577 [00:01<00:11, 43.84it/s] 10%|▉         | 57/577 [00:01<00:11, 44.16it/s] 11%|█         | 62/577 [00:01<00:11, 44.30it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.38it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.49it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.29it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.36it/s] 15%|█▌        | 87/577 [00:01<00:11, 44.06it/s] 16%|█▌        | 92/577 [00:02<00:11, 43.85it/s] 17%|█▋        | 97/577 [00:02<00:10, 43.88it/s] 18%|█▊        | 102/577 [00:02<00:10, 44.06it/s] 19%|█▊        | 107/577 [00:02<00:10, 44.20it/s] 19%|█▉        | 112/577 [00:02<00:10, 44.37it/s] 20%|██        | 117/577 [00:02<00:10, 44.39it/s] 21%|██        | 122/577 [00:02<00:10, 44.23it/s] 22%|██▏       | 127/577 [00:02<00:10, 44.11it/s] 23%|██▎       | 132/577 [00:02<00:10, 44.00it/s] 24%|██▎       | 137/577 [00:03<00:10, 43.96it/s] 25%|██▍       | 142/577 [00:03<00:09, 43.91it/s] 25%|██▌       | 147/577 [00:03<00:09, 44.04it/s] 26%|██▋       | 152/577 [00:03<00:09, 44.24it/s] 27%|██▋       | 157/577 [00:03<00:09, 44.35it/s] 28%|██▊       | 162/577 [00:03<00:09, 44.34it/s] 29%|██▉       | 167/577 [00:03<00:09, 44.10it/s] 30%|██▉       | 172/577 [00:03<00:09, 44.04it/s] 31%|███       | 177/577 [00:03<00:09, 43.96it/s] 32%|███▏      | 182/577 [00:04<00:08, 44.03it/s] 32%|███▏      | 187/577 [00:04<00:08, 44.03it/s] 33%|███▎      | 192/577 [00:04<00:08, 44.09it/s] 34%|███▍      | 197/577 [00:04<00:08, 44.18it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.39it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.30it/s] 37%|███▋      | 212/577 [00:04<00:08, 44.11it/s] 38%|███▊      | 217/577 [00:04<00:08, 43.99it/s] 38%|███▊      | 222/577 [00:04<00:08, 44.00it/s] 39%|███▉      | 227/577 [00:05<00:07, 43.97it/s] 40%|████      | 232/577 [00:05<00:07, 44.01it/s] 41%|████      | 237/577 [00:05<00:07, 44.00it/s] 42%|████▏     | 242/577 [00:05<00:07, 44.16it/s] 43%|████▎     | 247/577 [00:05<00:07, 44.27it/s] 44%|████▎     | 252/577 [00:05<00:07, 44.32it/s] 45%|████▍     | 257/577 [00:05<00:07, 44.11it/s] 45%|████▌     | 262/577 [00:05<00:07, 44.04it/s] 46%|████▋     | 267/577 [00:06<00:07, 44.01it/s] 47%|████▋     | 272/577 [00:06<00:06, 44.00it/s] 48%|████▊     | 277/577 [00:06<00:06, 44.08it/s] 49%|████▉     | 282/577 [00:06<00:06, 44.07it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.11it/s] 51%|█████     | 292/577 [00:06<00:06, 44.12it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.21it/s] 52%|█████▏    | 302/577 [00:06<00:06, 44.14it/s] 53%|█████▎    | 307/577 [00:06<00:06, 44.05it/s] 54%|█████▍    | 312/577 [00:07<00:06, 43.99it/s] 55%|█████▍    | 317/577 [00:07<00:05, 43.93it/s] 56%|█████▌    | 322/577 [00:07<00:05, 44.06it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.14it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.17it/s] 58%|█████▊    | 337/577 [00:07<00:05, 44.26it/s] 59%|█████▉    | 342/577 [00:07<00:05, 44.22it/s] 60%|██████    | 347/577 [00:07<00:05, 44.13it/s] 61%|██████    | 352/577 [00:07<00:05, 43.97it/s] 62%|██████▏   | 357/577 [00:08<00:05, 43.88it/s] 63%|██████▎   | 362/577 [00:08<00:05, 42.52it/s] 64%|██████▎   | 367/577 [00:08<00:04, 43.16it/s] 64%|██████▍   | 372/577 [00:08<00:04, 43.51it/s] 65%|██████▌   | 377/577 [00:08<00:04, 43.71it/s] 66%|██████▌   | 382/577 [00:08<00:04, 43.92it/s] 67%|██████▋   | 387/577 [00:08<00:04, 43.87it/s] 68%|██████▊   | 392/577 [00:08<00:04, 43.91it/s] 69%|██████▉   | 397/577 [00:08<00:04, 43.85it/s] 70%|██████▉   | 402/577 [00:09<00:04, 43.72it/s] 71%|███████   | 407/577 [00:09<00:03, 43.70it/s] 71%|███████▏  | 412/577 [00:09<00:03, 43.96it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.11it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.13it/s] 74%|███████▍  | 427/577 [00:09<00:03, 44.18it/s] 75%|███████▍  | 432/577 [00:09<00:03, 44.22it/s] 76%|███████▌  | 437/577 [00:09<00:03, 44.10it/s] 77%|███████▋  | 442/577 [00:10<00:03, 43.90it/s] 77%|███████▋  | 447/577 [00:10<00:02, 43.88it/s] 78%|███████▊  | 452/577 [00:10<00:02, 43.87it/s] 79%|███████▉  | 457/577 [00:10<00:02, 44.04it/s] 80%|████████  | 462/577 [00:10<00:02, 44.15it/s] 81%|████████  | 467/577 [00:10<00:02, 44.14it/s] 82%|████████▏ | 472/577 [00:10<00:02, 44.19it/s] 83%|████████▎ | 477/577 [00:10<00:02, 44.19it/s] 84%|████████▎ | 482/577 [00:10<00:02, 44.03it/s] 84%|████████▍ | 487/577 [00:11<00:02, 43.90it/s] 85%|████████▌ | 492/577 [00:11<00:01, 43.93it/s] 86%|████████▌ | 497/577 [00:11<00:02, 37.70it/s] 87%|████████▋ | 502/577 [00:11<00:01, 39.45it/s] 88%|████████▊ | 507/577 [00:11<00:01, 40.84it/s] 89%|████████▊ | 512/577 [00:11<00:01, 41.93it/s] 90%|████████▉ | 517/577 [00:11<00:01, 42.67it/s] 90%|█████████ | 522/577 [00:11<00:01, 43.24it/s] 91%|█████████▏| 527/577 [00:11<00:01, 43.66it/s] 92%|█████████▏| 532/577 [00:12<00:01, 43.80it/s] 93%|█████████▎| 537/577 [00:12<00:00, 43.48it/s] 94%|█████████▍| 542/577 [00:12<00:00, 43.30it/s] 95%|█████████▍| 547/577 [00:12<00:00, 43.38it/s] 96%|█████████▌| 552/577 [00:12<00:00, 43.75it/s] 97%|█████████▋| 557/577 [00:12<00:00, 43.94it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.09it/s] 98%|█████████▊| 567/577 [00:12<00:00, 44.22it/s] 99%|█████████▉| 572/577 [00:13<00:00, 44.28it/s]100%|██████████| 577/577 [00:13<00:00, 44.08it/s]100%|██████████| 577/577 [00:13<00:00, 43.95it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:57:18,524 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   eval_loss               =     1.0161
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   eval_runtime            = 0:00:13.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   eval_samples_per_second =    350.516
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   eval_steps_per_second   =     43.881
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:18,524 >>   perplexity              =     2.7623
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:27,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:27,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:27,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:57:28,242 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:57:28,243 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:57:28,831 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:57:29,902 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:57:29,902 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:32,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:32,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:32,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:32,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:32,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:57:32,465 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:57:32,467 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:57:32,725 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:57:32,902 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:57:32,902 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.27it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.33it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:07,  1.24it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.31it/s]Extractor Predicting: 16it [00:12,  1.32it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.29it/s]Extractor Predicting: 20it [00:15,  1.31it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.29it/s]Extractor Predicting: 23it [00:17,  1.28it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.28it/s]Extractor Predicting: 27it [00:20,  1.32it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.32it/s]Extractor Predicting: 30it [00:23,  1.34it/s]Extractor Predicting: 31it [00:23,  1.32it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.30it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.26it/s]Extractor Predicting: 42it [00:32,  1.19it/s]Extractor Predicting: 43it [00:33,  1.21it/s]Extractor Predicting: 44it [00:34,  1.21it/s]Extractor Predicting: 45it [00:35,  1.21it/s]Extractor Predicting: 46it [00:35,  1.21it/s]Extractor Predicting: 47it [00:36,  1.22it/s]Extractor Predicting: 48it [00:37,  1.22it/s]Extractor Predicting: 49it [00:38,  1.26it/s]Extractor Predicting: 50it [00:39,  1.26it/s]Extractor Predicting: 51it [00:39,  1.28it/s]Extractor Predicting: 52it [00:40,  1.25it/s]Extractor Predicting: 53it [00:41,  1.22it/s]Extractor Predicting: 54it [00:42,  1.23it/s]Extractor Predicting: 55it [00:43,  1.24it/s]Extractor Predicting: 56it [00:43,  1.26it/s]Extractor Predicting: 57it [00:44,  1.24it/s]Extractor Predicting: 58it [00:45,  1.23it/s]Extractor Predicting: 59it [00:46,  1.26it/s]Extractor Predicting: 60it [00:47,  1.27it/s]Extractor Predicting: 61it [00:47,  1.22it/s]Extractor Predicting: 62it [00:48,  1.21it/s]Extractor Predicting: 63it [00:49,  1.21it/s]Extractor Predicting: 64it [00:50,  1.21it/s]Extractor Predicting: 65it [00:51,  1.21it/s]Extractor Predicting: 66it [00:52,  1.25it/s]Extractor Predicting: 67it [00:52,  1.26it/s]Extractor Predicting: 68it [00:53,  1.29it/s]Extractor Predicting: 69it [00:54,  1.30it/s]Extractor Predicting: 70it [00:55,  1.28it/s]Extractor Predicting: 71it [00:55,  1.31it/s]Extractor Predicting: 72it [00:56,  1.34it/s]Extractor Predicting: 73it [00:57,  1.32it/s]Extractor Predicting: 74it [00:58,  1.34it/s]Extractor Predicting: 75it [00:58,  1.34it/s]Extractor Predicting: 76it [00:59,  1.31it/s]Extractor Predicting: 77it [01:00,  1.30it/s]Extractor Predicting: 78it [01:01,  1.32it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.30it/s]Extractor Predicting: 82it [01:04,  1.30it/s]Extractor Predicting: 83it [01:04,  1.29it/s]Extractor Predicting: 84it [01:05,  1.32it/s]Extractor Predicting: 85it [01:06,  1.37it/s]Extractor Predicting: 86it [01:07,  1.37it/s]Extractor Predicting: 87it [01:07,  1.39it/s]Extractor Predicting: 88it [01:08,  1.31it/s]Extractor Predicting: 89it [01:09,  1.29it/s]Extractor Predicting: 90it [01:10,  1.28it/s]Extractor Predicting: 91it [01:10,  1.29it/s]Extractor Predicting: 92it [01:11,  1.30it/s]Extractor Predicting: 93it [01:12,  1.23it/s]Extractor Predicting: 94it [01:13,  1.26it/s]Extractor Predicting: 95it [01:14,  1.27it/s]Extractor Predicting: 96it [01:14,  1.29it/s]Extractor Predicting: 97it [01:15,  1.24it/s]Extractor Predicting: 98it [01:16,  1.22it/s]Extractor Predicting: 99it [01:17,  1.25it/s]Extractor Predicting: 100it [01:18,  1.24it/s]Extractor Predicting: 101it [01:19,  1.05it/s]Extractor Predicting: 102it [01:20,  1.11it/s]Extractor Predicting: 103it [01:21,  1.15it/s]Extractor Predicting: 104it [01:21,  1.21it/s]Extractor Predicting: 105it [01:22,  1.24it/s]Extractor Predicting: 106it [01:23,  1.22it/s]Extractor Predicting: 107it [01:24,  1.27it/s]Extractor Predicting: 108it [01:24,  1.30it/s]Extractor Predicting: 109it [01:25,  1.27it/s]Extractor Predicting: 110it [01:26,  1.28it/s]Extractor Predicting: 111it [01:27,  1.27it/s]Extractor Predicting: 112it [01:28,  1.27it/s]Extractor Predicting: 113it [01:28,  1.28it/s]Extractor Predicting: 114it [01:29,  1.28it/s]Extractor Predicting: 115it [01:30,  1.29it/s]Extractor Predicting: 116it [01:31,  1.29it/s]Extractor Predicting: 117it [01:32,  1.26it/s]Extractor Predicting: 118it [01:32,  1.28it/s]Extractor Predicting: 119it [01:33,  1.33it/s]Extractor Predicting: 120it [01:34,  1.33it/s]Extractor Predicting: 121it [01:34,  1.32it/s]Extractor Predicting: 122it [01:35,  1.31it/s]Extractor Predicting: 123it [01:36,  1.30it/s]Extractor Predicting: 124it [01:37,  1.32it/s]Extractor Predicting: 125it [01:38,  1.28it/s]Extractor Predicting: 126it [01:38,  1.22it/s]Extractor Predicting: 127it [01:39,  1.27it/s]Extractor Predicting: 128it [01:40,  1.18it/s]Extractor Predicting: 129it [01:41,  1.20it/s]Extractor Predicting: 130it [01:42,  1.13it/s]Extractor Predicting: 131it [01:43,  1.17it/s]Extractor Predicting: 132it [01:44,  1.21it/s]Extractor Predicting: 133it [01:44,  1.24it/s]Extractor Predicting: 134it [01:45,  1.19it/s]Extractor Predicting: 135it [01:46,  1.21it/s]Extractor Predicting: 136it [01:47,  1.23it/s]Extractor Predicting: 137it [01:48,  1.25it/s]Extractor Predicting: 138it [01:49,  1.02it/s]Extractor Predicting: 139it [01:50,  1.07it/s]Extractor Predicting: 140it [01:51,  1.11it/s]Extractor Predicting: 141it [01:52,  1.02it/s]Extractor Predicting: 142it [01:53,  1.06it/s]Extractor Predicting: 143it [01:53,  1.12it/s]Extractor Predicting: 144it [01:54,  1.14it/s]Extractor Predicting: 145it [01:56,  1.07s/it]Extractor Predicting: 146it [01:57,  1.01s/it]Extractor Predicting: 147it [01:57,  1.05it/s]Extractor Predicting: 148it [01:58,  1.06it/s]Extractor Predicting: 149it [01:59,  1.12it/s]Extractor Predicting: 150it [02:00,  1.15it/s]Extractor Predicting: 151it [02:01,  1.20it/s]Extractor Predicting: 152it [02:02,  1.16it/s]Extractor Predicting: 153it [02:02,  1.21it/s]Extractor Predicting: 154it [02:03,  1.22it/s]Extractor Predicting: 155it [02:04,  1.24it/s]Extractor Predicting: 156it [02:05,  1.23it/s]Extractor Predicting: 157it [02:06,  1.26it/s]Extractor Predicting: 158it [02:06,  1.27it/s]Extractor Predicting: 159it [02:07,  1.26it/s]Extractor Predicting: 160it [02:08,  1.27it/s]Extractor Predicting: 161it [02:09,  1.30it/s]Extractor Predicting: 162it [02:09,  1.29it/s]Extractor Predicting: 163it [02:10,  1.26it/s]Extractor Predicting: 164it [02:11,  1.25it/s]Extractor Predicting: 165it [02:12,  1.26it/s]Extractor Predicting: 166it [02:13,  1.24it/s]Extractor Predicting: 167it [02:13,  1.26it/s]Extractor Predicting: 168it [02:14,  1.26it/s]Extractor Predicting: 169it [02:15,  1.26it/s]Extractor Predicting: 170it [02:16,  1.26it/s]Extractor Predicting: 171it [02:16,  1.49it/s]Extractor Predicting: 171it [02:16,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:01,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:01,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:01,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:01,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:01,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:00:03,402 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:00:03,403 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:04,721 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:05,845 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:05,922 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:10,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:10,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:10,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:10,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:10,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:00:11,183 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:00:11,184 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:11,751 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:11,924 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:11,924 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.28967813540510545,
  "recall": 0.05662833586461272,
  "score": 0.09473684210526316,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.33it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:07,  1.22it/s]Extractor Predicting: 10it [00:07,  1.19it/s]Extractor Predicting: 11it [00:08,  1.23it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:10,  1.27it/s]Extractor Predicting: 15it [00:11,  1.26it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.30it/s]Extractor Predicting: 18it [00:14,  1.31it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.29it/s]Extractor Predicting: 21it [00:16,  1.32it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:17,  1.27it/s]Extractor Predicting: 24it [00:18,  1.24it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.30it/s]Extractor Predicting: 27it [00:20,  1.31it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.31it/s]Extractor Predicting: 30it [00:23,  1.29it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.28it/s]Extractor Predicting: 33it [00:25,  1.29it/s]Extractor Predicting: 34it [00:26,  1.29it/s]Extractor Predicting: 35it [00:27,  1.30it/s]Extractor Predicting: 36it [00:28,  1.27it/s]Extractor Predicting: 37it [00:28,  1.27it/s]Extractor Predicting: 38it [00:29,  1.28it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:31,  1.30it/s]Extractor Predicting: 41it [00:31,  1.30it/s]Extractor Predicting: 42it [00:32,  1.30it/s]Extractor Predicting: 43it [00:33,  1.31it/s]Extractor Predicting: 44it [00:34,  1.32it/s]Extractor Predicting: 45it [00:34,  1.33it/s]Extractor Predicting: 46it [00:35,  1.34it/s]Extractor Predicting: 47it [00:36,  1.31it/s]Extractor Predicting: 48it [00:37,  1.31it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.30it/s]Extractor Predicting: 51it [00:39,  1.32it/s]Extractor Predicting: 52it [00:40,  1.33it/s]Extractor Predicting: 53it [00:40,  1.33it/s]Extractor Predicting: 54it [00:41,  1.30it/s]Extractor Predicting: 55it [00:42,  1.29it/s]Extractor Predicting: 56it [00:43,  1.31it/s]Extractor Predicting: 57it [00:44,  1.32it/s]Extractor Predicting: 58it [00:44,  1.28it/s]Extractor Predicting: 59it [00:45,  1.30it/s]Extractor Predicting: 60it [00:46,  1.28it/s]Extractor Predicting: 61it [00:47,  1.28it/s]Extractor Predicting: 62it [00:48,  1.24it/s]Extractor Predicting: 63it [00:48,  1.28it/s]Extractor Predicting: 64it [00:49,  1.29it/s]Extractor Predicting: 65it [00:50,  1.31it/s]Extractor Predicting: 66it [00:51,  1.26it/s]Extractor Predicting: 67it [00:51,  1.29it/s]Extractor Predicting: 68it [00:52,  1.31it/s]Extractor Predicting: 69it [00:53,  1.33it/s]Extractor Predicting: 70it [00:54,  1.30it/s]Extractor Predicting: 71it [00:54,  1.26it/s]Extractor Predicting: 72it [00:55,  1.28it/s]Extractor Predicting: 73it [00:56,  1.28it/s]Extractor Predicting: 74it [00:57,  1.30it/s]Extractor Predicting: 75it [00:58,  1.30it/s]Extractor Predicting: 76it [00:58,  1.33it/s]Extractor Predicting: 77it [00:59,  1.36it/s]Extractor Predicting: 78it [01:00,  1.20it/s]Extractor Predicting: 79it [01:01,  1.23it/s]Extractor Predicting: 80it [01:02,  1.27it/s]Extractor Predicting: 81it [01:02,  1.29it/s]Extractor Predicting: 82it [01:03,  1.21it/s]Extractor Predicting: 83it [01:04,  1.24it/s]Extractor Predicting: 84it [01:05,  1.25it/s]Extractor Predicting: 85it [01:05,  1.27it/s]Extractor Predicting: 86it [01:06,  1.28it/s]Extractor Predicting: 87it [01:07,  1.27it/s]Extractor Predicting: 88it [01:08,  1.29it/s]Extractor Predicting: 89it [01:09,  1.29it/s]Extractor Predicting: 90it [01:09,  1.27it/s]Extractor Predicting: 91it [01:10,  1.28it/s]Extractor Predicting: 92it [01:11,  1.29it/s]Extractor Predicting: 93it [01:12,  1.29it/s]Extractor Predicting: 94it [01:12,  1.28it/s]Extractor Predicting: 95it [01:13,  1.28it/s]Extractor Predicting: 96it [01:14,  1.25it/s]Extractor Predicting: 97it [01:15,  1.28it/s]Extractor Predicting: 98it [01:16,  1.29it/s]Extractor Predicting: 99it [01:16,  1.35it/s]Extractor Predicting: 100it [01:17,  1.36it/s]Extractor Predicting: 101it [01:18,  1.33it/s]Extractor Predicting: 102it [01:19,  1.34it/s]Extractor Predicting: 103it [01:19,  1.32it/s]Extractor Predicting: 104it [01:20,  1.33it/s]Extractor Predicting: 105it [01:21,  1.32it/s]Extractor Predicting: 106it [01:22,  1.34it/s]Extractor Predicting: 107it [01:22,  1.35it/s]Extractor Predicting: 108it [01:23,  1.33it/s]Extractor Predicting: 109it [01:24,  1.30it/s]Extractor Predicting: 110it [01:25,  1.24it/s]Extractor Predicting: 111it [01:26,  1.23it/s]Extractor Predicting: 112it [01:26,  1.24it/s]Extractor Predicting: 113it [01:27,  1.28it/s]Extractor Predicting: 114it [01:28,  1.30it/s]Extractor Predicting: 115it [01:29,  1.29it/s]Extractor Predicting: 116it [01:29,  1.29it/s]Extractor Predicting: 117it [01:30,  1.23it/s]Extractor Predicting: 118it [01:31,  1.22it/s]Extractor Predicting: 119it [01:32,  1.25it/s]Extractor Predicting: 120it [01:33,  1.24it/s]Extractor Predicting: 121it [01:34,  1.23it/s]Extractor Predicting: 122it [01:34,  1.25it/s]Extractor Predicting: 123it [01:35,  1.27it/s]Extractor Predicting: 124it [01:36,  1.27it/s]Extractor Predicting: 125it [01:36,  1.57it/s]Extractor Predicting: 125it [01:36,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:55,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:55,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:55,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:55,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:55,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:01:55,574 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:01:55,575 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:01:55,834 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:01:56,889 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:01:56,889 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:58,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:58,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:58,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:58,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:58,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:01:58,928 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:01:58,929 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:01:59,188 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:01:59,338 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:01:59,339 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5700197238658777,
  "recall": 0.09701242027526015,
  "score": 0.16580608146873207,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.13it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.20it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.61it/s]Extractor Predicting: 6it [00:04,  1.36it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:02:04,222 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:02:04,223 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:02:04,230 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:02:04,231 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:02:04,235 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:02:10,472 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:02:10,495 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:02:11,001 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:02:11,002 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:02:11,160 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:02:11,196 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5882352941176471,
  "recall": 0.03937007874015748,
  "score": 0.07380073800738006,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:02:11,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:12,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:12,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:13,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:14,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:15,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:15,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:16,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:17,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:18,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:18,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:19,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:20,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:21,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:21,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:22,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:23,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:24,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:24,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:25,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:14, 14.97s/it][WARNING|generation_utils.py:914] 2023-08-28 18:02:26,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:27,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:27,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:28,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:29,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:29,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:30,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:31,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:32,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:32,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:33,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:34,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:34,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:35,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:36,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:37,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:37,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:38,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:39,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:39,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:54, 14.31s/it][WARNING|generation_utils.py:914] 2023-08-28 18:02:40,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:41,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:41,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:42,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:43,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:44,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:44,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:45,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:46,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:46,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:47,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:48,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:49,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:49,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:50,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:51,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:52,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:53,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:53,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:54,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:43<01:41, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-28 18:02:55,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:56,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:56,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:57,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:58,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:58,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:02:59,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:00,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:01,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:01,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:02,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:03,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:04,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:04,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:05,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:06,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:07,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:08,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:08,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:09,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:10,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:30, 15.17s/it][WARNING|generation_utils.py:914] 2023-08-28 18:03:11,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:11,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:12,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:13,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:14,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:14,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:15,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:16,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:17,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:17,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:18,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:19,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:19,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:20,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:21,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:22,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:22,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:23,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:24,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:13<01:13, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 18:03:25,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:25,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:26,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:27,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:28,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:28,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:29,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:30,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:30,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:31,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:32,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:32,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:34,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:35,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:35,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:36,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:37,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:38,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:28<00:58, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-28 18:03:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:40,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:41,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:41,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:42,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:43,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:43,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:44,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:45,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:45,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:46,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:47,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:48,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:49,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:50,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:51,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:52,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:53,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:53,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:54,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:55,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:44<00:45, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-28 18:03:56,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:56,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:57,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:58,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:58,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:03:59,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:00,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:00,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:01,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:02,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:03,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:04,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:05,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:05,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:06,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:08,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:08,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:09,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:58<00:29, 14.91s/it][WARNING|generation_utils.py:914] 2023-08-28 18:04:10,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:11,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:11,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:12,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:13,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:13,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:14,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:15,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:15,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:16,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:17,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:17,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:18,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:19,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:19,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:20,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:21,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:21,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:22,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:23,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:24,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:24,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:14<00:15, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:04:25,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:26,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:27,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:27,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:28,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:29,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:29,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:30,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:31,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:32,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:33,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:34,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:34,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:35,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:36,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:36,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:37,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:38,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:38,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:39,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:40,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:41,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:30<00:00, 15.46s/it]Generating: 100%|██████████| 10/10 [02:30<00:00, 15.05s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:48,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:48,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:48,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:48,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:48,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:04:49,285 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:04:49,286 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:04:49,554 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:04:50,699 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:04:50,699 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:52,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:52,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:52,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:52,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:52,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:04:53,294 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:04:53,295 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:04:53,654 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:04:53,934 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:04:53,934 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : official language .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 542, 'raw': 544}
{'target': 600, 'success': 574, 'raw': 576}
{'target': 600, 'success': 606, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9967105263157895, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : voice type .', 'success_rate': 0.859375, 'errors': {'', "('John Ridley', 'voice type', '', 'He sang in several other shows , notably as the supporting character in the 2002 feature film The Man Who Saved the World , with John Ridley .')", "('It Came from the Blue', 'voice type', '', 'This song was featured on the soundtrack to the 1997 BBC comedy , It Came from the Blue .')", "('Jack White', 'voice type', '', 'She recorded a guest role as the mother of the British musician Jack White in the 1982 film The Big Bang Theory directed by Matthew Rhys .')", 'not enough values to unpack (expected 2, got 1)', "('Bill Withers', 'voice type', '', 'He played a version of the song he composed for Bill Withers in the 1997 film Donovan .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 7262
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7362, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.26it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.39it/s]Extractor Estimating: 4it [00:02,  1.39it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.43it/s]Extractor Estimating: 8it [00:05,  1.46it/s]Extractor Estimating: 9it [00:06,  1.40it/s]Extractor Estimating: 10it [00:07,  1.37it/s]Extractor Estimating: 11it [00:07,  1.39it/s]Extractor Estimating: 12it [00:08,  1.40it/s]Extractor Estimating: 13it [00:09,  1.40it/s]Extractor Estimating: 14it [00:10,  1.36it/s]Extractor Estimating: 15it [00:10,  1.28it/s]Extractor Estimating: 16it [00:11,  1.27it/s]Extractor Estimating: 17it [00:12,  1.31it/s]Extractor Estimating: 18it [00:13,  1.32it/s]Extractor Estimating: 19it [00:13,  1.34it/s]Extractor Estimating: 20it [00:14,  1.37it/s]Extractor Estimating: 21it [00:15,  1.37it/s]Extractor Estimating: 22it [00:16,  1.38it/s]Extractor Estimating: 23it [00:16,  1.39it/s]Extractor Estimating: 24it [00:17,  1.42it/s]Extractor Estimating: 25it [00:18,  1.36it/s]Extractor Estimating: 26it [00:18,  1.40it/s]Extractor Estimating: 27it [00:19,  1.42it/s]Extractor Estimating: 28it [00:20,  1.42it/s]Extractor Estimating: 29it [00:20,  1.48it/s]Extractor Estimating: 30it [00:21,  1.49it/s]Extractor Estimating: 31it [00:22,  1.49it/s]Extractor Estimating: 32it [00:22,  1.48it/s]Extractor Estimating: 33it [00:23,  1.50it/s]Extractor Estimating: 34it [00:24,  1.52it/s]Extractor Estimating: 35it [00:24,  1.55it/s]Extractor Estimating: 36it [00:25,  1.56it/s]Extractor Estimating: 37it [00:26,  1.53it/s]Extractor Estimating: 38it [00:26,  1.51it/s]Extractor Estimating: 39it [00:27,  1.50it/s]Extractor Estimating: 40it [00:28,  1.50it/s]Extractor Estimating: 41it [00:28,  1.47it/s]Extractor Estimating: 42it [00:29,  1.49it/s]Extractor Estimating: 43it [00:30,  1.49it/s]Extractor Estimating: 44it [00:30,  1.56it/s]Extractor Estimating: 45it [00:31,  1.57it/s]Extractor Estimating: 46it [00:32,  1.57it/s]Extractor Estimating: 47it [00:32,  1.56it/s]Extractor Estimating: 48it [00:33,  1.55it/s]Extractor Estimating: 49it [00:33,  1.59it/s]Extractor Estimating: 50it [00:34,  1.57it/s]Extractor Estimating: 51it [00:35,  1.51it/s]Extractor Estimating: 52it [00:36,  1.46it/s]Extractor Estimating: 53it [00:36,  1.39it/s]Extractor Estimating: 54it [00:37,  1.39it/s]Extractor Estimating: 55it [00:38,  1.38it/s]Extractor Estimating: 56it [00:39,  1.38it/s]Extractor Estimating: 57it [00:39,  1.38it/s]Extractor Estimating: 58it [00:40,  1.39it/s]Extractor Estimating: 59it [00:41,  1.39it/s]Extractor Estimating: 60it [00:41,  1.38it/s]Extractor Estimating: 61it [00:42,  1.40it/s]Extractor Estimating: 62it [00:43,  1.41it/s]Extractor Estimating: 63it [00:44,  1.40it/s]Extractor Estimating: 64it [00:44,  1.40it/s]Extractor Estimating: 65it [00:45,  1.36it/s]Extractor Estimating: 66it [00:46,  1.38it/s]Extractor Estimating: 67it [00:46,  1.37it/s]Extractor Estimating: 68it [00:47,  1.38it/s]Extractor Estimating: 69it [00:48,  1.37it/s]Extractor Estimating: 70it [00:49,  1.33it/s]Extractor Estimating: 71it [00:49,  1.38it/s]Extractor Estimating: 72it [00:50,  1.40it/s]Extractor Estimating: 73it [00:51,  1.41it/s]Extractor Estimating: 74it [00:51,  1.40it/s]Extractor Estimating: 75it [00:52,  1.39it/s]Extractor Estimating: 76it [00:53,  1.42it/s]Extractor Estimating: 77it [00:54,  1.41it/s]Extractor Estimating: 78it [00:54,  1.43it/s]Extractor Estimating: 79it [00:55,  1.45it/s]Extractor Estimating: 80it [00:56,  1.47it/s]Extractor Estimating: 81it [00:56,  1.50it/s]Extractor Estimating: 82it [00:57,  1.52it/s]Extractor Estimating: 83it [00:58,  1.49it/s]Extractor Estimating: 84it [00:58,  1.48it/s]Extractor Estimating: 85it [00:59,  1.45it/s]Extractor Estimating: 86it [01:00,  1.48it/s]Extractor Estimating: 87it [01:00,  1.49it/s]Extractor Estimating: 88it [01:01,  1.48it/s]Extractor Estimating: 89it [01:02,  1.46it/s]Extractor Estimating: 90it [01:02,  1.45it/s]Extractor Estimating: 91it [01:03,  1.44it/s]Extractor Estimating: 92it [01:04,  1.49it/s]Extractor Estimating: 93it [01:04,  1.49it/s]Extractor Estimating: 94it [01:05,  1.47it/s]Extractor Estimating: 95it [01:06,  1.47it/s]Extractor Estimating: 96it [01:06,  1.44it/s]Extractor Estimating: 97it [01:07,  1.46it/s]Extractor Estimating: 98it [01:08,  1.27it/s]Extractor Estimating: 99it [01:09,  1.33it/s]Extractor Estimating: 100it [01:10,  1.37it/s]Extractor Estimating: 101it [01:10,  1.34it/s]Extractor Estimating: 102it [01:11,  1.31it/s]Extractor Estimating: 103it [01:12,  1.29it/s]Extractor Estimating: 104it [01:13,  1.27it/s]Extractor Estimating: 105it [01:14,  1.27it/s]Extractor Estimating: 106it [01:14,  1.25it/s]Extractor Estimating: 107it [01:15,  1.23it/s]Extractor Estimating: 108it [01:16,  1.24it/s]Extractor Estimating: 109it [01:17,  1.25it/s]Extractor Estimating: 110it [01:18,  1.22it/s]Extractor Estimating: 111it [01:18,  1.22it/s]Extractor Estimating: 112it [01:19,  1.22it/s]Extractor Estimating: 113it [01:20,  1.22it/s]Extractor Estimating: 114it [01:21,  1.23it/s]Extractor Estimating: 115it [01:22,  1.22it/s]Extractor Estimating: 116it [01:23,  1.23it/s]Extractor Estimating: 117it [01:23,  1.22it/s]Extractor Estimating: 118it [01:24,  1.24it/s]Extractor Estimating: 119it [01:25,  1.25it/s]Extractor Estimating: 120it [01:26,  1.25it/s]Extractor Estimating: 121it [01:27,  1.24it/s]Extractor Estimating: 122it [01:27,  1.25it/s]Extractor Estimating: 123it [01:28,  1.23it/s]Extractor Estimating: 124it [01:29,  1.22it/s]Extractor Estimating: 125it [01:30,  1.22it/s]Extractor Estimating: 126it [01:31,  1.27it/s]Extractor Estimating: 127it [01:31,  1.27it/s]Extractor Estimating: 128it [01:32,  1.31it/s]Extractor Estimating: 129it [01:33,  1.33it/s]Extractor Estimating: 130it [01:33,  1.34it/s]Extractor Estimating: 131it [01:34,  1.35it/s]Extractor Estimating: 132it [01:35,  1.30it/s]Extractor Estimating: 133it [01:36,  1.31it/s]Extractor Estimating: 134it [01:37,  1.32it/s]Extractor Estimating: 135it [01:37,  1.36it/s]Extractor Estimating: 136it [01:38,  1.40it/s]Extractor Estimating: 137it [01:39,  1.41it/s]Extractor Estimating: 138it [01:39,  1.40it/s]Extractor Estimating: 139it [01:40,  1.43it/s]Extractor Estimating: 140it [01:41,  1.42it/s]Extractor Estimating: 141it [01:41,  1.41it/s]Extractor Estimating: 142it [01:42,  1.40it/s]Extractor Estimating: 143it [01:43,  1.36it/s]Extractor Estimating: 144it [01:44,  1.38it/s]Extractor Estimating: 145it [01:44,  1.40it/s]Extractor Estimating: 146it [01:45,  1.38it/s]Extractor Estimating: 147it [01:46,  1.35it/s]Extractor Estimating: 148it [01:47,  1.37it/s]Extractor Estimating: 149it [01:47,  1.40it/s]Extractor Estimating: 150it [01:48,  1.43it/s]Extractor Estimating: 151it [01:49,  1.40it/s]Extractor Estimating: 152it [01:49,  1.40it/s]Extractor Estimating: 153it [01:50,  1.42it/s]Extractor Estimating: 154it [01:51,  1.41it/s]Extractor Estimating: 155it [01:51,  1.43it/s]Extractor Estimating: 156it [01:52,  1.42it/s]Extractor Estimating: 157it [01:53,  1.43it/s]Extractor Estimating: 158it [01:54,  1.42it/s]Extractor Estimating: 159it [01:54,  1.41it/s]Extractor Estimating: 160it [01:55,  1.39it/s]Extractor Estimating: 161it [01:56,  1.25it/s]Extractor Estimating: 162it [01:57,  1.28it/s]Extractor Estimating: 163it [01:57,  1.29it/s]Extractor Estimating: 164it [01:58,  1.36it/s]Extractor Estimating: 165it [01:59,  1.37it/s]Extractor Estimating: 166it [02:00,  1.37it/s]Extractor Estimating: 167it [02:00,  1.38it/s]Extractor Estimating: 168it [02:01,  1.38it/s]Extractor Estimating: 169it [02:02,  1.36it/s]Extractor Estimating: 170it [02:03,  1.30it/s]Extractor Estimating: 171it [02:03,  1.31it/s]Extractor Estimating: 172it [02:04,  1.32it/s]Extractor Estimating: 173it [02:05,  1.31it/s]Extractor Estimating: 174it [02:06,  1.33it/s]Extractor Estimating: 175it [02:06,  1.32it/s]Extractor Estimating: 176it [02:07,  1.38it/s]Extractor Estimating: 177it [02:08,  1.40it/s]Extractor Estimating: 178it [02:08,  1.43it/s]Extractor Estimating: 179it [02:09,  1.38it/s]Extractor Estimating: 180it [02:10,  1.39it/s]Extractor Estimating: 181it [02:11,  1.42it/s]Extractor Estimating: 182it [02:11,  1.45it/s]Extractor Estimating: 183it [02:12,  1.46it/s]Extractor Estimating: 184it [02:13,  1.44it/s]Extractor Estimating: 185it [02:13,  1.44it/s]Extractor Estimating: 186it [02:14,  1.39it/s]Extractor Estimating: 187it [02:15,  1.37it/s]Extractor Estimating: 188it [02:15,  1.43it/s]Extractor Estimating: 189it [02:16,  1.41it/s]Extractor Estimating: 190it [02:17,  1.41it/s]Extractor Estimating: 191it [02:18,  1.43it/s]Extractor Estimating: 192it [02:18,  1.44it/s]Extractor Estimating: 193it [02:19,  1.47it/s]Extractor Estimating: 194it [02:20,  1.46it/s]Extractor Estimating: 195it [02:20,  1.43it/s]Extractor Estimating: 196it [02:21,  1.42it/s]Extractor Estimating: 197it [02:22,  1.41it/s]Extractor Estimating: 198it [02:22,  1.43it/s]Extractor Estimating: 199it [02:23,  1.43it/s]Extractor Estimating: 200it [02:24,  1.39it/s]Extractor Estimating: 201it [02:24,  1.51it/s]Extractor Estimating: 202it [02:25,  1.47it/s]Extractor Estimating: 203it [02:26,  1.52it/s]Extractor Estimating: 204it [02:26,  1.49it/s]Extractor Estimating: 205it [02:27,  1.50it/s]Extractor Estimating: 206it [02:28,  1.51it/s]Extractor Estimating: 207it [02:28,  1.49it/s]Extractor Estimating: 208it [02:29,  1.50it/s]Extractor Estimating: 209it [02:30,  1.53it/s]Extractor Estimating: 210it [02:30,  1.51it/s]Extractor Estimating: 211it [02:31,  1.48it/s]Extractor Estimating: 212it [02:32,  1.48it/s]Extractor Estimating: 213it [02:32,  1.50it/s]Extractor Estimating: 214it [02:33,  1.49it/s]Extractor Estimating: 215it [02:34,  1.52it/s]Extractor Estimating: 216it [02:34,  1.58it/s]Extractor Estimating: 217it [02:35,  1.63it/s]Extractor Estimating: 218it [02:36,  1.56it/s]Extractor Estimating: 219it [02:36,  1.61it/s]Extractor Estimating: 220it [02:37,  1.60it/s]Extractor Estimating: 221it [02:37,  1.62it/s]Extractor Estimating: 222it [02:38,  1.60it/s]Extractor Estimating: 223it [02:39,  1.51it/s]Extractor Estimating: 224it [02:40,  1.46it/s]Extractor Estimating: 225it [02:40,  1.40it/s]Extractor Estimating: 226it [02:41,  1.39it/s]Extractor Estimating: 227it [02:42,  1.28it/s]Extractor Estimating: 228it [02:43,  1.33it/s]Extractor Estimating: 229it [02:43,  1.37it/s]Extractor Estimating: 230it [02:44,  1.38it/s]Extractor Estimating: 231it [02:45,  1.41it/s]Extractor Estimating: 232it [02:45,  1.43it/s]Extractor Estimating: 233it [02:46,  1.43it/s]Extractor Estimating: 234it [02:47,  1.43it/s]Extractor Estimating: 235it [02:48,  1.43it/s]Extractor Estimating: 236it [02:48,  1.43it/s]Extractor Estimating: 237it [02:49,  1.44it/s]Extractor Estimating: 238it [02:50,  1.44it/s]Extractor Estimating: 239it [02:50,  1.44it/s]Extractor Estimating: 240it [02:51,  1.43it/s]Extractor Estimating: 241it [02:52,  1.45it/s]Extractor Estimating: 242it [02:52,  1.42it/s]Extractor Estimating: 243it [02:53,  1.43it/s]Extractor Estimating: 244it [02:54,  1.39it/s]Extractor Estimating: 245it [02:55,  1.39it/s]Extractor Estimating: 246it [02:55,  1.41it/s]Extractor Estimating: 247it [02:56,  1.41it/s]Extractor Estimating: 248it [02:57,  1.41it/s]Extractor Estimating: 249it [02:57,  1.40it/s]Extractor Estimating: 250it [02:58,  1.42it/s]Extractor Estimating: 250it [02:58,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:12,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:12,404 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:12,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:12,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:12,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:08:13,135 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:08:13,136 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:08:13,392 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:08:14,584 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:08:14,584 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:16,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:16,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:16,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:16,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:08:16,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:08:17,365 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:08:17,366 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:08:17,648 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:08:17,834 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:08:17,834 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:56:32,143 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:56:32,234 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4997 mean pseudo reward: 0.9293465187942451
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 16412
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16512, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16512, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.219, loss:516.7311
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.185, loss:478.3569
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.191, loss:430.9504
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.212, loss:429.7949
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.183, loss:399.9565
>> valid entity prec:0.4770, rec:0.4504, f1:0.4633
>> valid relation prec:0.2055, rec:0.0279, f1:0.0491
>> valid relation with NER prec:0.2055, rec:0.0279, f1:0.0491
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.059, loss:394.6383
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.191, loss:375.3539
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.212, loss:377.3644
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.189, loss:368.9943
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.197, loss:377.3671
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4800, rec:0.4412, f1:0.4598
>> valid relation prec:0.2011, rec:0.0314, f1:0.0543
>> valid relation with NER prec:0.2011, rec:0.0314, f1:0.0543
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 3.057, loss:363.3968
g_step 1200, step 155, avg_time 1.209, loss:358.4304
g_step 1300, step 46, avg_time 1.195, loss:357.1268
g_step 1400, step 146, avg_time 1.194, loss:344.4474
g_step 1500, step 37, avg_time 1.210, loss:324.5855
>> valid entity prec:0.4782, rec:0.4466, f1:0.4619
>> valid relation prec:0.1771, rec:0.0573, f1:0.0866
>> valid relation with NER prec:0.1771, rec:0.0573, f1:0.0866
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 3.034, loss:326.2205
g_step 1700, step 28, avg_time 1.192, loss:327.2177
g_step 1800, step 128, avg_time 1.207, loss:306.9204
g_step 1900, step 19, avg_time 1.199, loss:336.7223
g_step 2000, step 119, avg_time 1.197, loss:292.1546
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4692, rec:0.4457, f1:0.4571
>> valid relation prec:0.1935, rec:0.0390, f1:0.0649
>> valid relation with NER prec:0.1935, rec:0.0390, f1:0.0649
g_step 2100, step 10, avg_time 3.038, loss:315.6196
g_step 2200, step 110, avg_time 1.194, loss:266.6542
g_step 2300, step 1, avg_time 1.208, loss:296.6282
g_step 2400, step 101, avg_time 1.209, loss:271.2463
g_step 2500, step 201, avg_time 1.206, loss:281.3368
>> valid entity prec:0.4930, rec:0.4647, f1:0.4784
>> valid relation prec:0.2367, rec:0.0525, f1:0.0860
>> valid relation with NER prec:0.2367, rec:0.0525, f1:0.0860
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.043, loss:272.8420
g_step 2700, step 192, avg_time 1.204, loss:267.6944
g_step 2800, step 83, avg_time 1.196, loss:251.7830
g_step 2900, step 183, avg_time 1.199, loss:278.9991
g_step 3000, step 74, avg_time 1.204, loss:229.2289
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4906, rec:0.4096, f1:0.4465
>> valid relation prec:0.1491, rec:0.0377, f1:0.0602
>> valid relation with NER prec:0.1491, rec:0.0377, f1:0.0602
g_step 3100, step 174, avg_time 3.030, loss:269.3924
g_step 3200, step 65, avg_time 1.192, loss:248.2716
g_step 3300, step 165, avg_time 1.212, loss:243.2372
g_step 3400, step 56, avg_time 1.200, loss:239.2284
g_step 3500, step 156, avg_time 1.207, loss:233.0853
>> valid entity prec:0.5131, rec:0.3454, f1:0.4129
>> valid relation prec:0.1756, rec:0.0351, f1:0.0585
>> valid relation with NER prec:0.1756, rec:0.0351, f1:0.0585
g_step 3600, step 47, avg_time 3.026, loss:228.2365
g_step 3700, step 147, avg_time 1.210, loss:231.7486
g_step 3800, step 38, avg_time 1.206, loss:223.6196
g_step 3900, step 138, avg_time 1.206, loss:220.7239
g_step 4000, step 29, avg_time 1.203, loss:236.9461
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4936, rec:0.4130, f1:0.4497
>> valid relation prec:0.2119, rec:0.0442, f1:0.0732
>> valid relation with NER prec:0.2119, rec:0.0442, f1:0.0732
g_step 4100, step 129, avg_time 3.033, loss:219.7023
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:56:32 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:56:32 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-56-32_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:56:33 - WARNING - datasets.builder -   Using custom data configuration default-14dc4a2ff5475031
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-14dc4a2ff5475031/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:56:33,797 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:56:33,799 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:56:33,799 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:56:33,800 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:56:33,812 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:56:33,815 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:56:33,977 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:56:37,135 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:56:37,141 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-14dc4a2ff5475031/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.39ba/s] 40%|████      | 2/5 [00:00<00:00,  3.45ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.02ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.37ba/s]100%|██████████| 5/5 [00:01<00:00,  4.60ba/s]100%|██████████| 5/5 [00:01<00:00,  4.12ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  4.41ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.50ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.54ba/s]100%|██████████| 5/5 [00:01<00:00,  5.28ba/s]100%|██████████| 5/5 [00:01<00:00,  4.86ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.26ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.13ba/s]100%|██████████| 5/5 [00:00<00:00, 10.44ba/s]100%|██████████| 5/5 [00:00<00:00, 10.22ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.14ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.00ba/s]100%|██████████| 5/5 [00:00<00:00, 11.56ba/s]100%|██████████| 5/5 [00:00<00:00, 11.10ba/s]
[INFO|trainer.py:414] 2023-08-28 19:56:41,131 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:56:41,160 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:56:41,160 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 19:56:41,160 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:56:41,160 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:56:41,160 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:56:41,160 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:56:41,160 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:59,  3.25it/s]  1%|          | 2/390 [00:00<01:55,  3.35it/s]  1%|          | 3/390 [00:00<01:54,  3.38it/s]  1%|          | 4/390 [00:01<01:53,  3.39it/s]  1%|▏         | 5/390 [00:01<01:53,  3.40it/s]  2%|▏         | 6/390 [00:01<01:52,  3.40it/s]  2%|▏         | 7/390 [00:02<01:52,  3.40it/s]  2%|▏         | 8/390 [00:02<01:53,  3.37it/s]  2%|▏         | 9/390 [00:02<01:52,  3.38it/s]  3%|▎         | 10/390 [00:02<01:52,  3.39it/s]  3%|▎         | 11/390 [00:03<01:51,  3.39it/s]  3%|▎         | 12/390 [00:03<01:51,  3.39it/s]  3%|▎         | 13/390 [00:03<01:50,  3.40it/s]  4%|▎         | 14/390 [00:04<01:50,  3.40it/s]  4%|▍         | 15/390 [00:04<01:50,  3.40it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:05<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:49,  3.41it/s]  5%|▍         | 19/390 [00:05<01:54,  3.24it/s]  5%|▌         | 20/390 [00:05<01:53,  3.26it/s]  5%|▌         | 21/390 [00:06<01:52,  3.28it/s]  6%|▌         | 22/390 [00:06<01:51,  3.29it/s]  6%|▌         | 23/390 [00:06<01:51,  3.30it/s]  6%|▌         | 24/390 [00:07<01:50,  3.30it/s]  6%|▋         | 25/390 [00:07<01:50,  3.31it/s]  7%|▋         | 26/390 [00:07<01:49,  3.32it/s]  7%|▋         | 27/390 [00:08<01:49,  3.32it/s]  7%|▋         | 28/390 [00:08<01:49,  3.32it/s]  7%|▋         | 29/390 [00:08<01:50,  3.27it/s]  8%|▊         | 30/390 [00:08<01:49,  3.29it/s]  8%|▊         | 31/390 [00:09<01:48,  3.30it/s]  8%|▊         | 32/390 [00:09<01:48,  3.30it/s]  8%|▊         | 33/390 [00:09<01:47,  3.31it/s]  9%|▊         | 34/390 [00:10<01:47,  3.31it/s]  9%|▉         | 35/390 [00:10<01:47,  3.31it/s]  9%|▉         | 36/390 [00:10<01:46,  3.31it/s]  9%|▉         | 37/390 [00:11<01:46,  3.31it/s] 10%|▉         | 38/390 [00:11<01:46,  3.31it/s] 10%|█         | 39/390 [00:11<01:50,  3.19it/s] 10%|█         | 40/390 [00:12<01:48,  3.23it/s] 11%|█         | 41/390 [00:12<01:47,  3.25it/s] 11%|█         | 42/390 [00:12<01:46,  3.27it/s] 11%|█         | 43/390 [00:12<01:45,  3.29it/s] 11%|█▏        | 44/390 [00:13<01:44,  3.30it/s] 12%|█▏        | 45/390 [00:13<01:44,  3.30it/s] 12%|█▏        | 46/390 [00:13<01:43,  3.31it/s] 12%|█▏        | 47/390 [00:14<01:43,  3.31it/s] 12%|█▏        | 48/390 [00:14<01:43,  3.31it/s] 13%|█▎        | 49/390 [00:14<01:44,  3.27it/s] 13%|█▎        | 50/390 [00:15<01:43,  3.29it/s] 13%|█▎        | 51/390 [00:15<01:42,  3.30it/s] 13%|█▎        | 52/390 [00:15<01:42,  3.30it/s] 14%|█▎        | 53/390 [00:15<01:41,  3.31it/s] 14%|█▍        | 54/390 [00:16<01:41,  3.31it/s] 14%|█▍        | 55/390 [00:16<01:41,  3.31it/s] 14%|█▍        | 56/390 [00:16<01:40,  3.31it/s] 15%|█▍        | 57/390 [00:17<01:40,  3.31it/s] 15%|█▍        | 58/390 [00:17<01:40,  3.31it/s] 15%|█▌        | 59/390 [00:17<01:47,  3.08it/s] 15%|█▌        | 60/390 [00:18<01:44,  3.14it/s] 16%|█▌        | 61/390 [00:18<01:42,  3.19it/s] 16%|█▌        | 62/390 [00:18<01:41,  3.23it/s] 16%|█▌        | 63/390 [00:19<01:40,  3.26it/s] 16%|█▋        | 64/390 [00:19<01:39,  3.28it/s] 17%|█▋        | 65/390 [00:19<01:38,  3.29it/s] 17%|█▋        | 66/390 [00:19<01:38,  3.30it/s] 17%|█▋        | 67/390 [00:20<01:37,  3.30it/s] 17%|█▋        | 68/390 [00:20<01:37,  3.31it/s] 18%|█▊        | 69/390 [00:20<01:37,  3.29it/s] 18%|█▊        | 70/390 [00:21<01:36,  3.30it/s] 18%|█▊        | 71/390 [00:21<01:36,  3.30it/s] 18%|█▊        | 72/390 [00:21<01:36,  3.31it/s] 19%|█▊        | 73/390 [00:22<01:35,  3.31it/s] 19%|█▉        | 74/390 [00:22<01:35,  3.32it/s] 19%|█▉        | 75/390 [00:22<01:34,  3.32it/s] 19%|█▉        | 76/390 [00:22<01:34,  3.32it/s] 20%|█▉        | 77/390 [00:23<01:34,  3.31it/s] 20%|██        | 78/390 [00:23<01:34,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 19:57:04,804 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:57:04,805 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 19:57:04,805 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.45it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.11it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.74it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.68it/s][A
  5%|▍         | 27/577 [00:00<00:12, 44.93it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.48it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.32it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.01it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.06it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.25it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.30it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.42it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.41it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.13it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.75it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.83it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.82it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.91it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.23it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.27it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.33it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.30it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.15it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.95it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.75it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.76it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.88it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.94it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.24it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.24it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.16it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.08it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.90it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.90it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.82it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.89it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.07it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.20it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.25it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.18it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.08it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.92it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.74it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.91it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 43.98it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.13it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.30it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.38it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.25it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.95it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.83it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.83it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.82it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.94it/s][A
 48%|████▊     | 277/577 [00:06<00:07, 42.00it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 42.78it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 43.31it/s][A
 51%|█████     | 292/577 [00:06<00:06, 43.61it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.62it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.62it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.64it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.68it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.63it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.97it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.23it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.03it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.99it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.95it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.88it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.76it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.01it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.27it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.31it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.24it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.07it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.97it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.94it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 43.85it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.70it/s][A
 71%|███████▏  | 412/577 [00:09<00:04, 37.69it/s][A
 72%|███████▏  | 417/577 [00:09<00:04, 39.53it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 40.89it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 41.93it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 42.67it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.24it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.71it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.74it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.43it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.33it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.57it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.71it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.98it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.22it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.29it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.33it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.15it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.83it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.71it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.68it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.86it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.17it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.28it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 44.26it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.34it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.11it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 38.36it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 40.09it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 41.33it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 42.20it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 42.95it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.34it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 43.76it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.90it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:13<00:00, 43.90it/s][A 20%|██        | 78/390 [00:36<01:34,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:57:18,067 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 19:57:18,143 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:57:23,099 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:57:23,119 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:57:23,132 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:52<46:32,  8.98s/it] 21%|██        | 80/390 [00:53<32:56,  6.38s/it] 21%|██        | 81/390 [00:53<23:26,  4.55s/it] 21%|██        | 82/390 [00:53<16:49,  3.28s/it] 21%|██▏       | 83/390 [00:54<12:12,  2.38s/it] 22%|██▏       | 84/390 [00:54<08:58,  1.76s/it] 22%|██▏       | 85/390 [00:54<06:43,  1.32s/it] 22%|██▏       | 86/390 [00:54<05:08,  1.02s/it] 22%|██▏       | 87/390 [00:55<04:02,  1.25it/s] 23%|██▎       | 88/390 [00:55<03:16,  1.53it/s] 23%|██▎       | 89/390 [00:55<02:44,  1.83it/s] 23%|██▎       | 90/390 [00:56<02:21,  2.12it/s] 23%|██▎       | 91/390 [00:56<02:05,  2.37it/s] 24%|██▎       | 92/390 [00:56<01:54,  2.60it/s] 24%|██▍       | 93/390 [00:57<01:46,  2.78it/s] 24%|██▍       | 94/390 [00:57<01:41,  2.92it/s] 24%|██▍       | 95/390 [00:57<01:37,  3.04it/s] 25%|██▍       | 96/390 [00:57<01:35,  3.07it/s] 25%|██▍       | 97/390 [00:58<01:33,  3.15it/s] 25%|██▌       | 98/390 [00:58<01:31,  3.20it/s] 25%|██▌       | 99/390 [00:58<01:29,  3.24it/s] 26%|██▌       | 100/390 [00:59<01:28,  3.27it/s] 26%|██▌       | 101/390 [00:59<01:27,  3.28it/s] 26%|██▌       | 102/390 [00:59<01:27,  3.30it/s] 26%|██▋       | 103/390 [01:00<01:26,  3.31it/s] 27%|██▋       | 104/390 [01:00<01:26,  3.31it/s] 27%|██▋       | 105/390 [01:00<01:25,  3.32it/s] 27%|██▋       | 106/390 [01:00<01:26,  3.30it/s] 27%|██▋       | 107/390 [01:01<01:25,  3.30it/s] 28%|██▊       | 108/390 [01:01<01:25,  3.31it/s] 28%|██▊       | 109/390 [01:01<01:24,  3.31it/s] 28%|██▊       | 110/390 [01:02<01:24,  3.31it/s] 28%|██▊       | 111/390 [01:02<01:24,  3.31it/s] 29%|██▊       | 112/390 [01:02<01:23,  3.31it/s] 29%|██▉       | 113/390 [01:03<01:23,  3.32it/s] 29%|██▉       | 114/390 [01:03<01:23,  3.32it/s] 29%|██▉       | 115/390 [01:03<01:22,  3.32it/s] 30%|██▉       | 116/390 [01:03<01:24,  3.24it/s] 30%|███       | 117/390 [01:04<01:23,  3.26it/s] 30%|███       | 118/390 [01:04<01:22,  3.28it/s] 31%|███       | 119/390 [01:04<01:22,  3.29it/s] 31%|███       | 120/390 [01:05<01:21,  3.30it/s] 31%|███       | 121/390 [01:05<01:21,  3.30it/s] 31%|███▏      | 122/390 [01:05<01:21,  3.31it/s] 32%|███▏      | 123/390 [01:06<01:20,  3.31it/s] 32%|███▏      | 124/390 [01:06<01:20,  3.31it/s] 32%|███▏      | 125/390 [01:06<01:19,  3.32it/s] 32%|███▏      | 126/390 [01:07<01:20,  3.27it/s] 33%|███▎      | 127/390 [01:07<01:19,  3.29it/s] 33%|███▎      | 128/390 [01:07<01:19,  3.30it/s] 33%|███▎      | 129/390 [01:07<01:19,  3.30it/s] 33%|███▎      | 130/390 [01:08<01:18,  3.31it/s] 34%|███▎      | 131/390 [01:08<01:18,  3.31it/s] 34%|███▍      | 132/390 [01:08<01:18,  3.31it/s] 34%|███▍      | 133/390 [01:09<01:17,  3.31it/s] 34%|███▍      | 134/390 [01:09<01:17,  3.31it/s] 35%|███▍      | 135/390 [01:09<01:17,  3.31it/s] 35%|███▍      | 136/390 [01:10<01:16,  3.30it/s] 35%|███▌      | 137/390 [01:10<01:16,  3.30it/s] 35%|███▌      | 138/390 [01:10<01:16,  3.31it/s] 36%|███▌      | 139/390 [01:10<01:15,  3.31it/s] 36%|███▌      | 140/390 [01:11<01:15,  3.31it/s] 36%|███▌      | 141/390 [01:11<01:15,  3.32it/s] 36%|███▋      | 142/390 [01:11<01:14,  3.32it/s] 37%|███▋      | 143/390 [01:12<01:14,  3.32it/s] 37%|███▋      | 144/390 [01:12<01:14,  3.32it/s] 37%|███▋      | 145/390 [01:12<01:13,  3.32it/s] 37%|███▋      | 146/390 [01:13<01:16,  3.20it/s] 38%|███▊      | 147/390 [01:13<01:15,  3.24it/s] 38%|███▊      | 148/390 [01:13<01:14,  3.26it/s] 38%|███▊      | 149/390 [01:13<01:13,  3.27it/s] 38%|███▊      | 150/390 [01:14<01:12,  3.30it/s] 39%|███▊      | 151/390 [01:14<01:11,  3.33it/s] 39%|███▉      | 152/390 [01:14<01:11,  3.35it/s] 39%|███▉      | 153/390 [01:15<01:10,  3.36it/s] 39%|███▉      | 154/390 [01:15<01:09,  3.37it/s] 40%|███▉      | 155/390 [01:15<01:09,  3.38it/s] 40%|████      | 156/390 [01:16<01:11,  3.29it/s][INFO|trainer.py:2140] 2023-08-28 19:57:57,295 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:57:57,295 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 19:57:57,295 >>   Batch size = 8
{'eval_loss': 1.065826416015625, 'eval_runtime': 13.2144, 'eval_samples_per_second': 348.785, 'eval_steps_per_second': 43.664, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.55it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.53it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.74it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.85it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.05it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.36it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.15it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.90it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.96it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.11it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.31it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.45it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.45it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.20it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.99it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.78it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.78it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.93it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.20it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.36it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.40it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.31it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.14it/s][A
 21%|██        | 122/577 [00:02<00:10, 42.05it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 42.28it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 42.87it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.47it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.76it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.04it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.14it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.11it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 43.91it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.79it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.83it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.91it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.03it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.24it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.29it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.37it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.22it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.93it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.85it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.81it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.88it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.06it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.24it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.36it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.35it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.95it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.83it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.84it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.89it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.14it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.31it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.32it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.25it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.11it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.95it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.77it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.93it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.06it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.15it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.24it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.26it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.97it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.76it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.81it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.88it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.11it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.22it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.33it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.23it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.98it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.87it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.67it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 44.01it/s][A
 70%|██████▉   | 402/577 [00:09<00:04, 43.31it/s][A
 71%|███████   | 407/577 [00:09<00:03, 43.54it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 43.92it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.08it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.07it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 43.83it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.65it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.59it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.76it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.99it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.12it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.30it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.30it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.23it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.95it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.79it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.94it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.00it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.16it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.19it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.31it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.32it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.15it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.84it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.82it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 43.93it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 39.98it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 41.26it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 42.27it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 42.96it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 43.34it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.62it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.75it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.64it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 43.55it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.59it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 43.59it/s][A 40%|████      | 156/390 [01:29<01:11,  3.29it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:58:10,554 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 19:58:10,595 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:58:16,734 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:58:17,009 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:58:17,061 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:46<36:41,  9.45s/it] 41%|████      | 158/390 [01:47<25:55,  6.71s/it] 41%|████      | 159/390 [01:47<18:25,  4.78s/it] 41%|████      | 160/390 [01:47<13:10,  3.44s/it] 41%|████▏     | 161/390 [01:48<09:31,  2.50s/it] 42%|████▏     | 162/390 [01:48<06:59,  1.84s/it] 42%|████▏     | 163/390 [01:48<05:12,  1.38s/it] 42%|████▏     | 164/390 [01:48<03:58,  1.05s/it] 42%|████▏     | 165/390 [01:49<03:06,  1.21it/s] 43%|████▎     | 166/390 [01:49<02:30,  1.49it/s] 43%|████▎     | 167/390 [01:49<02:04,  1.79it/s] 43%|████▎     | 168/390 [01:50<01:47,  2.07it/s] 43%|████▎     | 169/390 [01:50<01:34,  2.33it/s] 44%|████▎     | 170/390 [01:50<01:25,  2.56it/s] 44%|████▍     | 171/390 [01:51<01:19,  2.75it/s] 44%|████▍     | 172/390 [01:51<01:15,  2.90it/s] 44%|████▍     | 173/390 [01:51<01:11,  3.02it/s] 45%|████▍     | 174/390 [01:51<01:09,  3.11it/s] 45%|████▍     | 175/390 [01:52<01:08,  3.16it/s] 45%|████▌     | 176/390 [01:52<01:27,  2.46it/s] 45%|████▌     | 177/390 [01:53<01:21,  2.61it/s] 46%|████▌     | 178/390 [01:53<01:16,  2.79it/s] 46%|████▌     | 179/390 [01:53<01:12,  2.93it/s] 46%|████▌     | 180/390 [01:54<01:08,  3.05it/s] 46%|████▋     | 181/390 [01:54<01:06,  3.15it/s] 47%|████▋     | 182/390 [01:54<01:04,  3.22it/s] 47%|████▋     | 183/390 [01:55<01:03,  3.28it/s] 47%|████▋     | 184/390 [01:55<01:02,  3.31it/s] 47%|████▋     | 185/390 [01:55<01:01,  3.34it/s] 48%|████▊     | 186/390 [01:55<01:00,  3.36it/s] 48%|████▊     | 187/390 [01:56<01:00,  3.37it/s] 48%|████▊     | 188/390 [01:56<00:59,  3.38it/s] 48%|████▊     | 189/390 [01:56<00:59,  3.39it/s] 49%|████▊     | 190/390 [01:57<00:58,  3.39it/s] 49%|████▉     | 191/390 [01:57<00:58,  3.40it/s] 49%|████▉     | 192/390 [01:57<00:58,  3.40it/s] 49%|████▉     | 193/390 [01:57<00:57,  3.40it/s] 50%|████▉     | 194/390 [01:58<00:57,  3.40it/s] 50%|█████     | 195/390 [01:58<00:57,  3.41it/s] 50%|█████     | 196/390 [01:58<00:56,  3.41it/s] 51%|█████     | 197/390 [01:59<00:56,  3.39it/s] 51%|█████     | 198/390 [01:59<00:56,  3.39it/s] 51%|█████     | 199/390 [01:59<00:56,  3.40it/s] 51%|█████▏    | 200/390 [02:00<00:55,  3.40it/s] 52%|█████▏    | 201/390 [02:00<00:55,  3.40it/s] 52%|█████▏    | 202/390 [02:00<00:55,  3.40it/s] 52%|█████▏    | 203/390 [02:00<00:54,  3.40it/s] 52%|█████▏    | 204/390 [02:01<00:54,  3.40it/s] 53%|█████▎    | 205/390 [02:01<00:54,  3.40it/s] 53%|█████▎    | 206/390 [02:01<00:54,  3.40it/s] 53%|█████▎    | 207/390 [02:02<00:53,  3.40it/s] 53%|█████▎    | 208/390 [02:02<00:53,  3.39it/s] 54%|█████▎    | 209/390 [02:02<00:53,  3.39it/s] 54%|█████▍    | 210/390 [02:02<00:52,  3.40it/s] 54%|█████▍    | 211/390 [02:03<00:52,  3.39it/s] 54%|█████▍    | 212/390 [02:03<00:52,  3.40it/s] 55%|█████▍    | 213/390 [02:03<00:52,  3.40it/s] 55%|█████▍    | 214/390 [02:04<00:51,  3.40it/s] 55%|█████▌    | 215/390 [02:04<00:51,  3.40it/s] 55%|█████▌    | 216/390 [02:04<00:51,  3.40it/s] 56%|█████▌    | 217/390 [02:05<00:50,  3.40it/s] 56%|█████▌    | 218/390 [02:05<00:50,  3.40it/s] 56%|█████▌    | 219/390 [02:05<00:50,  3.38it/s] 56%|█████▋    | 220/390 [02:05<00:50,  3.39it/s] 57%|█████▋    | 221/390 [02:06<00:49,  3.39it/s] 57%|█████▋    | 222/390 [02:06<00:49,  3.39it/s] 57%|█████▋    | 223/390 [02:06<00:49,  3.39it/s] 57%|█████▋    | 224/390 [02:07<00:48,  3.40it/s] 58%|█████▊    | 225/390 [02:07<00:48,  3.40it/s] 58%|█████▊    | 226/390 [02:07<00:48,  3.40it/s] 58%|█████▊    | 227/390 [02:07<00:47,  3.40it/s] 58%|█████▊    | 228/390 [02:08<00:47,  3.40it/s] 59%|█████▊    | 229/390 [02:08<00:47,  3.40it/s] 59%|█████▉    | 230/390 [02:08<00:47,  3.38it/s] 59%|█████▉    | 231/390 [02:09<00:46,  3.39it/s] 59%|█████▉    | 232/390 [02:09<00:46,  3.39it/s] 60%|█████▉    | 233/390 [02:09<00:46,  3.40it/s] 60%|██████    | 234/390 [02:10<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 19:58:51,240 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:58:51,240 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 19:58:51,240 >>   Batch size = 8
{'eval_loss': 1.0863043069839478, 'eval_runtime': 13.1425, 'eval_samples_per_second': 350.694, 'eval_steps_per_second': 43.903, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.53it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.46it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.48it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.14it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.01it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.68it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.41it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.09it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.98it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.22it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.21it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.38it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.36it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.30it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.24it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.89it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.84it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.94it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.19it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.32it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.32it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.14it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.04it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.71it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.87it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.03it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.13it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.26it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.32it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.32it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.18it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.95it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.78it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 43.93it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.20it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.38it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.36it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.24it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.18it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.04it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.86it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 43.72it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.01it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.15it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.30it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.25it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.29it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.16it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.00it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.77it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 43.91it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.00it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.25it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.33it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.22it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.32it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.22it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.10it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.81it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.86it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.24it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.24it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.25it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 41.89it/s][A
 60%|██████    | 347/577 [00:07<00:05, 42.49it/s][A
 61%|██████    | 352/577 [00:07<00:05, 42.81it/s][A
 62%|██████▏   | 357/577 [00:08<00:05, 43.25it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 43.66it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 43.80it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 43.99it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 43.86it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.92it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.84it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.88it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.03it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.15it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.19it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.30it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.16it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.04it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.88it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.93it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 44.10it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.08it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.16it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.24it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.26it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.14it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.03it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 40.31it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 41.58it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 42.56it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.12it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.44it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 43.63it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.84it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.72it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.45it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.67it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.01it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.15it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.28it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.28it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.28it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.17it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.82it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.66it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.79it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.09it/s][A
100%|██████████| 577/577 [00:13<00:00, 44.31it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 44.31it/s][A 60%|██████    | 234/390 [02:23<00:45,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:59:04,714 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:59:04,881 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:59:10,854 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:59:10,881 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:59:10,902 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:45<28:02, 10.85s/it] 61%|██████    | 236/390 [02:45<19:49,  7.73s/it] 61%|██████    | 237/390 [02:46<14:01,  5.50s/it] 61%|██████    | 238/390 [02:46<09:58,  3.94s/it] 61%|██████▏   | 239/390 [02:46<07:09,  2.85s/it] 62%|██████▏   | 240/390 [02:47<05:12,  2.08s/it] 62%|██████▏   | 241/390 [02:47<03:50,  1.55s/it] 62%|██████▏   | 242/390 [02:47<02:53,  1.17s/it] 62%|██████▏   | 243/390 [02:48<02:14,  1.10it/s] 63%|██████▎   | 244/390 [02:48<01:46,  1.37it/s] 63%|██████▎   | 245/390 [02:48<01:27,  1.67it/s] 63%|██████▎   | 246/390 [02:48<01:14,  1.93it/s] 63%|██████▎   | 247/390 [02:49<01:04,  2.21it/s] 64%|██████▎   | 248/390 [02:49<00:57,  2.46it/s] 64%|██████▍   | 249/390 [02:49<00:52,  2.67it/s] 64%|██████▍   | 250/390 [02:50<00:49,  2.83it/s] 64%|██████▍   | 251/390 [02:50<00:46,  2.96it/s] 65%|██████▍   | 252/390 [02:50<00:45,  3.06it/s] 65%|██████▍   | 253/390 [02:51<00:43,  3.13it/s] 65%|██████▌   | 254/390 [02:51<00:42,  3.19it/s] 65%|██████▌   | 255/390 [02:51<00:41,  3.23it/s] 66%|██████▌   | 256/390 [02:52<00:41,  3.20it/s] 66%|██████▌   | 257/390 [02:52<00:41,  3.24it/s] 66%|██████▌   | 258/390 [02:52<00:40,  3.24it/s] 66%|██████▋   | 259/390 [02:52<00:41,  3.12it/s] 67%|██████▋   | 260/390 [02:53<00:40,  3.18it/s] 67%|██████▋   | 261/390 [02:53<00:40,  3.22it/s] 67%|██████▋   | 262/390 [02:53<00:39,  3.25it/s] 67%|██████▋   | 263/390 [02:54<00:38,  3.27it/s] 68%|██████▊   | 264/390 [02:54<00:38,  3.29it/s] 68%|██████▊   | 265/390 [02:54<00:37,  3.30it/s] 68%|██████▊   | 266/390 [02:55<00:38,  3.18it/s] 68%|██████▊   | 267/390 [02:55<00:38,  3.22it/s] 69%|██████▊   | 268/390 [02:55<00:37,  3.25it/s] 69%|██████▉   | 269/390 [02:56<00:37,  3.27it/s] 69%|██████▉   | 270/390 [02:56<00:36,  3.29it/s] 69%|██████▉   | 271/390 [02:56<00:36,  3.30it/s] 70%|██████▉   | 272/390 [02:56<00:35,  3.31it/s] 70%|███████   | 273/390 [02:57<00:35,  3.31it/s] 70%|███████   | 274/390 [02:57<00:34,  3.32it/s] 71%|███████   | 275/390 [02:57<00:34,  3.32it/s] 71%|███████   | 276/390 [02:58<00:34,  3.32it/s] 71%|███████   | 277/390 [02:58<00:34,  3.32it/s] 71%|███████▏  | 278/390 [02:58<00:33,  3.31it/s] 72%|███████▏  | 279/390 [02:59<00:33,  3.32it/s] 72%|███████▏  | 280/390 [02:59<00:33,  3.32it/s] 72%|███████▏  | 281/390 [02:59<00:32,  3.32it/s] 72%|███████▏  | 282/390 [02:59<00:32,  3.32it/s] 73%|███████▎  | 283/390 [03:00<00:32,  3.32it/s] 73%|███████▎  | 284/390 [03:00<00:31,  3.32it/s] 73%|███████▎  | 285/390 [03:00<00:32,  3.23it/s] 73%|███████▎  | 286/390 [03:01<00:31,  3.25it/s] 74%|███████▎  | 287/390 [03:01<00:31,  3.27it/s] 74%|███████▍  | 288/390 [03:01<00:31,  3.29it/s] 74%|███████▍  | 289/390 [03:02<00:30,  3.29it/s] 74%|███████▍  | 290/390 [03:02<00:30,  3.30it/s] 75%|███████▍  | 291/390 [03:02<00:29,  3.31it/s] 75%|███████▍  | 292/390 [03:02<00:29,  3.31it/s] 75%|███████▌  | 293/390 [03:03<00:29,  3.31it/s] 75%|███████▌  | 294/390 [03:03<00:28,  3.32it/s] 76%|███████▌  | 295/390 [03:03<00:28,  3.28it/s] 76%|███████▌  | 296/390 [03:04<00:28,  3.29it/s] 76%|███████▌  | 297/390 [03:04<00:28,  3.30it/s] 76%|███████▋  | 298/390 [03:04<00:27,  3.30it/s] 77%|███████▋  | 299/390 [03:05<00:27,  3.31it/s] 77%|███████▋  | 300/390 [03:05<00:27,  3.31it/s] 77%|███████▋  | 301/390 [03:05<00:26,  3.31it/s] 77%|███████▋  | 302/390 [03:05<00:26,  3.32it/s] 78%|███████▊  | 303/390 [03:06<00:26,  3.31it/s] 78%|███████▊  | 304/390 [03:06<00:25,  3.31it/s] 78%|███████▊  | 305/390 [03:06<00:26,  3.25it/s] 78%|███████▊  | 306/390 [03:07<00:25,  3.27it/s] 79%|███████▊  | 307/390 [03:07<00:25,  3.28it/s] 79%|███████▉  | 308/390 [03:07<00:24,  3.30it/s] 79%|███████▉  | 309/390 [03:08<00:24,  3.30it/s] 79%|███████▉  | 310/390 [03:08<00:24,  3.30it/s] 80%|███████▉  | 311/390 [03:08<00:23,  3.31it/s] 80%|████████  | 312/390 [03:09<00:23,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 19:59:50,247 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:59:50,247 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 19:59:50,247 >>   Batch size = 8
{'eval_loss': 1.100744366645813, 'eval_runtime': 13.1242, 'eval_samples_per_second': 351.184, 'eval_steps_per_second': 43.965, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 55.38it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.26it/s][A
  3%|▎         | 17/577 [00:00<00:11, 46.69it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.83it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.02it/s][A
  6%|▌         | 32/577 [00:00<00:12, 44.52it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.43it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.14it/s][A
  8%|▊         | 47/577 [00:01<00:12, 44.09it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.07it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.10it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.44it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.47it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.21it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.27it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.06it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.91it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 44.02it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 43.98it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.10it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.29it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.41it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.30it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.31it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.15it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.01it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.01it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.13it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.17it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.23it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.35it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.26it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.49it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.67it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.77it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.72it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.04it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.11it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.24it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.27it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.13it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.06it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.04it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.00it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.01it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.12it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.23it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.20it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.25it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.08it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.08it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.09it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.08it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 43.96it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.21it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.32it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.26it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.19it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.06it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.08it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 44.11it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.12it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.16it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.06it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.04it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.97it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.07it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.15it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.21it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.23it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.11it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.99it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.95it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 43.96it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.00it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.14it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.25it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.26it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.05it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.09it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.09it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 42.17it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 42.93it/s][A
 77%|███████▋  | 447/577 [00:10<00:03, 43.27it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.57it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 43.84it/s][A
 80%|████████  | 462/577 [00:10<00:02, 43.91it/s][A
 81%|████████  | 467/577 [00:10<00:02, 43.96it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.95it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.85it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.87it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.06it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 43.93it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.17it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.26it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.37it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.18it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.94it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.99it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.95it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.16it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.08it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.27it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.27it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.29it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.09it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.96it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.88it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 43.26it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.76it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 43.76it/s][A 80%|████████  | 312/390 [03:22<00:23,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:00:03,369 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 20:00:03,440 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:00:09,590 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:00:09,664 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:00:09,693 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:40<12:33,  9.78s/it] 81%|████████  | 314/390 [03:41<08:47,  6.94s/it] 81%|████████  | 315/390 [03:41<06:11,  4.95s/it] 81%|████████  | 316/390 [03:41<04:23,  3.56s/it] 81%|████████▏ | 317/390 [03:42<03:08,  2.58s/it] 82%|████████▏ | 318/390 [03:42<02:16,  1.90s/it] 82%|████████▏ | 319/390 [03:42<01:40,  1.42s/it] 82%|████████▏ | 320/390 [03:43<01:15,  1.08s/it] 82%|████████▏ | 321/390 [03:43<00:58,  1.18it/s] 83%|████████▎ | 322/390 [03:43<00:46,  1.46it/s] 83%|████████▎ | 323/390 [03:43<00:38,  1.76it/s] 83%|████████▎ | 324/390 [03:44<00:32,  2.01it/s] 83%|████████▎ | 325/390 [03:44<00:28,  2.28it/s] 84%|████████▎ | 326/390 [03:44<00:25,  2.52it/s] 84%|████████▍ | 327/390 [03:45<00:23,  2.71it/s] 84%|████████▍ | 328/390 [03:45<00:21,  2.87it/s] 84%|████████▍ | 329/390 [03:45<00:20,  2.99it/s] 85%|████████▍ | 330/390 [03:46<00:19,  3.08it/s] 85%|████████▍ | 331/390 [03:46<00:18,  3.15it/s] 85%|████████▌ | 332/390 [03:46<00:18,  3.20it/s] 85%|████████▌ | 333/390 [03:46<00:17,  3.24it/s] 86%|████████▌ | 334/390 [03:47<00:17,  3.19it/s] 86%|████████▌ | 335/390 [03:47<00:17,  3.23it/s] 86%|████████▌ | 336/390 [03:47<00:16,  3.25it/s] 86%|████████▋ | 337/390 [03:48<00:16,  3.27it/s] 87%|████████▋ | 338/390 [03:48<00:15,  3.28it/s] 87%|████████▋ | 339/390 [03:48<00:15,  3.29it/s] 87%|████████▋ | 340/390 [03:49<00:15,  3.30it/s] 87%|████████▋ | 341/390 [03:49<00:14,  3.30it/s] 88%|████████▊ | 342/390 [03:49<00:14,  3.31it/s] 88%|████████▊ | 343/390 [03:50<00:14,  3.32it/s] 88%|████████▊ | 344/390 [03:50<00:14,  3.27it/s] 88%|████████▊ | 345/390 [03:50<00:13,  3.29it/s] 89%|████████▊ | 346/390 [03:50<00:13,  3.29it/s] 89%|████████▉ | 347/390 [03:51<00:13,  3.30it/s] 89%|████████▉ | 348/390 [03:51<00:12,  3.31it/s] 89%|████████▉ | 349/390 [03:51<00:12,  3.31it/s] 90%|████████▉ | 350/390 [03:52<00:12,  3.32it/s] 90%|█████████ | 351/390 [03:52<00:11,  3.32it/s] 90%|█████████ | 352/390 [03:52<00:11,  3.27it/s] 91%|█████████ | 353/390 [03:53<00:13,  2.82it/s] 91%|█████████ | 354/390 [03:53<00:12,  2.92it/s] 91%|█████████ | 355/390 [03:53<00:11,  3.03it/s] 91%|█████████▏| 356/390 [03:54<00:10,  3.12it/s] 92%|█████████▏| 357/390 [03:54<00:10,  3.17it/s] 92%|█████████▏| 358/390 [03:54<00:09,  3.22it/s] 92%|█████████▏| 359/390 [03:55<00:09,  3.25it/s] 92%|█████████▏| 360/390 [03:55<00:09,  3.27it/s] 93%|█████████▎| 361/390 [03:55<00:08,  3.28it/s] 93%|█████████▎| 362/390 [03:55<00:08,  3.29it/s] 93%|█████████▎| 363/390 [03:56<00:08,  3.30it/s] 93%|█████████▎| 364/390 [03:56<00:08,  3.23it/s] 94%|█████████▎| 365/390 [03:56<00:07,  3.25it/s] 94%|█████████▍| 366/390 [03:57<00:07,  3.27it/s] 94%|█████████▍| 367/390 [03:57<00:06,  3.29it/s] 94%|█████████▍| 368/390 [03:57<00:06,  3.30it/s] 95%|█████████▍| 369/390 [03:58<00:06,  3.30it/s] 95%|█████████▍| 370/390 [03:58<00:06,  3.31it/s] 95%|█████████▌| 371/390 [03:58<00:05,  3.31it/s] 95%|█████████▌| 372/390 [03:58<00:05,  3.31it/s] 96%|█████████▌| 373/390 [03:59<00:05,  3.32it/s] 96%|█████████▌| 374/390 [03:59<00:04,  3.31it/s] 96%|█████████▌| 375/390 [03:59<00:04,  3.31it/s] 96%|█████████▋| 376/390 [04:00<00:04,  3.31it/s] 97%|█████████▋| 377/390 [04:00<00:03,  3.31it/s] 97%|█████████▋| 378/390 [04:00<00:03,  3.32it/s] 97%|█████████▋| 379/390 [04:01<00:03,  3.33it/s] 97%|█████████▋| 380/390 [04:01<00:02,  3.35it/s] 98%|█████████▊| 381/390 [04:01<00:02,  3.36it/s] 98%|█████████▊| 382/390 [04:01<00:02,  3.37it/s] 98%|█████████▊| 383/390 [04:02<00:02,  3.22it/s] 98%|█████████▊| 384/390 [04:02<00:01,  3.27it/s] 99%|█████████▊| 385/390 [04:02<00:01,  3.31it/s] 99%|█████████▉| 386/390 [04:03<00:01,  3.34it/s] 99%|█████████▉| 387/390 [04:03<00:00,  3.36it/s] 99%|█████████▉| 388/390 [04:03<00:00,  3.37it/s]100%|█████████▉| 389/390 [04:04<00:00,  3.38it/s]100%|██████████| 390/390 [04:04<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 20:00:45,553 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:00:45,553 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 20:00:45,553 >>   Batch size = 8
{'eval_loss': 1.1108269691467285, 'eval_runtime': 13.0927, 'eval_samples_per_second': 352.029, 'eval_steps_per_second': 44.07, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.05it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.42it/s][A
  3%|▎         | 17/577 [00:00<00:12, 46.49it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.64it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.08it/s][A
  6%|▌         | 32/577 [00:00<00:12, 42.51it/s][A
  6%|▋         | 37/577 [00:00<00:12, 43.04it/s][A
  7%|▋         | 42/577 [00:00<00:12, 43.19it/s][A
  8%|▊         | 47/577 [00:01<00:12, 43.60it/s][A
  9%|▉         | 52/577 [00:01<00:11, 43.88it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.06it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.29it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.27it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.76it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 43.83it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 43.96it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.99it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.18it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.26it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.35it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.43it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.26it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.98it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.87it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 43.94it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 44.03it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.07it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.28it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.34it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.41it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.20it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.05it/s][A
 29%|██▉       | 167/577 [00:03<00:10, 40.29it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 41.59it/s][A
 31%|███       | 177/577 [00:04<00:09, 42.45it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.08it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.47it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 43.74it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.91it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.82it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.60it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.51it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.78it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.02it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.26it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.47it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.42it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.39it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.98it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 43.80it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 43.89it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 44.07it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.23it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.39it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.44it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.23it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.10it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 43.89it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.30it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.58it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 43.90it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.15it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.22it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.38it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.06it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.89it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.88it/s][A
 61%|██████    | 352/577 [00:08<00:05, 43.86it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.08it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.25it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.39it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.44it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.30it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.15it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.96it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.93it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.97it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.12it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.12it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.27it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.29it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.21it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.09it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 43.94it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 42.65it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.20it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 43.56it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.80it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.09it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.07it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.09it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.73it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 43.78it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.06it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 44.04it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.12it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.27it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.22it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.85it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.84it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 43.87it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 44.02it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.17it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.26it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.18it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.28it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.14it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.95it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.87it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 38.38it/s][A
100%|██████████| 577/577 [00:13<00:00, 40.27it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 40.27it/s][A100%|██████████| 390/390 [04:17<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:00:58,756 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 20:00:58,809 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:01:06,385 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:01:06,407 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:01:06,423 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:01:19,479 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:01:19,490 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78 (score: 1.065826416015625).
                                                 100%|██████████| 390/390 [04:44<00:00,  3.38it/s]100%|██████████| 390/390 [04:44<00:00,  1.37it/s]
[INFO|trainer.py:1894] 2023-08-28 20:01:25,944 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 20:01:26,000 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:01:31,406 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:01:31,424 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:01:31,437 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:01:31,761 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   train_loss               =     0.3908
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   train_runtime            = 0:04:44.72
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   train_samples_per_second =     87.804
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:31,761 >>   train_steps_per_second   =       1.37
{'eval_loss': 1.1163990497589111, 'eval_runtime': 13.1796, 'eval_samples_per_second': 349.706, 'eval_steps_per_second': 43.78, 'epoch': 4.99}
{'train_runtime': 284.7264, 'train_samples_per_second': 87.804, 'train_steps_per_second': 1.37, 'train_loss': 0.3907511001978165, 'epoch': 4.99}
08/28/2023 20:01:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:01:31,948 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:01:31,948 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-28 20:01:31,949 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 54.97it/s]  2%|▏         | 12/577 [00:00<00:11, 48.32it/s]  3%|▎         | 17/577 [00:00<00:11, 46.87it/s]  4%|▍         | 22/577 [00:00<00:12, 45.98it/s]  5%|▍         | 27/577 [00:00<00:12, 45.66it/s]  6%|▌         | 32/577 [00:00<00:12, 45.36it/s]  6%|▋         | 37/577 [00:00<00:11, 45.24it/s]  7%|▋         | 42/577 [00:00<00:11, 44.75it/s]  8%|▊         | 47/577 [00:01<00:11, 44.25it/s]  9%|▉         | 52/577 [00:01<00:11, 44.13it/s] 10%|▉         | 57/577 [00:01<00:11, 44.33it/s] 11%|█         | 62/577 [00:01<00:11, 44.44it/s] 12%|█▏        | 67/577 [00:01<00:11, 44.37it/s] 12%|█▏        | 72/577 [00:01<00:11, 44.41it/s] 13%|█▎        | 77/577 [00:01<00:11, 44.57it/s] 14%|█▍        | 82/577 [00:01<00:11, 44.61it/s] 15%|█▌        | 87/577 [00:01<00:11, 43.63it/s] 16%|█▌        | 92/577 [00:02<00:10, 44.31it/s] 17%|█▋        | 97/577 [00:02<00:10, 44.23it/s] 18%|█▊        | 102/577 [00:02<00:12, 37.05it/s] 18%|█▊        | 106/577 [00:02<00:19, 23.86it/s] 19%|█▉        | 111/577 [00:02<00:16, 27.79it/s] 20%|██        | 116/577 [00:02<00:14, 31.51it/s] 21%|██        | 121/577 [00:03<00:13, 34.59it/s] 22%|██▏       | 126/577 [00:03<00:12, 37.19it/s] 23%|██▎       | 131/577 [00:03<00:11, 39.13it/s] 24%|██▎       | 136/577 [00:03<00:10, 40.80it/s] 24%|██▍       | 141/577 [00:03<00:10, 41.78it/s] 25%|██▌       | 146/577 [00:03<00:10, 42.22it/s] 26%|██▌       | 151/577 [00:03<00:10, 42.51it/s] 27%|██▋       | 156/577 [00:03<00:09, 43.06it/s] 28%|██▊       | 161/577 [00:03<00:09, 43.45it/s] 29%|██▉       | 166/577 [00:04<00:09, 43.68it/s] 30%|██▉       | 171/577 [00:04<00:09, 43.98it/s] 31%|███       | 176/577 [00:04<00:09, 44.21it/s] 31%|███▏      | 181/577 [00:04<00:08, 44.43it/s] 32%|███▏      | 186/577 [00:04<00:08, 44.40it/s] 33%|███▎      | 191/577 [00:04<00:08, 44.15it/s] 34%|███▍      | 196/577 [00:04<00:08, 43.98it/s] 35%|███▍      | 201/577 [00:04<00:08, 44.03it/s] 36%|███▌      | 206/577 [00:04<00:08, 44.07it/s] 37%|███▋      | 211/577 [00:05<00:08, 44.10it/s] 37%|███▋      | 216/577 [00:05<00:08, 44.26it/s] 38%|███▊      | 221/577 [00:05<00:08, 44.47it/s] 39%|███▉      | 226/577 [00:05<00:07, 44.53it/s] 40%|████      | 231/577 [00:05<00:07, 44.38it/s] 41%|████      | 236/577 [00:05<00:07, 44.20it/s] 42%|████▏     | 241/577 [00:05<00:07, 44.08it/s] 43%|████▎     | 246/577 [00:05<00:07, 44.12it/s] 44%|████▎     | 251/577 [00:05<00:07, 44.19it/s] 44%|████▍     | 256/577 [00:06<00:07, 44.23it/s] 45%|████▌     | 261/577 [00:06<00:07, 44.34it/s] 46%|████▌     | 266/577 [00:06<00:06, 44.47it/s] 47%|████▋     | 271/577 [00:06<00:06, 44.36it/s] 48%|████▊     | 276/577 [00:06<00:06, 44.26it/s] 49%|████▊     | 281/577 [00:06<00:06, 44.09it/s] 50%|████▉     | 286/577 [00:06<00:06, 43.98it/s] 50%|█████     | 291/577 [00:06<00:06, 44.16it/s] 51%|█████▏    | 296/577 [00:06<00:06, 44.21it/s] 52%|█████▏    | 301/577 [00:07<00:06, 44.30it/s] 53%|█████▎    | 306/577 [00:07<00:06, 44.35it/s] 54%|█████▍    | 311/577 [00:07<00:05, 44.37it/s] 55%|█████▍    | 316/577 [00:07<00:05, 44.45it/s] 56%|█████▌    | 321/577 [00:07<00:05, 44.10it/s] 56%|█████▋    | 326/577 [00:07<00:05, 43.94it/s] 57%|█████▋    | 331/577 [00:07<00:05, 44.05it/s] 58%|█████▊    | 336/577 [00:07<00:05, 44.20it/s] 59%|█████▉    | 341/577 [00:07<00:05, 44.33it/s] 60%|█████▉    | 346/577 [00:08<00:05, 44.36it/s] 61%|██████    | 351/577 [00:08<00:05, 44.39it/s] 62%|██████▏   | 356/577 [00:08<00:04, 44.49it/s] 63%|██████▎   | 361/577 [00:08<00:04, 44.43it/s] 63%|██████▎   | 366/577 [00:08<00:04, 44.20it/s] 64%|██████▍   | 371/577 [00:08<00:04, 43.94it/s] 65%|██████▌   | 376/577 [00:08<00:04, 44.02it/s] 66%|██████▌   | 381/577 [00:08<00:05, 38.32it/s] 67%|██████▋   | 386/577 [00:09<00:04, 40.12it/s] 68%|██████▊   | 391/577 [00:09<00:04, 41.39it/s] 69%|██████▊   | 396/577 [00:09<00:04, 42.40it/s] 69%|██████▉   | 401/577 [00:09<00:04, 43.00it/s] 70%|███████   | 406/577 [00:09<00:03, 43.56it/s] 71%|███████   | 411/577 [00:09<00:03, 43.76it/s] 72%|███████▏  | 416/577 [00:09<00:03, 43.83it/s] 73%|███████▎  | 421/577 [00:09<00:03, 43.59it/s] 74%|███████▍  | 426/577 [00:09<00:03, 43.45it/s] 75%|███████▍  | 431/577 [00:10<00:03, 43.75it/s] 76%|███████▌  | 436/577 [00:10<00:03, 43.91it/s] 76%|███████▋  | 441/577 [00:10<00:03, 44.22it/s] 77%|███████▋  | 446/577 [00:10<00:02, 44.34it/s] 78%|███████▊  | 451/577 [00:10<00:02, 44.61it/s] 79%|███████▉  | 456/577 [00:10<00:02, 44.39it/s] 80%|███████▉  | 461/577 [00:10<00:02, 44.25it/s] 81%|████████  | 466/577 [00:10<00:02, 43.96it/s] 82%|████████▏ | 471/577 [00:10<00:02, 43.79it/s] 82%|████████▏ | 476/577 [00:11<00:02, 43.82it/s] 83%|████████▎ | 481/577 [00:11<00:02, 43.96it/s] 84%|████████▍ | 486/577 [00:11<00:02, 44.21it/s] 85%|████████▌ | 491/577 [00:11<00:01, 44.31it/s] 86%|████████▌ | 496/577 [00:11<00:01, 44.60it/s] 87%|████████▋ | 501/577 [00:11<00:01, 44.45it/s] 88%|████████▊ | 506/577 [00:11<00:01, 44.18it/s] 89%|████████▊ | 511/577 [00:11<00:01, 43.81it/s] 89%|████████▉ | 516/577 [00:12<00:01, 43.81it/s] 90%|█████████ | 521/577 [00:12<00:01, 43.87it/s] 91%|█████████ | 526/577 [00:12<00:01, 44.05it/s] 92%|█████████▏| 531/577 [00:12<00:01, 44.24it/s] 93%|█████████▎| 536/577 [00:12<00:00, 44.33it/s] 94%|█████████▍| 541/577 [00:12<00:00, 44.43it/s] 95%|█████████▍| 546/577 [00:12<00:00, 44.41it/s] 95%|█████████▌| 551/577 [00:12<00:00, 44.17it/s] 96%|█████████▋| 556/577 [00:12<00:00, 44.03it/s] 97%|█████████▋| 561/577 [00:13<00:00, 43.92it/s] 98%|█████████▊| 566/577 [00:13<00:00, 43.84it/s] 99%|█████████▉| 571/577 [00:13<00:00, 43.81it/s]100%|█████████▉| 576/577 [00:13<00:00, 44.20it/s]100%|██████████| 577/577 [00:13<00:00, 43.08it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:01:45,364 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   eval_loss               =     1.0658
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   eval_runtime            = 0:00:13.41
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   eval_samples_per_second =    343.569
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   eval_steps_per_second   =     43.011
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:01:45,364 >>   perplexity              =     2.9032
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:53,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:53,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:53,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:53,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:53,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:01:54,480 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:01:54,481 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:55,055 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:56,070 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:56,070 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:59,371 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:59,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:59,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:59,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:59,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:02:00,310 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:02:00,311 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:01,381 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:01,562 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:01,562 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.27it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:07,  1.24it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.31it/s]Extractor Predicting: 16it [00:12,  1.32it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:13,  1.33it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.26it/s]Extractor Predicting: 22it [00:17,  1.27it/s]Extractor Predicting: 23it [00:17,  1.26it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.30it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.32it/s]Extractor Predicting: 30it [00:23,  1.34it/s]Extractor Predicting: 31it [00:23,  1.31it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.30it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:27,  1.28it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.27it/s]Extractor Predicting: 38it [00:29,  1.27it/s]Extractor Predicting: 39it [00:30,  1.25it/s]Extractor Predicting: 40it [00:31,  1.26it/s]Extractor Predicting: 41it [00:31,  1.24it/s]Extractor Predicting: 42it [00:32,  1.26it/s]Extractor Predicting: 43it [00:33,  1.24it/s]Extractor Predicting: 44it [00:34,  1.23it/s]Extractor Predicting: 45it [00:35,  1.22it/s]Extractor Predicting: 46it [00:35,  1.22it/s]Extractor Predicting: 47it [00:36,  1.21it/s]Extractor Predicting: 48it [00:37,  1.22it/s]Extractor Predicting: 49it [00:38,  1.26it/s]Extractor Predicting: 50it [00:39,  1.25it/s]Extractor Predicting: 51it [00:39,  1.27it/s]Extractor Predicting: 52it [00:40,  1.25it/s]Extractor Predicting: 53it [00:41,  1.24it/s]Extractor Predicting: 54it [00:42,  1.24it/s]Extractor Predicting: 55it [00:43,  1.24it/s]Extractor Predicting: 56it [00:43,  1.27it/s]Extractor Predicting: 57it [00:44,  1.25it/s]Extractor Predicting: 58it [00:45,  1.24it/s]Extractor Predicting: 59it [00:46,  1.27it/s]Extractor Predicting: 60it [00:47,  1.28it/s]Extractor Predicting: 61it [00:47,  1.25it/s]Extractor Predicting: 62it [00:48,  1.23it/s]Extractor Predicting: 63it [00:49,  1.22it/s]Extractor Predicting: 64it [00:50,  1.21it/s]Extractor Predicting: 65it [00:51,  1.22it/s]Extractor Predicting: 66it [00:51,  1.26it/s]Extractor Predicting: 67it [00:52,  1.27it/s]Extractor Predicting: 68it [00:53,  1.30it/s]Extractor Predicting: 69it [00:54,  1.31it/s]Extractor Predicting: 70it [00:54,  1.29it/s]Extractor Predicting: 71it [00:55,  1.31it/s]Extractor Predicting: 72it [00:56,  1.35it/s]Extractor Predicting: 73it [00:57,  1.32it/s]Extractor Predicting: 74it [00:57,  1.34it/s]Extractor Predicting: 75it [00:58,  1.34it/s]Extractor Predicting: 76it [00:59,  1.30it/s]Extractor Predicting: 77it [01:00,  1.29it/s]Extractor Predicting: 78it [01:01,  1.32it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.31it/s]Extractor Predicting: 81it [01:03,  1.29it/s]Extractor Predicting: 82it [01:04,  1.29it/s]Extractor Predicting: 83it [01:04,  1.29it/s]Extractor Predicting: 84it [01:05,  1.32it/s]Extractor Predicting: 85it [01:06,  1.38it/s]Extractor Predicting: 86it [01:07,  1.37it/s]Extractor Predicting: 87it [01:07,  1.39it/s]Extractor Predicting: 88it [01:08,  1.32it/s]Extractor Predicting: 89it [01:09,  1.30it/s]Extractor Predicting: 90it [01:10,  1.29it/s]Extractor Predicting: 91it [01:10,  1.30it/s]Extractor Predicting: 92it [01:11,  1.31it/s]Extractor Predicting: 93it [01:12,  1.32it/s]Extractor Predicting: 94it [01:13,  1.32it/s]Extractor Predicting: 95it [01:13,  1.31it/s]Extractor Predicting: 96it [01:14,  1.32it/s]Extractor Predicting: 97it [01:15,  1.33it/s]Extractor Predicting: 98it [01:16,  1.29it/s]Extractor Predicting: 99it [01:16,  1.30it/s]Extractor Predicting: 100it [01:17,  1.28it/s]Extractor Predicting: 101it [01:18,  1.27it/s]Extractor Predicting: 102it [01:19,  1.29it/s]Extractor Predicting: 103it [01:20,  1.27it/s]Extractor Predicting: 104it [01:20,  1.30it/s]Extractor Predicting: 105it [01:21,  1.32it/s]Extractor Predicting: 106it [01:22,  1.28it/s]Extractor Predicting: 107it [01:23,  1.22it/s]Extractor Predicting: 108it [01:24,  1.26it/s]Extractor Predicting: 109it [01:24,  1.26it/s]Extractor Predicting: 110it [01:25,  1.27it/s]Extractor Predicting: 111it [01:26,  1.26it/s]Extractor Predicting: 112it [01:27,  1.25it/s]Extractor Predicting: 113it [01:28,  1.26it/s]Extractor Predicting: 114it [01:28,  1.27it/s]Extractor Predicting: 115it [01:29,  1.26it/s]Extractor Predicting: 116it [01:30,  1.27it/s]Extractor Predicting: 117it [01:31,  1.30it/s]Extractor Predicting: 118it [01:31,  1.31it/s]Extractor Predicting: 119it [01:32,  1.35it/s]Extractor Predicting: 120it [01:33,  1.33it/s]Extractor Predicting: 121it [01:34,  1.32it/s]Extractor Predicting: 122it [01:34,  1.31it/s]Extractor Predicting: 123it [01:35,  1.25it/s]Extractor Predicting: 124it [01:36,  1.28it/s]Extractor Predicting: 125it [01:37,  1.26it/s]Extractor Predicting: 126it [01:38,  1.30it/s]Extractor Predicting: 127it [01:38,  1.30it/s]Extractor Predicting: 128it [01:39,  1.29it/s]Extractor Predicting: 129it [01:40,  1.28it/s]Extractor Predicting: 130it [01:41,  1.26it/s]Extractor Predicting: 131it [01:42,  1.26it/s]Extractor Predicting: 132it [01:42,  1.28it/s]Extractor Predicting: 133it [01:43,  1.29it/s]Extractor Predicting: 134it [01:44,  1.28it/s]Extractor Predicting: 135it [01:45,  1.25it/s]Extractor Predicting: 136it [01:45,  1.27it/s]Extractor Predicting: 137it [01:46,  1.29it/s]Extractor Predicting: 138it [01:47,  1.28it/s]Extractor Predicting: 139it [01:48,  1.26it/s]Extractor Predicting: 140it [01:49,  1.25it/s]Extractor Predicting: 141it [01:49,  1.25it/s]Extractor Predicting: 142it [01:50,  1.23it/s]Extractor Predicting: 143it [01:51,  1.24it/s]Extractor Predicting: 144it [01:52,  1.23it/s]Extractor Predicting: 145it [01:53,  1.24it/s]Extractor Predicting: 146it [01:54,  1.20it/s]Extractor Predicting: 147it [01:54,  1.22it/s]Extractor Predicting: 148it [01:55,  1.24it/s]Extractor Predicting: 149it [01:56,  1.26it/s]Extractor Predicting: 150it [01:57,  1.25it/s]Extractor Predicting: 151it [01:57,  1.27it/s]Extractor Predicting: 152it [01:58,  1.27it/s]Extractor Predicting: 153it [01:59,  1.29it/s]Extractor Predicting: 154it [02:00,  1.28it/s]Extractor Predicting: 155it [02:01,  1.27it/s]Extractor Predicting: 156it [02:01,  1.25it/s]Extractor Predicting: 157it [02:02,  1.27it/s]Extractor Predicting: 158it [02:03,  1.27it/s]Extractor Predicting: 159it [02:04,  1.25it/s]Extractor Predicting: 160it [02:05,  1.27it/s]Extractor Predicting: 161it [02:05,  1.29it/s]Extractor Predicting: 162it [02:06,  1.28it/s]Extractor Predicting: 163it [02:07,  1.24it/s]Extractor Predicting: 164it [02:08,  1.25it/s]Extractor Predicting: 165it [02:09,  1.25it/s]Extractor Predicting: 166it [02:09,  1.24it/s]Extractor Predicting: 167it [02:10,  1.25it/s]Extractor Predicting: 168it [02:11,  1.26it/s]Extractor Predicting: 169it [02:12,  1.26it/s]Extractor Predicting: 170it [02:13,  1.26it/s]Extractor Predicting: 171it [02:13,  1.49it/s]Extractor Predicting: 171it [02:13,  1.28it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:24,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:24,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:24,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:24,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:24,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:04:24,998 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:04:24,999 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:04:25,646 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:04:26,726 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:04:26,726 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:29,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:29,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:29,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:29,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:04:29,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:04:30,274 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:04:30,275 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:04:30,555 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:04:30,742 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:04:30,743 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2701525054466231,
  "recall": 0.05380776741158603,
  "score": 0.08974127012846028,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.30it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:07,  1.24it/s]Extractor Predicting: 10it [00:07,  1.21it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.25it/s]Extractor Predicting: 14it [00:10,  1.26it/s]Extractor Predicting: 15it [00:11,  1.25it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:13,  1.25it/s]Extractor Predicting: 18it [00:14,  1.28it/s]Extractor Predicting: 19it [00:14,  1.30it/s]Extractor Predicting: 20it [00:15,  1.29it/s]Extractor Predicting: 21it [00:16,  1.32it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:18,  1.28it/s]Extractor Predicting: 25it [00:19,  1.31it/s]Extractor Predicting: 26it [00:20,  1.33it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:21,  1.33it/s]Extractor Predicting: 29it [00:22,  1.33it/s]Extractor Predicting: 30it [00:23,  1.31it/s]Extractor Predicting: 31it [00:23,  1.30it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.30it/s]Extractor Predicting: 35it [00:27,  1.31it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.31it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:30,  1.31it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:33,  1.31it/s]Extractor Predicting: 44it [00:33,  1.32it/s]Extractor Predicting: 45it [00:34,  1.33it/s]Extractor Predicting: 46it [00:35,  1.34it/s]Extractor Predicting: 47it [00:36,  1.31it/s]Extractor Predicting: 48it [00:36,  1.30it/s]Extractor Predicting: 49it [00:37,  1.30it/s]Extractor Predicting: 50it [00:38,  1.30it/s]Extractor Predicting: 51it [00:39,  1.32it/s]Extractor Predicting: 52it [00:40,  1.32it/s]Extractor Predicting: 53it [00:40,  1.32it/s]Extractor Predicting: 54it [00:41,  1.30it/s]Extractor Predicting: 55it [00:42,  1.29it/s]Extractor Predicting: 56it [00:43,  1.30it/s]Extractor Predicting: 57it [00:43,  1.31it/s]Extractor Predicting: 58it [00:44,  1.32it/s]Extractor Predicting: 59it [00:45,  1.32it/s]Extractor Predicting: 60it [00:46,  1.25it/s]Extractor Predicting: 61it [00:47,  1.26it/s]Extractor Predicting: 62it [00:47,  1.26it/s]Extractor Predicting: 63it [00:48,  1.30it/s]Extractor Predicting: 64it [00:49,  1.30it/s]Extractor Predicting: 65it [00:50,  1.31it/s]Extractor Predicting: 66it [00:50,  1.30it/s]Extractor Predicting: 67it [00:51,  1.29it/s]Extractor Predicting: 68it [00:52,  1.31it/s]Extractor Predicting: 69it [00:53,  1.33it/s]Extractor Predicting: 70it [00:53,  1.31it/s]Extractor Predicting: 71it [00:54,  1.26it/s]Extractor Predicting: 72it [00:55,  1.29it/s]Extractor Predicting: 73it [00:56,  1.28it/s]Extractor Predicting: 74it [00:57,  1.22it/s]Extractor Predicting: 75it [00:57,  1.23it/s]Extractor Predicting: 76it [00:58,  1.28it/s]Extractor Predicting: 77it [00:59,  1.32it/s]Extractor Predicting: 78it [01:00,  1.32it/s]Extractor Predicting: 79it [01:00,  1.31it/s]Extractor Predicting: 80it [01:01,  1.32it/s]Extractor Predicting: 81it [01:02,  1.31it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:03,  1.32it/s]Extractor Predicting: 84it [01:04,  1.25it/s]Extractor Predicting: 85it [01:05,  1.27it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:07,  1.28it/s]Extractor Predicting: 88it [01:07,  1.26it/s]Extractor Predicting: 89it [01:08,  1.27it/s]Extractor Predicting: 90it [01:09,  1.25it/s]Extractor Predicting: 91it [01:10,  1.27it/s]Extractor Predicting: 92it [01:11,  1.21it/s]Extractor Predicting: 93it [01:11,  1.24it/s]Extractor Predicting: 94it [01:12,  1.25it/s]Extractor Predicting: 95it [01:13,  1.25it/s]Extractor Predicting: 96it [01:14,  1.18it/s]Extractor Predicting: 97it [01:15,  1.23it/s]Extractor Predicting: 98it [01:16,  1.24it/s]Extractor Predicting: 99it [01:16,  1.32it/s]Extractor Predicting: 100it [01:17,  1.28it/s]Extractor Predicting: 101it [01:18,  1.28it/s]Extractor Predicting: 102it [01:19,  1.30it/s]Extractor Predicting: 103it [01:19,  1.30it/s]Extractor Predicting: 104it [01:20,  1.31it/s]Extractor Predicting: 105it [01:21,  1.31it/s]Extractor Predicting: 106it [01:22,  1.32it/s]Extractor Predicting: 107it [01:22,  1.34it/s]Extractor Predicting: 108it [01:23,  1.33it/s]Extractor Predicting: 109it [01:24,  1.29it/s]Extractor Predicting: 110it [01:25,  1.23it/s]Extractor Predicting: 111it [01:26,  1.22it/s]Extractor Predicting: 112it [01:26,  1.23it/s]Extractor Predicting: 113it [01:27,  1.27it/s]Extractor Predicting: 114it [01:28,  1.29it/s]Extractor Predicting: 115it [01:29,  1.28it/s]Extractor Predicting: 116it [01:29,  1.29it/s]Extractor Predicting: 117it [01:30,  1.28it/s]Extractor Predicting: 118it [01:31,  1.24it/s]Extractor Predicting: 119it [01:32,  1.26it/s]Extractor Predicting: 120it [01:33,  1.24it/s]Extractor Predicting: 121it [01:34,  1.24it/s]Extractor Predicting: 122it [01:34,  1.25it/s]Extractor Predicting: 123it [01:35,  1.27it/s]Extractor Predicting: 124it [01:36,  1.27it/s]Extractor Predicting: 125it [01:36,  1.57it/s]Extractor Predicting: 125it [01:36,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:17,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:17,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:17,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:17,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:17,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:06:17,818 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:06:17,819 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:06:18,386 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:06:19,435 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:06:19,435 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:22,379 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:22,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:22,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:22,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:06:22,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:06:23,045 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:06:23,046 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:06:23,635 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:06:23,804 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:06:23,804 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4346938775510204,
  "recall": 0.14300100704934543,
  "score": 0.2152058600656732,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.18it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.22it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.62it/s]Extractor Predicting: 6it [00:04,  1.38it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4864864864864865,
  "recall": 0.07086614173228346,
  "score": 0.12371134020618557,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
