/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4990 mean pseudo reward: 0.9790434639442265
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 22953
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23053, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23053, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.674, loss:3122.1709
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.158, loss:2155.7881
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.161, loss:1812.4106
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.173, loss:1684.8080
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.163, loss:1594.9970
>> valid entity prec:0.5386, rec:0.6380, f1:0.5841
>> valid relation prec:0.5082, rec:0.1597, f1:0.2431
>> valid relation with NER prec:0.5082, rec:0.1597, f1:0.2431
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.840, loss:1546.0846
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.161, loss:1382.8362
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.173, loss:1339.6571
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.172, loss:1211.5131
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.186, loss:1180.1323
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6121, rec:0.5048, f1:0.5533
>> valid relation prec:0.4902, rec:0.1283, f1:0.2033
>> valid relation with NER prec:0.4902, rec:0.1283, f1:0.2033
g_step 1100, step 60, avg_time 2.747, loss:1130.9000
g_step 1200, step 160, avg_time 1.162, loss:1068.5919
g_step 1300, step 52, avg_time 1.162, loss:1021.5931
g_step 1400, step 152, avg_time 1.171, loss:994.3958
g_step 1500, step 44, avg_time 1.174, loss:956.2395
>> valid entity prec:0.5576, rec:0.5949, f1:0.5756
>> valid relation prec:0.4037, rec:0.1260, f1:0.1920
>> valid relation with NER prec:0.4037, rec:0.1260, f1:0.1920
g_step 1600, step 144, avg_time 2.746, loss:954.1041
g_step 1700, step 36, avg_time 1.167, loss:907.9755
g_step 1800, step 136, avg_time 1.163, loss:867.4386
g_step 1900, step 28, avg_time 1.163, loss:840.2125
g_step 2000, step 128, avg_time 1.175, loss:830.4331
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5799, rec:0.5426, f1:0.5607
>> valid relation prec:0.3847, rec:0.1328, f1:0.1975
>> valid relation with NER prec:0.3847, rec:0.1328, f1:0.1975
g_step 2100, step 20, avg_time 2.727, loss:839.0907
g_step 2200, step 120, avg_time 1.166, loss:776.3932
g_step 2300, step 12, avg_time 1.168, loss:766.2213
g_step 2400, step 112, avg_time 1.173, loss:716.3816
g_step 2500, step 4, avg_time 1.157, loss:755.7591
>> valid entity prec:0.5827, rec:0.5682, f1:0.5754
>> valid relation prec:0.3533, rec:0.1331, f1:0.1934
>> valid relation with NER prec:0.3533, rec:0.1331, f1:0.1934
g_step 2600, step 104, avg_time 2.748, loss:684.5935
g_step 2700, step 204, avg_time 1.161, loss:731.7766
g_step 2800, step 96, avg_time 1.158, loss:645.0606
g_step 2900, step 196, avg_time 1.170, loss:685.6325
g_step 3000, step 88, avg_time 1.171, loss:619.8594
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6008, rec:0.4890, f1:0.5392
>> valid relation prec:0.3265, rec:0.1180, f1:0.1733
>> valid relation with NER prec:0.3265, rec:0.1180, f1:0.1733
g_step 3100, step 188, avg_time 2.746, loss:658.7428
g_step 3200, step 80, avg_time 1.167, loss:614.0345
g_step 3300, step 180, avg_time 1.168, loss:608.4634
g_step 3400, step 72, avg_time 1.161, loss:578.1168
g_step 3500, step 172, avg_time 1.179, loss:585.2006
>> valid entity prec:0.5394, rec:0.5736, f1:0.5559
>> valid relation prec:0.2418, rec:0.1122, f1:0.1533
>> valid relation with NER prec:0.2418, rec:0.1122, f1:0.1533
g_step 3600, step 64, avg_time 2.735, loss:553.2676
g_step 3700, step 164, avg_time 1.174, loss:556.4002
g_step 3800, step 56, avg_time 1.163, loss:525.9300
g_step 3900, step 156, avg_time 1.158, loss:532.7162
g_step 4000, step 48, avg_time 1.168, loss:526.9871
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5616, rec:0.5464, f1:0.5539
>> valid relation prec:0.2382, rec:0.1071, f1:0.1477
>> valid relation with NER prec:0.2382, rec:0.1071, f1:0.1477
g_step 4100, step 148, avg_time 2.754, loss:507.3798
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:42:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:42:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-42-22_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:42:24 - WARNING - datasets.builder -   Using custom data configuration default-cdde0453f8444597
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-cdde0453f8444597/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.38 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:42:26,106 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:42:26,115 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:42:26,130 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:42:26,131 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:42:26,192 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:42:26,225 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:42:26,760 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:42:34,880 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:42:34,911 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-cdde0453f8444597/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:42:34 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14d1de28b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:03,  1.46ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.45ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.12ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.10ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.53ba/s]100%|██████████| 6/6 [00:01<00:00,  3.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.43ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.05ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27ba/s]100%|██████████| 4/4 [00:00<00:00,  5.39ba/s]100%|██████████| 4/4 [00:00<00:00,  4.78ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.32ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.21ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.19ba/s]100%|██████████| 6/6 [00:00<00:00, 10.33ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.70ba/s] 50%|█████     | 2/4 [00:00<00:00,  6.81ba/s]100%|██████████| 4/4 [00:00<00:00,  9.87ba/s]100%|██████████| 4/4 [00:00<00:00,  8.66ba/s]
[INFO|trainer.py:414] 2023-08-27 22:42:39,645 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:42:39,799 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:42:39,799 >>   Num examples = 5004
[INFO|trainer.py:1149] 2023-08-27 22:42:39,799 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:42:39,799 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:42:39,799 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:42:39,799 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:42:39,799 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:01<09:29,  1.46s/it]  1%|          | 2/390 [00:01<05:02,  1.28it/s]  1%|          | 3/390 [00:02<03:36,  1.79it/s]  1%|          | 4/390 [00:02<02:55,  2.20it/s]  1%|▏         | 5/390 [00:02<02:33,  2.51it/s]  2%|▏         | 6/390 [00:02<02:19,  2.75it/s]  2%|▏         | 7/390 [00:03<02:10,  2.93it/s]  2%|▏         | 8/390 [00:03<02:05,  3.05it/s]  2%|▏         | 9/390 [00:03<02:01,  3.15it/s]  3%|▎         | 10/390 [00:04<01:58,  3.21it/s]  3%|▎         | 11/390 [00:04<01:56,  3.26it/s]  3%|▎         | 12/390 [00:04<01:54,  3.30it/s]  3%|▎         | 13/390 [00:05<01:53,  3.32it/s]  4%|▎         | 14/390 [00:05<01:52,  3.34it/s]  4%|▍         | 15/390 [00:05<01:51,  3.35it/s]  4%|▍         | 16/390 [00:05<01:51,  3.36it/s]  4%|▍         | 17/390 [00:06<01:50,  3.36it/s]  5%|▍         | 18/390 [00:06<01:50,  3.36it/s]  5%|▍         | 19/390 [00:06<01:50,  3.36it/s]  5%|▌         | 20/390 [00:07<01:49,  3.37it/s]  5%|▌         | 21/390 [00:07<01:53,  3.26it/s]  6%|▌         | 22/390 [00:07<01:51,  3.29it/s]  6%|▌         | 23/390 [00:08<01:50,  3.32it/s]  6%|▌         | 24/390 [00:08<01:49,  3.34it/s]  6%|▋         | 25/390 [00:08<01:48,  3.35it/s]  7%|▋         | 26/390 [00:08<01:48,  3.36it/s]  7%|▋         | 27/390 [00:09<01:48,  3.36it/s]  7%|▋         | 28/390 [00:09<01:47,  3.36it/s]  7%|▋         | 29/390 [00:09<01:47,  3.37it/s]  8%|▊         | 30/390 [00:10<01:46,  3.37it/s]  8%|▊         | 31/390 [00:10<01:46,  3.37it/s]  8%|▊         | 32/390 [00:10<01:46,  3.37it/s]  8%|▊         | 33/390 [00:10<01:46,  3.36it/s]  9%|▊         | 34/390 [00:11<01:45,  3.36it/s]  9%|▉         | 35/390 [00:11<01:45,  3.36it/s]  9%|▉         | 36/390 [00:11<01:45,  3.37it/s]  9%|▉         | 37/390 [00:12<01:44,  3.37it/s] 10%|▉         | 38/390 [00:12<01:44,  3.37it/s] 10%|█         | 39/390 [00:12<01:44,  3.36it/s] 10%|█         | 40/390 [00:13<01:44,  3.36it/s] 11%|█         | 41/390 [00:13<01:45,  3.31it/s] 11%|█         | 42/390 [00:13<01:44,  3.32it/s] 11%|█         | 43/390 [00:13<01:43,  3.34it/s] 11%|█▏        | 44/390 [00:14<01:43,  3.35it/s] 12%|█▏        | 45/390 [00:14<01:42,  3.36it/s] 12%|█▏        | 46/390 [00:14<01:42,  3.36it/s] 12%|█▏        | 47/390 [00:15<01:41,  3.36it/s] 12%|█▏        | 48/390 [00:15<01:41,  3.36it/s] 13%|█▎        | 49/390 [00:15<01:41,  3.36it/s] 13%|█▎        | 50/390 [00:16<01:41,  3.37it/s] 13%|█▎        | 51/390 [00:16<01:40,  3.37it/s] 13%|█▎        | 52/390 [00:16<01:40,  3.36it/s] 14%|█▎        | 53/390 [00:16<01:40,  3.36it/s] 14%|█▍        | 54/390 [00:17<01:40,  3.35it/s] 14%|█▍        | 55/390 [00:17<01:39,  3.35it/s] 14%|█▍        | 56/390 [00:17<01:39,  3.35it/s] 15%|█▍        | 57/390 [00:18<01:39,  3.36it/s] 15%|█▍        | 58/390 [00:18<01:38,  3.36it/s] 15%|█▌        | 59/390 [00:18<01:38,  3.36it/s] 15%|█▌        | 60/390 [00:19<01:38,  3.36it/s] 16%|█▌        | 61/390 [00:19<01:37,  3.36it/s] 16%|█▌        | 62/390 [00:19<01:37,  3.36it/s] 16%|█▌        | 63/390 [00:19<01:37,  3.36it/s] 16%|█▋        | 64/390 [00:20<01:37,  3.36it/s] 17%|█▋        | 65/390 [00:20<01:36,  3.36it/s] 17%|█▋        | 66/390 [00:20<01:36,  3.36it/s] 17%|█▋        | 67/390 [00:21<01:36,  3.35it/s] 17%|█▋        | 68/390 [00:21<01:36,  3.35it/s] 18%|█▊        | 69/390 [00:21<01:35,  3.35it/s] 18%|█▊        | 70/390 [00:22<01:35,  3.35it/s] 18%|█▊        | 71/390 [00:22<01:35,  3.36it/s] 18%|█▊        | 72/390 [00:22<01:34,  3.36it/s] 19%|█▊        | 73/390 [00:22<01:34,  3.36it/s] 19%|█▉        | 74/390 [00:23<01:34,  3.36it/s] 19%|█▉        | 75/390 [00:23<01:33,  3.35it/s] 19%|█▉        | 76/390 [00:23<01:33,  3.35it/s] 20%|█▉        | 77/390 [00:24<01:33,  3.36it/s] 20%|██        | 78/390 [00:24<01:32,  3.36it/s][INFO|trainer.py:2140] 2023-08-27 22:43:04,305 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:43:04,305 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:43:04,305 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.26it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.24it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.83it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.22it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.83it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.56it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.22it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.64it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.86it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.22it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.32it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.45it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.68it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.71it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.59it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.33it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.22it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.26it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.37it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.53it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.68it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.77it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.56it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.28it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.35it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.45it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.53it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.73it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.71it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.30it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.50it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.79it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.04it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.16it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.38it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.57it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.64it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.67it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.31it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.25it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.28it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.48it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.59it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.62it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.71it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.58it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.46it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.37it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.34it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.45it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.60it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.51it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.06it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.27it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.24it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.28it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.44it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.52it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.70it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.70it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.59it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.34it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.33it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.32it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.44it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.48it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.65it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.69it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.64it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.38it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.29it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.30it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.46it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.61it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.67it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.70it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.59it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:34<01:32,  3.36it/s]
100%|██████████| 438/438 [00:09<00:00, 44.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:43:14,294 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-27 22:43:14,420 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:43:16,870 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:43:16,927 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:43:16,960 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:43<31:09,  6.01s/it] 21%|██        | 80/390 [00:44<22:14,  4.30s/it] 21%|██        | 81/390 [00:44<15:58,  3.10s/it] 21%|██        | 82/390 [00:44<11:36,  2.26s/it] 21%|██▏       | 83/390 [00:44<08:33,  1.67s/it] 22%|██▏       | 84/390 [00:45<06:25,  1.26s/it] 22%|██▏       | 85/390 [00:45<04:55,  1.03it/s] 22%|██▏       | 86/390 [00:45<03:53,  1.30it/s] 22%|██▏       | 87/390 [00:46<03:10,  1.59it/s] 23%|██▎       | 88/390 [00:46<02:39,  1.89it/s] 23%|██▎       | 89/390 [00:46<02:18,  2.17it/s] 23%|██▎       | 90/390 [00:47<02:03,  2.43it/s] 23%|██▎       | 91/390 [00:47<01:52,  2.65it/s] 24%|██▎       | 92/390 [00:47<01:47,  2.78it/s] 24%|██▍       | 93/390 [00:47<01:41,  2.93it/s] 24%|██▍       | 94/390 [00:48<01:37,  3.05it/s] 24%|██▍       | 95/390 [00:48<01:34,  3.14it/s] 25%|██▍       | 96/390 [00:48<01:31,  3.20it/s] 25%|██▍       | 97/390 [00:49<01:30,  3.25it/s] 25%|██▌       | 98/390 [00:49<01:29,  3.28it/s] 25%|██▌       | 99/390 [00:49<01:28,  3.30it/s] 26%|██▌       | 100/390 [00:50<01:27,  3.32it/s] 26%|██▌       | 101/390 [00:50<01:26,  3.34it/s] 26%|██▌       | 102/390 [00:50<01:25,  3.35it/s] 26%|██▋       | 103/390 [00:50<01:26,  3.30it/s] 27%|██▋       | 104/390 [00:51<01:26,  3.32it/s] 27%|██▋       | 105/390 [00:51<01:25,  3.31it/s] 27%|██▋       | 106/390 [00:51<01:25,  3.33it/s] 27%|██▋       | 107/390 [00:52<01:24,  3.34it/s] 28%|██▊       | 108/390 [00:52<01:24,  3.35it/s] 28%|██▊       | 109/390 [00:52<01:23,  3.35it/s] 28%|██▊       | 110/390 [00:53<01:23,  3.36it/s] 28%|██▊       | 111/390 [00:53<01:22,  3.36it/s] 29%|██▊       | 112/390 [00:53<01:22,  3.36it/s] 29%|██▉       | 113/390 [00:53<01:22,  3.36it/s] 29%|██▉       | 114/390 [00:54<01:25,  3.23it/s] 29%|██▉       | 115/390 [00:54<01:24,  3.27it/s] 30%|██▉       | 116/390 [00:54<01:23,  3.30it/s] 30%|███       | 117/390 [00:55<01:22,  3.32it/s] 30%|███       | 118/390 [00:55<01:21,  3.33it/s] 31%|███       | 119/390 [00:55<01:21,  3.34it/s] 31%|███       | 120/390 [00:56<01:20,  3.35it/s] 31%|███       | 121/390 [00:56<01:20,  3.35it/s] 31%|███▏      | 122/390 [00:56<01:20,  3.35it/s] 32%|███▏      | 123/390 [00:56<01:19,  3.35it/s] 32%|███▏      | 124/390 [00:57<01:20,  3.32it/s] 32%|███▏      | 125/390 [00:57<01:19,  3.33it/s] 32%|███▏      | 126/390 [00:57<01:19,  3.34it/s] 33%|███▎      | 127/390 [00:58<01:18,  3.34it/s] 33%|███▎      | 128/390 [00:58<01:18,  3.35it/s] 33%|███▎      | 129/390 [00:58<01:17,  3.35it/s] 33%|███▎      | 130/390 [00:59<01:17,  3.36it/s] 34%|███▎      | 131/390 [00:59<01:16,  3.36it/s] 34%|███▍      | 132/390 [00:59<01:16,  3.37it/s] 34%|███▍      | 133/390 [00:59<01:16,  3.37it/s] 34%|███▍      | 134/390 [01:00<01:16,  3.36it/s] 35%|███▍      | 135/390 [01:00<01:20,  3.16it/s] 35%|███▍      | 136/390 [01:00<01:18,  3.22it/s] 35%|███▌      | 137/390 [01:01<01:17,  3.25it/s] 35%|███▌      | 138/390 [01:01<01:16,  3.28it/s] 36%|███▌      | 139/390 [01:01<01:16,  3.30it/s] 36%|███▌      | 140/390 [01:02<01:15,  3.32it/s] 36%|███▌      | 141/390 [01:02<01:14,  3.33it/s] 36%|███▋      | 142/390 [01:02<01:14,  3.34it/s] 37%|███▋      | 143/390 [01:02<01:13,  3.34it/s] 37%|███▋      | 144/390 [01:03<01:13,  3.35it/s] 37%|███▋      | 145/390 [01:03<01:16,  3.20it/s] 37%|███▋      | 146/390 [01:03<01:15,  3.25it/s] 38%|███▊      | 147/390 [01:04<01:14,  3.28it/s] 38%|███▊      | 148/390 [01:04<01:13,  3.30it/s] 38%|███▊      | 149/390 [01:04<01:12,  3.32it/s] 38%|███▊      | 150/390 [01:05<01:12,  3.33it/s] 39%|███▊      | 151/390 [01:05<01:11,  3.34it/s] 39%|███▉      | 152/390 [01:05<01:11,  3.34it/s] 39%|███▉      | 153/390 [01:05<01:10,  3.34it/s] 39%|███▉      | 154/390 [01:06<01:10,  3.34it/s] 40%|███▉      | 155/390 [01:06<01:13,  3.20it/s] 40%|████      | 156/390 [01:06<01:12,  3.24it/s][INFO|trainer.py:2140] 2023-08-27 22:43:46,781 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:43:46,781 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:43:46,781 >>   Batch size = 8
{'eval_loss': 0.9527348279953003, 'eval_runtime': 9.853, 'eval_samples_per_second': 355.02, 'eval_steps_per_second': 44.454, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.14it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.28it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.82it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.90it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.66it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.35it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.50it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.64it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.69it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.46it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.31it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.20it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.21it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.39it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.57it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.64it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.73it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 41.77it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.61it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.22it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.51it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.66it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.91it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.15it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.37it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.30it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.30it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.48it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.49it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.47it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.33it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.49it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.50it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.44it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.47it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.60it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.60it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.53it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.38it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.41it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.43it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.42it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 41.11it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 42.13it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 42.94it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.64it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.91it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.72it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.29it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 41.28it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 42.75it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 43.29it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.72it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.08it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.37it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.50it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.54it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.36it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.08it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.12it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.33it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.44it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.52it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.68it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.64it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.68it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.36it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.18it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.31it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.37it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.51it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.54it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.55it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.64it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.27it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.28it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.28it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.31it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.46it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.46it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.56it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:16<01:12,  3.24it/s]
100%|██████████| 438/438 [00:09<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:43:56,812 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-27 22:43:56,975 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:43:59,329 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:43:59,569 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:43:59,661 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:25<22:03,  5.68s/it] 41%|████      | 158/390 [01:25<15:44,  4.07s/it] 41%|████      | 159/390 [01:25<11:19,  2.94s/it] 41%|████      | 160/390 [01:26<08:13,  2.15s/it] 41%|████▏     | 161/390 [01:26<06:04,  1.59s/it] 42%|████▏     | 162/390 [01:26<04:34,  1.20s/it] 42%|████▏     | 163/390 [01:26<03:31,  1.07it/s] 42%|████▏     | 164/390 [01:27<02:47,  1.35it/s] 42%|████▏     | 165/390 [01:27<02:16,  1.65it/s] 43%|████▎     | 166/390 [01:27<01:55,  1.94it/s] 43%|████▎     | 167/390 [01:28<01:40,  2.23it/s] 43%|████▎     | 168/390 [01:28<01:29,  2.48it/s] 43%|████▎     | 169/390 [01:28<01:22,  2.66it/s] 44%|████▎     | 170/390 [01:29<01:17,  2.84it/s] 44%|████▍     | 171/390 [01:29<01:13,  2.98it/s] 44%|████▍     | 172/390 [01:29<01:10,  3.08it/s] 44%|████▍     | 173/390 [01:29<01:08,  3.16it/s] 45%|████▍     | 174/390 [01:30<01:07,  3.22it/s] 45%|████▍     | 175/390 [01:30<01:05,  3.26it/s] 45%|████▌     | 176/390 [01:30<01:05,  3.29it/s] 45%|████▌     | 177/390 [01:31<01:04,  3.31it/s] 46%|████▌     | 178/390 [01:31<01:03,  3.32it/s] 46%|████▌     | 179/390 [01:31<01:03,  3.33it/s] 46%|████▌     | 180/390 [01:32<01:03,  3.32it/s] 46%|████▋     | 181/390 [01:32<01:02,  3.33it/s] 47%|████▋     | 182/390 [01:32<01:02,  3.33it/s] 47%|████▋     | 183/390 [01:32<01:02,  3.34it/s] 47%|████▋     | 184/390 [01:33<01:01,  3.34it/s] 47%|████▋     | 185/390 [01:33<01:01,  3.34it/s] 48%|████▊     | 186/390 [01:33<01:00,  3.35it/s] 48%|████▊     | 187/390 [01:34<01:00,  3.35it/s] 48%|████▊     | 188/390 [01:34<01:00,  3.35it/s] 48%|████▊     | 189/390 [01:34<00:59,  3.35it/s] 49%|████▊     | 190/390 [01:35<00:59,  3.35it/s] 49%|████▉     | 191/390 [01:35<01:00,  3.29it/s] 49%|████▉     | 192/390 [01:35<00:59,  3.31it/s] 49%|████▉     | 193/390 [01:35<00:59,  3.32it/s] 50%|████▉     | 194/390 [01:36<00:58,  3.33it/s] 50%|█████     | 195/390 [01:36<00:58,  3.34it/s] 50%|█████     | 196/390 [01:36<00:57,  3.35it/s] 51%|█████     | 197/390 [01:37<00:57,  3.35it/s] 51%|█████     | 198/390 [01:37<00:57,  3.36it/s] 51%|█████     | 199/390 [01:37<00:56,  3.36it/s] 51%|█████▏    | 200/390 [01:38<00:56,  3.36it/s] 52%|█████▏    | 201/390 [01:38<00:56,  3.36it/s] 52%|█████▏    | 202/390 [01:38<00:56,  3.32it/s] 52%|█████▏    | 203/390 [01:38<00:56,  3.33it/s] 52%|█████▏    | 204/390 [01:39<00:55,  3.34it/s] 53%|█████▎    | 205/390 [01:39<00:55,  3.35it/s] 53%|█████▎    | 206/390 [01:39<00:54,  3.36it/s] 53%|█████▎    | 207/390 [01:40<00:54,  3.38it/s] 53%|█████▎    | 208/390 [01:40<00:53,  3.40it/s] 54%|█████▎    | 209/390 [01:40<00:53,  3.41it/s] 54%|█████▍    | 210/390 [01:40<00:52,  3.43it/s] 54%|█████▍    | 211/390 [01:41<00:52,  3.43it/s] 54%|█████▍    | 212/390 [01:41<00:51,  3.44it/s] 55%|█████▍    | 213/390 [01:41<00:52,  3.40it/s] 55%|█████▍    | 214/390 [01:42<00:51,  3.41it/s] 55%|█████▌    | 215/390 [01:42<00:51,  3.42it/s] 55%|█████▌    | 216/390 [01:42<00:53,  3.27it/s] 56%|█████▌    | 217/390 [01:43<01:08,  2.54it/s] 56%|█████▌    | 218/390 [01:43<01:02,  2.76it/s] 56%|█████▌    | 219/390 [01:43<00:58,  2.94it/s] 56%|█████▋    | 220/390 [01:44<00:55,  3.08it/s] 57%|█████▋    | 221/390 [01:44<00:53,  3.18it/s] 57%|█████▋    | 222/390 [01:44<00:51,  3.25it/s] 57%|█████▋    | 223/390 [01:45<00:52,  3.18it/s] 57%|█████▋    | 224/390 [01:45<00:51,  3.25it/s] 58%|█████▊    | 225/390 [01:45<00:49,  3.31it/s] 58%|█████▊    | 226/390 [01:46<00:49,  3.35it/s] 58%|█████▊    | 227/390 [01:46<00:48,  3.38it/s] 58%|█████▊    | 228/390 [01:46<00:47,  3.40it/s] 59%|█████▊    | 229/390 [01:46<00:47,  3.41it/s] 59%|█████▉    | 230/390 [01:47<00:46,  3.42it/s] 59%|█████▉    | 231/390 [01:47<00:46,  3.43it/s] 59%|█████▉    | 232/390 [01:47<00:45,  3.44it/s] 60%|█████▉    | 233/390 [01:48<00:45,  3.44it/s] 60%|██████    | 234/390 [01:48<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-27 22:44:28,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:44:28,177 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:44:28,177 >>   Batch size = 8
{'eval_loss': 0.9415568709373474, 'eval_runtime': 9.9222, 'eval_samples_per_second': 352.543, 'eval_steps_per_second': 44.143, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.07it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.07it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.42it/s][A
  5%|▌         | 22/438 [00:00<00:09, 43.93it/s][A
  6%|▌         | 27/438 [00:00<00:09, 44.13it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.25it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.45it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.73it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.35it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.38it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.47it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.42it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.52it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.39it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.46it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.62it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.50it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.48it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.45it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.47it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.41it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.39it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.50it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.58it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.63it/s][A
 36%|███▌      | 157/438 [00:03<00:07, 37.40it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 39.43it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 40.97it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 42.08it/s][A
 40%|████      | 177/438 [00:04<00:06, 42.94it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.54it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 43.97it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.06it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 43.85it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.65it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.80it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.10it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.32it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.51it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.71it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.73it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.62it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.30it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.04it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.25it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.36it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.57it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.64it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.70it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.78it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.49it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.16it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.11it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.20it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.37it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.59it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.69it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.82it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.71it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.48it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.31it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.24it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.29it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.45it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.50it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.65it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.71it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.67it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.49it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.36it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.24it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.35it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.45it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.58it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.63it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.76it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.62it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.51it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.78it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 43.96it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.16it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:58<00:45,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:44:38,222 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 22:44:38,319 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:44:41,329 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:44:41,589 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:44:41,743 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:09<16:59,  6.58s/it] 61%|██████    | 236/390 [02:09<12:04,  4.71s/it] 61%|██████    | 237/390 [02:10<08:37,  3.38s/it] 61%|██████    | 238/390 [02:10<06:13,  2.46s/it] 61%|██████▏   | 239/390 [02:10<04:33,  1.81s/it] 62%|██████▏   | 240/390 [02:11<03:23,  1.36s/it] 62%|██████▏   | 241/390 [02:11<02:34,  1.04s/it] 62%|██████▏   | 242/390 [02:11<02:00,  1.22it/s] 62%|██████▏   | 243/390 [02:12<01:37,  1.51it/s] 63%|██████▎   | 244/390 [02:12<01:20,  1.81it/s] 63%|██████▎   | 245/390 [02:12<01:08,  2.10it/s] 63%|██████▎   | 246/390 [02:12<01:02,  2.30it/s] 63%|██████▎   | 247/390 [02:13<00:56,  2.55it/s] 64%|██████▎   | 248/390 [02:13<00:51,  2.77it/s] 64%|██████▍   | 249/390 [02:13<00:47,  2.94it/s] 64%|██████▍   | 250/390 [02:14<00:45,  3.08it/s] 64%|██████▍   | 251/390 [02:14<00:43,  3.18it/s] 65%|██████▍   | 252/390 [02:14<00:42,  3.26it/s] 65%|██████▍   | 253/390 [02:14<00:41,  3.31it/s] 65%|██████▌   | 254/390 [02:15<00:40,  3.35it/s] 65%|██████▌   | 255/390 [02:15<00:39,  3.38it/s] 66%|██████▌   | 256/390 [02:15<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:16<00:40,  3.30it/s] 66%|██████▌   | 258/390 [02:16<00:39,  3.32it/s] 66%|██████▋   | 259/390 [02:16<00:40,  3.25it/s] 67%|██████▋   | 260/390 [02:17<00:39,  3.28it/s] 67%|██████▋   | 261/390 [02:17<00:38,  3.31it/s] 67%|██████▋   | 262/390 [02:17<00:38,  3.32it/s] 67%|██████▋   | 263/390 [02:17<00:38,  3.34it/s] 68%|██████▊   | 264/390 [02:18<00:37,  3.34it/s] 68%|██████▊   | 265/390 [02:18<00:37,  3.35it/s] 68%|██████▊   | 266/390 [02:18<00:37,  3.35it/s] 68%|██████▊   | 267/390 [02:19<00:36,  3.35it/s] 69%|██████▊   | 268/390 [02:19<00:36,  3.35it/s] 69%|██████▉   | 269/390 [02:19<00:37,  3.24it/s] 69%|██████▉   | 270/390 [02:20<00:36,  3.28it/s] 69%|██████▉   | 271/390 [02:20<00:36,  3.30it/s] 70%|██████▉   | 272/390 [02:20<00:35,  3.31it/s] 70%|███████   | 273/390 [02:20<00:35,  3.32it/s] 70%|███████   | 274/390 [02:21<00:34,  3.33it/s] 71%|███████   | 275/390 [02:21<00:34,  3.34it/s] 71%|███████   | 276/390 [02:21<00:34,  3.35it/s] 71%|███████   | 277/390 [02:22<00:33,  3.35it/s] 71%|███████▏  | 278/390 [02:22<00:33,  3.36it/s] 72%|███████▏  | 279/390 [02:22<00:33,  3.29it/s] 72%|███████▏  | 280/390 [02:23<00:33,  3.32it/s] 72%|███████▏  | 281/390 [02:23<00:32,  3.33it/s] 72%|███████▏  | 282/390 [02:23<00:32,  3.34it/s] 73%|███████▎  | 283/390 [02:23<00:31,  3.35it/s] 73%|███████▎  | 284/390 [02:24<00:31,  3.36it/s] 73%|███████▎  | 285/390 [02:24<00:31,  3.36it/s] 73%|███████▎  | 286/390 [02:24<00:30,  3.36it/s] 74%|███████▎  | 287/390 [02:25<00:30,  3.36it/s] 74%|███████▍  | 288/390 [02:25<00:30,  3.35it/s] 74%|███████▍  | 289/390 [02:25<00:30,  3.36it/s] 74%|███████▍  | 290/390 [02:26<00:30,  3.32it/s] 75%|███████▍  | 291/390 [02:26<00:29,  3.33it/s] 75%|███████▍  | 292/390 [02:26<00:29,  3.34it/s] 75%|███████▌  | 293/390 [02:26<00:28,  3.35it/s] 75%|███████▌  | 294/390 [02:27<00:28,  3.35it/s] 76%|███████▌  | 295/390 [02:27<00:28,  3.36it/s] 76%|███████▌  | 296/390 [02:27<00:27,  3.36it/s] 76%|███████▌  | 297/390 [02:28<00:27,  3.37it/s] 76%|███████▋  | 298/390 [02:28<00:27,  3.37it/s] 77%|███████▋  | 299/390 [02:28<00:27,  3.37it/s] 77%|███████▋  | 300/390 [02:29<00:26,  3.37it/s] 77%|███████▋  | 301/390 [02:29<00:27,  3.28it/s] 77%|███████▋  | 302/390 [02:29<00:26,  3.30it/s] 78%|███████▊  | 303/390 [02:29<00:26,  3.32it/s] 78%|███████▊  | 304/390 [02:30<00:25,  3.33it/s] 78%|███████▊  | 305/390 [02:30<00:25,  3.34it/s] 78%|███████▊  | 306/390 [02:30<00:25,  3.35it/s] 79%|███████▊  | 307/390 [02:31<00:24,  3.35it/s] 79%|███████▉  | 308/390 [02:31<00:24,  3.35it/s] 79%|███████▉  | 309/390 [02:31<00:24,  3.35it/s] 79%|███████▉  | 310/390 [02:32<00:23,  3.35it/s] 80%|███████▉  | 311/390 [02:32<00:24,  3.21it/s] 80%|████████  | 312/390 [02:32<00:23,  3.25it/s][INFO|trainer.py:2140] 2023-08-27 22:45:12,541 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:45:12,541 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:45:12,541 >>   Batch size = 8
{'eval_loss': 0.9427719712257385, 'eval_runtime': 9.9351, 'eval_samples_per_second': 352.085, 'eval_steps_per_second': 44.086, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.27it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.11it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.61it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.84it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.22it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.72it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.47it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.63it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.71it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.71it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.74it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.67it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.44it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.45it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.34it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.27it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.46it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.60it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.69it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.81it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.04it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.15it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.23it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.12it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.07it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.24it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.39it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.64it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.51it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.46it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.46it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.58it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.37it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.31it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.39it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.59it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.66it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.70it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.67it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.55it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.46it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 44.14it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.12it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.31it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.50it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.35it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.39it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.93it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.18it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.13it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.14it/s][A
 61%|██████    | 267/438 [00:05<00:03, 43.97it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.06it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.38it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.50it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.67it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.60it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.34it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.30it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.35it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.43it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.57it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.63it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.65it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.53it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.42it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.27it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.41it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.50it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.55it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.56it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.65it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 40.83it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 42.01it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 42.84it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 43.43it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.72it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.98it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.23it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.31it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.97it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.16it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.31it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.44it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:42<00:23,  3.25it/s]
100%|██████████| 438/438 [00:09<00:00, 44.44it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:45:22,560 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-27 22:45:22,711 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:45:26,280 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:45:26,428 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:45:26,488 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:53<08:06,  6.31s/it] 81%|████████  | 314/390 [02:53<05:43,  4.51s/it] 81%|████████  | 315/390 [02:53<04:03,  3.25s/it] 81%|████████  | 316/390 [02:53<02:54,  2.36s/it] 81%|████████▏ | 317/390 [02:54<02:07,  1.74s/it] 82%|████████▏ | 318/390 [02:54<01:34,  1.31s/it] 82%|████████▏ | 319/390 [02:54<01:11,  1.01s/it] 82%|████████▏ | 320/390 [02:55<00:55,  1.26it/s] 82%|████████▏ | 321/390 [02:55<00:44,  1.55it/s] 83%|████████▎ | 322/390 [02:55<00:36,  1.85it/s] 83%|████████▎ | 323/390 [02:56<00:31,  2.14it/s] 83%|████████▎ | 324/390 [02:56<00:27,  2.40it/s] 83%|████████▎ | 325/390 [02:56<00:25,  2.58it/s] 84%|████████▎ | 326/390 [02:56<00:23,  2.77it/s] 84%|████████▍ | 327/390 [02:57<00:21,  2.92it/s] 84%|████████▍ | 328/390 [02:57<00:20,  3.04it/s] 84%|████████▍ | 329/390 [02:57<00:19,  3.13it/s] 85%|████████▍ | 330/390 [02:58<00:18,  3.19it/s] 85%|████████▍ | 331/390 [02:58<00:18,  3.24it/s] 85%|████████▌ | 332/390 [02:58<00:17,  3.27it/s] 85%|████████▌ | 333/390 [02:59<00:17,  3.30it/s] 86%|████████▌ | 334/390 [02:59<00:16,  3.32it/s] 86%|████████▌ | 335/390 [02:59<00:16,  3.29it/s] 86%|████████▌ | 336/390 [02:59<00:16,  3.31it/s] 86%|████████▋ | 337/390 [03:00<00:15,  3.33it/s] 87%|████████▋ | 338/390 [03:00<00:15,  3.34it/s] 87%|████████▋ | 339/390 [03:00<00:15,  3.35it/s] 87%|████████▋ | 340/390 [03:01<00:14,  3.35it/s] 87%|████████▋ | 341/390 [03:01<00:14,  3.36it/s] 88%|████████▊ | 342/390 [03:01<00:14,  3.36it/s] 88%|████████▊ | 343/390 [03:01<00:13,  3.36it/s] 88%|████████▊ | 344/390 [03:02<00:13,  3.36it/s] 88%|████████▊ | 345/390 [03:02<00:13,  3.36it/s] 89%|████████▊ | 346/390 [03:02<00:13,  3.28it/s] 89%|████████▉ | 347/390 [03:03<00:13,  3.30it/s] 89%|████████▉ | 348/390 [03:03<00:12,  3.31it/s] 89%|████████▉ | 349/390 [03:03<00:12,  3.32it/s] 90%|████████▉ | 350/390 [03:04<00:12,  3.33it/s] 90%|█████████ | 351/390 [03:04<00:11,  3.34it/s] 90%|█████████ | 352/390 [03:04<00:11,  3.34it/s] 91%|█████████ | 353/390 [03:04<00:11,  3.34it/s] 91%|█████████ | 354/390 [03:05<00:10,  3.34it/s] 91%|█████████ | 355/390 [03:05<00:10,  3.34it/s] 91%|█████████▏| 356/390 [03:05<00:10,  3.23it/s] 92%|█████████▏| 357/390 [03:06<00:10,  3.26it/s] 92%|█████████▏| 358/390 [03:06<00:09,  3.28it/s] 92%|█████████▏| 359/390 [03:06<00:09,  3.30it/s] 92%|█████████▏| 360/390 [03:07<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:07<00:08,  3.33it/s] 93%|█████████▎| 362/390 [03:07<00:08,  3.33it/s] 93%|█████████▎| 363/390 [03:08<00:08,  3.34it/s] 93%|█████████▎| 364/390 [03:08<00:07,  3.35it/s] 94%|█████████▎| 365/390 [03:08<00:07,  3.35it/s] 94%|█████████▍| 366/390 [03:08<00:07,  3.23it/s] 94%|█████████▍| 367/390 [03:09<00:07,  3.26it/s] 94%|█████████▍| 368/390 [03:09<00:06,  3.29it/s] 95%|█████████▍| 369/390 [03:09<00:06,  3.30it/s] 95%|█████████▍| 370/390 [03:10<00:06,  3.32it/s] 95%|█████████▌| 371/390 [03:10<00:05,  3.32it/s] 95%|█████████▌| 372/390 [03:10<00:05,  3.33it/s] 96%|█████████▌| 373/390 [03:11<00:05,  3.34it/s] 96%|█████████▌| 374/390 [03:11<00:04,  3.35it/s] 96%|█████████▌| 375/390 [03:11<00:04,  3.35it/s] 96%|█████████▋| 376/390 [03:11<00:04,  3.16it/s] 97%|█████████▋| 377/390 [03:12<00:04,  3.22it/s] 97%|█████████▋| 378/390 [03:12<00:03,  3.25it/s] 97%|█████████▋| 379/390 [03:12<00:03,  3.28it/s] 97%|█████████▋| 380/390 [03:13<00:03,  3.30it/s] 98%|█████████▊| 381/390 [03:13<00:02,  3.16it/s] 98%|█████████▊| 382/390 [03:13<00:02,  3.22it/s] 98%|█████████▊| 383/390 [03:14<00:02,  3.26it/s] 98%|█████████▊| 384/390 [03:14<00:01,  3.29it/s] 99%|█████████▊| 385/390 [03:14<00:01,  3.31it/s] 99%|█████████▉| 386/390 [03:15<00:01,  3.20it/s] 99%|█████████▉| 387/390 [03:15<00:00,  3.25it/s] 99%|█████████▉| 388/390 [03:15<00:00,  3.29it/s]100%|█████████▉| 389/390 [03:15<00:00,  3.31it/s]100%|██████████| 390/390 [03:16<00:00,  3.33it/s][INFO|trainer.py:2140] 2023-08-27 22:45:56,058 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:45:56,058 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:45:56,058 >>   Batch size = 8
{'eval_loss': 0.9466835856437683, 'eval_runtime': 9.8903, 'eval_samples_per_second': 353.678, 'eval_steps_per_second': 44.286, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.17it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.26it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.88it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.89it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.38it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.96it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.72it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.53it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.81it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.75it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.79it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.74it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.67it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.44it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 43.76it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 43.94it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.01it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.18it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.34it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.52it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.56it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.32it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.39it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.42it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.45it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.58it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.63it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.73it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.62it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.57it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.37it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.35it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.28it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.40it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.50it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.52it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.74it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.58it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.55it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.35it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.40it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.47it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.52it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.48it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.54it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.66it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.69it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.54it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.50it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.47it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.47it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.53it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.47it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.43it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.55it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.62it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.53it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.44it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.41it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.57it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.58it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.53it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.45it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.55it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.61it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.57it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.55it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 42.62it/s][A
 80%|████████  | 352/438 [00:07<00:01, 43.33it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.77it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.93it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.16it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.34it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.39it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.40it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.26it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.22it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.41it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.50it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.53it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.52it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.57it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.64it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.48it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.39it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.37it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:26<00:00,  3.33it/s]
100%|██████████| 438/438 [00:09<00:00, 44.37it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:46:06,051 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-27 22:46:06,180 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:46:08,454 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:46:08,609 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:46:08,687 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:46:16,000 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:46:16,027 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156 (score: 0.9415568709373474).
                                                 100%|██████████| 390/390 [03:42<00:00,  3.33it/s]100%|██████████| 390/390 [03:42<00:00,  1.75it/s]
[INFO|trainer.py:1894] 2023-08-27 22:46:22,394 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:46:22,526 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:46:25,909 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:46:26,031 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:46:26,100 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:46:26,579 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   train_loss               =     0.8306
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   train_runtime            = 0:03:42.56
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   train_samples            =       5004
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   train_samples_per_second =    112.416
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:26,579 >>   train_steps_per_second   =      1.752
{'eval_loss': 0.9481102824211121, 'eval_runtime': 9.8566, 'eval_samples_per_second': 354.889, 'eval_steps_per_second': 44.437, 'epoch': 4.99}
{'train_runtime': 222.5655, 'train_samples_per_second': 112.416, 'train_steps_per_second': 1.752, 'train_loss': 0.8306406656901042, 'epoch': 4.99}
08/27/2023 22:46:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:46:26,877 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:46:26,877 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-27 22:46:26,877 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.33it/s]  3%|▎         | 12/438 [00:00<00:21, 20.21it/s]  4%|▍         | 17/438 [00:00<00:16, 26.25it/s]  5%|▌         | 22/438 [00:00<00:13, 31.11it/s]  6%|▌         | 27/438 [00:00<00:11, 34.87it/s]  7%|▋         | 32/438 [00:00<00:10, 37.74it/s]  8%|▊         | 37/438 [00:01<00:10, 39.87it/s] 10%|▉         | 42/438 [00:01<00:09, 41.42it/s] 11%|█         | 47/438 [00:01<00:09, 42.48it/s] 12%|█▏        | 52/438 [00:01<00:09, 42.85it/s] 13%|█▎        | 57/438 [00:01<00:08, 43.06it/s] 14%|█▍        | 62/438 [00:01<00:09, 40.95it/s] 15%|█▌        | 67/438 [00:01<00:08, 42.15it/s] 16%|█▋        | 72/438 [00:01<00:08, 43.10it/s] 18%|█▊        | 77/438 [00:02<00:08, 43.75it/s] 19%|█▊        | 82/438 [00:02<00:08, 44.16it/s] 20%|█▉        | 87/438 [00:02<00:07, 44.51it/s] 21%|██        | 92/438 [00:02<00:07, 44.61it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.37it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.16it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.21it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.37it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.64it/s] 28%|██▊       | 122/438 [00:03<00:07, 44.92it/s] 29%|██▉       | 127/438 [00:03<00:06, 44.66it/s] 30%|███       | 132/438 [00:03<00:06, 44.94it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.91it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.73it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.42it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.35it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.39it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.78it/s] 38%|███▊      | 167/438 [00:04<00:06, 44.84it/s] 39%|███▉      | 172/438 [00:04<00:05, 44.92it/s] 40%|████      | 177/438 [00:04<00:05, 45.06it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.96it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.67it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.59it/s] 45%|████▍     | 197/438 [00:04<00:05, 42.98it/s] 46%|████▌     | 202/438 [00:04<00:05, 43.62it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.00it/s] 48%|████▊     | 212/438 [00:05<00:05, 44.28it/s] 50%|████▉     | 217/438 [00:05<00:04, 44.49it/s] 51%|█████     | 222/438 [00:05<00:04, 44.77it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.83it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.55it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.41it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.47it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.67it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.72it/s] 59%|█████▊    | 257/438 [00:06<00:04, 44.81it/s] 60%|█████▉    | 262/438 [00:06<00:03, 44.60it/s] 61%|██████    | 267/438 [00:06<00:03, 44.83it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.83it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.66it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.53it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.51it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.59it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.70it/s] 69%|██████▉   | 302/438 [00:07<00:03, 44.75it/s] 70%|███████   | 307/438 [00:07<00:02, 44.90it/s] 71%|███████   | 312/438 [00:07<00:02, 44.86it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.87it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.70it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.63it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.57it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.56it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.60it/s] 79%|███████▉  | 347/438 [00:08<00:02, 44.69it/s] 80%|████████  | 352/438 [00:08<00:01, 44.78it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.87it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.84it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.77it/s] 85%|████████▍ | 372/438 [00:08<00:01, 44.64it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.63it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.70it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.64it/s] 89%|████████▉ | 392/438 [00:09<00:01, 44.78it/s] 91%|█████████ | 397/438 [00:09<00:00, 44.73it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.74it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.83it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.80it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.64it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.64it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.64it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.62it/s]100%|█████████▉| 437/438 [00:10<00:00, 44.66it/s]100%|██████████| 438/438 [00:10<00:00, 43.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:46:36,993 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   eval_loss               =     0.9416
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   eval_runtime            = 0:00:10.11
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   eval_samples_per_second =    345.794
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   eval_steps_per_second   =     43.298
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:46:36,993 >>   perplexity              =      2.564
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_0', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:21<03:11, 21.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:43<02:54, 21.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:19<03:17, 28.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:40<02:31, 25.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:02<02:00, 24.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:23<01:32, 23.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:44<01:07, 22.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:00<00:40, 20.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:18<00:19, 19.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:44<00:00, 21.51s/it]Generating: 100%|██████████| 10/10 [03:44<00:00, 22.43s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_5_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:19, 19.38s/it]Extractor Estimating: 2it [00:20,  8.80s/it]Extractor Estimating: 3it [00:21,  5.12s/it]Extractor Estimating: 4it [00:24,  4.36s/it]Extractor Estimating: 5it [00:25,  3.04s/it]Extractor Estimating: 6it [00:26,  2.29s/it]Extractor Estimating: 7it [00:27,  1.80s/it]Extractor Estimating: 8it [00:27,  1.47s/it]Extractor Estimating: 9it [00:28,  1.27s/it]Extractor Estimating: 10it [00:29,  1.14s/it]Extractor Estimating: 11it [00:30,  1.04s/it]Extractor Estimating: 12it [00:31,  1.00it/s]Extractor Estimating: 13it [00:31,  1.07it/s]Extractor Estimating: 14it [00:32,  1.15it/s]Extractor Estimating: 15it [00:33,  1.16it/s]Extractor Estimating: 16it [00:34,  1.19it/s]Extractor Estimating: 17it [00:35,  1.19it/s]Extractor Estimating: 18it [00:36,  1.20it/s]Extractor Estimating: 19it [00:36,  1.19it/s]Extractor Estimating: 20it [00:37,  1.18it/s]Extractor Estimating: 21it [00:38,  1.14it/s]Extractor Estimating: 22it [00:39,  1.13it/s]Extractor Estimating: 23it [00:40,  1.20it/s]Extractor Estimating: 24it [00:41,  1.23it/s]Extractor Estimating: 25it [00:41,  1.22it/s]Extractor Estimating: 26it [00:42,  1.19it/s]Extractor Estimating: 27it [00:43,  1.22it/s]Extractor Estimating: 28it [00:44,  1.20it/s]Extractor Estimating: 29it [00:45,  1.17it/s]Extractor Estimating: 30it [00:46,  1.14it/s]Extractor Estimating: 31it [00:47,  1.17it/s]Extractor Estimating: 32it [00:47,  1.20it/s]Extractor Estimating: 33it [00:48,  1.23it/s]Extractor Estimating: 34it [00:49,  1.23it/s]Extractor Estimating: 35it [00:50,  1.22it/s]Extractor Estimating: 36it [00:51,  1.14it/s]Extractor Estimating: 37it [00:52,  1.19it/s]Extractor Estimating: 38it [00:52,  1.16it/s]Extractor Estimating: 39it [00:53,  1.14it/s]Extractor Estimating: 40it [00:54,  1.12it/s]Extractor Estimating: 41it [00:55,  1.16it/s]Extractor Estimating: 42it [00:56,  1.10it/s]Extractor Estimating: 43it [00:57,  1.12it/s]Extractor Estimating: 44it [00:58,  1.08it/s]Extractor Estimating: 45it [00:59,  1.14it/s]Extractor Estimating: 46it [01:00,  1.15it/s]Extractor Estimating: 47it [01:00,  1.17it/s]Extractor Estimating: 48it [01:01,  1.14it/s]Extractor Estimating: 49it [01:02,  1.18it/s]Extractor Estimating: 50it [01:03,  1.14it/s]Extractor Estimating: 51it [01:04,  1.14it/s]Extractor Estimating: 52it [01:05,  1.16it/s]Extractor Estimating: 53it [01:06,  1.19it/s]Extractor Estimating: 54it [01:06,  1.21it/s]Extractor Estimating: 55it [01:07,  1.24it/s]Extractor Estimating: 56it [01:08,  1.24it/s]Extractor Estimating: 57it [01:09,  1.23it/s]Extractor Estimating: 58it [01:09,  1.25it/s]Extractor Estimating: 59it [01:10,  1.31it/s]Extractor Estimating: 60it [01:11,  1.31it/s]Extractor Estimating: 61it [01:12,  1.26it/s]Extractor Estimating: 62it [01:13,  1.26it/s]Extractor Estimating: 63it [01:13,  1.28it/s]Extractor Estimating: 64it [01:14,  1.30it/s]Extractor Estimating: 65it [01:15,  1.33it/s]Extractor Estimating: 66it [01:16,  1.32it/s]Extractor Estimating: 67it [01:16,  1.32it/s]Extractor Estimating: 68it [01:17,  1.29it/s]Extractor Estimating: 69it [01:18,  1.33it/s]Extractor Estimating: 70it [01:19,  1.28it/s]Extractor Estimating: 71it [01:19,  1.31it/s]Extractor Estimating: 72it [01:20,  1.34it/s]Extractor Estimating: 73it [01:21,  1.28it/s]Extractor Estimating: 74it [01:22,  1.29it/s]Extractor Estimating: 75it [01:23,  1.24it/s]Extractor Estimating: 76it [01:23,  1.30it/s]Extractor Estimating: 77it [01:24,  1.33it/s]Extractor Estimating: 78it [01:25,  1.26it/s]Extractor Estimating: 79it [01:26,  1.27it/s]Extractor Estimating: 80it [01:26,  1.31it/s]Extractor Estimating: 81it [01:27,  1.30it/s]Extractor Estimating: 82it [01:28,  1.31it/s]Extractor Estimating: 83it [01:29,  1.29it/s]Extractor Estimating: 84it [01:29,  1.34it/s]Extractor Estimating: 85it [01:30,  1.37it/s]Extractor Estimating: 86it [01:31,  1.38it/s]Extractor Estimating: 87it [01:32,  1.36it/s]Extractor Estimating: 88it [01:32,  1.39it/s]Extractor Estimating: 89it [01:33,  1.33it/s]Extractor Estimating: 90it [01:34,  1.36it/s]Extractor Estimating: 91it [01:34,  1.38it/s]Extractor Estimating: 92it [01:35,  1.31it/s]Extractor Estimating: 93it [01:36,  1.32it/s]Extractor Estimating: 94it [01:37,  1.33it/s]Extractor Estimating: 95it [01:38,  1.32it/s]Extractor Estimating: 96it [01:38,  1.34it/s]Extractor Estimating: 97it [01:39,  1.36it/s]Extractor Estimating: 98it [01:40,  1.34it/s]Extractor Estimating: 99it [01:40,  1.34it/s]Extractor Estimating: 100it [01:41,  1.31it/s]Extractor Estimating: 101it [01:42,  1.34it/s]Extractor Estimating: 102it [01:43,  1.38it/s]Extractor Estimating: 103it [01:43,  1.34it/s]Extractor Estimating: 104it [01:44,  1.34it/s]Extractor Estimating: 105it [01:45,  1.29it/s]Extractor Estimating: 106it [01:46,  1.30it/s]Extractor Estimating: 107it [01:47,  1.35it/s]Extractor Estimating: 108it [01:47,  1.40it/s]Extractor Estimating: 109it [01:48,  1.42it/s]Extractor Estimating: 110it [01:48,  1.45it/s]Extractor Estimating: 111it [01:49,  1.40it/s]Extractor Estimating: 112it [01:50,  1.40it/s]Extractor Estimating: 113it [01:51,  1.39it/s]Extractor Estimating: 114it [01:51,  1.43it/s]Extractor Estimating: 115it [01:52,  1.28it/s]Extractor Estimating: 116it [01:53,  1.29it/s]Extractor Estimating: 117it [01:54,  1.32it/s]Extractor Estimating: 118it [01:54,  1.38it/s]Extractor Estimating: 119it [01:55,  1.41it/s]Extractor Estimating: 120it [01:56,  1.39it/s]Extractor Estimating: 121it [01:57,  1.35it/s]Extractor Estimating: 122it [01:57,  1.31it/s]Extractor Estimating: 123it [01:58,  1.33it/s]Extractor Estimating: 124it [01:59,  1.33it/s]Extractor Estimating: 125it [02:00,  1.35it/s]Extractor Estimating: 126it [02:00,  1.37it/s]Extractor Estimating: 127it [02:01,  1.34it/s]Extractor Estimating: 128it [02:02,  1.35it/s]Extractor Estimating: 129it [02:03,  1.34it/s]Extractor Estimating: 130it [02:03,  1.31it/s]Extractor Estimating: 131it [02:04,  1.31it/s]Extractor Estimating: 132it [02:05,  1.38it/s]Extractor Estimating: 133it [02:06,  1.37it/s]Extractor Estimating: 134it [02:06,  1.38it/s]Extractor Estimating: 135it [02:07,  1.36it/s]Extractor Estimating: 136it [02:08,  1.33it/s]Extractor Estimating: 137it [02:09,  1.39it/s]Extractor Estimating: 138it [02:09,  1.41it/s]Extractor Estimating: 139it [02:10,  1.39it/s]Extractor Estimating: 140it [02:11,  1.37it/s]Extractor Estimating: 141it [02:11,  1.36it/s]Extractor Estimating: 142it [02:12,  1.37it/s]Extractor Estimating: 143it [02:13,  1.33it/s]Extractor Estimating: 144it [02:14,  1.28it/s]Extractor Estimating: 145it [02:15,  1.25it/s]Extractor Estimating: 146it [02:15,  1.27it/s]Extractor Estimating: 147it [02:16,  1.31it/s]Extractor Estimating: 148it [02:17,  1.33it/s]Extractor Estimating: 149it [02:18,  1.32it/s]Extractor Estimating: 150it [02:18,  1.31it/s]Extractor Estimating: 151it [02:19,  1.27it/s]Extractor Estimating: 152it [02:20,  1.26it/s]Extractor Estimating: 153it [02:21,  1.29it/s]Extractor Estimating: 154it [02:22,  1.28it/s]Extractor Estimating: 155it [02:22,  1.27it/s]Extractor Estimating: 156it [02:23,  1.21it/s]Extractor Estimating: 157it [02:24,  1.23it/s]Extractor Estimating: 158it [02:25,  1.23it/s]Extractor Estimating: 159it [02:26,  1.16it/s]Extractor Estimating: 160it [02:27,  1.20it/s]Extractor Estimating: 161it [02:27,  1.25it/s]Extractor Estimating: 162it [02:28,  1.24it/s]Extractor Estimating: 163it [02:29,  1.27it/s]Extractor Estimating: 164it [02:30,  1.26it/s]Extractor Estimating: 165it [02:30,  1.27it/s]Extractor Estimating: 166it [02:31,  1.29it/s]Extractor Estimating: 167it [02:32,  1.28it/s]Extractor Estimating: 168it [02:33,  1.27it/s]Extractor Estimating: 169it [02:34,  1.22it/s]Extractor Estimating: 170it [02:35,  1.22it/s]Extractor Estimating: 171it [02:35,  1.23it/s]Extractor Estimating: 172it [02:36,  1.27it/s]Extractor Estimating: 173it [02:37,  1.22it/s]Extractor Estimating: 174it [02:38,  1.23it/s]Extractor Estimating: 175it [02:39,  1.23it/s]Extractor Estimating: 176it [02:39,  1.20it/s]Extractor Estimating: 177it [02:40,  1.28it/s]Extractor Estimating: 178it [02:41,  1.32it/s]Extractor Estimating: 179it [02:42,  1.35it/s]Extractor Estimating: 180it [02:42,  1.38it/s]Extractor Estimating: 181it [02:43,  1.40it/s]Extractor Estimating: 182it [02:44,  1.38it/s]Extractor Estimating: 183it [02:44,  1.40it/s]Extractor Estimating: 184it [02:45,  1.40it/s]Extractor Estimating: 185it [02:46,  1.40it/s]Extractor Estimating: 186it [02:47,  1.36it/s]Extractor Estimating: 187it [02:47,  1.34it/s]Extractor Estimating: 188it [02:48,  1.38it/s]Extractor Estimating: 189it [02:49,  1.37it/s]Extractor Estimating: 190it [02:49,  1.39it/s]Extractor Estimating: 191it [02:50,  1.40it/s]Extractor Estimating: 192it [02:51,  1.32it/s]Extractor Estimating: 193it [02:52,  1.28it/s]Extractor Estimating: 194it [02:53,  1.30it/s]Extractor Estimating: 195it [02:53,  1.32it/s]Extractor Estimating: 196it [02:54,  1.31it/s]Extractor Estimating: 197it [02:55,  1.27it/s]Extractor Estimating: 198it [02:56,  1.34it/s]Extractor Estimating: 199it [02:56,  1.38it/s]Extractor Estimating: 200it [02:57,  1.36it/s]Extractor Estimating: 201it [02:58,  1.34it/s]Extractor Estimating: 202it [02:59,  1.29it/s]Extractor Estimating: 203it [02:59,  1.33it/s]Extractor Estimating: 204it [03:00,  1.22it/s]Extractor Estimating: 205it [03:01,  1.25it/s]Extractor Estimating: 206it [03:02,  1.29it/s]Extractor Estimating: 207it [03:03,  1.31it/s]Extractor Estimating: 208it [03:03,  1.26it/s]Extractor Estimating: 209it [03:04,  1.26it/s]Extractor Estimating: 210it [03:05,  1.24it/s]Extractor Estimating: 211it [03:06,  1.24it/s]Extractor Estimating: 212it [03:07,  1.25it/s]Extractor Estimating: 213it [03:07,  1.22it/s]Extractor Estimating: 214it [03:08,  1.23it/s]Extractor Estimating: 215it [03:09,  1.23it/s]Extractor Estimating: 216it [03:10,  1.25it/s]Extractor Estimating: 217it [03:11,  1.27it/s]Extractor Estimating: 218it [03:11,  1.28it/s]Extractor Estimating: 219it [03:12,  1.27it/s]Extractor Estimating: 220it [03:13,  1.24it/s]Extractor Estimating: 221it [03:14,  1.26it/s]Extractor Estimating: 222it [03:15,  1.26it/s]Extractor Estimating: 223it [03:15,  1.25it/s]Extractor Estimating: 224it [03:16,  1.24it/s]Extractor Estimating: 225it [03:17,  1.24it/s]Extractor Estimating: 226it [03:18,  1.27it/s]Extractor Estimating: 227it [03:19,  1.27it/s]Extractor Estimating: 228it [03:19,  1.29it/s]Extractor Estimating: 229it [03:20,  1.26it/s]Extractor Estimating: 230it [03:21,  1.24it/s]Extractor Estimating: 231it [03:22,  1.22it/s]Extractor Estimating: 232it [03:23,  1.22it/s]Extractor Estimating: 233it [03:23,  1.26it/s]Extractor Estimating: 234it [03:24,  1.18it/s]Extractor Estimating: 235it [03:25,  1.21it/s]Extractor Estimating: 236it [03:26,  1.23it/s]Extractor Estimating: 237it [03:27,  1.24it/s]Extractor Estimating: 238it [03:27,  1.24it/s]Extractor Estimating: 239it [03:28,  1.21it/s]Extractor Estimating: 240it [03:29,  1.24it/s]Extractor Estimating: 241it [03:30,  1.20it/s]Extractor Estimating: 242it [03:31,  1.18it/s]Extractor Estimating: 243it [03:32,  1.20it/s]Extractor Estimating: 244it [03:33,  1.19it/s]Extractor Estimating: 245it [03:33,  1.25it/s]Extractor Estimating: 246it [03:34,  1.25it/s]Extractor Estimating: 247it [03:35,  1.28it/s]Extractor Estimating: 248it [03:36,  1.27it/s]Extractor Estimating: 249it [03:36,  1.23it/s]Extractor Estimating: 250it [03:37,  1.24it/s]Extractor Estimating: 250it [03:37,  1.15it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5146 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 23155
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23255, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23255, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.632, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.228, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 85, avg_time 1.234, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 185, avg_time 1.256, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 70, avg_time 1.244, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 170, avg_time 2.568, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 55, avg_time 1.245, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 155, avg_time 1.253, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 40, avg_time 1.243, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 140, avg_time 1.249, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 25, avg_time 2.521, loss:nan
g_step 1200, step 125, avg_time 1.245, loss:nan
g_step 1300, step 10, avg_time 1.251, loss:nan
g_step 1400, step 110, avg_time 1.251, loss:nan
g_step 1500, step 210, avg_time 1.244, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 95, avg_time 2.528, loss:nan
g_step 1700, step 195, avg_time 1.254, loss:nan
g_step 1800, step 80, avg_time 1.244, loss:nan
g_step 1900, step 180, avg_time 1.228, loss:nan
g_step 2000, step 65, avg_time 1.261, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 165, avg_time 2.534, loss:nan
g_step 2200, step 50, avg_time 1.255, loss:nan
g_step 2300, step 150, avg_time 1.207, loss:nan
g_step 2400, step 35, avg_time 1.248, loss:nan
g_step 2500, step 135, avg_time 1.239, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 20, avg_time 2.528, loss:nan
g_step 2700, step 120, avg_time 1.219, loss:nan
g_step 2800, step 5, avg_time 1.253, loss:nan
g_step 2900, step 105, avg_time 1.237, loss:nan
g_step 3000, step 205, avg_time 1.244, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 90, avg_time 2.525, loss:nan
g_step 3200, step 190, avg_time 1.245, loss:nan
g_step 3300, step 75, avg_time 1.247, loss:nan
g_step 3400, step 175, avg_time 1.240, loss:nan
g_step 3500, step 60, avg_time 1.240, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 160, avg_time 2.541, loss:nan
g_step 3700, step 45, avg_time 1.217, loss:nan
g_step 3800, step 145, avg_time 1.230, loss:nan
g_step 3900, step 30, avg_time 1.256, loss:nan
g_step 4000, step 130, avg_time 1.245, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 15, avg_time 2.505, loss:nan
g_step 4200, step 115, avg_time 1.258, loss:nan
g_step 4300, step 215, avg_time 1.216, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:45:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:45:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-45-07_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:45:08 - WARNING - datasets.builder -   Using custom data configuration default-c2f348e270d82fd2
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c2f348e270d82fd2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:45:10,628 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:45:10,657 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:45:10,657 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:45:10,658 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:45:10,727 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:45:10,771 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:45:11,043 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:45:14,105 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:45:14,105 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c2f348e270d82fd2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:45:14 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14fc717a1200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.76ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.80ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.42ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.26ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.64ba/s]100%|██████████| 6/6 [00:01<00:00,  3.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.18ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.89ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.19ba/s]100%|██████████| 4/4 [00:00<00:00,  5.03ba/s]100%|██████████| 4/4 [00:00<00:00,  4.52ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.98ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.04ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.99ba/s]100%|██████████| 6/6 [00:00<00:00, 10.99ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.91ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.05ba/s]100%|██████████| 4/4 [00:00<00:00, 10.15ba/s]
[INFO|trainer.py:414] 2023-08-28 00:45:18,397 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:45:18,518 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:45:18,518 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:45:18,519 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:45:18,519 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:45:18,519 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:45:18,519 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:45:18,519 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:57,  3.43it/s]  0%|          | 2/405 [00:00<01:53,  3.55it/s]  1%|          | 3/405 [00:00<01:51,  3.59it/s]  1%|          | 4/405 [00:01<01:51,  3.61it/s]  1%|          | 5/405 [00:01<01:50,  3.62it/s]  1%|▏         | 6/405 [00:01<01:50,  3.60it/s]  2%|▏         | 7/405 [00:01<01:50,  3.59it/s]  2%|▏         | 8/405 [00:02<01:50,  3.59it/s]  2%|▏         | 9/405 [00:02<01:50,  3.58it/s]  2%|▏         | 10/405 [00:02<01:50,  3.58it/s]  3%|▎         | 11/405 [00:03<01:50,  3.58it/s]  3%|▎         | 12/405 [00:03<01:50,  3.57it/s]  3%|▎         | 13/405 [00:03<01:49,  3.57it/s]  3%|▎         | 14/405 [00:03<01:49,  3.57it/s]  4%|▎         | 15/405 [00:04<01:49,  3.56it/s]  4%|▍         | 16/405 [00:04<01:49,  3.56it/s]  4%|▍         | 17/405 [00:04<01:48,  3.56it/s]  4%|▍         | 18/405 [00:05<01:48,  3.56it/s]  5%|▍         | 19/405 [00:05<01:48,  3.56it/s]  5%|▍         | 20/405 [00:05<01:48,  3.56it/s]  5%|▌         | 21/405 [00:05<01:48,  3.56it/s]  5%|▌         | 22/405 [00:06<01:47,  3.56it/s]  6%|▌         | 23/405 [00:06<01:47,  3.56it/s]  6%|▌         | 24/405 [00:06<01:47,  3.56it/s]  6%|▌         | 25/405 [00:07<01:46,  3.56it/s]  6%|▋         | 26/405 [00:07<01:46,  3.56it/s]  7%|▋         | 27/405 [00:07<01:46,  3.56it/s]  7%|▋         | 28/405 [00:07<01:45,  3.56it/s]  7%|▋         | 29/405 [00:08<01:45,  3.56it/s]  7%|▋         | 30/405 [00:08<01:45,  3.57it/s]  8%|▊         | 31/405 [00:08<01:44,  3.57it/s]  8%|▊         | 32/405 [00:08<01:44,  3.56it/s]  8%|▊         | 33/405 [00:09<01:44,  3.56it/s]  8%|▊         | 34/405 [00:09<01:44,  3.56it/s]  9%|▊         | 35/405 [00:09<01:44,  3.56it/s]  9%|▉         | 36/405 [00:10<01:43,  3.55it/s]  9%|▉         | 37/405 [00:10<01:43,  3.57it/s]  9%|▉         | 38/405 [00:10<01:45,  3.47it/s] 10%|▉         | 39/405 [00:10<01:44,  3.51it/s] 10%|▉         | 40/405 [00:11<01:42,  3.55it/s] 10%|█         | 41/405 [00:11<01:41,  3.57it/s] 10%|█         | 42/405 [00:11<01:41,  3.59it/s] 11%|█         | 43/405 [00:12<01:40,  3.60it/s] 11%|█         | 44/405 [00:12<01:40,  3.61it/s] 11%|█         | 45/405 [00:12<01:39,  3.61it/s] 11%|█▏        | 46/405 [00:12<01:39,  3.61it/s] 12%|█▏        | 47/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 48/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 49/405 [00:13<01:38,  3.63it/s] 12%|█▏        | 50/405 [00:13<01:37,  3.63it/s] 13%|█▎        | 51/405 [00:14<01:37,  3.63it/s] 13%|█▎        | 52/405 [00:14<01:37,  3.63it/s] 13%|█▎        | 53/405 [00:14<01:36,  3.63it/s] 13%|█▎        | 54/405 [00:15<01:36,  3.63it/s] 14%|█▎        | 55/405 [00:15<01:36,  3.63it/s] 14%|█▍        | 56/405 [00:15<01:36,  3.63it/s] 14%|█▍        | 57/405 [00:15<01:35,  3.63it/s] 14%|█▍        | 58/405 [00:16<01:35,  3.63it/s] 15%|█▍        | 59/405 [00:16<01:35,  3.63it/s] 15%|█▍        | 60/405 [00:16<01:35,  3.63it/s] 15%|█▌        | 61/405 [00:17<01:34,  3.63it/s] 15%|█▌        | 62/405 [00:17<01:34,  3.63it/s] 16%|█▌        | 63/405 [00:17<01:34,  3.63it/s] 16%|█▌        | 64/405 [00:17<01:33,  3.63it/s] 16%|█▌        | 65/405 [00:18<01:33,  3.63it/s] 16%|█▋        | 66/405 [00:18<01:33,  3.63it/s] 17%|█▋        | 67/405 [00:18<01:33,  3.63it/s] 17%|█▋        | 68/405 [00:18<01:32,  3.63it/s] 17%|█▋        | 69/405 [00:19<01:32,  3.63it/s] 17%|█▋        | 70/405 [00:19<01:32,  3.63it/s] 18%|█▊        | 71/405 [00:19<01:31,  3.63it/s] 18%|█▊        | 72/405 [00:20<01:31,  3.63it/s] 18%|█▊        | 73/405 [00:20<01:31,  3.63it/s] 18%|█▊        | 74/405 [00:20<01:31,  3.63it/s] 19%|█▊        | 75/405 [00:20<01:30,  3.63it/s] 19%|█▉        | 76/405 [00:21<01:30,  3.63it/s] 19%|█▉        | 77/405 [00:21<01:30,  3.63it/s] 19%|█▉        | 78/405 [00:21<01:30,  3.63it/s] 20%|█▉        | 79/405 [00:21<01:29,  3.63it/s] 20%|█▉        | 80/405 [00:22<01:29,  3.63it/s] 20%|██        | 81/405 [00:22<01:20,  4.02it/s][INFO|trainer.py:2140] 2023-08-28 00:45:40,962 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:45:40,962 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:45:40,962 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.23it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.13it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.32it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.15it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.15it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.65it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.54it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.61it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.81it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.84it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.60it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.56it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.39it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.29it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.22it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.41it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.57it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.72it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.74it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.49it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.42it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.29it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.31it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.41it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.62it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.67it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.56it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.42it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.48it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.31it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.34it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.49it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.72it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.64it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.59it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.55it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.42it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.36it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.41it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.40it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.59it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.62it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.74it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.77it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.59it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.46it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.38it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.39it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.54it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.51it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.55it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.69it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.63it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.48it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.39it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.44it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.50it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.47it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.57it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.64it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.69it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.63it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.52it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.41it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.41it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.51it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.53it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.42it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.60it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.57it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.68it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.59it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.49it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.41it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.51it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.48it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.44it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.51it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.59it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.61it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:32<01:20,  4.02it/s]
100%|██████████| 438/438 [00:09<00:00, 44.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:45:50,968 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:45:51,098 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:45:53,877 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:45:53,969 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:45:54,021 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:36<23:35,  4.38s/it] 20%|██        | 83/405 [00:36<16:54,  3.15s/it] 21%|██        | 84/405 [00:37<12:15,  2.29s/it] 21%|██        | 85/405 [00:37<09:00,  1.69s/it] 21%|██        | 86/405 [00:37<06:43,  1.27s/it] 21%|██▏       | 87/405 [00:37<05:08,  1.03it/s] 22%|██▏       | 88/405 [00:38<04:02,  1.31it/s] 22%|██▏       | 89/405 [00:38<03:15,  1.62it/s] 22%|██▏       | 90/405 [00:38<02:42,  1.93it/s] 22%|██▏       | 91/405 [00:38<02:20,  2.24it/s] 23%|██▎       | 92/405 [00:39<02:04,  2.52it/s] 23%|██▎       | 93/405 [00:39<01:52,  2.76it/s] 23%|██▎       | 94/405 [00:39<01:44,  2.96it/s] 23%|██▎       | 95/405 [00:40<01:39,  3.12it/s] 24%|██▎       | 96/405 [00:40<01:35,  3.24it/s] 24%|██▍       | 97/405 [00:40<01:32,  3.33it/s] 24%|██▍       | 98/405 [00:40<01:30,  3.39it/s] 24%|██▍       | 99/405 [00:41<01:28,  3.44it/s] 25%|██▍       | 100/405 [00:41<01:27,  3.47it/s] 25%|██▍       | 101/405 [00:41<01:26,  3.50it/s] 25%|██▌       | 102/405 [00:42<01:26,  3.51it/s] 25%|██▌       | 103/405 [00:42<01:25,  3.52it/s] 26%|██▌       | 104/405 [00:42<01:25,  3.53it/s] 26%|██▌       | 105/405 [00:42<01:24,  3.54it/s] 26%|██▌       | 106/405 [00:43<01:24,  3.54it/s] 26%|██▋       | 107/405 [00:43<01:26,  3.43it/s] 27%|██▋       | 108/405 [00:43<01:25,  3.46it/s] 27%|██▋       | 109/405 [00:44<01:24,  3.49it/s] 27%|██▋       | 110/405 [00:44<01:24,  3.51it/s] 27%|██▋       | 111/405 [00:44<01:23,  3.52it/s] 28%|██▊       | 112/405 [00:44<01:22,  3.53it/s] 28%|██▊       | 113/405 [00:45<01:22,  3.54it/s] 28%|██▊       | 114/405 [00:45<01:22,  3.54it/s] 28%|██▊       | 115/405 [00:45<01:21,  3.54it/s] 29%|██▊       | 116/405 [00:46<01:21,  3.54it/s] 29%|██▉       | 117/405 [00:46<01:21,  3.54it/s] 29%|██▉       | 118/405 [00:46<01:24,  3.40it/s] 29%|██▉       | 119/405 [00:46<01:23,  3.44it/s] 30%|██▉       | 120/405 [00:47<01:23,  3.42it/s] 30%|██▉       | 121/405 [00:47<01:22,  3.44it/s] 30%|███       | 122/405 [00:47<01:21,  3.47it/s] 30%|███       | 123/405 [00:48<01:20,  3.49it/s] 31%|███       | 124/405 [00:48<01:20,  3.50it/s] 31%|███       | 125/405 [00:48<01:41,  2.75it/s] 31%|███       | 126/405 [00:49<01:34,  2.94it/s] 31%|███▏      | 127/405 [00:49<01:29,  3.10it/s] 32%|███▏      | 128/405 [00:49<01:27,  3.16it/s] 32%|███▏      | 129/405 [00:50<01:24,  3.26it/s] 32%|███▏      | 130/405 [00:50<01:22,  3.34it/s] 32%|███▏      | 131/405 [00:50<01:20,  3.40it/s] 33%|███▎      | 132/405 [00:50<01:19,  3.44it/s] 33%|███▎      | 133/405 [00:51<01:18,  3.47it/s] 33%|███▎      | 134/405 [00:51<01:17,  3.49it/s] 33%|███▎      | 135/405 [00:51<01:16,  3.51it/s] 34%|███▎      | 136/405 [00:52<01:16,  3.52it/s] 34%|███▍      | 137/405 [00:52<01:15,  3.53it/s] 34%|███▍      | 138/405 [00:52<01:15,  3.53it/s] 34%|███▍      | 139/405 [00:52<01:15,  3.53it/s] 35%|███▍      | 140/405 [00:53<01:17,  3.44it/s] 35%|███▍      | 141/405 [00:53<01:16,  3.46it/s] 35%|███▌      | 142/405 [00:53<01:15,  3.49it/s] 35%|███▌      | 143/405 [00:54<01:14,  3.50it/s] 36%|███▌      | 144/405 [00:54<01:14,  3.51it/s] 36%|███▌      | 145/405 [00:54<01:13,  3.52it/s] 36%|███▌      | 146/405 [00:54<01:13,  3.53it/s] 36%|███▋      | 147/405 [00:55<01:13,  3.53it/s] 37%|███▋      | 148/405 [00:55<01:12,  3.54it/s] 37%|███▋      | 149/405 [00:55<01:12,  3.54it/s] 37%|███▋      | 150/405 [00:56<01:12,  3.54it/s] 37%|███▋      | 151/405 [00:56<01:16,  3.34it/s] 38%|███▊      | 152/405 [00:56<01:14,  3.39it/s] 38%|███▊      | 153/405 [00:56<01:13,  3.43it/s] 38%|███▊      | 154/405 [00:57<01:12,  3.46it/s] 38%|███▊      | 155/405 [00:57<01:11,  3.48it/s] 39%|███▊      | 156/405 [00:57<01:11,  3.50it/s] 39%|███▉      | 157/405 [00:58<01:10,  3.52it/s] 39%|███▉      | 158/405 [00:58<01:09,  3.55it/s] 39%|███▉      | 159/405 [00:58<01:08,  3.58it/s] 40%|███▉      | 160/405 [00:58<01:08,  3.59it/s] 40%|███▉      | 161/405 [00:59<01:07,  3.60it/s] 40%|████      | 162/405 [00:59<01:04,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 00:46:17,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:46:17,934 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:46:17,934 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8265, 'eval_samples_per_second': 355.975, 'eval_steps_per_second': 44.573, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.16it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.93it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.28it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.44it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.93it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.51it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.83it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.42it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.51it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.76it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.90it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.80it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.54it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.22it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.08it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.25it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.18it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.70it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.78it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.92it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.76it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.50it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.93it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.96it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.03it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.23it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.46it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.76it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.74it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.50it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.25it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.24it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.44it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.50it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.71it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.84it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.79it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.59it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.34it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.23it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.26it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.39it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.51it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.68it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.79it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.73it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.59it/s][A
 61%|██████    | 267/438 [00:05<00:04, 42.19it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.87it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.29it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.64it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 43.97it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.25it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.46it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.50it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.28it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.25it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.32it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.27it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.62it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.75it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.70it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.55it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.44it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.33it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.37it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.41it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.47it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.56it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.64it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.59it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.53it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 41.68it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 42.55it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.08it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.62it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.90it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.20it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.29it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.38it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:09<01:04,  3.79it/s]
100%|██████████| 438/438 [00:09<00:00, 44.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:46:28,050 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:46:28,192 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:46:31,272 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:46:31,441 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:46:31,524 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:15<19:38,  4.87s/it] 40%|████      | 164/405 [01:15<14:01,  3.49s/it] 41%|████      | 165/405 [01:15<10:07,  2.53s/it] 41%|████      | 166/405 [01:15<07:23,  1.86s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:16<05:46,  1.45s/it] 41%|████▏     | 168/405 [01:16<04:21,  1.10s/it] 42%|████▏     | 169/405 [01:16<03:22,  1.17it/s] 42%|████▏     | 170/405 [01:17<02:41,  1.46it/s] 42%|████▏     | 171/405 [01:17<02:12,  1.77it/s] 42%|████▏     | 172/405 [01:17<01:51,  2.08it/s] 43%|████▎     | 173/405 [01:18<01:37,  2.38it/s] 43%|████▎     | 174/405 [01:18<01:27,  2.64it/s] 43%|████▎     | 175/405 [01:18<01:20,  2.86it/s] 43%|████▎     | 176/405 [01:18<01:15,  3.03it/s] 44%|████▎     | 177/405 [01:21<03:20,  1.13it/s] 44%|████▍     | 178/405 [01:21<02:39,  1.43it/s] 44%|████▍     | 179/405 [01:21<02:10,  1.74it/s] 44%|████▍     | 180/405 [01:21<01:49,  2.05it/s] 45%|████▍     | 181/405 [01:22<01:35,  2.35it/s] 45%|████▍     | 182/405 [01:22<01:25,  2.61it/s] 45%|████▌     | 183/405 [01:22<01:18,  2.84it/s] 45%|████▌     | 184/405 [01:23<01:13,  3.02it/s] 46%|████▌     | 185/405 [01:23<01:09,  3.16it/s] 46%|████▌     | 186/405 [01:23<01:07,  3.27it/s] 46%|████▌     | 187/405 [01:23<01:08,  3.18it/s] 46%|████▋     | 188/405 [01:24<01:06,  3.28it/s] 47%|████▋     | 189/405 [01:24<01:04,  3.36it/s] 47%|████▋     | 190/405 [01:24<01:03,  3.41it/s] 47%|████▋     | 191/405 [01:25<01:02,  3.45it/s] 47%|████▋     | 192/405 [01:25<01:01,  3.48it/s] 48%|████▊     | 193/405 [01:25<01:00,  3.50it/s] 48%|████▊     | 194/405 [01:25<01:00,  3.51it/s] 48%|████▊     | 195/405 [01:26<00:59,  3.52it/s] 48%|████▊     | 196/405 [01:26<00:59,  3.52it/s] 49%|████▊     | 197/405 [01:26<00:58,  3.53it/s] 49%|████▉     | 198/405 [01:27<01:02,  3.31it/s] 49%|████▉     | 199/405 [01:27<01:01,  3.37it/s] 49%|████▉     | 200/405 [01:27<00:59,  3.42it/s] 50%|████▉     | 201/405 [01:27<00:59,  3.45it/s] 50%|████▉     | 202/405 [01:28<00:58,  3.48it/s] 50%|█████     | 203/405 [01:28<00:57,  3.50it/s] 50%|█████     | 204/405 [01:28<00:57,  3.51it/s] 51%|█████     | 205/405 [01:29<00:56,  3.52it/s] 51%|█████     | 206/405 [01:29<00:56,  3.53it/s] 51%|█████     | 207/405 [01:29<00:56,  3.53it/s] 51%|█████▏    | 208/405 [01:29<00:55,  3.53it/s] 52%|█████▏    | 209/405 [01:30<00:56,  3.46it/s] 52%|█████▏    | 210/405 [01:30<00:56,  3.48it/s] 52%|█████▏    | 211/405 [01:30<00:55,  3.50it/s] 52%|█████▏    | 212/405 [01:31<00:54,  3.51it/s] 53%|█████▎    | 213/405 [01:31<00:54,  3.52it/s] 53%|█████▎    | 214/405 [01:31<00:54,  3.53it/s] 53%|█████▎    | 215/405 [01:31<00:53,  3.53it/s] 53%|█████▎    | 216/405 [01:32<00:53,  3.53it/s] 54%|█████▎    | 217/405 [01:32<00:53,  3.53it/s] 54%|█████▍    | 218/405 [01:32<00:52,  3.54it/s] 54%|█████▍    | 219/405 [01:33<00:52,  3.54it/s] 54%|█████▍    | 220/405 [01:33<00:52,  3.50it/s] 55%|█████▍    | 221/405 [01:33<00:52,  3.53it/s] 55%|█████▍    | 222/405 [01:33<00:51,  3.56it/s] 55%|█████▌    | 223/405 [01:34<00:50,  3.58it/s] 55%|█████▌    | 224/405 [01:34<00:50,  3.59it/s] 56%|█████▌    | 225/405 [01:34<00:50,  3.60it/s] 56%|█████▌    | 226/405 [01:35<00:49,  3.61it/s] 56%|█████▌    | 227/405 [01:35<00:49,  3.61it/s] 56%|█████▋    | 228/405 [01:35<00:48,  3.61it/s] 57%|█████▋    | 229/405 [01:35<00:48,  3.62it/s] 57%|█████▋    | 230/405 [01:36<00:48,  3.62it/s] 57%|█████▋    | 231/405 [01:36<00:48,  3.58it/s] 57%|█████▋    | 232/405 [01:36<00:48,  3.59it/s] 58%|█████▊    | 233/405 [01:36<00:47,  3.60it/s] 58%|█████▊    | 234/405 [01:37<00:47,  3.61it/s] 58%|█████▊    | 235/405 [01:37<00:47,  3.61it/s] 58%|█████▊    | 236/405 [01:37<00:46,  3.61it/s] 59%|█████▊    | 237/405 [01:38<00:46,  3.61it/s] 59%|█████▉    | 238/405 [01:38<00:46,  3.62it/s] 59%|█████▉    | 239/405 [01:38<00:46,  3.54it/s] 59%|█████▉    | 240/405 [01:38<00:46,  3.55it/s] 60%|█████▉    | 241/405 [01:39<00:45,  3.57it/s] 60%|█████▉    | 242/405 [01:39<00:46,  3.50it/s] 60%|██████    | 243/405 [01:39<00:41,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 00:46:58,235 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:46:58,235 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:46:58,235 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8724, 'eval_samples_per_second': 354.321, 'eval_steps_per_second': 44.366, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.91it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.18it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.32it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.66it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.64it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.70it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.82it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.69it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.51it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.29it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.17it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.25it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.54it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.71it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.77it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.71it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.65it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 42.13it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.67it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.21it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.66it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.00it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.34it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.23it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.22it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.21it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.38it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.50it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.63it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.80it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.70it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.59it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.35it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.21it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.26it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.40it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.53it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.68it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.76it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.78it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.63it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 42.42it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.05it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.43it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.71it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.08it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.36it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.46it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.19it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.22it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.23it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.35it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.44it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.39it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.55it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.66it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.51it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.27it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.37it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.38it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.40it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.42it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.50it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.70it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.65it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.58it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.43it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.47it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 42.26it/s][A
 92%|█████████▏| 402/438 [00:09<00:01, 31.10it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 34.88it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 37.45it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 39.42it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 40.96it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 42.16it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 42.98it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.47it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [01:49<00:41,  3.91it/s]
100%|██████████| 438/438 [00:10<00:00, 43.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:47:08,430 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:47:08,633 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:47:11,884 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:47:12,013 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:47:12,093 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [01:54<12:38,  4.71s/it] 60%|██████    | 245/405 [01:55<09:01,  3.39s/it] 61%|██████    | 246/405 [01:55<06:30,  2.45s/it] 61%|██████    | 247/405 [01:55<04:44,  1.80s/it] 61%|██████    | 248/405 [01:55<03:31,  1.35s/it] 61%|██████▏   | 249/405 [01:56<02:40,  1.03s/it] 62%|██████▏   | 250/405 [01:56<02:04,  1.25it/s] 62%|██████▏   | 251/405 [01:56<01:39,  1.55it/s] 62%|██████▏   | 252/405 [01:57<01:21,  1.87it/s] 62%|██████▏   | 253/405 [01:57<01:09,  2.19it/s] 63%|██████▎   | 254/405 [01:57<01:00,  2.48it/s] 63%|██████▎   | 255/405 [01:57<00:54,  2.74it/s] 63%|██████▎   | 256/405 [01:58<00:52,  2.86it/s] 63%|██████▎   | 257/405 [01:58<00:48,  3.06it/s] 64%|██████▎   | 258/405 [01:58<00:45,  3.20it/s] 64%|██████▍   | 259/405 [01:59<00:44,  3.32it/s] 64%|██████▍   | 260/405 [01:59<00:42,  3.40it/s] 64%|██████▍   | 261/405 [01:59<00:41,  3.47it/s] 65%|██████▍   | 262/405 [01:59<00:40,  3.52it/s] 65%|██████▍   | 263/405 [02:00<00:39,  3.55it/s] 65%|██████▌   | 264/405 [02:00<00:39,  3.57it/s] 65%|██████▌   | 265/405 [02:00<00:38,  3.59it/s] 66%|██████▌   | 266/405 [02:00<00:38,  3.60it/s] 66%|██████▌   | 267/405 [02:01<00:39,  3.47it/s] 66%|██████▌   | 268/405 [02:01<00:39,  3.51it/s] 66%|██████▋   | 269/405 [02:01<00:38,  3.54it/s] 67%|██████▋   | 270/405 [02:02<00:37,  3.57it/s] 67%|██████▋   | 271/405 [02:02<00:37,  3.58it/s] 67%|██████▋   | 272/405 [02:02<00:36,  3.59it/s] 67%|██████▋   | 273/405 [02:02<00:36,  3.61it/s] 68%|██████▊   | 274/405 [02:03<00:36,  3.61it/s] 68%|██████▊   | 275/405 [02:03<00:35,  3.62it/s] 68%|██████▊   | 276/405 [02:03<00:35,  3.62it/s] 68%|██████▊   | 277/405 [02:04<00:35,  3.62it/s] 69%|██████▊   | 278/405 [02:04<00:35,  3.54it/s] 69%|██████▉   | 279/405 [02:04<00:35,  3.56it/s] 69%|██████▉   | 280/405 [02:04<00:34,  3.58it/s] 69%|██████▉   | 281/405 [02:05<00:34,  3.59it/s] 70%|██████▉   | 282/405 [02:05<00:34,  3.60it/s] 70%|██████▉   | 283/405 [02:05<00:33,  3.61it/s] 70%|███████   | 284/405 [02:05<00:33,  3.61it/s] 70%|███████   | 285/405 [02:06<00:33,  3.62it/s] 71%|███████   | 286/405 [02:06<00:32,  3.62it/s] 71%|███████   | 287/405 [02:06<00:32,  3.62it/s] 71%|███████   | 288/405 [02:07<00:32,  3.62it/s] 71%|███████▏  | 289/405 [02:07<00:32,  3.53it/s] 72%|███████▏  | 290/405 [02:07<00:32,  3.55it/s] 72%|███████▏  | 291/405 [02:07<00:31,  3.58it/s] 72%|███████▏  | 292/405 [02:08<00:31,  3.59it/s] 72%|███████▏  | 293/405 [02:08<00:31,  3.60it/s] 73%|███████▎  | 294/405 [02:08<00:30,  3.61it/s] 73%|███████▎  | 295/405 [02:09<00:30,  3.61it/s] 73%|███████▎  | 296/405 [02:09<00:30,  3.61it/s] 73%|███████▎  | 297/405 [02:09<00:29,  3.61it/s] 74%|███████▎  | 298/405 [02:09<00:29,  3.62it/s] 74%|███████▍  | 299/405 [02:10<00:29,  3.62it/s] 74%|███████▍  | 300/405 [02:10<00:30,  3.47it/s] 74%|███████▍  | 301/405 [02:10<00:29,  3.51it/s] 75%|███████▍  | 302/405 [02:11<00:29,  3.54it/s] 75%|███████▍  | 303/405 [02:11<00:28,  3.57it/s] 75%|███████▌  | 304/405 [02:11<00:28,  3.59it/s] 75%|███████▌  | 305/405 [02:11<00:27,  3.59it/s] 76%|███████▌  | 306/405 [02:12<00:27,  3.60it/s] 76%|███████▌  | 307/405 [02:12<00:27,  3.61it/s] 76%|███████▌  | 308/405 [02:12<00:26,  3.61it/s] 76%|███████▋  | 309/405 [02:12<00:26,  3.61it/s] 77%|███████▋  | 310/405 [02:13<00:26,  3.62it/s] 77%|███████▋  | 311/405 [02:13<00:26,  3.52it/s] 77%|███████▋  | 312/405 [02:13<00:26,  3.55it/s] 77%|███████▋  | 313/405 [02:14<00:25,  3.57it/s] 78%|███████▊  | 314/405 [02:14<00:25,  3.59it/s] 78%|███████▊  | 315/405 [02:14<00:25,  3.59it/s] 78%|███████▊  | 316/405 [02:14<00:24,  3.60it/s] 78%|███████▊  | 317/405 [02:15<00:24,  3.61it/s] 79%|███████▊  | 318/405 [02:15<00:24,  3.61it/s] 79%|███████▉  | 319/405 [02:15<00:23,  3.61it/s] 79%|███████▉  | 320/405 [02:16<00:23,  3.61it/s] 79%|███████▉  | 321/405 [02:16<00:23,  3.62it/s] 80%|███████▉  | 322/405 [02:16<00:24,  3.46it/s] 80%|███████▉  | 323/405 [02:16<00:23,  3.50it/s] 80%|████████  | 324/405 [02:17<00:20,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 00:47:35,610 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:47:35,610 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:47:35,610 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 10.0199, 'eval_samples_per_second': 349.106, 'eval_steps_per_second': 43.713, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.26it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.82it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.21it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.38it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.72it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.11it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.66it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.55it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.74it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.84it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.85it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.78it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.52it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.18it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.12it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.19it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.39it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.57it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 40.18it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 41.62it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.60it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.26it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.63it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.93it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.13it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.20it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.00it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.07it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.48it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.60it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.66it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.54it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.47it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.23it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.27it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.37it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.53it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.63it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 37.09it/s][A
 51%|█████     | 222/438 [00:05<00:05, 39.25it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 40.81it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 41.98it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 42.86it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.46it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.93it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.04it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.89it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.77it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.97it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.17it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.38it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.68it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.58it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.31it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.14it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.18it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.33it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.43it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.67it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.75it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.76it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.56it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 37.22it/s][A
 80%|████████  | 352/438 [00:08<00:02, 39.29it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 40.87it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 42.02it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 42.92it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.49it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.94it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.07it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.86it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.63it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 43.79it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.05it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.36it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.65it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.72it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.79it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.59it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.37it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.01it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [02:27<00:20,  3.90it/s]
100%|██████████| 438/438 [00:09<00:00, 44.01it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:47:45,962 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:47:46,392 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:47:49,450 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:47:49,587 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:47:49,916 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [02:32<06:26,  4.83s/it] 80%|████████  | 326/405 [02:32<04:33,  3.47s/it] 81%|████████  | 327/405 [02:33<03:15,  2.51s/it] 81%|████████  | 328/405 [02:33<02:21,  1.84s/it] 81%|████████  | 329/405 [02:33<01:44,  1.37s/it] 81%|████████▏ | 330/405 [02:34<01:18,  1.05s/it] 82%|████████▏ | 331/405 [02:34<01:00,  1.22it/s] 82%|████████▏ | 332/405 [02:34<00:48,  1.52it/s] 82%|████████▏ | 333/405 [02:34<00:39,  1.83it/s] 82%|████████▏ | 334/405 [02:35<00:33,  2.14it/s] 83%|████████▎ | 335/405 [02:35<00:28,  2.43it/s] 83%|████████▎ | 336/405 [02:35<00:25,  2.68it/s] 83%|████████▎ | 337/405 [02:35<00:23,  2.89it/s] 83%|████████▎ | 338/405 [02:36<00:21,  3.06it/s] 84%|████████▎ | 339/405 [02:36<00:20,  3.19it/s] 84%|████████▍ | 340/405 [02:36<00:19,  3.29it/s] 84%|████████▍ | 341/405 [02:37<00:19,  3.36it/s] 84%|████████▍ | 342/405 [02:37<00:18,  3.37it/s] 85%|████████▍ | 343/405 [02:37<00:18,  3.42it/s] 85%|████████▍ | 344/405 [02:37<00:17,  3.46it/s] 85%|████████▌ | 345/405 [02:38<00:17,  3.48it/s] 85%|████████▌ | 346/405 [02:38<00:16,  3.50it/s] 86%|████████▌ | 347/405 [02:38<00:16,  3.52it/s] 86%|████████▌ | 348/405 [02:39<00:16,  3.52it/s] 86%|████████▌ | 349/405 [02:39<00:15,  3.53it/s] 86%|████████▋ | 350/405 [02:39<00:15,  3.53it/s] 87%|████████▋ | 351/405 [02:39<00:15,  3.53it/s] 87%|████████▋ | 352/405 [02:40<00:14,  3.54it/s] 87%|████████▋ | 353/405 [02:40<00:15,  3.47it/s] 87%|████████▋ | 354/405 [02:40<00:14,  3.49it/s] 88%|████████▊ | 355/405 [02:41<00:14,  3.50it/s] 88%|████████▊ | 356/405 [02:41<00:13,  3.51it/s] 88%|████████▊ | 357/405 [02:41<00:13,  3.52it/s] 88%|████████▊ | 358/405 [02:41<00:13,  3.52it/s] 89%|████████▊ | 359/405 [02:42<00:12,  3.54it/s] 89%|████████▉ | 360/405 [02:42<00:12,  3.56it/s] 89%|████████▉ | 361/405 [02:42<00:12,  3.58it/s] 89%|████████▉ | 362/405 [02:43<00:11,  3.60it/s] 90%|████████▉ | 363/405 [02:43<00:11,  3.61it/s] 90%|████████▉ | 364/405 [02:43<00:11,  3.51it/s] 90%|█████████ | 365/405 [02:43<00:11,  3.54it/s] 90%|█████████ | 366/405 [02:44<00:10,  3.57it/s] 91%|█████████ | 367/405 [02:44<00:10,  3.58it/s] 91%|█████████ | 368/405 [02:44<00:10,  3.59it/s] 91%|█████████ | 369/405 [02:45<00:09,  3.60it/s] 91%|█████████▏| 370/405 [02:45<00:09,  3.61it/s] 92%|█████████▏| 371/405 [02:45<00:09,  3.62it/s] 92%|█████████▏| 372/405 [02:45<00:09,  3.62it/s] 92%|█████████▏| 373/405 [02:46<00:08,  3.62it/s] 92%|█████████▏| 374/405 [02:46<00:08,  3.62it/s] 93%|█████████▎| 375/405 [02:46<00:08,  3.46it/s] 93%|█████████▎| 376/405 [02:46<00:08,  3.51it/s] 93%|█████████▎| 377/405 [02:47<00:08,  3.37it/s] 93%|█████████▎| 378/405 [02:47<00:07,  3.42it/s] 94%|█████████▎| 379/405 [02:47<00:07,  3.48it/s] 94%|█████████▍| 380/405 [02:48<00:07,  3.52it/s] 94%|█████████▍| 381/405 [02:48<00:06,  3.55it/s] 94%|█████████▍| 382/405 [02:48<00:08,  2.77it/s] 95%|█████████▍| 383/405 [02:49<00:07,  2.96it/s] 95%|█████████▍| 384/405 [02:49<00:06,  3.14it/s] 95%|█████████▌| 385/405 [02:49<00:06,  3.15it/s] 95%|█████████▌| 386/405 [02:50<00:05,  3.28it/s] 96%|█████████▌| 387/405 [02:50<00:05,  3.37it/s] 96%|█████████▌| 388/405 [02:50<00:04,  3.45it/s] 96%|█████████▌| 389/405 [02:50<00:04,  3.50it/s] 96%|█████████▋| 390/405 [02:51<00:04,  3.53it/s] 97%|█████████▋| 391/405 [02:51<00:03,  3.56it/s] 97%|█████████▋| 392/405 [02:51<00:03,  3.57it/s] 97%|█████████▋| 393/405 [02:52<00:03,  3.59it/s] 97%|█████████▋| 394/405 [02:52<00:03,  3.60it/s] 98%|█████████▊| 395/405 [02:52<00:02,  3.60it/s] 98%|█████████▊| 396/405 [02:52<00:02,  3.58it/s] 98%|█████████▊| 397/405 [02:53<00:02,  3.59it/s] 98%|█████████▊| 398/405 [02:53<00:01,  3.60it/s] 99%|█████████▊| 399/405 [02:53<00:01,  3.61it/s] 99%|█████████▉| 400/405 [02:53<00:01,  3.61it/s] 99%|█████████▉| 401/405 [02:54<00:01,  3.62it/s] 99%|█████████▉| 402/405 [02:54<00:00,  3.62it/s]100%|█████████▉| 403/405 [02:54<00:00,  3.61it/s]100%|█████████▉| 404/405 [02:55<00:00,  3.61it/s]100%|██████████| 405/405 [02:55<00:00,  4.00it/s][INFO|trainer.py:2140] 2023-08-28 00:48:13,811 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:48:13,811 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:48:13,811 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 10.0111, 'eval_samples_per_second': 349.411, 'eval_steps_per_second': 43.751, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.76it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.91it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.27it/s][A
  5%|▌         | 22/438 [00:00<00:09, 43.05it/s][A
  6%|▌         | 27/438 [00:00<00:09, 43.63it/s][A
  7%|▋         | 32/438 [00:00<00:09, 43.79it/s][A
  8%|▊         | 37/438 [00:00<00:09, 43.85it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.76it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.04it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.36it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.58it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.41it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.58it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.62it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.50it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.24it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.02it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.55it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.55it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.58it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.74it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.33it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.19it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.29it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.50it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.61it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.54it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 42.03it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 42.95it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.44it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.68it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.83it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.02it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.45it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.20it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.31it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.48it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.50it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.41it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.45it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.52it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.68it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.58it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.46it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.40it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.52it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.48it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.43it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.43it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.28it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.48it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.54it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.46it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.27it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.36it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.34it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.30it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.41it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.50it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.53it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.51it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.50it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.52it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.56it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.48it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.45it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.46it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.52it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.44it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.50it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.48it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.43it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.53it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.45it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.52it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.46it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.37it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.53it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.54it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.50it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 42.91it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 43.50it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.83it/s][A                                                 
                                                 [A100%|██████████| 405/405 [03:05<00:00,  4.00it/s]
100%|██████████| 438/438 [00:09<00:00, 43.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:48:23,795 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:48:23,888 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:48:26,976 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:48:27,174 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:48:27,257 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:48:28,534 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:48:28,534 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81 (score: 1.0478343963623047).
                                                 100%|██████████| 405/405 [03:22<00:00,  4.00it/s]100%|██████████| 405/405 [03:22<00:00,  2.00it/s]
[INFO|trainer.py:1894] 2023-08-28 00:48:40,963 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:48:41,289 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:48:46,464 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:48:46,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:48:47,027 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:48:47,931 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   train_runtime            = 0:03:22.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   train_samples_per_second =    127.514
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:47,931 >>   train_steps_per_second   =      2.002
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8894, 'eval_samples_per_second': 353.711, 'eval_steps_per_second': 44.29, 'epoch': 5.0}
{'train_runtime': 202.331, 'train_samples_per_second': 127.514, 'train_steps_per_second': 2.002, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:48:48 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:48:48,485 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:48:48,486 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 00:48:48,486 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.86it/s]  3%|▎         | 12/438 [00:00<00:08, 49.36it/s]  4%|▍         | 17/438 [00:00<00:08, 47.67it/s]  5%|▌         | 22/438 [00:00<00:08, 46.72it/s]  6%|▌         | 27/438 [00:00<00:08, 46.22it/s]  7%|▋         | 32/438 [00:00<00:08, 45.93it/s]  8%|▊         | 37/438 [00:00<00:08, 45.72it/s] 10%|▉         | 42/438 [00:00<00:08, 45.32it/s] 11%|█         | 47/438 [00:01<00:08, 44.73it/s] 12%|█▏        | 52/438 [00:01<00:10, 36.50it/s] 13%|█▎        | 57/438 [00:01<00:09, 38.82it/s] 14%|█▍        | 62/438 [00:01<00:09, 40.55it/s] 15%|█▌        | 67/438 [00:01<00:08, 41.93it/s] 16%|█▋        | 72/438 [00:01<00:08, 42.90it/s] 18%|█▊        | 77/438 [00:01<00:08, 43.63it/s] 19%|█▊        | 82/438 [00:01<00:08, 44.10it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.26it/s] 21%|██        | 92/438 [00:02<00:07, 44.03it/s] 22%|██▏       | 97/438 [00:02<00:07, 43.85it/s] 23%|██▎       | 102/438 [00:02<00:07, 43.89it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.24it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.51it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.69it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.82it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.91it/s] 30%|███       | 132/438 [00:02<00:06, 44.89it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.74it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.53it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.39it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.60it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.58it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.71it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.91it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.94it/s] 40%|████      | 177/438 [00:04<00:05, 44.89it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.70it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.41it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.41it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.45it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.64it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.70it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.81it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.89it/s] 51%|█████     | 222/438 [00:05<00:04, 44.75it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.69it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.49it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.37it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.43it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.58it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.75it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.78it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.84it/s] 61%|██████    | 267/438 [00:06<00:03, 44.83it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.77it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.62it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.58it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.61it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.70it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.68it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.81it/s] 70%|███████   | 307/438 [00:06<00:02, 44.86it/s] 71%|███████   | 312/438 [00:07<00:02, 44.86it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.76it/s] 74%|███████▎  | 322/438 [00:07<00:02, 43.99it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.24it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.30it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.41it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.53it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.67it/s] 80%|████████  | 352/438 [00:07<00:01, 44.77it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.85it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.74it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.73it/s] 85%|████████▍ | 372/438 [00:08<00:01, 44.66it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.65it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.66it/s] 88%|████████▊ | 387/438 [00:08<00:01, 43.34it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.23it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.51it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.64it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.66it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.66it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.74it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.66it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.66it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.57it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.65it/s]100%|██████████| 438/438 [00:09<00:00, 44.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:48:58,367 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   eval_loss               =     1.0478
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   eval_samples_per_second =    354.008
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   eval_steps_per_second   =     44.327
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:48:58,367 >>   perplexity              =     2.8515
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,708 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:49:08,878 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:49:08,879 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:49:09,515 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:49:10,633 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:49:10,633 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:49:14,599 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:49:14,600 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:49:15,436 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:49:15,668 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:49:15,704 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.26it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.39it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.39it/s]Extractor Predicting: 30it [00:20,  1.35it/s]Extractor Predicting: 31it [00:21,  1.33it/s]Extractor Predicting: 32it [00:22,  1.31it/s]Extractor Predicting: 33it [00:23,  1.34it/s]Extractor Predicting: 34it [00:23,  1.32it/s]Extractor Predicting: 35it [00:24,  1.33it/s]Extractor Predicting: 36it [00:25,  1.34it/s]Extractor Predicting: 37it [00:26,  1.31it/s]Extractor Predicting: 38it [00:26,  1.29it/s]Extractor Predicting: 39it [00:27,  1.27it/s]Extractor Predicting: 40it [00:28,  1.28it/s]Extractor Predicting: 41it [00:29,  1.29it/s]Extractor Predicting: 42it [00:30,  1.27it/s]Extractor Predicting: 43it [00:30,  1.26it/s]Extractor Predicting: 44it [00:31,  1.27it/s]Extractor Predicting: 45it [00:32,  1.28it/s]Extractor Predicting: 46it [00:33,  1.27it/s]Extractor Predicting: 47it [00:34,  1.29it/s]Extractor Predicting: 48it [00:34,  1.31it/s]Extractor Predicting: 49it [00:35,  1.30it/s]Extractor Predicting: 50it [00:36,  1.29it/s]Extractor Predicting: 51it [00:37,  1.28it/s]Extractor Predicting: 52it [00:37,  1.28it/s]Extractor Predicting: 53it [00:38,  1.27it/s]Extractor Predicting: 54it [00:39,  1.31it/s]Extractor Predicting: 55it [00:40,  1.30it/s]Extractor Predicting: 56it [00:40,  1.32it/s]Extractor Predicting: 57it [00:41,  1.32it/s]Extractor Predicting: 58it [00:42,  1.30it/s]Extractor Predicting: 59it [00:43,  1.27it/s]Extractor Predicting: 60it [00:43,  1.35it/s]Extractor Predicting: 61it [00:44,  1.36it/s]Extractor Predicting: 62it [00:45,  1.37it/s]Extractor Predicting: 63it [00:46,  1.35it/s]Extractor Predicting: 64it [00:46,  1.36it/s]Extractor Predicting: 65it [00:47,  1.39it/s]Extractor Predicting: 66it [00:48,  1.37it/s]Extractor Predicting: 67it [00:49,  1.38it/s]Extractor Predicting: 68it [00:49,  1.36it/s]Extractor Predicting: 69it [00:50,  1.32it/s]Extractor Predicting: 70it [00:51,  1.37it/s]Extractor Predicting: 71it [00:51,  1.38it/s]Extractor Predicting: 72it [00:52,  1.40it/s]Extractor Predicting: 73it [00:53,  1.42it/s]Extractor Predicting: 74it [00:54,  1.42it/s]Extractor Predicting: 75it [00:54,  1.44it/s]Extractor Predicting: 76it [00:55,  1.43it/s]Extractor Predicting: 77it [00:56,  1.44it/s]Extractor Predicting: 78it [00:56,  1.44it/s]Extractor Predicting: 79it [00:57,  1.43it/s]Extractor Predicting: 80it [00:58,  1.39it/s]Extractor Predicting: 81it [00:59,  1.38it/s]Extractor Predicting: 82it [00:59,  1.39it/s]Extractor Predicting: 83it [01:00,  1.41it/s]Extractor Predicting: 84it [01:01,  1.40it/s]Extractor Predicting: 85it [01:01,  1.38it/s]Extractor Predicting: 86it [01:02,  1.36it/s]Extractor Predicting: 87it [01:03,  1.37it/s]Extractor Predicting: 88it [01:04,  1.41it/s]Extractor Predicting: 89it [01:04,  1.47it/s]Extractor Predicting: 90it [01:05,  1.45it/s]Extractor Predicting: 91it [01:05,  1.48it/s]Extractor Predicting: 92it [01:06,  1.52it/s]Extractor Predicting: 93it [01:07,  1.54it/s]Extractor Predicting: 94it [01:07,  1.56it/s]Extractor Predicting: 95it [01:08,  1.54it/s]Extractor Predicting: 96it [01:09,  1.49it/s]Extractor Predicting: 97it [01:09,  1.51it/s]Extractor Predicting: 98it [01:10,  1.53it/s]Extractor Predicting: 99it [01:11,  1.52it/s]Extractor Predicting: 100it [01:11,  1.48it/s]Extractor Predicting: 101it [01:12,  1.47it/s]Extractor Predicting: 102it [01:13,  1.48it/s]Extractor Predicting: 103it [01:13,  1.50it/s]Extractor Predicting: 104it [01:14,  1.51it/s]Extractor Predicting: 105it [01:15,  1.47it/s]Extractor Predicting: 106it [01:15,  1.49it/s]Extractor Predicting: 107it [01:16,  1.47it/s]Extractor Predicting: 108it [01:17,  1.50it/s]Extractor Predicting: 109it [01:17,  1.51it/s]Extractor Predicting: 110it [01:18,  1.53it/s]Extractor Predicting: 111it [01:19,  1.53it/s]Extractor Predicting: 112it [01:19,  1.55it/s]Extractor Predicting: 113it [01:20,  1.42it/s]Extractor Predicting: 114it [01:21,  1.41it/s]Extractor Predicting: 115it [01:22,  1.39it/s]Extractor Predicting: 116it [01:22,  1.39it/s]Extractor Predicting: 117it [01:23,  1.38it/s]Extractor Predicting: 118it [01:24,  1.39it/s]Extractor Predicting: 119it [01:25,  1.37it/s]Extractor Predicting: 120it [01:25,  1.36it/s]Extractor Predicting: 121it [01:26,  1.35it/s]Extractor Predicting: 122it [01:27,  1.37it/s]Extractor Predicting: 123it [01:28,  1.35it/s]Extractor Predicting: 124it [01:28,  1.31it/s]Extractor Predicting: 125it [01:29,  1.32it/s]Extractor Predicting: 126it [01:30,  1.32it/s]Extractor Predicting: 127it [01:31,  1.31it/s]Extractor Predicting: 128it [01:31,  1.31it/s]Extractor Predicting: 129it [01:32,  1.32it/s]Extractor Predicting: 130it [01:33,  1.35it/s]Extractor Predicting: 131it [01:34,  1.35it/s]Extractor Predicting: 132it [01:34,  1.34it/s]Extractor Predicting: 133it [01:35,  1.32it/s]Extractor Predicting: 134it [01:36,  1.34it/s]Extractor Predicting: 135it [01:37,  1.34it/s]Extractor Predicting: 136it [01:37,  1.33it/s]Extractor Predicting: 137it [01:38,  1.31it/s]Extractor Predicting: 138it [01:39,  1.33it/s]Extractor Predicting: 139it [01:40,  1.35it/s]Extractor Predicting: 140it [01:40,  1.37it/s]Extractor Predicting: 141it [01:41,  1.35it/s]Extractor Predicting: 142it [01:42,  1.32it/s]Extractor Predicting: 143it [01:43,  1.34it/s]Extractor Predicting: 144it [01:43,  1.33it/s]Extractor Predicting: 145it [01:43,  1.78it/s]Extractor Predicting: 145it [01:43,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:11,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:11,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:11,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:11,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:11,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:51:12,782 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:51:12,783 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:51:13,430 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:51:14,518 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:51:14,519 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:17,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:17,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:17,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:17,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:17,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:51:18,578 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:51:18,610 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:51:19,215 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:51:19,428 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:51:19,428 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.37it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.37it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:08,  1.38it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:12,  1.39it/s]Extractor Predicting: 18it [00:13,  1.40it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.37it/s]Extractor Predicting: 21it [00:15,  1.38it/s]Extractor Predicting: 22it [00:16,  1.28it/s]Extractor Predicting: 23it [00:16,  1.32it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.36it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.36it/s]Extractor Predicting: 30it [00:21,  1.33it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.39it/s]Extractor Predicting: 33it [00:24,  1.41it/s]Extractor Predicting: 34it [00:24,  1.43it/s]Extractor Predicting: 35it [00:25,  1.46it/s]Extractor Predicting: 36it [00:26,  1.41it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:27,  1.44it/s]Extractor Predicting: 39it [00:28,  1.38it/s]Extractor Predicting: 40it [00:28,  1.40it/s]Extractor Predicting: 41it [00:29,  1.40it/s]Extractor Predicting: 42it [00:30,  1.42it/s]Extractor Predicting: 43it [00:31,  1.43it/s]Extractor Predicting: 44it [00:31,  1.47it/s]Extractor Predicting: 45it [00:32,  1.46it/s]Extractor Predicting: 46it [00:32,  1.52it/s]Extractor Predicting: 47it [00:33,  1.48it/s]Extractor Predicting: 48it [00:34,  1.44it/s]Extractor Predicting: 49it [00:35,  1.42it/s]Extractor Predicting: 50it [00:35,  1.39it/s]Extractor Predicting: 51it [00:36,  1.43it/s]Extractor Predicting: 52it [00:37,  1.42it/s]Extractor Predicting: 53it [00:37,  1.45it/s]Extractor Predicting: 54it [00:38,  1.47it/s]Extractor Predicting: 55it [00:39,  1.44it/s]Extractor Predicting: 56it [00:40,  1.42it/s]Extractor Predicting: 57it [00:40,  1.42it/s]Extractor Predicting: 58it [00:41,  1.42it/s]Extractor Predicting: 59it [00:42,  1.41it/s]Extractor Predicting: 60it [00:42,  1.42it/s]Extractor Predicting: 61it [00:43,  1.41it/s]Extractor Predicting: 62it [00:44,  1.42it/s]Extractor Predicting: 63it [00:45,  1.39it/s]Extractor Predicting: 64it [00:45,  1.34it/s]Extractor Predicting: 65it [00:46,  1.40it/s]Extractor Predicting: 66it [00:47,  1.40it/s]Extractor Predicting: 67it [00:47,  1.45it/s]Extractor Predicting: 68it [00:48,  1.45it/s]Extractor Predicting: 69it [00:49,  1.41it/s]Extractor Predicting: 70it [00:49,  1.42it/s]Extractor Predicting: 71it [00:50,  1.41it/s]Extractor Predicting: 72it [00:51,  1.40it/s]Extractor Predicting: 73it [00:52,  1.39it/s]Extractor Predicting: 74it [00:52,  1.39it/s]Extractor Predicting: 75it [00:53,  1.38it/s]Extractor Predicting: 76it [00:54,  1.36it/s]Extractor Predicting: 77it [00:55,  1.37it/s]Extractor Predicting: 78it [00:55,  1.38it/s]Extractor Predicting: 79it [00:56,  1.40it/s]Extractor Predicting: 80it [00:57,  1.44it/s]Extractor Predicting: 81it [00:57,  1.37it/s]Extractor Predicting: 82it [00:58,  1.37it/s]Extractor Predicting: 83it [00:59,  1.38it/s]Extractor Predicting: 84it [01:00,  1.41it/s]Extractor Predicting: 85it [01:00,  1.42it/s]Extractor Predicting: 86it [01:01,  1.43it/s]Extractor Predicting: 87it [01:02,  1.43it/s]Extractor Predicting: 88it [01:02,  1.46it/s]Extractor Predicting: 89it [01:03,  1.47it/s]Extractor Predicting: 90it [01:04,  1.49it/s]Extractor Predicting: 91it [01:04,  1.46it/s]Extractor Predicting: 92it [01:05,  1.44it/s]Extractor Predicting: 93it [01:06,  1.45it/s]Extractor Predicting: 94it [01:06,  1.43it/s]Extractor Predicting: 95it [01:07,  1.43it/s]Extractor Predicting: 96it [01:08,  1.31it/s]Extractor Predicting: 97it [01:09,  1.32it/s]Extractor Predicting: 98it [01:10,  1.34it/s]Extractor Predicting: 99it [01:10,  1.36it/s]Extractor Predicting: 100it [01:11,  1.38it/s]Extractor Predicting: 101it [01:12,  1.39it/s]Extractor Predicting: 102it [01:12,  1.36it/s]Extractor Predicting: 103it [01:13,  1.38it/s]Extractor Predicting: 104it [01:14,  1.39it/s]Extractor Predicting: 105it [01:15,  1.40it/s]Extractor Predicting: 106it [01:15,  1.40it/s]Extractor Predicting: 107it [01:16,  1.39it/s]Extractor Predicting: 108it [01:17,  1.41it/s]Extractor Predicting: 109it [01:17,  1.42it/s]Extractor Predicting: 110it [01:18,  1.40it/s]Extractor Predicting: 111it [01:19,  1.44it/s]Extractor Predicting: 112it [01:19,  1.40it/s]Extractor Predicting: 113it [01:20,  1.39it/s]Extractor Predicting: 114it [01:21,  1.37it/s]Extractor Predicting: 115it [01:22,  1.40it/s]Extractor Predicting: 116it [01:22,  1.41it/s]Extractor Predicting: 117it [01:23,  1.39it/s]Extractor Predicting: 118it [01:24,  1.41it/s]Extractor Predicting: 119it [01:25,  1.40it/s]Extractor Predicting: 120it [01:25,  1.40it/s]Extractor Predicting: 121it [01:26,  1.42it/s]Extractor Predicting: 122it [01:27,  1.43it/s]Extractor Predicting: 123it [01:27,  1.45it/s]Extractor Predicting: 124it [01:28,  1.45it/s]Extractor Predicting: 125it [01:29,  1.45it/s]Extractor Predicting: 126it [01:29,  1.38it/s]Extractor Predicting: 127it [01:30,  1.39it/s]Extractor Predicting: 128it [01:31,  1.43it/s]Extractor Predicting: 129it [01:32,  1.43it/s]Extractor Predicting: 130it [01:32,  1.46it/s]Extractor Predicting: 131it [01:33,  1.45it/s]Extractor Predicting: 132it [01:34,  1.43it/s]Extractor Predicting: 133it [01:34,  1.43it/s]Extractor Predicting: 134it [01:35,  1.42it/s]Extractor Predicting: 135it [01:36,  1.44it/s]Extractor Predicting: 136it [01:36,  1.43it/s]Extractor Predicting: 137it [01:37,  1.43it/s]Extractor Predicting: 138it [01:38,  1.45it/s]Extractor Predicting: 139it [01:39,  1.41it/s]Extractor Predicting: 140it [01:39,  1.45it/s]Extractor Predicting: 140it [01:39,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:08,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:08,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:08,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:08,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:08,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:53:09,406 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:53:09,407 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:53:10,040 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:53:11,162 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:53:11,162 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:14,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:14,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:14,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:14,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:53:14,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:53:14,925 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:53:14,926 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:53:15,518 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:53:15,737 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:53:15,737 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 3it [00:02,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:53:19,682 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:53:19,683 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:53:19,732 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:53:19,733 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:53:19,764 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:53:32,144 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:53:32,166 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:53:32,351 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:53:32,352 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:53:32,443 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:53:32,510 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:53:32,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:33,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:34,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:35,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:36,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:37,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:38,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:39,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:40,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:41,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:41,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:43,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:44,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:44,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:46,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:46,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:47,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:48,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:49,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:50,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:51,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:52,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:59, 19.98s/it][WARNING|generation_utils.py:914] 2023-08-28 00:53:52,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:53,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:54,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:55,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:56,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:57,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:58,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:59,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:59,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:01,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:02,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:03,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:04,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:05,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:06,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:06,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:07,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:08,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:09,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:10,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:11,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:12,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:13,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:13,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:41<02:48, 21.12s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:14,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:15,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:16,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:17,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:18,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:18,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:19,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:20,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:21,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:21,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:22,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:23,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:23,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:24,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:25,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:25,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:26,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:27,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:28,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:28,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:29,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:30,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:31,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:32,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:32,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:33,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:34,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:35,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:36,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:36,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:37,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:38,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:39,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:39,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:40,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:41,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:41,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:42,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:43,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:44,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:44,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:45,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:46,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:47,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:47,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:48,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:16<03:12, 27.47s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:49,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:50,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:51,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:52,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:53,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:53,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:55,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:55,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:56,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:57,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:58,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:59,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:59,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:00,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:01,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:02,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:03,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:03,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:04,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:05,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:06,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:06,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:07,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:08,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:09,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:28, 24.74s/it][WARNING|generation_utils.py:914] 2023-08-28 00:55:10,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:11,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:11,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:12,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:13,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:14,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:15,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:15,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:16,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:17,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:17,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:18,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:19,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:20,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:20,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:21,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:22,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:23,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:23,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:24,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:25,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:26,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:27,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:28,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:28,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:29,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:30,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:31,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:59<01:58, 23.65s/it][WARNING|generation_utils.py:914] 2023-08-28 00:55:32,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:33,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:33,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:34,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:35,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:36,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:36,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:38,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:39,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:39,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:40,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:41,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:42,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:43,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:43,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:44,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:45,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:46,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:47,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:48,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:49,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:50,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:51,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:52,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:20<01:30, 22.72s/it][WARNING|generation_utils.py:914] 2023-08-28 00:55:53,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:53,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:54,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:55,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:56,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:56,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:57,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:58,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:59,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:00,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:01,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:01,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:02,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:03,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:04,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:05,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:05,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:06,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:07,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:08,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:09,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:10,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:10,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:11,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:12,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:13,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:40<01:06, 22.10s/it][WARNING|generation_utils.py:914] 2023-08-28 00:56:13,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:14,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:15,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:16,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:16,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:17,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:18,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:19,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:19,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:20,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:21,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:21,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:22,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:23,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:23,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:24,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:25,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:26,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:26,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:27,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:28,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:28,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:56<00:40, 20.09s/it][WARNING|generation_utils.py:914] 2023-08-28 00:56:29,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:30,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:31,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:32,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:32,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:33,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:34,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:35,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:35,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:36,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:37,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:38,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:39,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:39,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:40,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:41,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:42,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:42,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:43,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:44,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:45,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:46,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:46,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:14<00:19, 19.41s/it][WARNING|generation_utils.py:914] 2023-08-28 00:56:47,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:48,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:49,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:50,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:51,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:51,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:52,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:53,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:54,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:55,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:56,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:57,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:58,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:59,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:00,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:01,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:01,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:02,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:03,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:04,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:05,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:06,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:07,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:08,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:08,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:09,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:10,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:11,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:39<00:00, 21.18s/it]Generating: 100%|██████████| 10/10 [03:39<00:00, 21.98s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:20,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:20,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:20,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:20,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:20,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:57:20,989 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:57:20,990 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:57:21,599 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:57:22,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:57:22,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:25,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:25,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:25,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:25,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:57:25,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:57:26,617 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:57:26,618 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:57:27,231 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:57:27,480 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:57:27,519 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.01s/it]Extractor Estimating: 2it [00:01,  1.12it/s]Extractor Estimating: 3it [00:02,  1.22it/s]Extractor Estimating: 4it [00:03,  1.11it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:05,  1.20it/s]Extractor Estimating: 7it [00:05,  1.22it/s]Extractor Estimating: 8it [00:06,  1.25it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.22it/s]Extractor Estimating: 11it [00:09,  1.22it/s]Extractor Estimating: 12it [00:10,  1.19it/s]Extractor Estimating: 13it [00:10,  1.21it/s]Extractor Estimating: 14it [00:11,  1.26it/s]Extractor Estimating: 15it [00:12,  1.24it/s]Extractor Estimating: 16it [00:13,  1.25it/s]Extractor Estimating: 17it [00:14,  1.22it/s]Extractor Estimating: 18it [00:14,  1.22it/s]Extractor Estimating: 19it [00:15,  1.21it/s]Extractor Estimating: 20it [00:16,  1.20it/s]Extractor Estimating: 21it [00:17,  1.14it/s]Extractor Estimating: 22it [00:18,  1.13it/s]Extractor Estimating: 23it [00:19,  1.20it/s]Extractor Estimating: 24it [00:19,  1.24it/s]Extractor Estimating: 25it [00:20,  1.20it/s]Extractor Estimating: 26it [00:21,  1.17it/s]Extractor Estimating: 27it [00:22,  1.21it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.15it/s]Extractor Estimating: 30it [00:25,  1.14it/s]Extractor Estimating: 31it [00:25,  1.17it/s]Extractor Estimating: 32it [00:26,  1.21it/s]Extractor Estimating: 33it [00:27,  1.24it/s]Extractor Estimating: 34it [00:28,  1.21it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:30,  1.15it/s]Extractor Estimating: 37it [00:30,  1.20it/s]Extractor Estimating: 38it [00:31,  1.14it/s]Extractor Estimating: 39it [00:32,  1.13it/s]Extractor Estimating: 40it [00:33,  1.13it/s]Extractor Estimating: 41it [00:34,  1.16it/s]Extractor Estimating: 42it [00:35,  1.15it/s]Extractor Estimating: 43it [00:36,  1.16it/s]Extractor Estimating: 44it [00:37,  1.12it/s]Extractor Estimating: 45it [00:37,  1.17it/s]Extractor Estimating: 46it [00:38,  1.16it/s]Extractor Estimating: 47it [00:39,  1.18it/s]Extractor Estimating: 48it [00:40,  1.17it/s]Extractor Estimating: 49it [00:41,  1.13it/s]Extractor Estimating: 50it [00:42,  1.09it/s]Extractor Estimating: 51it [00:43,  1.13it/s]Extractor Estimating: 52it [00:44,  1.16it/s]Extractor Estimating: 53it [00:44,  1.18it/s]Extractor Estimating: 54it [00:45,  1.20it/s]Extractor Estimating: 55it [00:46,  1.23it/s]Extractor Estimating: 56it [00:47,  1.24it/s]Extractor Estimating: 57it [00:48,  1.24it/s]Extractor Estimating: 58it [00:48,  1.24it/s]Extractor Estimating: 59it [00:49,  1.30it/s]Extractor Estimating: 60it [00:50,  1.29it/s]Extractor Estimating: 61it [00:51,  1.27it/s]Extractor Estimating: 62it [00:51,  1.25it/s]Extractor Estimating: 63it [00:52,  1.27it/s]Extractor Estimating: 64it [00:53,  1.29it/s]Extractor Estimating: 65it [00:54,  1.33it/s]Extractor Estimating: 66it [00:54,  1.31it/s]Extractor Estimating: 67it [00:55,  1.32it/s]Extractor Estimating: 68it [00:56,  1.29it/s]Extractor Estimating: 69it [00:57,  1.34it/s]Extractor Estimating: 70it [00:58,  1.28it/s]Extractor Estimating: 71it [00:58,  1.29it/s]Extractor Estimating: 72it [00:59,  1.33it/s]Extractor Estimating: 73it [01:00,  1.27it/s]Extractor Estimating: 74it [01:01,  1.29it/s]Extractor Estimating: 75it [01:02,  1.24it/s]Extractor Estimating: 76it [01:02,  1.29it/s]Extractor Estimating: 77it [01:03,  1.33it/s]Extractor Estimating: 78it [01:04,  1.27it/s]Extractor Estimating: 79it [01:05,  1.26it/s]Extractor Estimating: 80it [01:05,  1.31it/s]Extractor Estimating: 81it [01:06,  1.30it/s]Extractor Estimating: 82it [01:07,  1.31it/s]Extractor Estimating: 83it [01:08,  1.27it/s]Extractor Estimating: 84it [01:08,  1.33it/s]Extractor Estimating: 85it [01:09,  1.36it/s]Extractor Estimating: 86it [01:10,  1.38it/s]Extractor Estimating: 87it [01:10,  1.36it/s]Extractor Estimating: 88it [01:11,  1.37it/s]Extractor Estimating: 89it [01:12,  1.32it/s]Extractor Estimating: 90it [01:13,  1.35it/s]Extractor Estimating: 91it [01:13,  1.38it/s]Extractor Estimating: 92it [01:14,  1.32it/s]Extractor Estimating: 93it [01:15,  1.31it/s]Extractor Estimating: 94it [01:16,  1.33it/s]Extractor Estimating: 95it [01:17,  1.32it/s]Extractor Estimating: 96it [01:17,  1.35it/s]Extractor Estimating: 97it [01:18,  1.36it/s]Extractor Estimating: 98it [01:19,  1.32it/s]Extractor Estimating: 99it [01:20,  1.32it/s]Extractor Estimating: 100it [01:20,  1.30it/s]Extractor Estimating: 101it [01:21,  1.34it/s]Extractor Estimating: 102it [01:22,  1.35it/s]Extractor Estimating: 103it [01:23,  1.32it/s]Extractor Estimating: 104it [01:23,  1.32it/s]Extractor Estimating: 105it [01:24,  1.26it/s]Extractor Estimating: 106it [01:25,  1.28it/s]Extractor Estimating: 107it [01:26,  1.34it/s]Extractor Estimating: 108it [01:26,  1.39it/s]Extractor Estimating: 109it [01:27,  1.41it/s]Extractor Estimating: 110it [01:28,  1.44it/s]Extractor Estimating: 111it [01:28,  1.43it/s]Extractor Estimating: 112it [01:29,  1.39it/s]Extractor Estimating: 113it [01:30,  1.38it/s]Extractor Estimating: 114it [01:30,  1.43it/s]Extractor Estimating: 115it [01:31,  1.36it/s]Extractor Estimating: 116it [01:32,  1.37it/s]Extractor Estimating: 117it [01:33,  1.28it/s]Extractor Estimating: 118it [01:33,  1.35it/s]Extractor Estimating: 119it [01:34,  1.39it/s]Extractor Estimating: 120it [01:35,  1.37it/s]Extractor Estimating: 121it [01:36,  1.37it/s]Extractor Estimating: 122it [01:37,  1.28it/s]Extractor Estimating: 123it [01:37,  1.31it/s]Extractor Estimating: 124it [01:38,  1.32it/s]Extractor Estimating: 125it [01:39,  1.35it/s]Extractor Estimating: 126it [01:39,  1.35it/s]Extractor Estimating: 127it [01:40,  1.33it/s]Extractor Estimating: 128it [01:41,  1.33it/s]Extractor Estimating: 129it [01:42,  1.33it/s]Extractor Estimating: 130it [01:43,  1.30it/s]Extractor Estimating: 131it [01:43,  1.31it/s]Extractor Estimating: 132it [01:44,  1.37it/s]Extractor Estimating: 133it [01:45,  1.36it/s]Extractor Estimating: 134it [01:45,  1.37it/s]Extractor Estimating: 135it [01:46,  1.35it/s]Extractor Estimating: 136it [01:47,  1.32it/s]Extractor Estimating: 137it [01:48,  1.38it/s]Extractor Estimating: 138it [01:48,  1.40it/s]Extractor Estimating: 139it [01:49,  1.39it/s]Extractor Estimating: 140it [01:50,  1.34it/s]Extractor Estimating: 141it [01:51,  1.34it/s]Extractor Estimating: 142it [01:51,  1.35it/s]Extractor Estimating: 143it [01:52,  1.33it/s]Extractor Estimating: 144it [01:53,  1.26it/s]Extractor Estimating: 145it [01:54,  1.25it/s]Extractor Estimating: 146it [01:55,  1.27it/s]Extractor Estimating: 147it [01:55,  1.32it/s]Extractor Estimating: 148it [01:56,  1.32it/s]Extractor Estimating: 149it [01:57,  1.32it/s]Extractor Estimating: 150it [01:58,  1.31it/s]Extractor Estimating: 151it [01:58,  1.27it/s]Extractor Estimating: 152it [01:59,  1.27it/s]Extractor Estimating: 153it [02:00,  1.28it/s]Extractor Estimating: 154it [02:01,  1.28it/s]Extractor Estimating: 155it [02:02,  1.27it/s]Extractor Estimating: 156it [02:02,  1.21it/s]Extractor Estimating: 157it [02:03,  1.22it/s]Extractor Estimating: 158it [02:04,  1.22it/s]Extractor Estimating: 159it [02:05,  1.16it/s]Extractor Estimating: 160it [02:06,  1.21it/s]Extractor Estimating: 161it [02:07,  1.24it/s]Extractor Estimating: 162it [02:07,  1.23it/s]Extractor Estimating: 163it [02:08,  1.26it/s]Extractor Estimating: 164it [02:09,  1.25it/s]Extractor Estimating: 165it [02:10,  1.25it/s]Extractor Estimating: 166it [02:10,  1.27it/s]Extractor Estimating: 167it [02:11,  1.27it/s]Extractor Estimating: 168it [02:12,  1.28it/s]Extractor Estimating: 169it [02:13,  1.22it/s]Extractor Estimating: 170it [02:14,  1.22it/s]Extractor Estimating: 171it [02:15,  1.22it/s]Extractor Estimating: 172it [02:15,  1.28it/s]Extractor Estimating: 173it [02:16,  1.22it/s]Extractor Estimating: 174it [02:17,  1.23it/s]Extractor Estimating: 175it [02:18,  1.22it/s]Extractor Estimating: 176it [02:19,  1.19it/s]Extractor Estimating: 177it [02:19,  1.26it/s]Extractor Estimating: 178it [02:20,  1.30it/s]Extractor Estimating: 179it [02:21,  1.33it/s]Extractor Estimating: 180it [02:22,  1.37it/s]Extractor Estimating: 181it [02:22,  1.30it/s]Extractor Estimating: 182it [02:23,  1.29it/s]Extractor Estimating: 183it [02:24,  1.34it/s]Extractor Estimating: 184it [02:25,  1.35it/s]Extractor Estimating: 185it [02:25,  1.36it/s]Extractor Estimating: 186it [02:26,  1.34it/s]Extractor Estimating: 187it [02:27,  1.33it/s]Extractor Estimating: 188it [02:27,  1.37it/s]Extractor Estimating: 189it [02:28,  1.37it/s]Extractor Estimating: 190it [02:29,  1.38it/s]Extractor Estimating: 191it [02:30,  1.39it/s]Extractor Estimating: 192it [02:30,  1.32it/s]Extractor Estimating: 193it [02:31,  1.28it/s]Extractor Estimating: 194it [02:32,  1.31it/s]Extractor Estimating: 195it [02:33,  1.32it/s]Extractor Estimating: 196it [02:34,  1.31it/s]Extractor Estimating: 197it [02:34,  1.27it/s]Extractor Estimating: 198it [02:35,  1.33it/s]Extractor Estimating: 199it [02:36,  1.38it/s]Extractor Estimating: 200it [02:37,  1.36it/s]Extractor Estimating: 201it [02:37,  1.34it/s]Extractor Estimating: 202it [02:38,  1.29it/s]Extractor Estimating: 203it [02:39,  1.33it/s]Extractor Estimating: 204it [02:40,  1.31it/s]Extractor Estimating: 205it [02:40,  1.32it/s]Extractor Estimating: 206it [02:41,  1.34it/s]Extractor Estimating: 207it [02:42,  1.34it/s]Extractor Estimating: 208it [02:43,  1.28it/s]Extractor Estimating: 209it [02:43,  1.27it/s]Extractor Estimating: 210it [02:44,  1.25it/s]Extractor Estimating: 211it [02:45,  1.25it/s]Extractor Estimating: 212it [02:46,  1.26it/s]Extractor Estimating: 213it [02:47,  1.22it/s]Extractor Estimating: 214it [02:48,  1.23it/s]Extractor Estimating: 215it [02:48,  1.21it/s]Extractor Estimating: 216it [02:49,  1.24it/s]Extractor Estimating: 217it [02:50,  1.26it/s]Extractor Estimating: 218it [02:51,  1.28it/s]Extractor Estimating: 219it [02:51,  1.27it/s]Extractor Estimating: 220it [02:52,  1.24it/s]Extractor Estimating: 221it [02:53,  1.25it/s]Extractor Estimating: 222it [02:54,  1.26it/s]Extractor Estimating: 223it [02:55,  1.25it/s]Extractor Estimating: 224it [02:56,  1.24it/s]Extractor Estimating: 225it [02:56,  1.23it/s]Extractor Estimating: 226it [02:57,  1.26it/s]Extractor Estimating: 227it [02:58,  1.26it/s]Extractor Estimating: 228it [02:59,  1.29it/s]Extractor Estimating: 229it [03:00,  1.25it/s]Extractor Estimating: 230it [03:00,  1.23it/s]Extractor Estimating: 231it [03:01,  1.19it/s]Extractor Estimating: 232it [03:02,  1.20it/s]Extractor Estimating: 233it [03:03,  1.24it/s]Extractor Estimating: 234it [03:04,  1.18it/s]Extractor Estimating: 235it [03:05,  1.19it/s]Extractor Estimating: 236it [03:05,  1.22it/s]Extractor Estimating: 237it [03:06,  1.24it/s]Extractor Estimating: 238it [03:07,  1.25it/s]Extractor Estimating: 239it [03:08,  1.21it/s]Extractor Estimating: 240it [03:09,  1.23it/s]Extractor Estimating: 241it [03:09,  1.19it/s]Extractor Estimating: 242it [03:10,  1.18it/s]Extractor Estimating: 243it [03:11,  1.19it/s]Extractor Estimating: 244it [03:12,  1.19it/s]Extractor Estimating: 245it [03:13,  1.24it/s]Extractor Estimating: 246it [03:14,  1.25it/s]Extractor Estimating: 247it [03:14,  1.26it/s]Extractor Estimating: 248it [03:15,  1.25it/s]Extractor Estimating: 249it [03:16,  1.22it/s]Extractor Estimating: 250it [03:17,  1.23it/s]Extractor Estimating: 250it [03:17,  1.27it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:05,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:05,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:05,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:05,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:05,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:01:06,314 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:01:06,315 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:01:06,943 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:01:08,075 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:01:08,075 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:11,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:11,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:11,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:11,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:11,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:01:11,926 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:01:11,927 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:01:12,585 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:01:12,852 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:01:12,852 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 02:51:38,760 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 02:51:39,211 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5231 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 21994
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22094, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22094, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.269, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.279, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.280, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.289, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.276, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.584, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.260, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.284, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.275, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.278, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.579, loss:nan
g_step 1200, step 110, avg_time 1.273, loss:nan
g_step 1300, step 210, avg_time 1.267, loss:nan
g_step 1400, step 92, avg_time 1.298, loss:nan
g_step 1500, step 192, avg_time 1.286, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.566, loss:nan
g_step 1700, step 174, avg_time 1.275, loss:nan
g_step 1800, step 56, avg_time 1.285, loss:nan
g_step 1900, step 156, avg_time 1.277, loss:nan
g_step 2000, step 38, avg_time 1.269, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.564, loss:nan
g_step 2200, step 20, avg_time 1.289, loss:nan
g_step 2300, step 120, avg_time 1.278, loss:nan
g_step 2400, step 2, avg_time 1.291, loss:nan
g_step 2500, step 102, avg_time 1.264, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.575, loss:nan
g_step 2700, step 84, avg_time 1.269, loss:nan
g_step 2800, step 184, avg_time 1.295, loss:nan
g_step 2900, step 66, avg_time 1.288, loss:nan
g_step 3000, step 166, avg_time 1.275, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.578, loss:nan
g_step 3200, step 148, avg_time 1.268, loss:nan
g_step 3300, step 30, avg_time 1.296, loss:nan
g_step 3400, step 130, avg_time 1.291, loss:nan
g_step 3500, step 12, avg_time 1.265, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.595, loss:nan
g_step 3700, step 212, avg_time 1.280, loss:nan
g_step 3800, step 94, avg_time 1.289, loss:nan
g_step 3900, step 194, avg_time 1.272, loss:nan
g_step 4000, step 76, avg_time 1.293, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.600, loss:nan
g_step 4200, step 58, avg_time 1.279, loss:nan
g_step 4300, step 158, avg_time 1.284, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:51:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:51:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-51-38_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:51:40 - WARNING - datasets.builder -   Using custom data configuration default-5c56408408adb04e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5c56408408adb04e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:51:44,237 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:51:44,239 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:51:44,239 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:51:44,240 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:51:44,434 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,526 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,526 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,526 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,526 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,527 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:51:44,527 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:51:45,071 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:51:48,218 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:51:48,262 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5c56408408adb04e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.95ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.82ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.42ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.81ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.06ba/s]100%|██████████| 6/6 [00:01<00:00,  4.39ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.44ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.01ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.25ba/s]100%|██████████| 4/4 [00:00<00:00,  5.36ba/s]100%|██████████| 4/4 [00:00<00:00,  4.75ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.93ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.17ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.18ba/s]100%|██████████| 6/6 [00:00<00:00, 10.68ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.24ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.79ba/s]100%|██████████| 4/4 [00:00<00:00,  9.80ba/s]
[INFO|trainer.py:414] 2023-08-28 02:51:52,435 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:51:52,487 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:51:52,487 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 02:51:52,487 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:51:52,487 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:51:52,487 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:51:52,487 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:51:52,487 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.49it/s]  0%|          | 2/410 [00:00<01:54,  3.57it/s]  1%|          | 3/410 [00:00<01:53,  3.59it/s]  1%|          | 4/410 [00:01<01:53,  3.59it/s]  1%|          | 5/410 [00:01<01:53,  3.58it/s]  1%|▏         | 6/410 [00:01<01:53,  3.57it/s]  2%|▏         | 7/410 [00:01<01:53,  3.56it/s]  2%|▏         | 8/410 [00:02<01:55,  3.47it/s]  2%|▏         | 9/410 [00:02<01:54,  3.49it/s]  2%|▏         | 10/410 [00:02<01:53,  3.51it/s]  3%|▎         | 11/410 [00:03<01:53,  3.53it/s]  3%|▎         | 12/410 [00:03<01:52,  3.54it/s]  3%|▎         | 13/410 [00:03<01:52,  3.54it/s]  3%|▎         | 14/410 [00:03<01:51,  3.55it/s]  4%|▎         | 15/410 [00:04<01:51,  3.55it/s]  4%|▍         | 16/410 [00:04<01:50,  3.56it/s]  4%|▍         | 17/410 [00:04<01:50,  3.56it/s]  4%|▍         | 18/410 [00:05<01:50,  3.56it/s]  5%|▍         | 19/410 [00:05<01:52,  3.49it/s]  5%|▍         | 20/410 [00:05<01:51,  3.50it/s]  5%|▌         | 21/410 [00:05<01:50,  3.52it/s]  5%|▌         | 22/410 [00:06<01:50,  3.53it/s]  6%|▌         | 23/410 [00:06<01:49,  3.53it/s]  6%|▌         | 24/410 [00:06<01:49,  3.53it/s]  6%|▌         | 25/410 [00:07<01:48,  3.54it/s]  6%|▋         | 26/410 [00:07<01:48,  3.54it/s]  7%|▋         | 27/410 [00:07<01:47,  3.55it/s]  7%|▋         | 28/410 [00:07<01:47,  3.55it/s]  7%|▋         | 29/410 [00:08<01:47,  3.55it/s]  7%|▋         | 30/410 [00:08<01:48,  3.50it/s]  8%|▊         | 31/410 [00:08<01:47,  3.51it/s]  8%|▊         | 32/410 [00:09<01:47,  3.53it/s]  8%|▊         | 33/410 [00:09<01:46,  3.54it/s]  8%|▊         | 34/410 [00:09<01:46,  3.54it/s]  9%|▊         | 35/410 [00:09<01:45,  3.55it/s]  9%|▉         | 36/410 [00:10<01:45,  3.55it/s]  9%|▉         | 37/410 [00:10<01:45,  3.55it/s]  9%|▉         | 38/410 [00:10<01:44,  3.55it/s] 10%|▉         | 39/410 [00:11<01:44,  3.55it/s] 10%|▉         | 40/410 [00:11<01:44,  3.55it/s] 10%|█         | 41/410 [00:11<01:45,  3.48it/s] 10%|█         | 42/410 [00:11<01:45,  3.50it/s] 10%|█         | 43/410 [00:12<01:44,  3.52it/s] 11%|█         | 44/410 [00:12<01:43,  3.53it/s] 11%|█         | 45/410 [00:12<01:43,  3.53it/s] 11%|█         | 46/410 [00:13<01:42,  3.54it/s] 11%|█▏        | 47/410 [00:13<01:42,  3.55it/s] 12%|█▏        | 48/410 [00:13<01:42,  3.55it/s] 12%|█▏        | 49/410 [00:13<01:41,  3.55it/s] 12%|█▏        | 50/410 [00:14<01:41,  3.54it/s] 12%|█▏        | 51/410 [00:14<01:41,  3.54it/s] 13%|█▎        | 52/410 [00:14<01:43,  3.45it/s] 13%|█▎        | 53/410 [00:15<01:42,  3.48it/s] 13%|█▎        | 54/410 [00:15<01:41,  3.49it/s] 13%|█▎        | 55/410 [00:15<01:41,  3.51it/s] 14%|█▎        | 56/410 [00:15<01:40,  3.51it/s] 14%|█▍        | 57/410 [00:16<01:40,  3.52it/s] 14%|█▍        | 58/410 [00:16<01:39,  3.52it/s] 14%|█▍        | 59/410 [00:16<01:39,  3.53it/s] 15%|█▍        | 60/410 [00:17<01:44,  3.36it/s] 15%|█▍        | 61/410 [00:17<01:42,  3.41it/s] 15%|█▌        | 62/410 [00:17<01:40,  3.45it/s] 15%|█▌        | 63/410 [00:17<01:39,  3.48it/s] 16%|█▌        | 64/410 [00:18<01:38,  3.50it/s] 16%|█▌        | 65/410 [00:18<01:38,  3.51it/s] 16%|█▌        | 66/410 [00:18<01:37,  3.52it/s] 16%|█▋        | 67/410 [00:19<01:37,  3.52it/s] 17%|█▋        | 68/410 [00:19<01:37,  3.53it/s] 17%|█▋        | 69/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 70/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 71/410 [00:20<01:49,  3.11it/s] 18%|█▊        | 72/410 [00:20<01:44,  3.22it/s] 18%|█▊        | 73/410 [00:20<01:41,  3.31it/s] 18%|█▊        | 74/410 [00:21<01:39,  3.38it/s] 18%|█▊        | 75/410 [00:21<01:37,  3.42it/s] 19%|█▊        | 76/410 [00:21<01:36,  3.46it/s] 19%|█▉        | 77/410 [00:21<01:35,  3.48it/s] 19%|█▉        | 78/410 [00:22<01:34,  3.50it/s] 19%|█▉        | 79/410 [00:22<01:34,  3.51it/s] 20%|█▉        | 80/410 [00:22<01:33,  3.51it/s] 20%|█▉        | 81/410 [00:23<01:33,  3.52it/s] 20%|██        | 82/410 [00:23<01:33,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 02:52:15,880 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:52:15,880 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:52:15,880 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.05it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.72it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.32it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.49it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.91it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.15it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.51it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.11it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.21it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.30it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 43.86it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.22it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.49it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.65it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.54it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.28it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.02it/s][A
 21%|██        | 92/438 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.18it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.46it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.29it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.83it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.92it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.81it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.01it/s][A
 30%|███       | 132/438 [00:02<00:07, 42.66it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.17it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.58it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.91it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.22it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.43it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.59it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.29it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.35it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.73it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.17it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.36it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.56it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.64it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.62it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.39it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.24it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.28it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.38it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.50it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.50it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.73it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.78it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.74it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.48it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.33it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.70it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.91it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.01it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.47it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.61it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.50it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.36it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.74it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.91it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.14it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.25it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.31it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.59it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.59it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.36it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.35it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.34it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.35it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.51it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.55it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.55it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.67it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.66it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.51it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.38it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 42.91it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.47it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.86it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.08it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.24it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.41it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.45it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.43it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.14it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.14it/s][A 20%|██        | 82/410 [00:33<01:33,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:52:25,899 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 02:52:26,053 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:52:29,778 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:52:30,025 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:52:30,147 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:39<27:46,  5.10s/it] 20%|██        | 84/410 [00:39<19:50,  3.65s/it] 21%|██        | 85/410 [00:40<14:18,  2.64s/it] 21%|██        | 86/410 [00:40<10:26,  1.93s/it] 21%|██        | 87/410 [00:40<07:49,  1.45s/it] 21%|██▏       | 88/410 [00:41<05:54,  1.10s/it] 22%|██▏       | 89/410 [00:41<04:34,  1.17it/s] 22%|██▏       | 90/410 [00:41<03:38,  1.46it/s] 22%|██▏       | 91/410 [00:42<02:59,  1.78it/s] 22%|██▏       | 92/410 [00:42<02:32,  2.09it/s] 23%|██▎       | 93/410 [00:42<02:12,  2.38it/s] 23%|██▎       | 94/410 [00:42<01:59,  2.65it/s] 23%|██▎       | 95/410 [00:43<01:49,  2.87it/s] 23%|██▎       | 96/410 [00:43<01:43,  3.03it/s] 24%|██▎       | 97/410 [00:43<01:38,  3.17it/s] 24%|██▍       | 98/410 [00:44<01:44,  2.99it/s] 24%|██▍       | 99/410 [00:44<01:39,  3.13it/s] 24%|██▍       | 100/410 [00:44<01:35,  3.24it/s] 25%|██▍       | 101/410 [00:44<01:32,  3.33it/s] 25%|██▍       | 102/410 [00:45<01:30,  3.39it/s] 25%|██▌       | 103/410 [00:45<01:29,  3.44it/s] 25%|██▌       | 104/410 [00:45<01:28,  3.47it/s] 26%|██▌       | 105/410 [00:46<01:27,  3.50it/s] 26%|██▌       | 106/410 [00:46<02:06,  2.40it/s] 26%|██▌       | 107/410 [00:47<01:58,  2.56it/s] 26%|██▋       | 108/410 [00:47<01:48,  2.79it/s] 27%|██▋       | 109/410 [00:47<01:46,  2.81it/s] 27%|██▋       | 110/410 [00:48<01:40,  3.00it/s] 27%|██▋       | 111/410 [00:48<01:35,  3.14it/s] 27%|██▋       | 112/410 [00:48<01:31,  3.25it/s] 28%|██▊       | 113/410 [00:48<01:29,  3.33it/s] 28%|██▊       | 114/410 [00:49<01:27,  3.39it/s] 28%|██▊       | 115/410 [00:49<01:26,  3.43it/s] 28%|██▊       | 116/410 [00:49<01:24,  3.46it/s] 29%|██▊       | 117/410 [00:50<01:24,  3.48it/s] 29%|██▉       | 118/410 [00:50<01:23,  3.49it/s] 29%|██▉       | 119/410 [00:50<01:22,  3.51it/s] 29%|██▉       | 120/410 [00:50<01:27,  3.33it/s] 30%|██▉       | 121/410 [00:51<01:25,  3.39it/s] 30%|██▉       | 122/410 [00:51<01:23,  3.43it/s] 30%|███       | 123/410 [00:51<01:22,  3.46it/s] 30%|███       | 124/410 [00:52<01:22,  3.48it/s] 30%|███       | 125/410 [00:52<01:21,  3.49it/s] 31%|███       | 126/410 [00:52<01:21,  3.50it/s] 31%|███       | 127/410 [00:52<01:20,  3.51it/s] 31%|███       | 128/410 [00:53<01:20,  3.52it/s] 31%|███▏      | 129/410 [00:53<01:19,  3.52it/s] 32%|███▏      | 130/410 [00:53<01:19,  3.52it/s] 32%|███▏      | 131/410 [00:54<01:23,  3.35it/s] 32%|███▏      | 132/410 [00:54<01:21,  3.40it/s] 32%|███▏      | 133/410 [00:54<01:20,  3.44it/s] 33%|███▎      | 134/410 [00:54<01:19,  3.47it/s] 33%|███▎      | 135/410 [00:55<01:18,  3.49it/s] 33%|███▎      | 136/410 [00:55<01:18,  3.50it/s] 33%|███▎      | 137/410 [00:55<01:17,  3.51it/s] 34%|███▎      | 138/410 [00:56<01:17,  3.52it/s] 34%|███▍      | 139/410 [00:56<01:16,  3.53it/s] 34%|███▍      | 140/410 [00:56<01:16,  3.53it/s] 34%|███▍      | 141/410 [00:56<01:16,  3.53it/s] 35%|███▍      | 142/410 [00:57<01:18,  3.42it/s] 35%|███▍      | 143/410 [00:57<01:17,  3.45it/s] 35%|███▌      | 144/410 [00:57<01:16,  3.48it/s] 35%|███▌      | 145/410 [00:58<01:15,  3.49it/s] 36%|███▌      | 146/410 [00:58<01:15,  3.50it/s] 36%|███▌      | 147/410 [00:58<01:14,  3.51it/s] 36%|███▌      | 148/410 [00:58<01:14,  3.52it/s] 36%|███▋      | 149/410 [00:59<01:14,  3.52it/s] 37%|███▋      | 150/410 [00:59<01:13,  3.52it/s] 37%|███▋      | 151/410 [00:59<01:13,  3.52it/s] 37%|███▋      | 152/410 [01:00<01:13,  3.53it/s] 37%|███▋      | 153/410 [01:00<01:14,  3.46it/s] 38%|███▊      | 154/410 [01:00<01:13,  3.49it/s] 38%|███▊      | 155/410 [01:00<01:12,  3.50it/s] 38%|███▊      | 156/410 [01:01<01:12,  3.51it/s] 38%|███▊      | 157/410 [01:01<01:12,  3.51it/s] 39%|███▊      | 158/410 [01:01<01:11,  3.52it/s] 39%|███▉      | 159/410 [01:02<01:11,  3.52it/s] 39%|███▉      | 160/410 [01:02<01:10,  3.53it/s] 39%|███▉      | 161/410 [01:02<01:10,  3.53it/s] 40%|███▉      | 162/410 [01:02<01:10,  3.53it/s] 40%|███▉      | 163/410 [01:03<01:10,  3.53it/s] 40%|████      | 164/410 [01:03<01:08,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 02:52:55,953 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:52:55,953 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:52:55,953 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8978, 'eval_samples_per_second': 353.411, 'eval_steps_per_second': 44.252, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.42it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.32it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.12it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.38it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.87it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.14it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.62it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.37it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.63it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.65it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.78it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.77it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.50it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.25it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.25it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.44it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.68it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.74it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.66it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.66it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.91it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.37it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.70it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.12it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.29it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.52it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.58it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.43it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.17it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.08it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.26it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.38it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.48it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.58it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.63it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.75it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.90it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 43.94it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 43.93it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.06it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 43.97it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.54it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.66it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.67it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.52it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.35it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.16it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.22it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.30it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.43it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.69it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.49it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.33it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.25it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.29it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.45it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.46it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.57it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.66it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.64it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.48it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.33it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.25it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.30it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.41it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.53it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.67it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.67it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.67it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.44it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.35it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 42.94it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.46it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.76it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.98it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.21it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.43it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.46it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.32it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.10it/s][A 40%|████      | 164/410 [01:13<01:08,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:53:06,022 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 02:53:06,180 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:53:09,227 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:53:09,350 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:53:09,412 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:18<18:44,  4.59s/it] 40%|████      | 166/410 [01:18<13:27,  3.31s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:18<09:43,  2.40s/it] 41%|████      | 168/410 [01:18<07:07,  1.77s/it] 41%|████      | 169/410 [01:19<05:18,  1.32s/it] 41%|████▏     | 170/410 [01:19<04:02,  1.01s/it] 42%|████▏     | 171/410 [01:19<03:09,  1.26it/s] 42%|████▏     | 172/410 [01:20<02:32,  1.57it/s] 42%|████▏     | 173/410 [01:20<02:05,  1.88it/s] 42%|████▏     | 174/410 [01:20<01:47,  2.19it/s] 43%|████▎     | 175/410 [01:20<01:34,  2.47it/s] 43%|████▎     | 176/410 [01:21<01:25,  2.72it/s] 43%|████▎     | 177/410 [01:21<01:22,  2.81it/s] 43%|████▎     | 178/410 [01:21<01:17,  2.99it/s] 44%|████▎     | 179/410 [01:22<01:13,  3.14it/s] 44%|████▍     | 180/410 [01:22<01:10,  3.25it/s] 44%|████▍     | 181/410 [01:22<01:08,  3.33it/s] 44%|████▍     | 182/410 [01:22<01:07,  3.39it/s] 45%|████▍     | 183/410 [01:23<01:06,  3.43it/s] 45%|████▍     | 184/410 [01:23<01:05,  3.45it/s] 45%|████▌     | 185/410 [01:23<01:04,  3.48it/s] 45%|████▌     | 186/410 [01:24<01:04,  3.49it/s] 46%|████▌     | 187/410 [01:24<01:03,  3.51it/s] 46%|████▌     | 188/410 [01:24<01:06,  3.36it/s] 46%|████▌     | 189/410 [01:25<01:04,  3.41it/s] 46%|████▋     | 190/410 [01:25<01:03,  3.45it/s] 47%|████▋     | 191/410 [01:25<01:03,  3.47it/s] 47%|████▋     | 192/410 [01:25<01:02,  3.49it/s] 47%|████▋     | 193/410 [01:26<01:02,  3.50it/s] 47%|████▋     | 194/410 [01:26<01:01,  3.50it/s] 48%|████▊     | 195/410 [01:26<01:01,  3.51it/s] 48%|████▊     | 196/410 [01:27<01:00,  3.52it/s] 48%|████▊     | 197/410 [01:27<01:00,  3.52it/s] 48%|████▊     | 198/410 [01:27<01:00,  3.53it/s] 49%|████▊     | 199/410 [01:27<01:01,  3.42it/s] 49%|████▉     | 200/410 [01:28<01:00,  3.46it/s] 49%|████▉     | 201/410 [01:28<01:00,  3.48it/s] 49%|████▉     | 202/410 [01:28<00:59,  3.50it/s] 50%|████▉     | 203/410 [01:29<00:59,  3.51it/s] 50%|████▉     | 204/410 [01:29<00:58,  3.51it/s] 50%|█████     | 205/410 [01:29<00:57,  3.54it/s] 50%|█████     | 206/410 [01:29<00:57,  3.56it/s] 50%|█████     | 207/410 [01:30<00:56,  3.58it/s] 51%|█████     | 208/410 [01:30<00:56,  3.59it/s] 51%|█████     | 209/410 [01:30<00:55,  3.60it/s] 51%|█████     | 210/410 [01:30<00:56,  3.51it/s] 51%|█████▏    | 211/410 [01:31<00:56,  3.54it/s] 52%|█████▏    | 212/410 [01:31<00:55,  3.57it/s] 52%|█████▏    | 213/410 [01:31<00:54,  3.58it/s] 52%|█████▏    | 214/410 [01:32<00:54,  3.59it/s] 52%|█████▏    | 215/410 [01:32<00:54,  3.60it/s] 53%|█████▎    | 216/410 [01:32<00:53,  3.61it/s] 53%|█████▎    | 217/410 [01:32<00:53,  3.61it/s] 53%|█████▎    | 218/410 [01:33<00:53,  3.61it/s] 53%|█████▎    | 219/410 [01:33<00:52,  3.62it/s] 54%|█████▎    | 220/410 [01:33<00:52,  3.62it/s] 54%|█████▍    | 221/410 [01:34<00:54,  3.46it/s] 54%|█████▍    | 222/410 [01:34<00:53,  3.51it/s] 54%|█████▍    | 223/410 [01:34<00:52,  3.54it/s] 55%|█████▍    | 224/410 [01:34<00:52,  3.56it/s] 55%|█████▍    | 225/410 [01:35<00:51,  3.58it/s] 55%|█████▌    | 226/410 [01:35<00:51,  3.59it/s] 55%|█████▌    | 227/410 [01:35<00:50,  3.60it/s] 56%|█████▌    | 228/410 [01:35<00:50,  3.61it/s] 56%|█████▌    | 229/410 [01:36<00:50,  3.61it/s] 56%|█████▌    | 230/410 [01:36<00:49,  3.61it/s] 56%|█████▋    | 231/410 [01:36<00:49,  3.62it/s] 57%|█████▋    | 232/410 [01:37<00:50,  3.50it/s] 57%|█████▋    | 233/410 [01:37<00:50,  3.53it/s] 57%|█████▋    | 234/410 [01:37<00:49,  3.55it/s] 57%|█████▋    | 235/410 [01:37<00:48,  3.57it/s] 58%|█████▊    | 236/410 [01:38<00:48,  3.59it/s] 58%|█████▊    | 237/410 [01:38<00:48,  3.59it/s] 58%|█████▊    | 238/410 [01:38<00:47,  3.59it/s] 58%|█████▊    | 239/410 [01:39<00:47,  3.60it/s] 59%|█████▊    | 240/410 [01:39<00:47,  3.60it/s] 59%|█████▉    | 241/410 [01:39<00:46,  3.61it/s] 59%|█████▉    | 242/410 [01:39<00:46,  3.61it/s] 59%|█████▉    | 243/410 [01:40<00:48,  3.44it/s] 60%|█████▉    | 244/410 [01:40<00:47,  3.49it/s] 60%|█████▉    | 245/410 [01:40<00:46,  3.53it/s] 60%|██████    | 246/410 [01:41<00:44,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:53:33,513 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:53:33,513 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:53:33,513 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8712, 'eval_samples_per_second': 354.365, 'eval_steps_per_second': 44.372, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.12it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.91it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.03it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.09it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.92it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.45it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.74it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.66it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.61it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.25it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.06it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.11it/s][A
 21%|██        | 92/438 [00:02<00:08, 41.20it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 42.28it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.09it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.65it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.03it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.25it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.16it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.14it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.93it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 41.00it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 42.17it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.04it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.62it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.00it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.30it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.42it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.35it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.08it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.82it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.01it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.25it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.49it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.69it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.68it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.72it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.52it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.24it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 40.97it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 42.13it/s][A
 54%|█████▍    | 237/438 [00:06<00:14, 14.11it/s][A
 55%|█████▌    | 242/438 [00:06<00:11, 17.79it/s][A
 56%|█████▋    | 247/438 [00:06<00:08, 21.75it/s][A
 58%|█████▊    | 252/438 [00:06<00:07, 25.72it/s][A
 59%|█████▊    | 257/438 [00:06<00:06, 29.53it/s][A
 60%|█████▉    | 262/438 [00:06<00:05, 32.95it/s][A
 61%|██████    | 267/438 [00:06<00:04, 35.85it/s][A
 62%|██████▏   | 272/438 [00:06<00:04, 38.18it/s][A
 63%|██████▎   | 277/438 [00:07<00:04, 39.66it/s][A
 64%|██████▍   | 282/438 [00:07<00:03, 40.70it/s][A
 66%|██████▌   | 287/438 [00:07<00:03, 41.52it/s][A
 67%|██████▋   | 292/438 [00:07<00:03, 42.37it/s][A
 68%|██████▊   | 297/438 [00:07<00:03, 43.01it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 43.63it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.05it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.35it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 43.37it/s][A
 74%|███████▎  | 322/438 [00:08<00:02, 43.67it/s][A
 75%|███████▍  | 327/438 [00:08<00:02, 43.60it/s][A
 76%|███████▌  | 332/438 [00:08<00:02, 43.68it/s][A
 77%|███████▋  | 337/438 [00:08<00:02, 43.99it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 44.18it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 44.33it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.46it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.58it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.70it/s][A
 84%|████████▍ | 367/438 [00:09<00:01, 44.51it/s][A
 85%|████████▍ | 372/438 [00:09<00:01, 44.39it/s][A
 86%|████████▌ | 377/438 [00:09<00:01, 44.32it/s][A
 87%|████████▋ | 382/438 [00:09<00:01, 44.37it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.36it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.43it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.53it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.69it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.86it/s][A
 94%|█████████▍| 412/438 [00:10<00:00, 44.74it/s][A
 95%|█████████▌| 417/438 [00:10<00:00, 44.48it/s][A
 96%|█████████▋| 422/438 [00:10<00:00, 44.37it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 44.37it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.28it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.33it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.33it/s][A 60%|██████    | 246/410 [01:51<00:44,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:53:44,318 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 02:53:44,516 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:53:47,391 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:53:47,485 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:53:47,531 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:55<12:42,  4.68s/it] 60%|██████    | 248/410 [01:56<09:04,  3.36s/it] 61%|██████    | 249/410 [01:56<06:32,  2.44s/it] 61%|██████    | 250/410 [01:56<04:46,  1.79s/it] 61%|██████    | 251/410 [01:57<03:32,  1.34s/it] 61%|██████▏   | 252/410 [01:57<02:41,  1.02s/it] 62%|██████▏   | 253/410 [01:57<02:05,  1.25it/s] 62%|██████▏   | 254/410 [01:57<01:40,  1.55it/s] 62%|██████▏   | 255/410 [01:58<01:24,  1.84it/s] 62%|██████▏   | 256/410 [01:58<01:11,  2.15it/s] 63%|██████▎   | 257/410 [01:58<01:02,  2.44it/s] 63%|██████▎   | 258/410 [01:59<00:56,  2.69it/s] 63%|██████▎   | 259/410 [01:59<00:52,  2.89it/s] 63%|██████▎   | 260/410 [01:59<00:49,  3.06it/s] 64%|██████▎   | 261/410 [01:59<00:46,  3.19it/s] 64%|██████▍   | 262/410 [02:00<00:45,  3.29it/s] 64%|██████▍   | 263/410 [02:00<00:43,  3.36it/s] 64%|██████▍   | 264/410 [02:00<00:42,  3.41it/s] 65%|██████▍   | 265/410 [02:01<00:42,  3.45it/s] 65%|██████▍   | 266/410 [02:01<00:42,  3.37it/s] 65%|██████▌   | 267/410 [02:01<00:41,  3.42it/s] 65%|██████▌   | 268/410 [02:01<00:41,  3.46it/s] 66%|██████▌   | 269/410 [02:02<00:40,  3.48it/s] 66%|██████▌   | 270/410 [02:02<00:40,  3.50it/s] 66%|██████▌   | 271/410 [02:02<00:39,  3.51it/s] 66%|██████▋   | 272/410 [02:03<00:39,  3.51it/s] 67%|██████▋   | 273/410 [02:03<00:38,  3.52it/s] 67%|██████▋   | 274/410 [02:03<00:38,  3.53it/s] 67%|██████▋   | 275/410 [02:03<00:38,  3.53it/s] 67%|██████▋   | 276/410 [02:04<00:37,  3.53it/s] 68%|██████▊   | 277/410 [02:04<00:38,  3.46it/s] 68%|██████▊   | 278/410 [02:04<00:37,  3.48it/s] 68%|██████▊   | 279/410 [02:05<00:37,  3.49it/s] 68%|██████▊   | 280/410 [02:05<00:37,  3.51it/s] 69%|██████▊   | 281/410 [02:05<00:36,  3.52it/s] 69%|██████▉   | 282/410 [02:05<00:36,  3.53it/s] 69%|██████▉   | 283/410 [02:06<00:35,  3.53it/s] 69%|██████▉   | 284/410 [02:06<00:35,  3.53it/s] 70%|██████▉   | 285/410 [02:06<00:35,  3.53it/s] 70%|██████▉   | 286/410 [02:07<00:35,  3.53it/s] 70%|███████   | 287/410 [02:07<00:34,  3.53it/s] 70%|███████   | 288/410 [02:07<00:35,  3.46it/s] 70%|███████   | 289/410 [02:07<00:34,  3.48it/s] 71%|███████   | 290/410 [02:08<00:34,  3.50it/s] 71%|███████   | 291/410 [02:08<00:33,  3.50it/s] 71%|███████   | 292/410 [02:08<00:33,  3.51it/s] 71%|███████▏  | 293/410 [02:09<00:33,  3.52it/s] 72%|███████▏  | 294/410 [02:09<00:32,  3.52it/s] 72%|███████▏  | 295/410 [02:09<00:32,  3.53it/s] 72%|███████▏  | 296/410 [02:09<00:32,  3.53it/s] 72%|███████▏  | 297/410 [02:10<00:32,  3.53it/s] 73%|███████▎  | 298/410 [02:10<00:31,  3.53it/s] 73%|███████▎  | 299/410 [02:10<00:32,  3.45it/s] 73%|███████▎  | 300/410 [02:11<00:31,  3.48it/s] 73%|███████▎  | 301/410 [02:11<00:31,  3.49it/s] 74%|███████▎  | 302/410 [02:11<00:30,  3.51it/s] 74%|███████▍  | 303/410 [02:11<00:30,  3.51it/s] 74%|███████▍  | 304/410 [02:12<00:30,  3.52it/s] 74%|███████▍  | 305/410 [02:12<00:29,  3.52it/s] 75%|███████▍  | 306/410 [02:12<00:29,  3.53it/s] 75%|███████▍  | 307/410 [02:13<00:29,  3.53it/s] 75%|███████▌  | 308/410 [02:13<00:28,  3.55it/s] 75%|███████▌  | 309/410 [02:13<00:28,  3.57it/s] 76%|███████▌  | 310/410 [02:13<00:29,  3.43it/s] 76%|███████▌  | 311/410 [02:14<00:28,  3.49it/s] 76%|███████▌  | 312/410 [02:14<00:27,  3.52it/s] 76%|███████▋  | 313/410 [02:14<00:27,  3.55it/s] 77%|███████▋  | 314/410 [02:15<00:26,  3.57it/s] 77%|███████▋  | 315/410 [02:15<00:26,  3.58it/s] 77%|███████▋  | 316/410 [02:15<00:26,  3.59it/s] 77%|███████▋  | 317/410 [02:15<00:25,  3.60it/s] 78%|███████▊  | 318/410 [02:16<00:25,  3.60it/s] 78%|███████▊  | 319/410 [02:16<00:25,  3.61it/s] 78%|███████▊  | 320/410 [02:16<00:24,  3.61it/s] 78%|███████▊  | 321/410 [02:17<00:25,  3.44it/s] 79%|███████▊  | 322/410 [02:17<00:25,  3.50it/s] 79%|███████▉  | 323/410 [02:17<00:24,  3.53it/s] 79%|███████▉  | 324/410 [02:17<00:24,  3.56it/s] 79%|███████▉  | 325/410 [02:18<00:23,  3.57it/s] 80%|███████▉  | 326/410 [02:18<00:23,  3.58it/s] 80%|███████▉  | 327/410 [02:18<00:23,  3.59it/s] 80%|████████  | 328/410 [02:18<00:21,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 02:54:11,429 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:54:11,430 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:54:11,430 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 10.715, 'eval_samples_per_second': 326.458, 'eval_steps_per_second': 40.877, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.07it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.99it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.12it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.14it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.72it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.55it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.47it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.51it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.69it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.77it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.81it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.65it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.29it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.30it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.34it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.45it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.53it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.57it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.66it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.61it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.43it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.31it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.20it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.25it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.44it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.56it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.61it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.58it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.41it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.26it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.65it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.97it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.18it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.30it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.29it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.21it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.01it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.27it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.51it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.42it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.51it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.48it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.56it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.29it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.19it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.31it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.51it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.45it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.51it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.41it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.49it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.37it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.29it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.46it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.48it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.60it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.49it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.52it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.44it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.38it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.38it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.27it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.38it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.52it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.52it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.45it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.24it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.47it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.36it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.27it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.30it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.37it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.48it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.52it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.59it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.55it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.55it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.45it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.32it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.35it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.64it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 43.64it/s][A 80%|████████  | 328/410 [02:28<00:21,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:54:21,458 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 02:54:21,569 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:54:24,756 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:54:24,842 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:54:24,885 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:33<06:07,  4.53s/it] 80%|████████  | 330/410 [02:33<04:20,  3.26s/it] 81%|████████  | 331/410 [02:33<03:06,  2.36s/it] 81%|████████  | 332/410 [02:34<02:15,  1.74s/it] 81%|████████  | 333/410 [02:34<01:40,  1.30s/it] 81%|████████▏ | 334/410 [02:34<01:15,  1.01it/s] 82%|████████▏ | 335/410 [02:35<00:58,  1.28it/s] 82%|████████▏ | 336/410 [02:35<00:46,  1.59it/s] 82%|████████▏ | 337/410 [02:35<00:38,  1.90it/s] 82%|████████▏ | 338/410 [02:35<00:32,  2.21it/s] 83%|████████▎ | 339/410 [02:36<00:28,  2.50it/s] 83%|████████▎ | 340/410 [02:36<00:25,  2.76it/s] 83%|████████▎ | 341/410 [02:36<00:23,  2.97it/s] 83%|████████▎ | 342/410 [02:37<00:21,  3.14it/s] 84%|████████▎ | 343/410 [02:37<00:20,  3.27it/s] 84%|████████▍ | 344/410 [02:37<00:19,  3.37it/s] 84%|████████▍ | 345/410 [02:37<00:18,  3.44it/s] 84%|████████▍ | 346/410 [02:38<00:18,  3.49it/s] 85%|████████▍ | 347/410 [02:38<00:17,  3.52it/s] 85%|████████▍ | 348/410 [02:38<00:18,  3.43it/s] 85%|████████▌ | 349/410 [02:39<00:17,  3.48it/s] 85%|████████▌ | 350/410 [02:39<00:17,  3.52it/s] 86%|████████▌ | 351/410 [02:39<00:16,  3.55it/s] 86%|████████▌ | 352/410 [02:39<00:16,  3.57it/s] 86%|████████▌ | 353/410 [02:40<00:15,  3.58it/s] 86%|████████▋ | 354/410 [02:40<00:15,  3.59it/s] 87%|████████▋ | 355/410 [02:40<00:15,  3.59it/s] 87%|████████▋ | 356/410 [02:40<00:15,  3.60it/s] 87%|████████▋ | 357/410 [02:41<00:14,  3.60it/s] 87%|████████▋ | 358/410 [02:41<00:14,  3.60it/s] 88%|████████▊ | 359/410 [02:41<00:14,  3.50it/s] 88%|████████▊ | 360/410 [02:42<00:14,  3.53it/s] 88%|████████▊ | 361/410 [02:42<00:13,  3.56it/s] 88%|████████▊ | 362/410 [02:42<00:13,  3.58it/s] 89%|████████▊ | 363/410 [02:42<00:13,  3.59it/s] 89%|████████▉ | 364/410 [02:43<00:12,  3.60it/s] 89%|████████▉ | 365/410 [02:43<00:12,  3.60it/s] 89%|████████▉ | 366/410 [02:43<00:12,  3.61it/s] 90%|████████▉ | 367/410 [02:44<00:11,  3.61it/s] 90%|████████▉ | 368/410 [02:44<00:12,  3.38it/s] 90%|█████████ | 369/410 [02:44<00:11,  3.43it/s] 90%|█████████ | 370/410 [02:44<00:12,  3.28it/s] 90%|█████████ | 371/410 [02:45<00:11,  3.37it/s] 91%|█████████ | 372/410 [02:45<00:11,  3.44it/s] 91%|█████████ | 373/410 [02:45<00:10,  3.49it/s] 91%|█████████ | 374/410 [02:46<00:10,  3.53it/s] 91%|█████████▏| 375/410 [02:46<00:11,  2.93it/s] 92%|█████████▏| 376/410 [02:46<00:10,  3.11it/s] 92%|█████████▏| 377/410 [02:47<00:13,  2.43it/s] 92%|█████████▏| 378/410 [02:47<00:11,  2.69it/s] 92%|█████████▏| 379/410 [02:48<00:11,  2.76it/s] 93%|█████████▎| 380/410 [02:48<00:10,  2.97it/s] 93%|█████████▎| 381/410 [02:48<00:09,  3.14it/s] 93%|█████████▎| 382/410 [02:48<00:08,  3.26it/s] 93%|█████████▎| 383/410 [02:49<00:08,  3.36it/s] 94%|█████████▎| 384/410 [02:49<00:07,  3.43it/s] 94%|█████████▍| 385/410 [02:49<00:07,  3.48it/s] 94%|█████████▍| 386/410 [02:50<00:06,  3.52it/s] 94%|█████████▍| 387/410 [02:50<00:06,  3.55it/s] 95%|█████████▍| 388/410 [02:50<00:06,  3.44it/s] 95%|█████████▍| 389/410 [02:50<00:06,  3.49it/s] 95%|█████████▌| 390/410 [02:51<00:05,  3.52it/s] 95%|█████████▌| 391/410 [02:51<00:05,  3.55it/s] 96%|█████████▌| 392/410 [02:51<00:05,  3.57it/s] 96%|█████████▌| 393/410 [02:51<00:04,  3.58it/s] 96%|█████████▌| 394/410 [02:52<00:04,  3.59it/s] 96%|█████████▋| 395/410 [02:52<00:04,  3.59it/s] 97%|█████████▋| 396/410 [02:52<00:03,  3.60it/s] 97%|█████████▋| 397/410 [02:53<00:03,  3.60it/s] 97%|█████████▋| 398/410 [02:53<00:03,  3.60it/s] 97%|█████████▋| 399/410 [02:53<00:03,  3.52it/s] 98%|█████████▊| 400/410 [02:53<00:02,  3.54it/s] 98%|█████████▊| 401/410 [02:54<00:02,  3.56it/s] 98%|█████████▊| 402/410 [02:54<00:02,  3.58it/s] 98%|█████████▊| 403/410 [02:54<00:01,  3.59it/s] 99%|█████████▊| 404/410 [02:55<00:01,  3.59it/s] 99%|█████████▉| 405/410 [02:55<00:01,  3.60it/s] 99%|█████████▉| 406/410 [02:55<00:01,  3.60it/s] 99%|█████████▉| 407/410 [02:55<00:00,  3.61it/s]100%|█████████▉| 408/410 [02:56<00:00,  3.61it/s]100%|█████████▉| 409/410 [02:56<00:00,  3.61it/s]100%|██████████| 410/410 [02:56<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:54:49,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:54:49,199 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:54:49,199 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8614, 'eval_samples_per_second': 354.716, 'eval_steps_per_second': 44.416, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.81it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.80it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.12it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.32it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.79it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.07it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.29it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.31it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.59it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.68it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.87it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.88it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.81it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.51it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.21it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.10it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.29it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.45it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.69it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.78it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.73it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.65it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.41it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.18it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.70it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.07it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.52it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.63it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.64it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.53it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.39it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.15it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.14it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.32it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.59it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.68it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.79it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.70it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.57it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.32it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.26it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.27it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.42it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.51it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.67it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.74it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.71it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.59it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.44it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.19it/s][A
 61%|██████    | 267/438 [00:05<00:03, 43.31it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.80it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.08it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.36it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.45it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.45it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.33it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.29it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.06it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.20it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.37it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.62it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.70it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.72it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.63it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.35it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.22it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.16it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.26it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.44it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.55it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.71it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.77it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.59it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.41it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.30it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.21it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.39it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.92it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.15it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.38it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.49it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.43it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.26it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.27it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.27it/s][A100%|██████████| 410/410 [03:06<00:00,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:54:59,136 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 02:54:59,304 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:55:02,313 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:55:02,429 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:55:02,472 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:55:03,399 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:55:03,399 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82 (score: 1.0478343963623047).
                                                 100%|██████████| 410/410 [03:18<00:00,  3.69it/s]100%|██████████| 410/410 [03:18<00:00,  2.06it/s]
[INFO|trainer.py:1894] 2023-08-28 02:55:11,314 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 02:55:11,486 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:55:14,529 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:55:14,617 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:55:14,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:55:15,106 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   train_runtime            = 0:03:18.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   train_samples_per_second =    131.809
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:15,106 >>   train_steps_per_second   =      2.063
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8545, 'eval_samples_per_second': 354.966, 'eval_steps_per_second': 44.447, 'epoch': 5.0}
{'train_runtime': 198.7725, 'train_samples_per_second': 131.809, 'train_steps_per_second': 2.063, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:55:15 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:55:15,256 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:55:15,256 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 02:55:15,257 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.47it/s]  3%|▎         | 12/438 [00:00<00:08, 48.61it/s]  4%|▍         | 17/438 [00:00<00:08, 47.47it/s]  5%|▌         | 22/438 [00:00<00:08, 46.49it/s]  6%|▌         | 27/438 [00:00<00:08, 46.07it/s]  7%|▋         | 32/438 [00:00<00:08, 45.61it/s]  8%|▊         | 37/438 [00:00<00:08, 45.62it/s] 10%|▉         | 42/438 [00:00<00:08, 45.21it/s] 11%|█         | 47/438 [00:01<00:08, 44.69it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.31it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.35it/s] 14%|█▍        | 62/438 [00:01<00:08, 42.53it/s] 15%|█▌        | 67/438 [00:01<00:08, 43.19it/s] 16%|█▋        | 72/438 [00:01<00:08, 43.70it/s] 18%|█▊        | 77/438 [00:01<00:08, 44.15it/s] 19%|█▊        | 82/438 [00:01<00:08, 44.39it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.51it/s] 21%|██        | 92/438 [00:02<00:07, 44.45it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.41it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.20it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.21it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.51it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.64it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.79it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.94it/s] 30%|███       | 132/438 [00:02<00:06, 44.98it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.46it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.38it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.43it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.44it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.59it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.68it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.82it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.92it/s] 40%|████      | 177/438 [00:03<00:05, 44.97it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.83it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.71it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.60it/s] 45%|████▍     | 197/438 [00:04<00:05, 42.77it/s] 46%|████▌     | 202/438 [00:04<00:05, 43.52it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.02it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.31it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.46it/s] 51%|█████     | 222/438 [00:04<00:04, 44.59it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.60it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.45it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.27it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.26it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.45it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.67it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.78it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.86it/s] 61%|██████    | 267/438 [00:05<00:03, 44.91it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.74it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.59it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.47it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.45it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.59it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.72it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.84it/s] 70%|███████   | 307/438 [00:06<00:02, 44.84it/s] 71%|███████   | 312/438 [00:06<00:02, 44.85it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.73it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.53it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.57it/s] 76%|███████▌  | 332/438 [00:07<00:02, 43.00it/s] 77%|███████▋  | 337/438 [00:07<00:02, 43.62it/s] 78%|███████▊  | 342/438 [00:07<00:02, 43.99it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.34it/s] 80%|████████  | 352/438 [00:07<00:01, 44.60it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.68it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.73it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.58it/s] 85%|████████▍ | 372/438 [00:08<00:01, 44.33it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.32it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.55it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.65it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.73it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.78it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.86it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.71it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.53it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.42it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.39it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.60it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.73it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.79it/s]100%|██████████| 438/438 [00:09<00:00, 44.60it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:55:25,100 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   eval_loss               =     1.0478
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   eval_samples_per_second =    355.362
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   eval_steps_per_second   =     44.496
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:55:25,100 >>   perplexity              =     2.8515
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:34,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:34,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:34,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:34,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:34,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:55:35,253 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:55:35,254 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:55:36,003 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:55:37,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:55:37,270 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:39,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:39,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:39,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:39,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:55:39,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:55:40,870 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:55:40,908 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:55:41,576 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:55:41,880 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:55:41,880 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.45it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:07,  1.49it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:09,  1.45it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:20,  1.41it/s]Extractor Predicting: 30it [00:21,  1.36it/s]Extractor Predicting: 31it [00:21,  1.38it/s]Extractor Predicting: 32it [00:22,  1.31it/s]Extractor Predicting: 33it [00:23,  1.33it/s]Extractor Predicting: 34it [00:24,  1.32it/s]Extractor Predicting: 35it [00:24,  1.33it/s]Extractor Predicting: 36it [00:25,  1.27it/s]Extractor Predicting: 37it [00:26,  1.26it/s]Extractor Predicting: 38it [00:27,  1.26it/s]Extractor Predicting: 39it [00:28,  1.28it/s]Extractor Predicting: 40it [00:28,  1.28it/s]Extractor Predicting: 41it [00:29,  1.28it/s]Extractor Predicting: 42it [00:30,  1.27it/s]Extractor Predicting: 43it [00:31,  1.27it/s]Extractor Predicting: 44it [00:31,  1.26it/s]Extractor Predicting: 45it [00:32,  1.27it/s]Extractor Predicting: 46it [00:33,  1.26it/s]Extractor Predicting: 47it [00:34,  1.28it/s]Extractor Predicting: 48it [00:35,  1.29it/s]Extractor Predicting: 49it [00:35,  1.28it/s]Extractor Predicting: 50it [00:36,  1.28it/s]Extractor Predicting: 51it [00:37,  1.27it/s]Extractor Predicting: 52it [00:38,  1.26it/s]Extractor Predicting: 53it [00:39,  1.26it/s]Extractor Predicting: 54it [00:39,  1.30it/s]Extractor Predicting: 55it [00:40,  1.31it/s]Extractor Predicting: 56it [00:41,  1.31it/s]Extractor Predicting: 57it [00:42,  1.30it/s]Extractor Predicting: 58it [00:42,  1.29it/s]Extractor Predicting: 59it [00:43,  1.27it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.34it/s]Extractor Predicting: 62it [00:45,  1.35it/s]Extractor Predicting: 63it [00:46,  1.34it/s]Extractor Predicting: 64it [00:47,  1.37it/s]Extractor Predicting: 65it [00:47,  1.38it/s]Extractor Predicting: 66it [00:48,  1.36it/s]Extractor Predicting: 67it [00:49,  1.37it/s]Extractor Predicting: 68it [00:50,  1.35it/s]Extractor Predicting: 69it [00:50,  1.34it/s]Extractor Predicting: 70it [00:51,  1.35it/s]Extractor Predicting: 71it [00:52,  1.37it/s]Extractor Predicting: 72it [00:53,  1.39it/s]Extractor Predicting: 73it [00:53,  1.30it/s]Extractor Predicting: 74it [00:54,  1.33it/s]Extractor Predicting: 75it [00:55,  1.37it/s]Extractor Predicting: 76it [00:56,  1.38it/s]Extractor Predicting: 77it [00:56,  1.39it/s]Extractor Predicting: 78it [00:57,  1.41it/s]Extractor Predicting: 79it [00:58,  1.38it/s]Extractor Predicting: 80it [00:58,  1.38it/s]Extractor Predicting: 81it [00:59,  1.37it/s]Extractor Predicting: 82it [01:00,  1.38it/s]Extractor Predicting: 83it [01:01,  1.39it/s]Extractor Predicting: 84it [01:01,  1.38it/s]Extractor Predicting: 85it [01:02,  1.37it/s]Extractor Predicting: 86it [01:03,  1.35it/s]Extractor Predicting: 87it [01:04,  1.36it/s]Extractor Predicting: 88it [01:04,  1.40it/s]Extractor Predicting: 89it [01:05,  1.45it/s]Extractor Predicting: 90it [01:06,  1.43it/s]Extractor Predicting: 91it [01:06,  1.47it/s]Extractor Predicting: 92it [01:07,  1.50it/s]Extractor Predicting: 93it [01:08,  1.51it/s]Extractor Predicting: 94it [01:08,  1.54it/s]Extractor Predicting: 95it [01:09,  1.51it/s]Extractor Predicting: 96it [01:10,  1.47it/s]Extractor Predicting: 97it [01:10,  1.48it/s]Extractor Predicting: 98it [01:11,  1.50it/s]Extractor Predicting: 99it [01:12,  1.50it/s]Extractor Predicting: 100it [01:12,  1.46it/s]Extractor Predicting: 101it [01:13,  1.46it/s]Extractor Predicting: 102it [01:14,  1.46it/s]Extractor Predicting: 103it [01:14,  1.48it/s]Extractor Predicting: 104it [01:15,  1.49it/s]Extractor Predicting: 105it [01:16,  1.45it/s]Extractor Predicting: 106it [01:16,  1.48it/s]Extractor Predicting: 107it [01:17,  1.45it/s]Extractor Predicting: 108it [01:18,  1.48it/s]Extractor Predicting: 109it [01:18,  1.50it/s]Extractor Predicting: 110it [01:19,  1.51it/s]Extractor Predicting: 111it [01:20,  1.50it/s]Extractor Predicting: 112it [01:20,  1.53it/s]Extractor Predicting: 113it [01:21,  1.51it/s]Extractor Predicting: 114it [01:22,  1.48it/s]Extractor Predicting: 115it [01:22,  1.45it/s]Extractor Predicting: 116it [01:23,  1.43it/s]Extractor Predicting: 117it [01:24,  1.40it/s]Extractor Predicting: 118it [01:25,  1.39it/s]Extractor Predicting: 119it [01:25,  1.37it/s]Extractor Predicting: 120it [01:26,  1.33it/s]Extractor Predicting: 121it [01:27,  1.33it/s]Extractor Predicting: 122it [01:28,  1.35it/s]Extractor Predicting: 123it [01:28,  1.33it/s]Extractor Predicting: 124it [01:29,  1.30it/s]Extractor Predicting: 125it [01:30,  1.31it/s]Extractor Predicting: 126it [01:31,  1.31it/s]Extractor Predicting: 127it [01:31,  1.30it/s]Extractor Predicting: 128it [01:32,  1.32it/s]Extractor Predicting: 129it [01:33,  1.31it/s]Extractor Predicting: 130it [01:34,  1.34it/s]Extractor Predicting: 131it [01:34,  1.34it/s]Extractor Predicting: 132it [01:35,  1.34it/s]Extractor Predicting: 133it [01:36,  1.34it/s]Extractor Predicting: 134it [01:37,  1.34it/s]Extractor Predicting: 135it [01:37,  1.34it/s]Extractor Predicting: 136it [01:38,  1.32it/s]Extractor Predicting: 137it [01:39,  1.32it/s]Extractor Predicting: 138it [01:40,  1.32it/s]Extractor Predicting: 139it [01:40,  1.34it/s]Extractor Predicting: 140it [01:41,  1.37it/s]Extractor Predicting: 141it [01:42,  1.34it/s]Extractor Predicting: 142it [01:43,  1.34it/s]Extractor Predicting: 143it [01:43,  1.34it/s]Extractor Predicting: 144it [01:44,  1.32it/s]Extractor Predicting: 145it [01:44,  1.77it/s]Extractor Predicting: 145it [01:44,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:39,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:39,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:39,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:39,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:39,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:57:41,101 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:57:41,102 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:57:41,818 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:57:42,963 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:57:43,016 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:46,143 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:46,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:46,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:46,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:46,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:57:47,079 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:57:47,165 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:57:47,798 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:57:48,017 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:57:48,017 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.36it/s]Extractor Predicting: 9it [00:06,  1.35it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:08,  1.37it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.37it/s]Extractor Predicting: 18it [00:13,  1.38it/s]Extractor Predicting: 19it [00:13,  1.36it/s]Extractor Predicting: 20it [00:14,  1.37it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:16,  1.32it/s]Extractor Predicting: 23it [00:16,  1.35it/s]Extractor Predicting: 24it [00:17,  1.38it/s]Extractor Predicting: 25it [00:18,  1.39it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.35it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:22,  1.36it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.42it/s]Extractor Predicting: 34it [00:24,  1.44it/s]Extractor Predicting: 35it [00:25,  1.44it/s]Extractor Predicting: 36it [00:26,  1.39it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:28,  1.41it/s]Extractor Predicting: 40it [00:28,  1.42it/s]Extractor Predicting: 41it [00:29,  1.42it/s]Extractor Predicting: 42it [00:30,  1.44it/s]Extractor Predicting: 43it [00:31,  1.42it/s]Extractor Predicting: 44it [00:31,  1.47it/s]Extractor Predicting: 45it [00:32,  1.46it/s]Extractor Predicting: 46it [00:32,  1.52it/s]Extractor Predicting: 47it [00:33,  1.48it/s]Extractor Predicting: 48it [00:34,  1.41it/s]Extractor Predicting: 49it [00:35,  1.41it/s]Extractor Predicting: 50it [00:35,  1.38it/s]Extractor Predicting: 51it [00:36,  1.43it/s]Extractor Predicting: 52it [00:37,  1.42it/s]Extractor Predicting: 53it [00:38,  1.42it/s]Extractor Predicting: 54it [00:38,  1.46it/s]Extractor Predicting: 55it [00:39,  1.44it/s]Extractor Predicting: 56it [00:40,  1.42it/s]Extractor Predicting: 57it [00:40,  1.42it/s]Extractor Predicting: 58it [00:41,  1.40it/s]Extractor Predicting: 59it [00:42,  1.41it/s]Extractor Predicting: 60it [00:42,  1.41it/s]Extractor Predicting: 61it [00:43,  1.41it/s]Extractor Predicting: 62it [00:44,  1.42it/s]Extractor Predicting: 63it [00:45,  1.37it/s]Extractor Predicting: 64it [00:45,  1.35it/s]Extractor Predicting: 65it [00:46,  1.39it/s]Extractor Predicting: 66it [00:47,  1.40it/s]Extractor Predicting: 67it [00:47,  1.44it/s]Extractor Predicting: 68it [00:48,  1.43it/s]Extractor Predicting: 69it [00:49,  1.31it/s]Extractor Predicting: 70it [00:50,  1.34it/s]Extractor Predicting: 71it [00:50,  1.35it/s]Extractor Predicting: 72it [00:51,  1.32it/s]Extractor Predicting: 73it [00:52,  1.33it/s]Extractor Predicting: 74it [00:53,  1.37it/s]Extractor Predicting: 75it [00:53,  1.37it/s]Extractor Predicting: 76it [00:54,  1.35it/s]Extractor Predicting: 77it [00:55,  1.32it/s]Extractor Predicting: 78it [00:56,  1.34it/s]Extractor Predicting: 79it [00:56,  1.37it/s]Extractor Predicting: 80it [00:57,  1.41it/s]Extractor Predicting: 81it [00:58,  1.35it/s]Extractor Predicting: 82it [00:59,  1.35it/s]Extractor Predicting: 83it [00:59,  1.38it/s]Extractor Predicting: 84it [01:00,  1.40it/s]Extractor Predicting: 85it [01:01,  1.41it/s]Extractor Predicting: 86it [01:01,  1.39it/s]Extractor Predicting: 87it [01:02,  1.40it/s]Extractor Predicting: 88it [01:03,  1.45it/s]Extractor Predicting: 89it [01:03,  1.45it/s]Extractor Predicting: 90it [01:04,  1.47it/s]Extractor Predicting: 91it [01:05,  1.42it/s]Extractor Predicting: 92it [01:06,  1.41it/s]Extractor Predicting: 93it [01:06,  1.42it/s]Extractor Predicting: 94it [01:07,  1.40it/s]Extractor Predicting: 95it [01:08,  1.41it/s]Extractor Predicting: 96it [01:08,  1.38it/s]Extractor Predicting: 97it [01:09,  1.38it/s]Extractor Predicting: 98it [01:10,  1.37it/s]Extractor Predicting: 99it [01:11,  1.38it/s]Extractor Predicting: 100it [01:11,  1.40it/s]Extractor Predicting: 101it [01:12,  1.38it/s]Extractor Predicting: 102it [01:13,  1.38it/s]Extractor Predicting: 103it [01:14,  1.39it/s]Extractor Predicting: 104it [01:14,  1.40it/s]Extractor Predicting: 105it [01:15,  1.40it/s]Extractor Predicting: 106it [01:16,  1.39it/s]Extractor Predicting: 107it [01:16,  1.39it/s]Extractor Predicting: 108it [01:17,  1.41it/s]Extractor Predicting: 109it [01:18,  1.42it/s]Extractor Predicting: 110it [01:19,  1.40it/s]Extractor Predicting: 111it [01:19,  1.41it/s]Extractor Predicting: 112it [01:20,  1.42it/s]Extractor Predicting: 113it [01:21,  1.41it/s]Extractor Predicting: 114it [01:21,  1.39it/s]Extractor Predicting: 115it [01:22,  1.41it/s]Extractor Predicting: 116it [01:23,  1.40it/s]Extractor Predicting: 117it [01:23,  1.42it/s]Extractor Predicting: 118it [01:24,  1.43it/s]Extractor Predicting: 119it [01:25,  1.42it/s]Extractor Predicting: 120it [01:26,  1.41it/s]Extractor Predicting: 121it [01:26,  1.41it/s]Extractor Predicting: 122it [01:27,  1.42it/s]Extractor Predicting: 123it [01:28,  1.44it/s]Extractor Predicting: 124it [01:28,  1.43it/s]Extractor Predicting: 125it [01:29,  1.44it/s]Extractor Predicting: 126it [01:30,  1.41it/s]Extractor Predicting: 127it [01:31,  1.42it/s]Extractor Predicting: 128it [01:31,  1.45it/s]Extractor Predicting: 129it [01:32,  1.44it/s]Extractor Predicting: 130it [01:33,  1.29it/s]Extractor Predicting: 131it [01:33,  1.36it/s]Extractor Predicting: 132it [01:34,  1.37it/s]Extractor Predicting: 133it [01:35,  1.39it/s]Extractor Predicting: 134it [01:36,  1.37it/s]Extractor Predicting: 135it [01:36,  1.40it/s]Extractor Predicting: 136it [01:37,  1.42it/s]Extractor Predicting: 137it [01:38,  1.32it/s]Extractor Predicting: 138it [01:39,  1.37it/s]Extractor Predicting: 139it [01:39,  1.32it/s]Extractor Predicting: 140it [01:40,  1.38it/s]Extractor Predicting: 140it [01:40,  1.39it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:37,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:37,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:37,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:37,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:37,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:59:38,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:59:38,397 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:59:39,022 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:59:40,158 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:59:40,159 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:43,415 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:43,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:43,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:43,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:59:43,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:59:44,545 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:59:44,546 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:59:45,188 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:59:45,462 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:59:45,462 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 3it [00:02,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:59:50,143 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:59:50,144 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:59:50,238 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:59:50,239 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:59:50,275 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:00:00,228 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:00:00,251 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:00:00,320 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:00:00,321 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:00:00,368 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:00:00,400 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:00:00,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:01,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:02,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:03,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:04,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:05,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:06,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:06,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:08,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:08,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:09,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:10,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:11,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:12,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:13,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:14,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:15,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:16,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:17,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:18,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:19,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:19,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:59, 19.99s/it][WARNING|generation_utils.py:914] 2023-08-28 03:00:20,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:21,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:22,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:23,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:24,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:25,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:25,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:26,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:27,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:28,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:29,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:30,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:31,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:32,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:33,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:34,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:35,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:36,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:37,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:38,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:39,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:41,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:41,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:42<02:50, 21.27s/it][WARNING|generation_utils.py:914] 2023-08-28 03:00:42,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:43,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:44,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:45,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:46,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:47,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:47,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:48,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:49,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:49,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:50,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:51,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:52,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:52,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:53,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:54,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:54,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:55,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:56,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:57,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:57,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:58,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:59,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:00,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:01,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:01,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:02,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:03,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:03,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:04,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:05,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:05,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:06,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:08,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:08,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:09,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:10,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:10,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:11,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:12,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:13,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:13,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:14,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:15,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:16,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:17,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<03:13, 27.62s/it][WARNING|generation_utils.py:914] 2023-08-28 03:01:18,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:18,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:19,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:20,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:21,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:22,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:23,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:24,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:25,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:25,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:26,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:27,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:27,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:28,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:29,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:30,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:31,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:31,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:32,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:33,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:35,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:35,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:36,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:37,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:28, 24.82s/it][WARNING|generation_utils.py:914] 2023-08-28 03:01:38,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:39,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:40,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:40,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:41,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:42,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:43,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:44,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:44,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:45,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:46,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:46,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:47,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:48,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:49,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:49,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:50,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:51,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:52,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:52,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:54,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:54,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:55,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:56,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:57,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:58,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:58,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:59,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:59<01:58, 23.74s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:00,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:01,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:02,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:03,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:04,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:05,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:06,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:07,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:08,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:09,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:09,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:10,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:11,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:12,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:12,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:13,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:14,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:15,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:15,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:16,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:17,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:18,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:19,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:19,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:20,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:20<01:31, 22.79s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:21,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:22,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:22,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:23,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:24,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:25,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:26,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:26,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:27,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:28,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:29,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:30,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:31,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:31,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:32,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:33,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:34,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:35,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:35,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:36,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:37,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:38,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:39,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:39,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:40,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:41,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:41<01:06, 22.15s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:42,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:43,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:43,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:44,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:45,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:45,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:46,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:47,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:48,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:48,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:49,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:50,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:50,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:51,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:52,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:53,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:53,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:54,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:55,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:55,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:56,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:57,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:57<00:40, 20.20s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:58,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:58,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:59,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:00,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:01,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:02,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:03,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:03,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:04,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:05,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:06,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:06,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:07,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:08,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:09,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:10,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:11,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:11,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:12,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:13,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:14,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:14,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:15,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:15<00:19, 19.55s/it][WARNING|generation_utils.py:914] 2023-08-28 03:03:16,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:17,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:18,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:18,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:19,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:20,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:21,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:22,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:23,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:23,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:25,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:25,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:26,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:27,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:28,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:29,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:29,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:30,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:31,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:32,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:33,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:34,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:35,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:36,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:36,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:37,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:38,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:39,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:40,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:40<00:00, 21.23s/it]Generating: 100%|██████████| 10/10 [03:40<00:00, 22.06s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:48,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:48,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:48,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:48,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:48,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:03:50,018 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:03:50,020 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:03:50,652 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:03:51,838 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:03:51,839 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:54,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:55,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:55,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:55,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:03:55,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:03:55,947 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:03:55,948 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:03:56,624 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:03:56,933 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:03:56,934 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.01s/it]Extractor Estimating: 2it [00:01,  1.11it/s]Extractor Estimating: 3it [00:02,  1.21it/s]Extractor Estimating: 4it [00:03,  1.08it/s]Extractor Estimating: 5it [00:04,  1.19it/s]Extractor Estimating: 6it [00:05,  1.18it/s]Extractor Estimating: 7it [00:05,  1.21it/s]Extractor Estimating: 8it [00:06,  1.23it/s]Extractor Estimating: 9it [00:07,  1.22it/s]Extractor Estimating: 10it [00:08,  1.21it/s]Extractor Estimating: 11it [00:09,  1.21it/s]Extractor Estimating: 12it [00:10,  1.17it/s]Extractor Estimating: 13it [00:10,  1.20it/s]Extractor Estimating: 14it [00:11,  1.25it/s]Extractor Estimating: 15it [00:12,  1.23it/s]Extractor Estimating: 16it [00:13,  1.24it/s]Extractor Estimating: 17it [00:14,  1.21it/s]Extractor Estimating: 18it [00:15,  1.21it/s]Extractor Estimating: 19it [00:15,  1.20it/s]Extractor Estimating: 20it [00:16,  1.18it/s]Extractor Estimating: 21it [00:17,  1.12it/s]Extractor Estimating: 22it [00:18,  1.12it/s]Extractor Estimating: 23it [00:19,  1.19it/s]Extractor Estimating: 24it [00:20,  1.23it/s]Extractor Estimating: 25it [00:20,  1.20it/s]Extractor Estimating: 26it [00:21,  1.16it/s]Extractor Estimating: 27it [00:22,  1.19it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.16it/s]Extractor Estimating: 30it [00:25,  1.13it/s]Extractor Estimating: 31it [00:26,  1.16it/s]Extractor Estimating: 32it [00:27,  1.16it/s]Extractor Estimating: 33it [00:27,  1.19it/s]Extractor Estimating: 34it [00:28,  1.20it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:30,  1.13it/s]Extractor Estimating: 37it [00:31,  1.18it/s]Extractor Estimating: 38it [00:32,  1.15it/s]Extractor Estimating: 39it [00:33,  1.13it/s]Extractor Estimating: 40it [00:34,  1.12it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:35,  1.15it/s]Extractor Estimating: 43it [00:36,  1.16it/s]Extractor Estimating: 44it [00:37,  1.10it/s]Extractor Estimating: 45it [00:38,  1.15it/s]Extractor Estimating: 46it [00:39,  1.16it/s]Extractor Estimating: 47it [00:40,  1.17it/s]Extractor Estimating: 48it [00:40,  1.14it/s]Extractor Estimating: 49it [00:41,  1.17it/s]Extractor Estimating: 50it [00:42,  1.13it/s]Extractor Estimating: 51it [00:43,  1.15it/s]Extractor Estimating: 52it [00:44,  1.17it/s]Extractor Estimating: 53it [00:45,  1.19it/s]Extractor Estimating: 54it [00:45,  1.21it/s]Extractor Estimating: 55it [00:46,  1.23it/s]Extractor Estimating: 56it [00:47,  1.22it/s]Extractor Estimating: 57it [00:48,  1.22it/s]Extractor Estimating: 58it [00:49,  1.25it/s]Extractor Estimating: 59it [00:49,  1.30it/s]Extractor Estimating: 60it [00:50,  1.27it/s]Extractor Estimating: 61it [00:51,  1.26it/s]Extractor Estimating: 62it [00:52,  1.25it/s]Extractor Estimating: 63it [00:53,  1.27it/s]Extractor Estimating: 64it [00:53,  1.26it/s]Extractor Estimating: 65it [00:54,  1.31it/s]Extractor Estimating: 66it [00:55,  1.30it/s]Extractor Estimating: 67it [00:56,  1.31it/s]Extractor Estimating: 68it [00:56,  1.25it/s]Extractor Estimating: 69it [00:57,  1.31it/s]Extractor Estimating: 70it [00:58,  1.26it/s]Extractor Estimating: 71it [00:59,  1.29it/s]Extractor Estimating: 72it [00:59,  1.33it/s]Extractor Estimating: 73it [01:00,  1.25it/s]Extractor Estimating: 74it [01:01,  1.28it/s]Extractor Estimating: 75it [01:02,  1.24it/s]Extractor Estimating: 76it [01:03,  1.29it/s]Extractor Estimating: 77it [01:03,  1.30it/s]Extractor Estimating: 78it [01:04,  1.25it/s]Extractor Estimating: 79it [01:05,  1.26it/s]Extractor Estimating: 80it [01:06,  1.31it/s]Extractor Estimating: 81it [01:07,  1.29it/s]Extractor Estimating: 82it [01:07,  1.31it/s]Extractor Estimating: 83it [01:08,  1.28it/s]Extractor Estimating: 84it [01:09,  1.33it/s]Extractor Estimating: 85it [01:10,  1.34it/s]Extractor Estimating: 86it [01:10,  1.36it/s]Extractor Estimating: 87it [01:11,  1.35it/s]Extractor Estimating: 88it [01:12,  1.38it/s]Extractor Estimating: 89it [01:12,  1.33it/s]Extractor Estimating: 90it [01:13,  1.34it/s]Extractor Estimating: 91it [01:14,  1.37it/s]Extractor Estimating: 92it [01:15,  1.32it/s]Extractor Estimating: 93it [01:15,  1.32it/s]Extractor Estimating: 94it [01:16,  1.33it/s]Extractor Estimating: 95it [01:17,  1.31it/s]Extractor Estimating: 96it [01:18,  1.33it/s]Extractor Estimating: 97it [01:18,  1.34it/s]Extractor Estimating: 98it [01:19,  1.32it/s]Extractor Estimating: 99it [01:20,  1.31it/s]Extractor Estimating: 100it [01:21,  1.29it/s]Extractor Estimating: 101it [01:22,  1.33it/s]Extractor Estimating: 102it [01:22,  1.36it/s]Extractor Estimating: 103it [01:23,  1.33it/s]Extractor Estimating: 104it [01:24,  1.22it/s]Extractor Estimating: 105it [01:25,  1.19it/s]Extractor Estimating: 106it [01:26,  1.25it/s]Extractor Estimating: 107it [01:26,  1.31it/s]Extractor Estimating: 108it [01:27,  1.35it/s]Extractor Estimating: 109it [01:28,  1.37it/s]Extractor Estimating: 110it [01:28,  1.41it/s]Extractor Estimating: 111it [01:29,  1.40it/s]Extractor Estimating: 112it [01:30,  1.39it/s]Extractor Estimating: 113it [01:31,  1.38it/s]Extractor Estimating: 114it [01:31,  1.41it/s]Extractor Estimating: 115it [01:32,  1.34it/s]Extractor Estimating: 116it [01:33,  1.37it/s]Extractor Estimating: 117it [01:33,  1.37it/s]Extractor Estimating: 118it [01:34,  1.42it/s]Extractor Estimating: 119it [01:35,  1.42it/s]Extractor Estimating: 120it [01:36,  1.39it/s]Extractor Estimating: 121it [01:36,  1.38it/s]Extractor Estimating: 122it [01:37,  1.32it/s]Extractor Estimating: 123it [01:38,  1.31it/s]Extractor Estimating: 124it [01:39,  1.31it/s]Extractor Estimating: 125it [01:39,  1.35it/s]Extractor Estimating: 126it [01:40,  1.36it/s]Extractor Estimating: 127it [01:41,  1.34it/s]Extractor Estimating: 128it [01:42,  1.32it/s]Extractor Estimating: 129it [01:42,  1.32it/s]Extractor Estimating: 130it [01:43,  1.31it/s]Extractor Estimating: 131it [01:44,  1.31it/s]Extractor Estimating: 132it [01:45,  1.36it/s]Extractor Estimating: 133it [01:45,  1.35it/s]Extractor Estimating: 134it [01:46,  1.37it/s]Extractor Estimating: 135it [01:47,  1.37it/s]Extractor Estimating: 136it [01:48,  1.32it/s]Extractor Estimating: 137it [01:48,  1.35it/s]Extractor Estimating: 138it [01:49,  1.38it/s]Extractor Estimating: 139it [01:50,  1.37it/s]Extractor Estimating: 140it [01:51,  1.35it/s]Extractor Estimating: 141it [01:51,  1.34it/s]Extractor Estimating: 142it [01:52,  1.32it/s]Extractor Estimating: 143it [01:53,  1.31it/s]Extractor Estimating: 144it [01:54,  1.26it/s]Extractor Estimating: 145it [01:55,  1.24it/s]Extractor Estimating: 146it [01:55,  1.23it/s]Extractor Estimating: 147it [01:56,  1.28it/s]Extractor Estimating: 148it [01:57,  1.31it/s]Extractor Estimating: 149it [01:58,  1.30it/s]Extractor Estimating: 150it [01:58,  1.25it/s]Extractor Estimating: 151it [01:59,  1.23it/s]Extractor Estimating: 152it [02:00,  1.24it/s]Extractor Estimating: 153it [02:01,  1.28it/s]Extractor Estimating: 154it [02:02,  1.25it/s]Extractor Estimating: 155it [02:02,  1.26it/s]Extractor Estimating: 156it [02:03,  1.20it/s]Extractor Estimating: 157it [02:04,  1.23it/s]Extractor Estimating: 158it [02:05,  1.19it/s]Extractor Estimating: 159it [02:06,  1.14it/s]Extractor Estimating: 160it [02:07,  1.19it/s]Extractor Estimating: 161it [02:07,  1.24it/s]Extractor Estimating: 162it [02:08,  1.22it/s]Extractor Estimating: 163it [02:09,  1.25it/s]Extractor Estimating: 164it [02:10,  1.25it/s]Extractor Estimating: 165it [02:11,  1.27it/s]Extractor Estimating: 166it [02:11,  1.27it/s]Extractor Estimating: 167it [02:12,  1.26it/s]Extractor Estimating: 168it [02:13,  1.20it/s]Extractor Estimating: 169it [02:14,  1.17it/s]Extractor Estimating: 170it [02:15,  1.16it/s]Extractor Estimating: 171it [02:16,  1.18it/s]Extractor Estimating: 172it [02:16,  1.25it/s]Extractor Estimating: 173it [02:17,  1.21it/s]Extractor Estimating: 174it [02:18,  1.22it/s]Extractor Estimating: 175it [02:19,  1.20it/s]Extractor Estimating: 176it [02:20,  1.17it/s]Extractor Estimating: 177it [02:21,  1.25it/s]Extractor Estimating: 178it [02:21,  1.28it/s]Extractor Estimating: 179it [02:22,  1.32it/s]Extractor Estimating: 180it [02:23,  1.36it/s]Extractor Estimating: 181it [02:23,  1.38it/s]Extractor Estimating: 182it [02:24,  1.38it/s]Extractor Estimating: 183it [02:25,  1.39it/s]Extractor Estimating: 184it [02:26,  1.39it/s]Extractor Estimating: 185it [02:26,  1.38it/s]Extractor Estimating: 186it [02:27,  1.36it/s]Extractor Estimating: 187it [02:28,  1.36it/s]Extractor Estimating: 188it [02:28,  1.37it/s]Extractor Estimating: 189it [02:29,  1.36it/s]Extractor Estimating: 190it [02:30,  1.37it/s]Extractor Estimating: 191it [02:31,  1.38it/s]Extractor Estimating: 192it [02:31,  1.34it/s]Extractor Estimating: 193it [02:32,  1.25it/s]Extractor Estimating: 194it [02:33,  1.28it/s]Extractor Estimating: 195it [02:34,  1.30it/s]Extractor Estimating: 196it [02:35,  1.30it/s]Extractor Estimating: 197it [02:36,  1.24it/s]Extractor Estimating: 198it [02:36,  1.31it/s]Extractor Estimating: 199it [02:37,  1.36it/s]Extractor Estimating: 200it [02:38,  1.34it/s]Extractor Estimating: 201it [02:38,  1.32it/s]Extractor Estimating: 202it [02:39,  1.27it/s]Extractor Estimating: 203it [02:40,  1.31it/s]Extractor Estimating: 204it [02:41,  1.29it/s]Extractor Estimating: 205it [02:42,  1.30it/s]Extractor Estimating: 206it [02:42,  1.32it/s]Extractor Estimating: 207it [02:43,  1.32it/s]Extractor Estimating: 208it [02:44,  1.27it/s]Extractor Estimating: 209it [02:45,  1.25it/s]Extractor Estimating: 210it [02:46,  1.23it/s]Extractor Estimating: 211it [02:46,  1.23it/s]Extractor Estimating: 212it [02:47,  1.25it/s]Extractor Estimating: 213it [02:48,  1.21it/s]Extractor Estimating: 214it [02:49,  1.22it/s]Extractor Estimating: 215it [02:50,  1.21it/s]Extractor Estimating: 216it [02:50,  1.24it/s]Extractor Estimating: 217it [02:51,  1.24it/s]Extractor Estimating: 218it [02:52,  1.26it/s]Extractor Estimating: 219it [02:53,  1.26it/s]Extractor Estimating: 220it [02:54,  1.22it/s]Extractor Estimating: 221it [02:54,  1.23it/s]Extractor Estimating: 222it [02:55,  1.24it/s]Extractor Estimating: 223it [02:56,  1.23it/s]Extractor Estimating: 224it [02:57,  1.22it/s]Extractor Estimating: 225it [02:58,  1.21it/s]Extractor Estimating: 226it [02:58,  1.26it/s]Extractor Estimating: 227it [02:59,  1.26it/s]Extractor Estimating: 228it [03:00,  1.28it/s]Extractor Estimating: 229it [03:01,  1.25it/s]Extractor Estimating: 230it [03:02,  1.22it/s]Extractor Estimating: 231it [03:03,  1.21it/s]Extractor Estimating: 232it [03:03,  1.20it/s]Extractor Estimating: 233it [03:04,  1.24it/s]Extractor Estimating: 234it [03:05,  1.17it/s]Extractor Estimating: 235it [03:06,  1.20it/s]Extractor Estimating: 236it [03:07,  1.21it/s]Extractor Estimating: 237it [03:08,  1.23it/s]Extractor Estimating: 238it [03:08,  1.24it/s]Extractor Estimating: 239it [03:09,  1.21it/s]Extractor Estimating: 240it [03:10,  1.15it/s]Extractor Estimating: 241it [03:11,  1.14it/s]Extractor Estimating: 242it [03:12,  1.14it/s]Extractor Estimating: 243it [03:13,  1.17it/s]Extractor Estimating: 244it [03:14,  1.16it/s]Extractor Estimating: 245it [03:14,  1.22it/s]Extractor Estimating: 246it [03:15,  1.23it/s]Extractor Estimating: 247it [03:16,  1.26it/s]Extractor Estimating: 248it [03:17,  1.23it/s]Extractor Estimating: 249it [03:18,  1.20it/s]Extractor Estimating: 250it [03:18,  1.21it/s]Extractor Estimating: 250it [03:18,  1.26it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:32,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:32,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:32,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:32,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:32,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:07:33,107 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:07:33,108 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:07:33,736 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:07:34,853 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:07:34,873 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:37,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:37,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:37,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:37,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:07:37,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:07:38,641 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:07:38,642 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:07:39,239 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:07:39,450 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:07:39,450 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:00:34,748 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:00:35,263 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5299 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 20538
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20638, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20638, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.294, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.293, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.288, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.305, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.266, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.618, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.276, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.308, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.302, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.284, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.612, loss:nan
g_step 1200, step 95, avg_time 1.275, loss:nan
g_step 1300, step 195, avg_time 1.299, loss:nan
g_step 1400, step 74, avg_time 1.301, loss:nan
g_step 1500, step 174, avg_time 1.292, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.619, loss:nan
g_step 1700, step 153, avg_time 1.297, loss:nan
g_step 1800, step 32, avg_time 1.301, loss:nan
g_step 1900, step 132, avg_time 1.292, loss:nan
g_step 2000, step 11, avg_time 1.290, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.597, loss:nan
g_step 2200, step 211, avg_time 1.301, loss:nan
g_step 2300, step 90, avg_time 1.283, loss:nan
g_step 2400, step 190, avg_time 1.303, loss:nan
g_step 2500, step 69, avg_time 1.316, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.581, loss:nan
g_step 2700, step 48, avg_time 1.290, loss:nan
g_step 2800, step 148, avg_time 1.300, loss:nan
g_step 2900, step 27, avg_time 1.304, loss:nan
g_step 3000, step 127, avg_time 1.317, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.566, loss:nan
g_step 3200, step 106, avg_time 1.278, loss:nan
g_step 3300, step 206, avg_time 1.309, loss:nan
g_step 3400, step 85, avg_time 1.319, loss:nan
g_step 3500, step 185, avg_time 1.295, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.602, loss:nan
g_step 3700, step 164, avg_time 1.299, loss:nan
g_step 3800, step 43, avg_time 1.278, loss:nan
g_step 3900, step 143, avg_time 1.304, loss:nan
g_step 4000, step 22, avg_time 1.308, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.590, loss:nan
g_step 4200, step 1, avg_time 1.295, loss:nan
g_step 4300, step 101, avg_time 1.274, loss:nan
g_step 4400, step 201, avg_time 1.316, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:00:35 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:00:35 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-00-34_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:00:36 - WARNING - datasets.builder -   Using custom data configuration default-6975cb0e648d0205
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6975cb0e648d0205/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:00:40,195 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:00:40,196 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:00:40,196 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:00:40,197 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:00:40,351 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:00:40,418 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:00:40,950 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:00:44,106 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:00:44,130 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6975cb0e648d0205/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.06ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.10ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.68ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.03ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.25ba/s]100%|██████████| 6/6 [00:01<00:00,  4.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.53ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.47ba/s]100%|██████████| 4/4 [00:00<00:00,  4.85ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.84ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.10ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.19ba/s]100%|██████████| 6/6 [00:00<00:00, 10.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.17ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.29ba/s]100%|██████████| 4/4 [00:00<00:00, 10.40ba/s]
[INFO|trainer.py:414] 2023-08-28 05:00:48,122 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:00:48,172 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:00:48,172 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 05:00:48,172 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:00:48,173 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:00:48,173 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:00:48,173 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:00:48,173 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<02:00,  3.45it/s]  0%|          | 2/415 [00:00<01:56,  3.55it/s]  1%|          | 3/415 [00:00<01:54,  3.59it/s]  1%|          | 4/415 [00:01<01:53,  3.61it/s]  1%|          | 5/415 [00:01<01:54,  3.59it/s]  1%|▏         | 6/415 [00:01<01:54,  3.58it/s]  2%|▏         | 7/415 [00:01<01:54,  3.57it/s]  2%|▏         | 8/415 [00:02<01:54,  3.56it/s]  2%|▏         | 9/415 [00:02<01:57,  3.46it/s]  2%|▏         | 10/415 [00:02<01:56,  3.49it/s]  3%|▎         | 11/415 [00:03<01:55,  3.51it/s]  3%|▎         | 12/415 [00:03<01:54,  3.52it/s]  3%|▎         | 13/415 [00:03<01:53,  3.53it/s]  3%|▎         | 14/415 [00:03<01:53,  3.54it/s]  4%|▎         | 15/415 [00:04<01:52,  3.55it/s]  4%|▍         | 16/415 [00:04<01:52,  3.55it/s]  4%|▍         | 17/415 [00:04<01:52,  3.55it/s]  4%|▍         | 18/415 [00:05<01:51,  3.55it/s]  5%|▍         | 19/415 [00:05<01:51,  3.55it/s]  5%|▍         | 20/415 [00:05<01:52,  3.51it/s]  5%|▌         | 21/415 [00:05<01:51,  3.52it/s]  5%|▌         | 22/415 [00:06<01:51,  3.53it/s]  6%|▌         | 23/415 [00:06<01:50,  3.54it/s]  6%|▌         | 24/415 [00:06<01:50,  3.54it/s]  6%|▌         | 25/415 [00:07<01:50,  3.54it/s]  6%|▋         | 26/415 [00:07<01:49,  3.54it/s]  7%|▋         | 27/415 [00:07<01:49,  3.54it/s]  7%|▋         | 28/415 [00:07<01:49,  3.54it/s]  7%|▋         | 29/415 [00:08<01:48,  3.55it/s]  7%|▋         | 30/415 [00:08<01:48,  3.55it/s]  7%|▋         | 31/415 [00:08<01:49,  3.49it/s]  8%|▊         | 32/415 [00:09<01:49,  3.51it/s]  8%|▊         | 33/415 [00:09<01:48,  3.52it/s]  8%|▊         | 34/415 [00:09<01:47,  3.53it/s]  8%|▊         | 35/415 [00:09<01:47,  3.54it/s]  9%|▊         | 36/415 [00:10<01:47,  3.54it/s]  9%|▉         | 37/415 [00:10<01:46,  3.54it/s]  9%|▉         | 38/415 [00:10<01:46,  3.54it/s]  9%|▉         | 39/415 [00:11<01:46,  3.54it/s] 10%|▉         | 40/415 [00:11<01:45,  3.54it/s] 10%|▉         | 41/415 [00:11<01:45,  3.54it/s] 10%|█         | 42/415 [00:11<01:47,  3.46it/s] 10%|█         | 43/415 [00:12<01:46,  3.49it/s] 11%|█         | 44/415 [00:12<01:45,  3.51it/s] 11%|█         | 45/415 [00:12<01:45,  3.52it/s] 11%|█         | 46/415 [00:13<01:44,  3.53it/s] 11%|█▏        | 47/415 [00:13<01:44,  3.53it/s] 12%|█▏        | 48/415 [00:13<01:43,  3.55it/s] 12%|█▏        | 49/415 [00:13<01:42,  3.58it/s] 12%|█▏        | 50/415 [00:14<01:41,  3.59it/s] 12%|█▏        | 51/415 [00:14<01:41,  3.60it/s] 13%|█▎        | 52/415 [00:14<01:40,  3.61it/s] 13%|█▎        | 53/415 [00:14<01:42,  3.54it/s] 13%|█▎        | 54/415 [00:15<01:41,  3.57it/s] 13%|█▎        | 55/415 [00:15<01:40,  3.59it/s] 13%|█▎        | 56/415 [00:15<01:39,  3.60it/s] 14%|█▎        | 57/415 [00:16<01:39,  3.60it/s] 14%|█▍        | 58/415 [00:16<01:38,  3.61it/s] 14%|█▍        | 59/415 [00:16<01:38,  3.62it/s] 14%|█▍        | 60/415 [00:16<01:38,  3.62it/s] 15%|█▍        | 61/415 [00:17<01:37,  3.62it/s] 15%|█▍        | 62/415 [00:17<01:37,  3.62it/s] 15%|█▌        | 63/415 [00:17<01:37,  3.62it/s] 15%|█▌        | 64/415 [00:18<01:38,  3.57it/s] 16%|█▌        | 65/415 [00:18<01:37,  3.59it/s] 16%|█▌        | 66/415 [00:18<01:36,  3.60it/s] 16%|█▌        | 67/415 [00:18<01:36,  3.61it/s] 16%|█▋        | 68/415 [00:19<01:35,  3.62it/s] 17%|█▋        | 69/415 [00:19<01:35,  3.62it/s] 17%|█▋        | 70/415 [00:19<01:35,  3.62it/s] 17%|█▋        | 71/415 [00:19<01:34,  3.62it/s] 17%|█▋        | 72/415 [00:20<01:34,  3.63it/s] 18%|█▊        | 73/415 [00:20<01:34,  3.63it/s] 18%|█▊        | 74/415 [00:20<01:34,  3.63it/s] 18%|█▊        | 75/415 [00:21<01:33,  3.62it/s] 18%|█▊        | 76/415 [00:21<01:33,  3.63it/s] 19%|█▊        | 77/415 [00:21<01:33,  3.63it/s] 19%|█▉        | 78/415 [00:21<01:32,  3.63it/s] 19%|█▉        | 79/415 [00:22<01:32,  3.62it/s] 19%|█▉        | 80/415 [00:22<01:32,  3.62it/s] 20%|█▉        | 81/415 [00:22<01:32,  3.63it/s] 20%|█▉        | 82/415 [00:22<01:31,  3.63it/s] 20%|██        | 83/415 [00:23<01:26,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 05:01:11,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:01:11,412 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:01:11,412 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.24it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.43it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.31it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.47it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.92it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.29it/s][A
  9%|▊         | 38/438 [00:00<00:08, 44.86it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.18it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.41it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 44.74it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 44.81it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 44.88it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 44.64it/s][A
 18%|█▊        | 78/438 [00:01<00:08, 44.52it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 44.41it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.25it/s][A
 21%|██        | 93/438 [00:02<00:07, 44.43it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.52it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 44.66it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.76it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.79it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.73it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.55it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.52it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.40it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 43.58it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.06it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.32it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.43it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.55it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.50it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.46it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.20it/s][A
 41%|████      | 178/438 [00:03<00:05, 44.12it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 44.59it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.82it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.52it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.60it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 44.59it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 44.33it/s][A
 51%|█████     | 223/438 [00:04<00:04, 44.28it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 44.42it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.49it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.74it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.78it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.79it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.71it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.59it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.44it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.29it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 42.84it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 43.43it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 43.93it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.52it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.50it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.37it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.25it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.17it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.26it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 44.39it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.55it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.76it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.87it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.78it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 44.62it/s][A
 81%|████████  | 353/438 [00:07<00:01, 44.36it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.22it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.38it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.46it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.58it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.71it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.85it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.72it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.57it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 44.36it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.33it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 43.67it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.13it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.30it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.48it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.58it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.54it/s][A
100%|██████████| 438/438 [00:09<00:00, 44.45it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.45it/s][A 20%|██        | 83/415 [00:33<01:26,  3.83it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:01:21,334 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 05:01:21,437 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:01:25,075 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:01:25,197 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:01:25,249 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:38<26:34,  4.82s/it] 20%|██        | 85/415 [00:38<19:00,  3.45s/it] 21%|██        | 86/415 [00:39<13:42,  2.50s/it] 21%|██        | 87/415 [00:39<10:01,  1.83s/it] 21%|██        | 88/415 [00:39<07:26,  1.37s/it] 21%|██▏       | 89/415 [00:40<05:38,  1.04s/it] 22%|██▏       | 90/415 [00:40<04:25,  1.22it/s] 22%|██▏       | 91/415 [00:40<03:31,  1.53it/s] 22%|██▏       | 92/415 [00:40<02:54,  1.85it/s] 22%|██▏       | 93/415 [00:41<02:28,  2.17it/s] 23%|██▎       | 94/415 [00:41<02:10,  2.47it/s] 23%|██▎       | 95/415 [00:41<01:57,  2.73it/s] 23%|██▎       | 96/415 [00:41<01:48,  2.95it/s] 23%|██▎       | 97/415 [00:42<01:41,  3.13it/s] 24%|██▎       | 98/415 [00:42<01:37,  3.26it/s] 24%|██▍       | 99/415 [00:42<01:34,  3.36it/s] 24%|██▍       | 100/415 [00:43<01:31,  3.43it/s] 24%|██▍       | 101/415 [00:43<01:36,  3.24it/s] 25%|██▍       | 102/415 [00:43<01:33,  3.35it/s] 25%|██▍       | 103/415 [00:44<01:35,  3.28it/s] 25%|██▌       | 104/415 [00:44<01:32,  3.36it/s] 25%|██▌       | 105/415 [00:44<01:30,  3.44it/s] 26%|██▌       | 106/415 [00:44<01:28,  3.49it/s] 26%|██▌       | 107/415 [00:45<01:27,  3.53it/s] 26%|██▌       | 108/415 [00:45<01:26,  3.56it/s] 26%|██▋       | 109/415 [00:45<01:25,  3.57it/s] 27%|██▋       | 110/415 [00:45<01:24,  3.59it/s] 27%|██▋       | 111/415 [00:46<01:24,  3.60it/s] 27%|██▋       | 112/415 [00:46<01:29,  3.38it/s] 27%|██▋       | 113/415 [00:47<02:08,  2.35it/s] 27%|██▋       | 114/415 [00:47<01:55,  2.62it/s] 28%|██▊       | 115/415 [00:47<01:45,  2.85it/s] 28%|██▊       | 116/415 [00:48<01:38,  3.05it/s] 28%|██▊       | 117/415 [00:48<01:33,  3.20it/s] 28%|██▊       | 118/415 [00:48<01:29,  3.32it/s] 29%|██▊       | 119/415 [00:48<01:26,  3.41it/s] 29%|██▉       | 120/415 [00:49<01:25,  3.47it/s] 29%|██▉       | 121/415 [00:49<01:27,  3.37it/s] 29%|██▉       | 122/415 [00:49<01:25,  3.44it/s] 30%|██▉       | 123/415 [00:50<01:23,  3.50it/s] 30%|██▉       | 124/415 [00:50<01:22,  3.54it/s] 30%|███       | 125/415 [00:50<01:21,  3.56it/s] 30%|███       | 126/415 [00:50<01:20,  3.58it/s] 31%|███       | 127/415 [00:51<01:20,  3.59it/s] 31%|███       | 128/415 [00:51<01:19,  3.61it/s] 31%|███       | 129/415 [00:51<01:19,  3.61it/s] 31%|███▏      | 130/415 [00:52<01:18,  3.61it/s] 32%|███▏      | 131/415 [00:52<01:18,  3.62it/s] 32%|███▏      | 132/415 [00:52<01:18,  3.62it/s] 32%|███▏      | 133/415 [00:52<01:17,  3.62it/s] 32%|███▏      | 134/415 [00:53<01:17,  3.62it/s] 33%|███▎      | 135/415 [00:53<01:17,  3.62it/s] 33%|███▎      | 136/415 [00:53<01:17,  3.62it/s] 33%|███▎      | 137/415 [00:53<01:16,  3.62it/s] 33%|███▎      | 138/415 [00:54<01:22,  3.36it/s] 33%|███▎      | 139/415 [00:54<01:20,  3.44it/s] 34%|███▎      | 140/415 [00:54<01:18,  3.50it/s] 34%|███▍      | 141/415 [00:55<01:17,  3.53it/s] 34%|███▍      | 142/415 [00:55<01:16,  3.56it/s] 34%|███▍      | 143/415 [00:55<01:16,  3.58it/s] 35%|███▍      | 144/415 [00:55<01:15,  3.59it/s] 35%|███▍      | 145/415 [00:56<01:14,  3.60it/s] 35%|███▌      | 146/415 [00:56<01:14,  3.61it/s] 35%|███▌      | 147/415 [00:56<01:14,  3.61it/s] 36%|███▌      | 148/415 [00:57<01:13,  3.61it/s] 36%|███▌      | 149/415 [00:57<01:15,  3.53it/s] 36%|███▌      | 150/415 [00:57<01:14,  3.56it/s] 36%|███▋      | 151/415 [00:57<01:13,  3.57it/s] 37%|███▋      | 152/415 [00:58<01:13,  3.59it/s] 37%|███▋      | 153/415 [00:58<01:12,  3.60it/s] 37%|███▋      | 154/415 [00:58<01:12,  3.60it/s] 37%|███▋      | 155/415 [00:59<01:12,  3.61it/s] 38%|███▊      | 156/415 [00:59<01:11,  3.61it/s] 38%|███▊      | 157/415 [00:59<01:11,  3.62it/s] 38%|███▊      | 158/415 [00:59<01:10,  3.62it/s] 38%|███▊      | 159/415 [01:00<01:10,  3.62it/s] 39%|███▊      | 160/415 [01:00<01:11,  3.55it/s] 39%|███▉      | 161/415 [01:00<01:11,  3.57it/s] 39%|███▉      | 162/415 [01:00<01:10,  3.59it/s] 39%|███▉      | 163/415 [01:01<01:09,  3.60it/s] 40%|███▉      | 164/415 [01:01<01:09,  3.60it/s] 40%|███▉      | 165/415 [01:01<01:09,  3.61it/s] 40%|████      | 166/415 [01:02<01:05,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 05:01:50,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:01:50,227 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:01:50,227 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8437, 'eval_samples_per_second': 355.355, 'eval_steps_per_second': 44.496, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.87it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.02it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.25it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.22it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.42it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.95it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.49it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.62it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.67it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 42.41it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 43.17it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 43.76it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.03it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.03it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.01it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.44it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.62it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.66it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.73it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.58it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.45it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.29it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.47it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.60it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.62it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.71it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.79it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.79it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.63it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.43it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 43.63it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.04it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.33it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.40it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.43it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.45it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.37it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.31it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.41it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.57it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.72it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.76it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.60it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.56it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.39it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.32it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.45it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.55it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.69it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.76it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.75it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.61it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.50it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.38it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.34it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.71it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.37it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.52it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.47it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.42it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.38it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.34it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.02it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.42it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.56it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.65it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.66it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.64it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.51it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.50it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.38it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.37it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.42it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.66it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.70it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.65it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.62it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.62it/s][A 40%|████      | 166/415 [01:11<01:05,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:02:00,309 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 05:02:00,459 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:02:03,093 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:02:03,271 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:02:03,380 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:16<19:00,  4.60s/it] 40%|████      | 168/415 [01:17<13:35,  3.30s/it] 41%|████      | 169/415 [01:17<09:49,  2.40s/it] 41%|████      | 170/415 [01:17<07:11,  1.76s/it] 41%|████      | 171/415 [01:17<05:21,  1.32s/it] 41%|████▏     | 172/415 [01:18<04:04,  1.01s/it] 42%|████▏     | 173/415 [01:18<03:13,  1.25it/s] 42%|████▏     | 174/415 [01:18<02:34,  1.56it/s] 42%|████▏     | 175/415 [01:19<02:07,  1.88it/s] 42%|████▏     | 176/415 [01:19<01:48,  2.19it/s] 43%|████▎     | 177/415 [01:19<01:35,  2.49it/s] 43%|████▎     | 178/415 [01:19<01:26,  2.75it/s] 43%|████▎     | 179/415 [01:20<01:19,  2.96it/s] 43%|████▎     | 180/415 [01:20<01:14,  3.13it/s] 44%|████▎     | 181/415 [01:20<01:11,  3.27it/s] 44%|████▍     | 182/415 [01:20<01:09,  3.37it/s] 44%|████▍     | 183/415 [01:21<01:07,  3.44it/s] 44%|████▍     | 184/415 [01:21<01:08,  3.37it/s] 45%|████▍     | 185/415 [01:21<01:10,  3.27it/s] 45%|████▍     | 186/415 [01:22<01:07,  3.37it/s] 45%|████▌     | 187/415 [01:22<01:06,  3.44it/s] 45%|████▌     | 188/415 [01:22<01:04,  3.50it/s] 46%|████▌     | 189/415 [01:22<01:03,  3.54it/s] 46%|████▌     | 190/415 [01:23<01:03,  3.56it/s] 46%|████▌     | 191/415 [01:23<01:02,  3.57it/s] 46%|████▋     | 192/415 [01:23<01:02,  3.59it/s] 47%|████▋     | 193/415 [01:24<01:01,  3.60it/s] 47%|████▋     | 194/415 [01:24<01:01,  3.61it/s] 47%|████▋     | 195/415 [01:24<01:00,  3.61it/s] 47%|████▋     | 196/415 [01:24<01:01,  3.53it/s] 47%|████▋     | 197/415 [01:25<01:01,  3.56it/s] 48%|████▊     | 198/415 [01:25<01:00,  3.58it/s] 48%|████▊     | 199/415 [01:25<01:00,  3.59it/s] 48%|████▊     | 200/415 [01:26<00:59,  3.60it/s] 48%|████▊     | 201/415 [01:26<00:59,  3.61it/s] 49%|████▊     | 202/415 [01:26<00:58,  3.61it/s] 49%|████▉     | 203/415 [01:26<00:58,  3.62it/s] 49%|████▉     | 204/415 [01:27<00:58,  3.62it/s] 49%|████▉     | 205/415 [01:27<00:58,  3.62it/s] 50%|████▉     | 206/415 [01:27<00:57,  3.62it/s] 50%|████▉     | 207/415 [01:28<00:59,  3.51it/s] 50%|█████     | 208/415 [01:28<00:58,  3.54it/s] 50%|█████     | 209/415 [01:28<00:57,  3.57it/s] 51%|█████     | 210/415 [01:28<00:57,  3.58it/s] 51%|█████     | 211/415 [01:29<00:56,  3.60it/s] 51%|█████     | 212/415 [01:29<00:56,  3.60it/s] 51%|█████▏    | 213/415 [01:29<00:55,  3.61it/s] 52%|█████▏    | 214/415 [01:29<00:55,  3.61it/s] 52%|█████▏    | 215/415 [01:30<00:55,  3.61it/s] 52%|█████▏    | 216/415 [01:30<00:55,  3.62it/s] 52%|█████▏    | 217/415 [01:30<00:54,  3.62it/s] 53%|█████▎    | 218/415 [01:31<00:56,  3.50it/s] 53%|█████▎    | 219/415 [01:31<00:55,  3.53it/s] 53%|█████▎    | 220/415 [01:31<00:54,  3.56it/s] 53%|█████▎    | 221/415 [01:31<00:54,  3.58it/s] 53%|█████▎    | 222/415 [01:32<00:53,  3.59it/s] 54%|█████▎    | 223/415 [01:32<00:53,  3.60it/s] 54%|█████▍    | 224/415 [01:32<00:52,  3.61it/s] 54%|█████▍    | 225/415 [01:33<00:52,  3.61it/s] 54%|█████▍    | 226/415 [01:33<00:52,  3.62it/s] 55%|█████▍    | 227/415 [01:33<00:51,  3.62it/s] 55%|█████▍    | 228/415 [01:33<00:51,  3.62it/s] 55%|█████▌    | 229/415 [01:34<00:53,  3.49it/s] 55%|█████▌    | 230/415 [01:34<00:52,  3.53it/s] 56%|█████▌    | 231/415 [01:34<00:51,  3.55it/s] 56%|█████▌    | 232/415 [01:34<00:51,  3.57it/s] 56%|█████▌    | 233/415 [01:35<00:50,  3.59it/s] 56%|█████▋    | 234/415 [01:35<00:50,  3.60it/s] 57%|█████▋    | 235/415 [01:35<00:49,  3.61it/s] 57%|█████▋    | 236/415 [01:36<00:49,  3.61it/s] 57%|█████▋    | 237/415 [01:36<00:49,  3.61it/s] 57%|█████▋    | 238/415 [01:36<00:48,  3.61it/s] 58%|█████▊    | 239/415 [01:36<00:48,  3.62it/s] 58%|█████▊    | 240/415 [01:37<00:50,  3.45it/s] 58%|█████▊    | 241/415 [01:37<00:49,  3.50it/s] 58%|█████▊    | 242/415 [01:37<00:48,  3.54it/s] 59%|█████▊    | 243/415 [01:38<00:48,  3.56it/s] 59%|█████▉    | 244/415 [01:38<00:47,  3.57it/s] 59%|█████▉    | 245/415 [01:38<00:47,  3.59it/s] 59%|█████▉    | 246/415 [01:38<00:46,  3.60it/s] 60%|█████▉    | 247/415 [01:39<00:46,  3.61it/s] 60%|█████▉    | 248/415 [01:39<00:46,  3.61it/s] 60%|██████    | 249/415 [01:39<00:43,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 05:02:27,840 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:02:27,841 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:02:27,841 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.853, 'eval_samples_per_second': 355.02, 'eval_steps_per_second': 44.454, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.37it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.03it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.52it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.77it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.49it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.45it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.64it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.75it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.83it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.80it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.51it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.28it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.31it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.31it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.60it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.73it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.83it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.71it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.54it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.12it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.21it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.36it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.42it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.69it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 42.50it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.24it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.96it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.97it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.13it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.23it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 42.47it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.34it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.85it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.02it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 44.16it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.21it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.33it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.29it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.14it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.28it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.50it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.64it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.66it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.69it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.56it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.46it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.36it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 40.40it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 41.63it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 42.67it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.28it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.83it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.23it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.29it/s][A
 74%|███████▎  | 322/438 [00:07<00:06, 18.88it/s][A
 75%|███████▍  | 327/438 [00:07<00:04, 22.89it/s][A
 76%|███████▌  | 332/438 [00:08<00:03, 26.76it/s][A
 77%|███████▋  | 337/438 [00:08<00:03, 30.62it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 33.86it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 36.60it/s][A
 80%|████████  | 352/438 [00:08<00:02, 38.75it/s][A
 82%|████████▏ | 357/438 [00:08<00:02, 40.35it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 41.29it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 41.85it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 42.49it/s][A
 86%|████████▌ | 377/438 [00:09<00:01, 43.12it/s][A
 87%|████████▋ | 382/438 [00:09<00:01, 43.65it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 43.99it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.33it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 42.12it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 42.94it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.31it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.45it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.74it/s][A
 96%|█████████▋| 422/438 [00:10<00:00, 43.94it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 44.20it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.43it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.39it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.39it/s][A 60%|██████    | 249/415 [01:50<00:43,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:02:38,384 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 05:02:38,637 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:02:42,189 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:02:42,407 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:02:42,553 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [01:56<14:01,  5.10s/it] 60%|██████    | 251/415 [01:56<10:01,  3.67s/it] 61%|██████    | 252/415 [01:56<07:11,  2.65s/it] 61%|██████    | 253/415 [01:56<05:13,  1.94s/it] 61%|██████    | 254/415 [01:57<03:51,  1.44s/it] 61%|██████▏   | 255/415 [01:57<02:54,  1.09s/it] 62%|██████▏   | 256/415 [01:57<02:14,  1.18it/s] 62%|██████▏   | 257/415 [01:58<01:46,  1.48it/s] 62%|██████▏   | 258/415 [01:58<01:27,  1.80it/s] 62%|██████▏   | 259/415 [01:58<01:13,  2.12it/s] 63%|██████▎   | 260/415 [01:58<01:04,  2.42it/s] 63%|██████▎   | 261/415 [01:59<00:57,  2.69it/s] 63%|██████▎   | 262/415 [01:59<00:53,  2.85it/s] 63%|██████▎   | 263/415 [01:59<00:49,  3.05it/s] 64%|██████▎   | 264/415 [01:59<00:47,  3.20it/s] 64%|██████▍   | 265/415 [02:00<00:45,  3.31it/s] 64%|██████▍   | 266/415 [02:00<00:43,  3.40it/s] 64%|██████▍   | 267/415 [02:00<00:42,  3.47it/s] 65%|██████▍   | 268/415 [02:01<00:41,  3.51it/s] 65%|██████▍   | 269/415 [02:01<00:41,  3.55it/s] 65%|██████▌   | 270/415 [02:01<00:40,  3.57it/s] 65%|██████▌   | 271/415 [02:01<00:40,  3.59it/s] 66%|██████▌   | 272/415 [02:02<00:39,  3.60it/s] 66%|██████▌   | 273/415 [02:02<00:40,  3.50it/s] 66%|██████▌   | 274/415 [02:02<00:39,  3.54it/s] 66%|██████▋   | 275/415 [02:03<00:39,  3.56it/s] 67%|██████▋   | 276/415 [02:03<00:38,  3.58it/s] 67%|██████▋   | 277/415 [02:03<00:38,  3.59it/s] 67%|██████▋   | 278/415 [02:03<00:38,  3.59it/s] 67%|██████▋   | 279/415 [02:04<00:37,  3.60it/s] 67%|██████▋   | 280/415 [02:04<00:37,  3.61it/s] 68%|██████▊   | 281/415 [02:04<00:37,  3.61it/s] 68%|██████▊   | 282/415 [02:04<00:36,  3.62it/s] 68%|██████▊   | 283/415 [02:05<00:36,  3.62it/s] 68%|██████▊   | 284/415 [02:05<00:37,  3.47it/s] 69%|██████▊   | 285/415 [02:05<00:37,  3.51it/s] 69%|██████▉   | 286/415 [02:06<00:36,  3.55it/s] 69%|██████▉   | 287/415 [02:06<00:35,  3.57it/s] 69%|██████▉   | 288/415 [02:06<00:35,  3.58it/s] 70%|██████▉   | 289/415 [02:06<00:35,  3.60it/s] 70%|██████▉   | 290/415 [02:07<00:34,  3.61it/s] 70%|███████   | 291/415 [02:07<00:34,  3.61it/s] 70%|███████   | 292/415 [02:07<00:34,  3.62it/s] 71%|███████   | 293/415 [02:08<00:33,  3.62it/s] 71%|███████   | 294/415 [02:08<00:33,  3.62it/s] 71%|███████   | 295/415 [02:08<00:33,  3.56it/s] 71%|███████▏  | 296/415 [02:08<00:33,  3.57it/s] 72%|███████▏  | 297/415 [02:09<00:32,  3.59it/s] 72%|███████▏  | 298/415 [02:09<00:32,  3.60it/s] 72%|███████▏  | 299/415 [02:09<00:32,  3.61it/s] 72%|███████▏  | 300/415 [02:10<00:31,  3.61it/s] 73%|███████▎  | 301/415 [02:10<00:31,  3.61it/s] 73%|███████▎  | 302/415 [02:10<00:31,  3.61it/s] 73%|███████▎  | 303/415 [02:10<00:30,  3.61it/s] 73%|███████▎  | 304/415 [02:11<00:30,  3.62it/s] 73%|███████▎  | 305/415 [02:11<00:30,  3.62it/s] 74%|███████▎  | 306/415 [02:11<00:30,  3.53it/s] 74%|███████▍  | 307/415 [02:11<00:30,  3.56it/s] 74%|███████▍  | 308/415 [02:12<00:29,  3.58it/s] 74%|███████▍  | 309/415 [02:12<00:29,  3.59it/s] 75%|███████▍  | 310/415 [02:12<00:29,  3.60it/s] 75%|███████▍  | 311/415 [02:13<00:28,  3.60it/s] 75%|███████▌  | 312/415 [02:13<00:28,  3.61it/s] 75%|███████▌  | 313/415 [02:13<00:28,  3.61it/s] 76%|███████▌  | 314/415 [02:13<00:27,  3.61it/s] 76%|███████▌  | 315/415 [02:14<00:27,  3.61it/s] 76%|███████▌  | 316/415 [02:14<00:27,  3.61it/s] 76%|███████▋  | 317/415 [02:14<00:28,  3.49it/s] 77%|███████▋  | 318/415 [02:15<00:27,  3.53it/s] 77%|███████▋  | 319/415 [02:15<00:26,  3.56it/s] 77%|███████▋  | 320/415 [02:15<00:26,  3.58it/s] 77%|███████▋  | 321/415 [02:15<00:26,  3.59it/s] 78%|███████▊  | 322/415 [02:16<00:25,  3.60it/s] 78%|███████▊  | 323/415 [02:16<00:25,  3.61it/s] 78%|███████▊  | 324/415 [02:16<00:25,  3.61it/s] 78%|███████▊  | 325/415 [02:16<00:24,  3.62it/s] 79%|███████▊  | 326/415 [02:17<00:24,  3.62it/s] 79%|███████▉  | 327/415 [02:17<00:24,  3.62it/s] 79%|███████▉  | 328/415 [02:17<00:24,  3.56it/s] 79%|███████▉  | 329/415 [02:18<00:24,  3.58it/s] 80%|███████▉  | 330/415 [02:18<00:23,  3.59it/s] 80%|███████▉  | 331/415 [02:18<00:23,  3.60it/s] 80%|████████  | 332/415 [02:18<00:21,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 05:03:07,050 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:03:07,050 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:03:07,050 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 10.4176, 'eval_samples_per_second': 335.779, 'eval_steps_per_second': 42.044, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.02it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.31it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.23it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.46it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.97it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.50it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.30it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.47it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.55it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.74it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.83it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.81it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.74it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.48it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 43.81it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 43.96it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.38it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.58it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.68it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.64it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.41it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.29it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.29it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 41.89it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.82it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.45it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.66it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.63it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.46it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.35it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.25it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.29it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.45it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.63it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.74it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.65it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.53it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.37it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.25it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.34it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.40it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.57it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.67it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.76it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.79it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.61it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.36it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.30it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.43it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.50it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.60it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.77it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.54it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.33it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.27it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.31it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 42.64it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.41it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.79it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.22it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.37it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.28it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.13it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.10it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.24it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.47it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.60it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.74it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.57it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.61it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.40it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.96it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.30it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.36it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.46it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.60it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.63it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.75it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.62it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.62it/s][A 80%|████████  | 332/415 [02:28<00:21,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:03:17,207 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 05:03:17,431 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:03:19,816 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:03:19,919 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:03:19,981 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [02:32<05:59,  4.38s/it] 80%|████████  | 334/415 [02:33<04:15,  3.16s/it] 81%|████████  | 335/415 [02:33<03:03,  2.30s/it] 81%|████████  | 336/415 [02:33<02:13,  1.69s/it] 81%|████████  | 337/415 [02:34<01:38,  1.27s/it] 81%|████████▏ | 338/415 [02:34<01:14,  1.03it/s] 82%|████████▏ | 339/415 [02:34<00:58,  1.31it/s] 82%|████████▏ | 340/415 [02:34<00:46,  1.61it/s] 82%|████████▏ | 341/415 [02:35<00:38,  1.93it/s] 82%|████████▏ | 342/415 [02:35<00:32,  2.23it/s] 83%|████████▎ | 343/415 [02:35<00:28,  2.51it/s] 83%|████████▎ | 344/415 [02:35<00:25,  2.76it/s] 83%|████████▎ | 345/415 [02:36<00:24,  2.90it/s] 83%|████████▎ | 346/415 [02:36<00:22,  3.07it/s] 84%|████████▎ | 347/415 [02:36<00:21,  3.20it/s] 84%|████████▍ | 348/415 [02:37<00:20,  3.30it/s] 84%|████████▍ | 349/415 [02:37<00:19,  3.37it/s] 84%|████████▍ | 350/415 [02:37<00:19,  3.42it/s] 85%|████████▍ | 351/415 [02:37<00:18,  3.46it/s] 85%|████████▍ | 352/415 [02:38<00:18,  3.48it/s] 85%|████████▌ | 353/415 [02:38<00:17,  3.50it/s] 85%|████████▌ | 354/415 [02:38<00:17,  3.52it/s] 86%|████████▌ | 355/415 [02:39<00:16,  3.53it/s] 86%|████████▌ | 356/415 [02:39<00:17,  3.44it/s] 86%|████████▌ | 357/415 [02:39<00:16,  3.47it/s] 86%|████████▋ | 358/415 [02:39<00:16,  3.49it/s] 87%|████████▋ | 359/415 [02:40<00:15,  3.51it/s] 87%|████████▋ | 360/415 [02:40<00:15,  3.52it/s] 87%|████████▋ | 361/415 [02:40<00:15,  3.53it/s] 87%|████████▋ | 362/415 [02:41<00:15,  3.53it/s] 87%|████████▋ | 363/415 [02:41<00:14,  3.54it/s] 88%|████████▊ | 364/415 [02:41<00:14,  3.54it/s] 88%|████████▊ | 365/415 [02:41<00:14,  3.54it/s] 88%|████████▊ | 366/415 [02:42<00:13,  3.54it/s] 88%|████████▊ | 367/415 [02:42<00:13,  3.43it/s] 89%|████████▊ | 368/415 [02:42<00:13,  3.46it/s] 89%|████████▉ | 369/415 [02:43<00:13,  3.48it/s] 89%|████████▉ | 370/415 [02:43<00:12,  3.50it/s] 89%|████████▉ | 371/415 [02:43<00:12,  3.51it/s] 90%|████████▉ | 372/415 [02:43<00:12,  3.52it/s] 90%|████████▉ | 373/415 [02:44<00:12,  3.36it/s] 90%|█████████ | 374/415 [02:44<00:12,  3.41it/s] 90%|█████████ | 375/415 [02:44<00:11,  3.45it/s] 91%|█████████ | 376/415 [02:45<00:11,  3.47it/s] 91%|█████████ | 377/415 [02:45<00:10,  3.49it/s] 91%|█████████ | 378/415 [02:45<00:10,  3.39it/s] 91%|█████████▏| 379/415 [02:46<00:10,  3.43it/s] 92%|█████████▏| 380/415 [02:46<00:10,  3.46it/s] 92%|█████████▏| 381/415 [02:46<00:09,  3.48it/s] 92%|█████████▏| 382/415 [02:46<00:09,  3.40it/s] 92%|█████████▏| 383/415 [02:47<00:11,  2.68it/s] 93%|█████████▎| 384/415 [02:47<00:10,  2.88it/s] 93%|█████████▎| 385/415 [02:48<00:09,  3.05it/s] 93%|█████████▎| 386/415 [02:48<00:09,  3.18it/s] 93%|█████████▎| 387/415 [02:48<00:08,  3.28it/s] 93%|█████████▎| 388/415 [02:48<00:08,  3.21it/s] 94%|█████████▎| 389/415 [02:49<00:07,  3.30it/s] 94%|█████████▍| 390/415 [02:49<00:07,  3.37it/s] 94%|█████████▍| 391/415 [02:49<00:07,  3.42it/s] 94%|█████████▍| 392/415 [02:50<00:06,  3.45it/s] 95%|█████████▍| 393/415 [02:50<00:06,  3.48it/s] 95%|█████████▍| 394/415 [02:50<00:06,  3.49it/s] 95%|█████████▌| 395/415 [02:50<00:05,  3.50it/s] 95%|█████████▌| 396/415 [02:51<00:05,  3.51it/s] 96%|█████████▌| 397/415 [02:51<00:05,  3.52it/s] 96%|█████████▌| 398/415 [02:51<00:04,  3.52it/s] 96%|█████████▌| 399/415 [02:52<00:04,  3.30it/s] 96%|█████████▋| 400/415 [02:52<00:04,  3.37it/s] 97%|█████████▋| 401/415 [02:52<00:04,  3.42it/s] 97%|█████████▋| 402/415 [02:52<00:03,  3.45it/s] 97%|█████████▋| 403/415 [02:53<00:03,  3.48it/s] 97%|█████████▋| 404/415 [02:53<00:03,  3.50it/s] 98%|█████████▊| 405/415 [02:53<00:02,  3.51it/s] 98%|█████████▊| 406/415 [02:54<00:02,  3.45it/s] 98%|█████████▊| 407/415 [02:54<00:02,  3.47it/s] 98%|█████████▊| 408/415 [02:54<00:02,  3.49it/s] 99%|█████████▊| 409/415 [02:54<00:01,  3.51it/s] 99%|█████████▉| 410/415 [02:55<00:01,  3.52it/s] 99%|█████████▉| 411/415 [02:55<00:01,  3.53it/s] 99%|█████████▉| 412/415 [02:55<00:00,  3.53it/s]100%|█████████▉| 413/415 [02:56<00:00,  3.53it/s]100%|█████████▉| 414/415 [02:56<00:00,  3.53it/s]100%|██████████| 415/415 [02:56<00:00,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 05:03:44,766 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:03:44,766 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:03:44,766 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8593, 'eval_samples_per_second': 354.791, 'eval_steps_per_second': 44.425, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.53it/s][A
  3%|▎         | 12/438 [00:00<00:10, 39.66it/s][A
  4%|▍         | 17/438 [00:00<00:10, 41.63it/s][A
  5%|▌         | 22/438 [00:00<00:09, 43.01it/s][A
  6%|▌         | 27/438 [00:00<00:09, 43.63it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.08it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.20it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.29it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.99it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.04it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.33it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.59it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.71it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.81it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.76it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.66it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.44it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.26it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.33it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.45it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.50it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.62it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.72it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.70it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.66it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.49it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.28it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.07it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.33it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.44it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.61it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.67it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.67it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.53it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.34it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.39it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.54it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.57it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.61it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.65it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.70it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.63it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.38it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.30it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.42it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.55it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.69it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.76it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.68it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.76it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.58it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.48it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.35it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.85it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.21it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.57it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.64it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.47it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.43it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.45it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.34it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.54it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.67it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.81it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.73it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.65it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.53it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.34it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.19it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.24it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.36it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.58it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.78it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.76it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.68it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.58it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.35it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.28it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 42.72it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.44it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.77it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.20it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.36it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.36it/s][A100%|██████████| 415/415 [03:06<00:00,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:03:54,767 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 05:03:54,971 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:03:57,592 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:03:57,689 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:03:57,724 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:03:58,444 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:03:58,445 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83 (score: 1.0478343963623047).
                                                 100%|██████████| 415/415 [03:17<00:00,  3.73it/s]100%|██████████| 415/415 [03:17<00:00,  2.10it/s]
[INFO|trainer.py:1894] 2023-08-28 05:04:05,915 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 05:04:05,998 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:04:09,603 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:04:09,765 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:04:09,852 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:04:10,331 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   train_runtime            = 0:03:17.70
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   train_samples_per_second =    134.036
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:10,332 >>   train_steps_per_second   =      2.099
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.886, 'eval_samples_per_second': 353.833, 'eval_steps_per_second': 44.305, 'epoch': 5.0}
{'train_runtime': 197.7083, 'train_samples_per_second': 134.036, 'train_steps_per_second': 2.099, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:04:10 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:04:10,557 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:04:10,557 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 05:04:10,557 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.03it/s]  3%|▎         | 12/438 [00:00<00:08, 48.55it/s]  4%|▍         | 17/438 [00:00<00:08, 47.28it/s]  5%|▌         | 22/438 [00:00<00:08, 46.43it/s]  6%|▌         | 27/438 [00:00<00:08, 45.92it/s]  7%|▋         | 32/438 [00:00<00:08, 45.60it/s]  8%|▊         | 37/438 [00:00<00:08, 45.44it/s] 10%|▉         | 42/438 [00:00<00:08, 45.11it/s] 11%|█         | 47/438 [00:01<00:08, 44.59it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.49it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.46it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.58it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.72it/s] 16%|█▋        | 72/438 [00:01<00:08, 44.74it/s] 18%|█▊        | 77/438 [00:01<00:08, 44.80it/s] 19%|█▊        | 82/438 [00:01<00:07, 44.91it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.69it/s] 21%|██        | 92/438 [00:02<00:07, 44.38it/s] 22%|██▏       | 97/438 [00:02<00:08, 39.09it/s] 23%|██▎       | 102/438 [00:02<00:08, 40.74it/s] 24%|██▍       | 107/438 [00:02<00:07, 41.92it/s] 26%|██▌       | 112/438 [00:02<00:07, 42.81it/s] 27%|██▋       | 117/438 [00:02<00:07, 43.48it/s] 28%|██▊       | 122/438 [00:02<00:07, 43.96it/s] 29%|██▉       | 127/438 [00:02<00:07, 44.25it/s] 30%|███       | 132/438 [00:02<00:06, 44.36it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.05it/s] 32%|███▏      | 142/438 [00:03<00:06, 43.88it/s] 34%|███▎      | 147/438 [00:03<00:06, 43.96it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.17it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.33it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.54it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.73it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.86it/s] 40%|████      | 177/438 [00:03<00:05, 44.80it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.56it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.39it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.32it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.35it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.47it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.66it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.83it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.95it/s] 51%|█████     | 222/438 [00:04<00:04, 44.91it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.45it/s] 53%|█████▎    | 232/438 [00:05<00:04, 43.35it/s] 54%|█████▍    | 237/438 [00:05<00:04, 43.80it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.05it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.36it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.54it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.78it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.87it/s] 61%|██████    | 267/438 [00:06<00:03, 44.79it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.52it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.52it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.55it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.61it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.74it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.81it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.93it/s] 70%|███████   | 307/438 [00:06<00:02, 44.92it/s] 71%|███████   | 312/438 [00:07<00:02, 44.75it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.59it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.48it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.58it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.67it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.77it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.79it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.95it/s] 80%|████████  | 352/438 [00:07<00:01, 44.91it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.77it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.58it/s] 84%|████████▍ | 367/438 [00:08<00:01, 41.82it/s] 85%|████████▍ | 372/438 [00:08<00:01, 42.76it/s] 86%|████████▌ | 377/438 [00:08<00:01, 43.43it/s] 87%|████████▋ | 382/438 [00:08<00:01, 43.84it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.09it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.40it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.48it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.47it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.24it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.02it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.52it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.76it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.76it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.79it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.84it/s]100%|██████████| 438/438 [00:09<00:00, 44.42it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:04:20,441 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   eval_loss               =     1.0478
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   eval_samples_per_second =    353.891
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   eval_steps_per_second   =     44.312
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:04:20,442 >>   perplexity              =     2.8515
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:29,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:29,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:29,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:29,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:29,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:04:30,047 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:04:30,049 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:04:30,650 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:04:31,737 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:04:31,738 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:34,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:34,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:34,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:34,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:04:34,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:04:35,703 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:04:35,704 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:04:36,828 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:04:37,193 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:04:37,193 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.26it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:03,  1.35it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:07,  1.46it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:09,  1.39it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:16,  1.47it/s]Extractor Predicting: 24it [00:16,  1.47it/s]Extractor Predicting: 25it [00:17,  1.44it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.46it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:21,  1.37it/s]Extractor Predicting: 32it [00:22,  1.33it/s]Extractor Predicting: 33it [00:23,  1.33it/s]Extractor Predicting: 34it [00:24,  1.32it/s]Extractor Predicting: 35it [00:24,  1.32it/s]Extractor Predicting: 36it [00:25,  1.33it/s]Extractor Predicting: 37it [00:26,  1.30it/s]Extractor Predicting: 38it [00:27,  1.28it/s]Extractor Predicting: 39it [00:27,  1.29it/s]Extractor Predicting: 40it [00:28,  1.29it/s]Extractor Predicting: 41it [00:29,  1.27it/s]Extractor Predicting: 42it [00:30,  1.26it/s]Extractor Predicting: 43it [00:31,  1.26it/s]Extractor Predicting: 44it [00:31,  1.27it/s]Extractor Predicting: 45it [00:32,  1.27it/s]Extractor Predicting: 46it [00:33,  1.25it/s]Extractor Predicting: 47it [00:34,  1.25it/s]Extractor Predicting: 48it [00:35,  1.26it/s]Extractor Predicting: 49it [00:35,  1.27it/s]Extractor Predicting: 50it [00:36,  1.26it/s]Extractor Predicting: 51it [00:37,  1.24it/s]Extractor Predicting: 52it [00:38,  1.25it/s]Extractor Predicting: 53it [00:39,  1.24it/s]Extractor Predicting: 54it [00:39,  1.28it/s]Extractor Predicting: 55it [00:40,  1.28it/s]Extractor Predicting: 56it [00:41,  1.30it/s]Extractor Predicting: 57it [00:42,  1.30it/s]Extractor Predicting: 58it [00:42,  1.28it/s]Extractor Predicting: 59it [00:43,  1.25it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.33it/s]Extractor Predicting: 62it [00:45,  1.32it/s]Extractor Predicting: 63it [00:46,  1.30it/s]Extractor Predicting: 64it [00:47,  1.33it/s]Extractor Predicting: 65it [00:48,  1.36it/s]Extractor Predicting: 66it [00:48,  1.35it/s]Extractor Predicting: 67it [00:49,  1.36it/s]Extractor Predicting: 68it [00:50,  1.32it/s]Extractor Predicting: 69it [00:51,  1.32it/s]Extractor Predicting: 70it [00:51,  1.36it/s]Extractor Predicting: 71it [00:52,  1.37it/s]Extractor Predicting: 72it [00:53,  1.39it/s]Extractor Predicting: 73it [00:54,  1.37it/s]Extractor Predicting: 74it [00:54,  1.42it/s]Extractor Predicting: 75it [00:55,  1.43it/s]Extractor Predicting: 76it [00:56,  1.42it/s]Extractor Predicting: 77it [00:56,  1.43it/s]Extractor Predicting: 78it [00:57,  1.40it/s]Extractor Predicting: 79it [00:58,  1.41it/s]Extractor Predicting: 80it [00:58,  1.40it/s]Extractor Predicting: 81it [00:59,  1.39it/s]Extractor Predicting: 82it [01:00,  1.39it/s]Extractor Predicting: 83it [01:01,  1.38it/s]Extractor Predicting: 84it [01:01,  1.37it/s]Extractor Predicting: 85it [01:02,  1.38it/s]Extractor Predicting: 86it [01:03,  1.36it/s]Extractor Predicting: 87it [01:04,  1.37it/s]Extractor Predicting: 88it [01:04,  1.38it/s]Extractor Predicting: 89it [01:05,  1.44it/s]Extractor Predicting: 90it [01:06,  1.45it/s]Extractor Predicting: 91it [01:06,  1.48it/s]Extractor Predicting: 92it [01:07,  1.51it/s]Extractor Predicting: 93it [01:08,  1.51it/s]Extractor Predicting: 94it [01:08,  1.54it/s]Extractor Predicting: 95it [01:09,  1.54it/s]Extractor Predicting: 96it [01:10,  1.50it/s]Extractor Predicting: 97it [01:10,  1.51it/s]Extractor Predicting: 98it [01:11,  1.52it/s]Extractor Predicting: 99it [01:11,  1.52it/s]Extractor Predicting: 100it [01:12,  1.50it/s]Extractor Predicting: 101it [01:13,  1.49it/s]Extractor Predicting: 102it [01:14,  1.49it/s]Extractor Predicting: 103it [01:14,  1.50it/s]Extractor Predicting: 104it [01:15,  1.39it/s]Extractor Predicting: 105it [01:16,  1.40it/s]Extractor Predicting: 106it [01:16,  1.43it/s]Extractor Predicting: 107it [01:17,  1.42it/s]Extractor Predicting: 108it [01:18,  1.45it/s]Extractor Predicting: 109it [01:18,  1.47it/s]Extractor Predicting: 110it [01:19,  1.51it/s]Extractor Predicting: 111it [01:20,  1.51it/s]Extractor Predicting: 112it [01:20,  1.54it/s]Extractor Predicting: 113it [01:21,  1.51it/s]Extractor Predicting: 114it [01:22,  1.48it/s]Extractor Predicting: 115it [01:22,  1.48it/s]Extractor Predicting: 116it [01:23,  1.46it/s]Extractor Predicting: 117it [01:24,  1.42it/s]Extractor Predicting: 118it [01:25,  1.40it/s]Extractor Predicting: 119it [01:25,  1.37it/s]Extractor Predicting: 120it [01:26,  1.35it/s]Extractor Predicting: 121it [01:27,  1.35it/s]Extractor Predicting: 122it [01:28,  1.36it/s]Extractor Predicting: 123it [01:28,  1.34it/s]Extractor Predicting: 124it [01:29,  1.33it/s]Extractor Predicting: 125it [01:30,  1.33it/s]Extractor Predicting: 126it [01:31,  1.30it/s]Extractor Predicting: 127it [01:31,  1.29it/s]Extractor Predicting: 128it [01:32,  1.32it/s]Extractor Predicting: 129it [01:33,  1.33it/s]Extractor Predicting: 130it [01:34,  1.35it/s]Extractor Predicting: 131it [01:34,  1.34it/s]Extractor Predicting: 132it [01:35,  1.32it/s]Extractor Predicting: 133it [01:36,  1.32it/s]Extractor Predicting: 134it [01:37,  1.33it/s]Extractor Predicting: 135it [01:37,  1.34it/s]Extractor Predicting: 136it [01:38,  1.29it/s]Extractor Predicting: 137it [01:39,  1.30it/s]Extractor Predicting: 138it [01:40,  1.33it/s]Extractor Predicting: 139it [01:40,  1.35it/s]Extractor Predicting: 140it [01:41,  1.36it/s]Extractor Predicting: 141it [01:42,  1.33it/s]Extractor Predicting: 142it [01:43,  1.33it/s]Extractor Predicting: 143it [01:43,  1.34it/s]Extractor Predicting: 144it [01:44,  1.33it/s]Extractor Predicting: 145it [01:44,  1.75it/s]Extractor Predicting: 145it [01:44,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:35,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:35,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:35,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:35,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:35,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:06:35,871 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:06:35,872 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:06:36,826 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:06:37,930 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:06:37,930 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:40,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:40,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:40,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:40,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:06:40,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:06:41,245 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:06:41,246 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:06:42,027 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:06:42,252 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:06:42,252 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.26it/s]Extractor Predicting: 2it [00:01,  1.26it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.35it/s]Extractor Predicting: 8it [00:05,  1.36it/s]Extractor Predicting: 9it [00:06,  1.35it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:08,  1.36it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:11,  1.42it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:12,  1.39it/s]Extractor Predicting: 18it [00:13,  1.33it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:14,  1.33it/s]Extractor Predicting: 21it [00:15,  1.36it/s]Extractor Predicting: 22it [00:16,  1.32it/s]Extractor Predicting: 23it [00:17,  1.35it/s]Extractor Predicting: 24it [00:17,  1.38it/s]Extractor Predicting: 25it [00:18,  1.38it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.35it/s]Extractor Predicting: 31it [00:22,  1.36it/s]Extractor Predicting: 32it [00:23,  1.39it/s]Extractor Predicting: 33it [00:24,  1.41it/s]Extractor Predicting: 34it [00:24,  1.44it/s]Extractor Predicting: 35it [00:25,  1.44it/s]Extractor Predicting: 36it [00:26,  1.40it/s]Extractor Predicting: 37it [00:27,  1.41it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:28,  1.41it/s]Extractor Predicting: 40it [00:29,  1.41it/s]Extractor Predicting: 41it [00:29,  1.41it/s]Extractor Predicting: 42it [00:30,  1.42it/s]Extractor Predicting: 43it [00:31,  1.43it/s]Extractor Predicting: 44it [00:31,  1.46it/s]Extractor Predicting: 45it [00:32,  1.46it/s]Extractor Predicting: 46it [00:33,  1.51it/s]Extractor Predicting: 47it [00:33,  1.47it/s]Extractor Predicting: 48it [00:34,  1.44it/s]Extractor Predicting: 49it [00:35,  1.41it/s]Extractor Predicting: 50it [00:36,  1.39it/s]Extractor Predicting: 51it [00:36,  1.44it/s]Extractor Predicting: 52it [00:37,  1.42it/s]Extractor Predicting: 53it [00:38,  1.45it/s]Extractor Predicting: 54it [00:38,  1.47it/s]Extractor Predicting: 55it [00:39,  1.44it/s]Extractor Predicting: 56it [00:40,  1.42it/s]Extractor Predicting: 57it [00:40,  1.41it/s]Extractor Predicting: 58it [00:41,  1.42it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:43,  1.41it/s]Extractor Predicting: 61it [00:43,  1.40it/s]Extractor Predicting: 62it [00:44,  1.41it/s]Extractor Predicting: 63it [00:45,  1.38it/s]Extractor Predicting: 64it [00:46,  1.34it/s]Extractor Predicting: 65it [00:46,  1.39it/s]Extractor Predicting: 66it [00:47,  1.39it/s]Extractor Predicting: 67it [00:48,  1.44it/s]Extractor Predicting: 68it [00:48,  1.44it/s]Extractor Predicting: 69it [00:49,  1.39it/s]Extractor Predicting: 70it [00:50,  1.41it/s]Extractor Predicting: 71it [00:51,  1.40it/s]Extractor Predicting: 72it [00:51,  1.38it/s]Extractor Predicting: 73it [00:52,  1.38it/s]Extractor Predicting: 74it [00:53,  1.39it/s]Extractor Predicting: 75it [00:53,  1.39it/s]Extractor Predicting: 76it [00:54,  1.37it/s]Extractor Predicting: 77it [00:55,  1.38it/s]Extractor Predicting: 78it [00:56,  1.39it/s]Extractor Predicting: 79it [00:56,  1.38it/s]Extractor Predicting: 80it [00:57,  1.43it/s]Extractor Predicting: 81it [00:58,  1.35it/s]Extractor Predicting: 82it [00:59,  1.36it/s]Extractor Predicting: 83it [00:59,  1.39it/s]Extractor Predicting: 84it [01:00,  1.37it/s]Extractor Predicting: 85it [01:01,  1.38it/s]Extractor Predicting: 86it [01:01,  1.39it/s]Extractor Predicting: 87it [01:02,  1.41it/s]Extractor Predicting: 88it [01:03,  1.46it/s]Extractor Predicting: 89it [01:03,  1.47it/s]Extractor Predicting: 90it [01:04,  1.49it/s]Extractor Predicting: 91it [01:05,  1.45it/s]Extractor Predicting: 92it [01:06,  1.40it/s]Extractor Predicting: 93it [01:06,  1.43it/s]Extractor Predicting: 94it [01:07,  1.31it/s]Extractor Predicting: 95it [01:08,  1.34it/s]Extractor Predicting: 96it [01:09,  1.33it/s]Extractor Predicting: 97it [01:09,  1.35it/s]Extractor Predicting: 98it [01:10,  1.35it/s]Extractor Predicting: 99it [01:11,  1.37it/s]Extractor Predicting: 100it [01:11,  1.38it/s]Extractor Predicting: 101it [01:12,  1.37it/s]Extractor Predicting: 102it [01:13,  1.36it/s]Extractor Predicting: 103it [01:14,  1.38it/s]Extractor Predicting: 104it [01:14,  1.39it/s]Extractor Predicting: 105it [01:15,  1.38it/s]Extractor Predicting: 106it [01:16,  1.38it/s]Extractor Predicting: 107it [01:17,  1.38it/s]Extractor Predicting: 108it [01:17,  1.40it/s]Extractor Predicting: 109it [01:18,  1.41it/s]Extractor Predicting: 110it [01:19,  1.40it/s]Extractor Predicting: 111it [01:19,  1.40it/s]Extractor Predicting: 112it [01:20,  1.42it/s]Extractor Predicting: 113it [01:21,  1.40it/s]Extractor Predicting: 114it [01:22,  1.38it/s]Extractor Predicting: 115it [01:22,  1.40it/s]Extractor Predicting: 116it [01:23,  1.39it/s]Extractor Predicting: 117it [01:24,  1.41it/s]Extractor Predicting: 118it [01:24,  1.42it/s]Extractor Predicting: 119it [01:25,  1.41it/s]Extractor Predicting: 120it [01:26,  1.41it/s]Extractor Predicting: 121it [01:26,  1.40it/s]Extractor Predicting: 122it [01:27,  1.42it/s]Extractor Predicting: 123it [01:28,  1.43it/s]Extractor Predicting: 124it [01:29,  1.42it/s]Extractor Predicting: 125it [01:29,  1.43it/s]Extractor Predicting: 126it [01:30,  1.39it/s]Extractor Predicting: 127it [01:31,  1.39it/s]Extractor Predicting: 128it [01:31,  1.43it/s]Extractor Predicting: 129it [01:32,  1.41it/s]Extractor Predicting: 130it [01:33,  1.44it/s]Extractor Predicting: 131it [01:33,  1.46it/s]Extractor Predicting: 132it [01:34,  1.44it/s]Extractor Predicting: 133it [01:35,  1.43it/s]Extractor Predicting: 134it [01:36,  1.42it/s]Extractor Predicting: 135it [01:36,  1.42it/s]Extractor Predicting: 136it [01:37,  1.43it/s]Extractor Predicting: 137it [01:38,  1.43it/s]Extractor Predicting: 138it [01:38,  1.45it/s]Extractor Predicting: 139it [01:39,  1.40it/s]Extractor Predicting: 140it [01:40,  1.42it/s]Extractor Predicting: 140it [01:40,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:32,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:32,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:32,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:32,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:32,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:08:33,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:08:33,447 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:08:33,778 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:08:34,869 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:08:34,869 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:38,029 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:38,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:38,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:38,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:08:38,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:08:38,801 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:08:38,802 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:08:39,440 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:08:39,679 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:08:39,679 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 3it [00:02,  1.32it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:08:44,226 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:08:44,227 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:08:44,308 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:08:44,309 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:08:44,365 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:08:58,190 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:08:58,211 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:08:58,301 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:08:58,302 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:08:58,371 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:08:58,428 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:08:58,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:08:59,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:00,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:01,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:02,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:03,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:04,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:05,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:06,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:07,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:07,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:09,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:09,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:10,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:11,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:12,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:13,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:14,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:15,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:16,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:17,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:17,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:59, 19.98s/it][WARNING|generation_utils.py:914] 2023-08-28 05:09:18,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:19,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:20,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:21,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:22,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:23,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:24,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:24,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:25,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:27,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:27,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:29,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:30,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:31,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:32,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:32,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:33,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:34,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:35,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:36,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:37,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:38,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:39,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:40,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:42<02:49, 21.22s/it][WARNING|generation_utils.py:914] 2023-08-28 05:09:40,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:41,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:42,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:43,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:44,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:45,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:45,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:46,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:47,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:47,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:48,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:49,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:50,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:50,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:51,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:52,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:52,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:54,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:55,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:55,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:56,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:57,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:58,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:59,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:09:59,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:00,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:01,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:01,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:02,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:03,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:03,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:04,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:05,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:06,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:06,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:07,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:08,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:08,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:09,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:10,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:11,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:11,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:12,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:13,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:14,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:14,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<03:12, 27.56s/it][WARNING|generation_utils.py:914] 2023-08-28 05:10:15,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:16,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:17,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:18,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:19,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:20,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:21,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:22,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:23,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:23,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:24,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:25,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:25,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:26,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:27,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:28,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:29,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:29,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:30,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:31,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:32,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:33,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:33,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:34,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:35,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:29, 24.84s/it][WARNING|generation_utils.py:914] 2023-08-28 05:10:36,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:37,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:38,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:38,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:39,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:40,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:41,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:42,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:42,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:43,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:44,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:44,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:45,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:46,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:47,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:47,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:48,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:49,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:50,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:50,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:52,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:52,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:53,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:54,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:55,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:56,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:57,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:57,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:59<01:58, 23.77s/it][WARNING|generation_utils.py:914] 2023-08-28 05:10:58,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:10:59,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:00,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:00,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:01,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:02,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:03,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:04,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:04,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:05,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:06,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:07,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:08,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:08,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:09,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:10,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:11,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:11,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:12,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:13,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:14,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:14,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:15,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:16,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:17,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:17,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:18,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:20<01:31, 22.80s/it][WARNING|generation_utils.py:914] 2023-08-28 05:11:19,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:20,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:20,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:21,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:22,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:23,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:24,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:24,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:25,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:26,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:27,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:28,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:29,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:29,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:30,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:31,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:32,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:33,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:33,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:34,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:35,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:36,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:37,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:37,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:38,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:39,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:41<01:06, 22.13s/it][WARNING|generation_utils.py:914] 2023-08-28 05:11:40,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:41,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:41,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:42,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:43,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:43,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:44,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:45,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:46,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:46,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:47,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:48,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:48,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:49,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:50,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:51,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:51,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:52,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:53,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:53,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:54,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:55,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:57<00:40, 20.15s/it][WARNING|generation_utils.py:914] 2023-08-28 05:11:56,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:56,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:57,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:58,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:11:59,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:00,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:00,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:02,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:03,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:04,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:04,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:05,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:06,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:07,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:08,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:09,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:10,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:11,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:11,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:12,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:13,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:15<00:19, 19.46s/it][WARNING|generation_utils.py:914] 2023-08-28 05:12:14,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:14,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:15,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:16,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:17,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:18,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:20,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:20,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:21,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:22,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:23,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:24,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:25,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:26,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:27,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:28,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:29,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:30,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:30,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:31,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:33,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:33,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:34,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:35,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:36,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:37,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:12:38,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:40<00:00, 21.20s/it]Generating: 100%|██████████| 10/10 [03:40<00:00, 22.03s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:46,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:46,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:46,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:46,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:46,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:12:47,306 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:12:47,307 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:12:47,939 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:12:49,099 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:12:49,099 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:52,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:52,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:52,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:52,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:12:52,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:12:53,142 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:12:53,143 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:12:53,763 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:12:54,043 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:12:54,043 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.01s/it]Extractor Estimating: 2it [00:01,  1.11it/s]Extractor Estimating: 3it [00:02,  1.20it/s]Extractor Estimating: 4it [00:03,  1.10it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:05,  1.19it/s]Extractor Estimating: 7it [00:05,  1.21it/s]Extractor Estimating: 8it [00:06,  1.24it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.21it/s]Extractor Estimating: 11it [00:09,  1.17it/s]Extractor Estimating: 12it [00:10,  1.15it/s]Extractor Estimating: 13it [00:10,  1.19it/s]Extractor Estimating: 14it [00:11,  1.23it/s]Extractor Estimating: 15it [00:12,  1.22it/s]Extractor Estimating: 16it [00:13,  1.23it/s]Extractor Estimating: 17it [00:14,  1.22it/s]Extractor Estimating: 18it [00:15,  1.21it/s]Extractor Estimating: 19it [00:15,  1.20it/s]Extractor Estimating: 20it [00:16,  1.19it/s]Extractor Estimating: 21it [00:17,  1.13it/s]Extractor Estimating: 22it [00:18,  1.09it/s]Extractor Estimating: 23it [00:19,  1.17it/s]Extractor Estimating: 24it [00:20,  1.22it/s]Extractor Estimating: 25it [00:21,  1.20it/s]Extractor Estimating: 26it [00:21,  1.16it/s]Extractor Estimating: 27it [00:22,  1.19it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.17it/s]Extractor Estimating: 30it [00:25,  1.13it/s]Extractor Estimating: 31it [00:26,  1.16it/s]Extractor Estimating: 32it [00:27,  1.20it/s]Extractor Estimating: 33it [00:27,  1.23it/s]Extractor Estimating: 34it [00:28,  1.21it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:30,  1.15it/s]Extractor Estimating: 37it [00:31,  1.19it/s]Extractor Estimating: 38it [00:32,  1.15it/s]Extractor Estimating: 39it [00:33,  1.13it/s]Extractor Estimating: 40it [00:33,  1.13it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:35,  1.14it/s]Extractor Estimating: 43it [00:36,  1.15it/s]Extractor Estimating: 44it [00:37,  1.11it/s]Extractor Estimating: 45it [00:38,  1.16it/s]Extractor Estimating: 46it [00:39,  1.16it/s]Extractor Estimating: 47it [00:39,  1.17it/s]Extractor Estimating: 48it [00:40,  1.16it/s]Extractor Estimating: 49it [00:41,  1.18it/s]Extractor Estimating: 50it [00:42,  1.13it/s]Extractor Estimating: 51it [00:43,  1.15it/s]Extractor Estimating: 52it [00:44,  1.18it/s]Extractor Estimating: 53it [00:45,  1.20it/s]Extractor Estimating: 54it [00:45,  1.21it/s]Extractor Estimating: 55it [00:46,  1.24it/s]Extractor Estimating: 56it [00:47,  1.25it/s]Extractor Estimating: 57it [00:48,  1.24it/s]Extractor Estimating: 58it [00:49,  1.25it/s]Extractor Estimating: 59it [00:49,  1.31it/s]Extractor Estimating: 60it [00:50,  1.31it/s]Extractor Estimating: 61it [00:51,  1.28it/s]Extractor Estimating: 62it [00:52,  1.24it/s]Extractor Estimating: 63it [00:52,  1.26it/s]Extractor Estimating: 64it [00:53,  1.28it/s]Extractor Estimating: 65it [00:54,  1.32it/s]Extractor Estimating: 66it [00:55,  1.29it/s]Extractor Estimating: 67it [00:55,  1.30it/s]Extractor Estimating: 68it [00:56,  1.27it/s]Extractor Estimating: 69it [00:57,  1.32it/s]Extractor Estimating: 70it [00:58,  1.25it/s]Extractor Estimating: 71it [00:59,  1.28it/s]Extractor Estimating: 72it [00:59,  1.32it/s]Extractor Estimating: 73it [01:00,  1.26it/s]Extractor Estimating: 74it [01:01,  1.29it/s]Extractor Estimating: 75it [01:02,  1.24it/s]Extractor Estimating: 76it [01:03,  1.27it/s]Extractor Estimating: 77it [01:03,  1.31it/s]Extractor Estimating: 78it [01:04,  1.26it/s]Extractor Estimating: 79it [01:05,  1.18it/s]Extractor Estimating: 80it [01:06,  1.23it/s]Extractor Estimating: 81it [01:07,  1.24it/s]Extractor Estimating: 82it [01:07,  1.26it/s]Extractor Estimating: 83it [01:08,  1.25it/s]Extractor Estimating: 84it [01:09,  1.31it/s]Extractor Estimating: 85it [01:10,  1.34it/s]Extractor Estimating: 86it [01:10,  1.36it/s]Extractor Estimating: 87it [01:11,  1.35it/s]Extractor Estimating: 88it [01:12,  1.38it/s]Extractor Estimating: 89it [01:13,  1.31it/s]Extractor Estimating: 90it [01:13,  1.34it/s]Extractor Estimating: 91it [01:14,  1.36it/s]Extractor Estimating: 92it [01:15,  1.31it/s]Extractor Estimating: 93it [01:16,  1.30it/s]Extractor Estimating: 94it [01:16,  1.33it/s]Extractor Estimating: 95it [01:17,  1.31it/s]Extractor Estimating: 96it [01:18,  1.34it/s]Extractor Estimating: 97it [01:19,  1.35it/s]Extractor Estimating: 98it [01:19,  1.32it/s]Extractor Estimating: 99it [01:20,  1.31it/s]Extractor Estimating: 100it [01:21,  1.29it/s]Extractor Estimating: 101it [01:22,  1.32it/s]Extractor Estimating: 102it [01:22,  1.35it/s]Extractor Estimating: 103it [01:23,  1.31it/s]Extractor Estimating: 104it [01:24,  1.32it/s]Extractor Estimating: 105it [01:25,  1.26it/s]Extractor Estimating: 106it [01:25,  1.29it/s]Extractor Estimating: 107it [01:26,  1.34it/s]Extractor Estimating: 108it [01:27,  1.39it/s]Extractor Estimating: 109it [01:28,  1.40it/s]Extractor Estimating: 110it [01:28,  1.43it/s]Extractor Estimating: 111it [01:29,  1.40it/s]Extractor Estimating: 112it [01:30,  1.40it/s]Extractor Estimating: 113it [01:30,  1.39it/s]Extractor Estimating: 114it [01:31,  1.43it/s]Extractor Estimating: 115it [01:32,  1.36it/s]Extractor Estimating: 116it [01:33,  1.37it/s]Extractor Estimating: 117it [01:33,  1.36it/s]Extractor Estimating: 118it [01:34,  1.41it/s]Extractor Estimating: 119it [01:35,  1.43it/s]Extractor Estimating: 120it [01:35,  1.39it/s]Extractor Estimating: 121it [01:36,  1.38it/s]Extractor Estimating: 122it [01:37,  1.31it/s]Extractor Estimating: 123it [01:38,  1.33it/s]Extractor Estimating: 124it [01:39,  1.32it/s]Extractor Estimating: 125it [01:39,  1.36it/s]Extractor Estimating: 126it [01:40,  1.35it/s]Extractor Estimating: 127it [01:41,  1.33it/s]Extractor Estimating: 128it [01:41,  1.34it/s]Extractor Estimating: 129it [01:42,  1.34it/s]Extractor Estimating: 130it [01:43,  1.31it/s]Extractor Estimating: 131it [01:44,  1.31it/s]Extractor Estimating: 132it [01:44,  1.37it/s]Extractor Estimating: 133it [01:45,  1.36it/s]Extractor Estimating: 134it [01:46,  1.37it/s]Extractor Estimating: 135it [01:47,  1.36it/s]Extractor Estimating: 136it [01:47,  1.32it/s]Extractor Estimating: 137it [01:48,  1.38it/s]Extractor Estimating: 138it [01:49,  1.41it/s]Extractor Estimating: 139it [01:50,  1.39it/s]Extractor Estimating: 140it [01:50,  1.34it/s]Extractor Estimating: 141it [01:51,  1.24it/s]Extractor Estimating: 142it [01:52,  1.28it/s]Extractor Estimating: 143it [01:53,  1.29it/s]Extractor Estimating: 144it [01:54,  1.22it/s]Extractor Estimating: 145it [01:55,  1.21it/s]Extractor Estimating: 146it [01:55,  1.24it/s]Extractor Estimating: 147it [01:56,  1.29it/s]Extractor Estimating: 148it [01:57,  1.29it/s]Extractor Estimating: 149it [01:58,  1.29it/s]Extractor Estimating: 150it [01:58,  1.29it/s]Extractor Estimating: 151it [01:59,  1.25it/s]Extractor Estimating: 152it [02:00,  1.23it/s]Extractor Estimating: 153it [02:01,  1.27it/s]Extractor Estimating: 154it [02:02,  1.26it/s]Extractor Estimating: 155it [02:02,  1.26it/s]Extractor Estimating: 156it [02:03,  1.20it/s]Extractor Estimating: 157it [02:04,  1.21it/s]Extractor Estimating: 158it [02:05,  1.21it/s]Extractor Estimating: 159it [02:06,  1.15it/s]Extractor Estimating: 160it [02:07,  1.20it/s]Extractor Estimating: 161it [02:07,  1.23it/s]Extractor Estimating: 162it [02:08,  1.23it/s]Extractor Estimating: 163it [02:09,  1.25it/s]Extractor Estimating: 164it [02:10,  1.25it/s]Extractor Estimating: 165it [02:11,  1.26it/s]Extractor Estimating: 166it [02:11,  1.27it/s]Extractor Estimating: 167it [02:12,  1.25it/s]Extractor Estimating: 168it [02:13,  1.26it/s]Extractor Estimating: 169it [02:14,  1.20it/s]Extractor Estimating: 170it [02:15,  1.20it/s]Extractor Estimating: 171it [02:15,  1.21it/s]Extractor Estimating: 172it [02:16,  1.27it/s]Extractor Estimating: 173it [02:17,  1.21it/s]Extractor Estimating: 174it [02:18,  1.22it/s]Extractor Estimating: 175it [02:19,  1.21it/s]Extractor Estimating: 176it [02:20,  1.17it/s]Extractor Estimating: 177it [02:20,  1.23it/s]Extractor Estimating: 178it [02:21,  1.28it/s]Extractor Estimating: 179it [02:22,  1.32it/s]Extractor Estimating: 180it [02:22,  1.36it/s]Extractor Estimating: 181it [02:23,  1.38it/s]Extractor Estimating: 182it [02:24,  1.36it/s]Extractor Estimating: 183it [02:25,  1.39it/s]Extractor Estimating: 184it [02:25,  1.39it/s]Extractor Estimating: 185it [02:26,  1.39it/s]Extractor Estimating: 186it [02:27,  1.36it/s]Extractor Estimating: 187it [02:28,  1.35it/s]Extractor Estimating: 188it [02:28,  1.39it/s]Extractor Estimating: 189it [02:29,  1.37it/s]Extractor Estimating: 190it [02:30,  1.38it/s]Extractor Estimating: 191it [02:30,  1.39it/s]Extractor Estimating: 192it [02:31,  1.32it/s]Extractor Estimating: 193it [02:32,  1.27it/s]Extractor Estimating: 194it [02:33,  1.30it/s]Extractor Estimating: 195it [02:34,  1.32it/s]Extractor Estimating: 196it [02:34,  1.31it/s]Extractor Estimating: 197it [02:35,  1.26it/s]Extractor Estimating: 198it [02:36,  1.33it/s]Extractor Estimating: 199it [02:37,  1.37it/s]Extractor Estimating: 200it [02:37,  1.35it/s]Extractor Estimating: 201it [02:38,  1.33it/s]Extractor Estimating: 202it [02:39,  1.28it/s]Extractor Estimating: 203it [02:40,  1.32it/s]Extractor Estimating: 204it [02:40,  1.30it/s]Extractor Estimating: 205it [02:41,  1.31it/s]Extractor Estimating: 206it [02:42,  1.33it/s]Extractor Estimating: 207it [02:43,  1.33it/s]Extractor Estimating: 208it [02:44,  1.19it/s]Extractor Estimating: 209it [02:45,  1.21it/s]Extractor Estimating: 210it [02:45,  1.21it/s]Extractor Estimating: 211it [02:46,  1.21it/s]Extractor Estimating: 212it [02:47,  1.23it/s]Extractor Estimating: 213it [02:48,  1.20it/s]Extractor Estimating: 214it [02:49,  1.21it/s]Extractor Estimating: 215it [02:49,  1.21it/s]Extractor Estimating: 216it [02:50,  1.24it/s]Extractor Estimating: 217it [02:51,  1.26it/s]Extractor Estimating: 218it [02:52,  1.27it/s]Extractor Estimating: 219it [02:53,  1.26it/s]Extractor Estimating: 220it [02:53,  1.22it/s]Extractor Estimating: 221it [02:54,  1.22it/s]Extractor Estimating: 222it [02:55,  1.24it/s]Extractor Estimating: 223it [02:56,  1.24it/s]Extractor Estimating: 224it [02:57,  1.23it/s]Extractor Estimating: 225it [02:58,  1.21it/s]Extractor Estimating: 226it [02:58,  1.25it/s]Extractor Estimating: 227it [02:59,  1.25it/s]Extractor Estimating: 228it [03:00,  1.27it/s]Extractor Estimating: 229it [03:01,  1.22it/s]Extractor Estimating: 230it [03:02,  1.20it/s]Extractor Estimating: 231it [03:02,  1.20it/s]Extractor Estimating: 232it [03:03,  1.19it/s]Extractor Estimating: 233it [03:04,  1.23it/s]Extractor Estimating: 234it [03:05,  1.16it/s]Extractor Estimating: 235it [03:06,  1.16it/s]Extractor Estimating: 236it [03:07,  1.19it/s]Extractor Estimating: 237it [03:07,  1.21it/s]Extractor Estimating: 238it [03:08,  1.23it/s]Extractor Estimating: 239it [03:09,  1.18it/s]Extractor Estimating: 240it [03:10,  1.21it/s]Extractor Estimating: 241it [03:11,  1.18it/s]Extractor Estimating: 242it [03:12,  1.17it/s]Extractor Estimating: 243it [03:13,  1.19it/s]Extractor Estimating: 244it [03:13,  1.19it/s]Extractor Estimating: 245it [03:14,  1.24it/s]Extractor Estimating: 246it [03:15,  1.25it/s]Extractor Estimating: 247it [03:16,  1.27it/s]Extractor Estimating: 248it [03:16,  1.25it/s]Extractor Estimating: 249it [03:17,  1.22it/s]Extractor Estimating: 250it [03:18,  1.23it/s]Extractor Estimating: 250it [03:18,  1.26it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:28,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:28,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:28,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:28,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:28,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:16:28,969 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:16:28,970 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:16:29,247 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:16:30,341 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:16:30,341 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:33,399 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:33,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:33,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:33,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:16:33,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:16:34,186 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:16:34,187 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:16:34,807 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:16:35,033 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:16:35,034 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:09:56,297 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:09:56,446 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5239 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 19151
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19251, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19251, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.307, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.305, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.351, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.303, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.633, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.314, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.296, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.310, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.329, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.625, loss:nan
g_step 1200, step 105, avg_time 1.305, loss:nan
g_step 1300, step 205, avg_time 1.321, loss:nan
g_step 1400, step 86, avg_time 1.308, loss:nan
g_step 1500, step 186, avg_time 1.322, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.616, loss:nan
g_step 1700, step 167, avg_time 1.317, loss:nan
g_step 1800, step 48, avg_time 1.328, loss:nan
g_step 1900, step 148, avg_time 1.308, loss:nan
g_step 2000, step 29, avg_time 1.304, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.603, loss:nan
g_step 2200, step 10, avg_time 1.321, loss:nan
g_step 2300, step 110, avg_time 1.316, loss:nan
g_step 2400, step 210, avg_time 1.307, loss:nan
g_step 2500, step 91, avg_time 1.284, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.634, loss:nan
g_step 2700, step 72, avg_time 1.323, loss:nan
g_step 2800, step 172, avg_time 1.289, loss:nan
g_step 2900, step 53, avg_time 1.318, loss:nan
g_step 3000, step 153, avg_time 1.303, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.628, loss:nan
g_step 3200, step 134, avg_time 1.321, loss:nan
g_step 3300, step 15, avg_time 1.305, loss:nan
g_step 3400, step 115, avg_time 1.310, loss:nan
g_step 3500, step 215, avg_time 1.315, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.609, loss:nan
g_step 3700, step 196, avg_time 1.316, loss:nan
g_step 3800, step 77, avg_time 1.315, loss:nan
g_step 3900, step 177, avg_time 1.311, loss:nan
g_step 4000, step 58, avg_time 1.316, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.623, loss:nan
g_step 4200, step 39, avg_time 1.297, loss:nan
g_step 4300, step 139, avg_time 1.327, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:09:56 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:09:56 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-09-56_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:09:57 - WARNING - datasets.builder -   Using custom data configuration default-7b4533f69b11f78e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7b4533f69b11f78e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:09:58,955 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:09:58,956 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:09:58,956 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:09:58,957 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:09:59,040 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:09:59,084 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:09:59,416 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:10:02,555 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:10:02,575 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7b4533f69b11f78e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.90ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.79ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.45ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.86ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.13ba/s]100%|██████████| 6/6 [00:01<00:00,  4.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.09ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.85ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18ba/s]100%|██████████| 4/4 [00:00<00:00,  5.33ba/s]100%|██████████| 4/4 [00:00<00:00,  4.64ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.77ba/s] 33%|███▎      | 2/6 [00:00<00:00,  6.05ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  8.56ba/s]100%|██████████| 6/6 [00:00<00:00, 11.45ba/s]100%|██████████| 6/6 [00:00<00:00,  9.54ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.63ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.37ba/s]100%|██████████| 4/4 [00:00<00:00,  9.26ba/s]
[INFO|trainer.py:414] 2023-08-28 07:10:07,392 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:10:07,504 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:10:07,504 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 07:10:07,504 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:10:07,504 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:10:07,504 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:10:07,504 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:10:07,504 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:56,  3.51it/s]  0%|          | 2/410 [00:00<01:53,  3.59it/s]  1%|          | 3/410 [00:00<01:52,  3.61it/s]  1%|          | 4/410 [00:01<01:52,  3.61it/s]  1%|          | 5/410 [00:01<01:52,  3.59it/s]  1%|▏         | 6/410 [00:01<01:57,  3.44it/s]  2%|▏         | 7/410 [00:01<01:55,  3.48it/s]  2%|▏         | 8/410 [00:02<01:54,  3.51it/s]  2%|▏         | 9/410 [00:02<01:53,  3.52it/s]  2%|▏         | 10/410 [00:02<01:53,  3.54it/s]  3%|▎         | 11/410 [00:03<01:52,  3.54it/s]  3%|▎         | 12/410 [00:03<01:52,  3.54it/s]  3%|▎         | 13/410 [00:03<01:52,  3.54it/s]  3%|▎         | 14/410 [00:03<01:51,  3.54it/s]  4%|▎         | 15/410 [00:04<01:51,  3.54it/s]  4%|▍         | 16/410 [00:04<01:51,  3.54it/s]  4%|▍         | 17/410 [00:04<01:53,  3.46it/s]  4%|▍         | 18/410 [00:05<01:52,  3.48it/s]  5%|▍         | 19/410 [00:05<01:51,  3.49it/s]  5%|▍         | 20/410 [00:05<01:51,  3.51it/s]  5%|▌         | 21/410 [00:05<01:50,  3.52it/s]  5%|▌         | 22/410 [00:06<01:50,  3.53it/s]  6%|▌         | 23/410 [00:06<01:49,  3.54it/s]  6%|▌         | 24/410 [00:06<01:48,  3.54it/s]  6%|▌         | 25/410 [00:07<01:48,  3.54it/s]  6%|▋         | 26/410 [00:07<01:48,  3.54it/s]  7%|▋         | 27/410 [00:07<01:48,  3.54it/s]  7%|▋         | 28/410 [00:07<01:49,  3.47it/s]  7%|▋         | 29/410 [00:08<01:49,  3.49it/s]  7%|▋         | 30/410 [00:08<01:48,  3.50it/s]  8%|▊         | 31/410 [00:08<01:47,  3.51it/s]  8%|▊         | 32/410 [00:09<01:47,  3.52it/s]  8%|▊         | 33/410 [00:09<01:46,  3.53it/s]  8%|▊         | 34/410 [00:09<01:46,  3.52it/s]  9%|▊         | 35/410 [00:09<01:46,  3.51it/s]  9%|▉         | 36/410 [00:10<01:46,  3.51it/s]  9%|▉         | 37/410 [00:10<01:46,  3.51it/s]  9%|▉         | 38/410 [00:10<01:46,  3.50it/s] 10%|▉         | 39/410 [00:11<01:48,  3.43it/s] 10%|▉         | 40/410 [00:11<01:46,  3.47it/s] 10%|█         | 41/410 [00:11<01:45,  3.50it/s] 10%|█         | 42/410 [00:11<01:44,  3.51it/s] 10%|█         | 43/410 [00:12<01:44,  3.52it/s] 11%|█         | 44/410 [00:12<01:43,  3.53it/s] 11%|█         | 45/410 [00:12<01:43,  3.53it/s] 11%|█         | 46/410 [00:13<01:43,  3.53it/s] 11%|█▏        | 47/410 [00:13<01:42,  3.53it/s] 12%|█▏        | 48/410 [00:13<01:42,  3.53it/s] 12%|█▏        | 49/410 [00:13<01:42,  3.54it/s] 12%|█▏        | 50/410 [00:14<01:45,  3.40it/s] 12%|█▏        | 51/410 [00:14<01:44,  3.44it/s] 13%|█▎        | 52/410 [00:14<01:43,  3.47it/s] 13%|█▎        | 53/410 [00:15<01:42,  3.49it/s] 13%|█▎        | 54/410 [00:15<01:41,  3.50it/s] 13%|█▎        | 55/410 [00:15<01:41,  3.51it/s] 14%|█▎        | 56/410 [00:15<01:40,  3.52it/s] 14%|█▍        | 57/410 [00:16<01:40,  3.53it/s] 14%|█▍        | 58/410 [00:16<01:39,  3.53it/s] 14%|█▍        | 59/410 [00:16<01:39,  3.54it/s] 15%|█▍        | 60/410 [00:17<01:38,  3.54it/s] 15%|█▍        | 61/410 [00:17<01:41,  3.45it/s] 15%|█▌        | 62/410 [00:17<01:40,  3.47it/s] 15%|█▌        | 63/410 [00:17<01:39,  3.49it/s] 16%|█▌        | 64/410 [00:18<01:38,  3.51it/s] 16%|█▌        | 65/410 [00:18<01:38,  3.52it/s] 16%|█▌        | 66/410 [00:18<01:37,  3.52it/s] 16%|█▋        | 67/410 [00:19<01:37,  3.52it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 69/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 70/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 71/410 [00:20<01:35,  3.54it/s] 18%|█▊        | 72/410 [00:20<01:35,  3.54it/s] 18%|█▊        | 73/410 [00:20<01:35,  3.54it/s] 18%|█▊        | 74/410 [00:21<01:35,  3.54it/s] 18%|█▊        | 75/410 [00:21<01:34,  3.53it/s] 19%|█▊        | 76/410 [00:21<01:34,  3.54it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.57it/s] 19%|█▉        | 78/410 [00:22<01:32,  3.58it/s] 19%|█▉        | 79/410 [00:22<01:32,  3.59it/s] 20%|█▉        | 80/410 [00:22<01:35,  3.46it/s] 20%|█▉        | 81/410 [00:23<01:33,  3.51it/s] 20%|██        | 82/410 [00:23<01:29,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 07:10:30,780 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:10:30,780 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:10:30,780 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.26it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.30it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.24it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.57it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.95it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.43it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.56it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.72it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.82it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.85it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.87it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.52it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.28it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.27it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.42it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.54it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.65it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 40.71it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 41.91it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.85it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.41it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.68it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.92it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.35it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.11it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.20it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.45it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.53it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.65it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.53it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.58it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.51it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.43it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.33it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.32it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.44it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.64it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.62it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.66it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.57it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.55it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.38it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 40.34it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 41.67it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 42.65it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.38it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.81it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.20it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.31it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.24it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.96it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 43.88it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 43.93it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.34it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.54it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.69it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.75it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.74it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.49it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.22it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.29it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.40it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.58it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.69it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.79it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.72it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.58it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.29it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 41.96it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 42.76it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.44it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.80it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.19it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.37it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.51it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.38it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.11it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.07it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.30it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.40it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.61it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.61it/s][A 20%|██        | 82/410 [00:33<01:29,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:10:40,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 07:10:41,319 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:10:45,036 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:10:45,275 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:10:45,426 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:40<29:05,  5.34s/it] 20%|██        | 84/410 [00:40<20:45,  3.82s/it] 21%|██        | 85/410 [00:41<15:00,  2.77s/it] 21%|██        | 86/410 [00:41<10:55,  2.02s/it] 21%|██        | 87/410 [00:41<08:04,  1.50s/it] 21%|██▏       | 88/410 [00:41<06:05,  1.14s/it] 22%|██▏       | 89/410 [00:42<04:42,  1.14it/s] 22%|██▏       | 90/410 [00:42<03:44,  1.43it/s] 22%|██▏       | 91/410 [00:42<03:03,  1.74it/s] 22%|██▏       | 92/410 [00:43<02:35,  2.05it/s] 23%|██▎       | 93/410 [00:43<02:15,  2.35it/s] 23%|██▎       | 94/410 [00:43<02:00,  2.61it/s] 23%|██▎       | 95/410 [00:43<01:51,  2.83it/s] 23%|██▎       | 96/410 [00:44<01:46,  2.94it/s] 24%|██▎       | 97/410 [00:44<01:40,  3.10it/s] 24%|██▍       | 98/410 [00:44<01:36,  3.22it/s] 24%|██▍       | 99/410 [00:45<01:33,  3.31it/s] 24%|██▍       | 100/410 [00:45<01:31,  3.38it/s] 25%|██▍       | 101/410 [00:45<01:30,  3.42it/s] 25%|██▍       | 102/410 [00:45<01:29,  3.45it/s] 25%|██▌       | 103/410 [00:46<01:28,  3.48it/s] 25%|██▌       | 104/410 [00:46<01:27,  3.49it/s] 26%|██▌       | 105/410 [00:46<01:26,  3.51it/s] 26%|██▌       | 106/410 [00:46<01:26,  3.52it/s] 26%|██▌       | 107/410 [00:47<01:30,  3.36it/s] 26%|██▋       | 108/410 [00:47<01:28,  3.40it/s] 27%|██▋       | 109/410 [00:47<01:27,  3.44it/s] 27%|██▋       | 110/410 [00:48<01:26,  3.47it/s] 27%|██▋       | 111/410 [00:48<01:25,  3.49it/s] 27%|██▋       | 112/410 [00:48<01:25,  3.50it/s] 28%|██▊       | 113/410 [00:49<01:46,  2.80it/s] 28%|██▊       | 114/410 [00:49<01:39,  2.97it/s] 28%|██▊       | 115/410 [00:49<01:34,  3.12it/s] 28%|██▊       | 116/410 [00:50<01:30,  3.23it/s] 29%|██▊       | 117/410 [00:50<01:28,  3.31it/s] 29%|██▉       | 118/410 [00:50<01:26,  3.37it/s] 29%|██▉       | 119/410 [00:50<01:25,  3.42it/s] 29%|██▉       | 120/410 [00:51<01:23,  3.46it/s] 30%|██▉       | 121/410 [00:51<01:23,  3.48it/s] 30%|██▉       | 122/410 [00:51<01:22,  3.49it/s] 30%|███       | 123/410 [00:52<01:21,  3.50it/s] 30%|███       | 124/410 [00:52<01:21,  3.51it/s] 30%|███       | 125/410 [00:52<01:21,  3.52it/s] 31%|███       | 126/410 [00:52<01:20,  3.52it/s] 31%|███       | 127/410 [00:53<01:20,  3.53it/s] 31%|███       | 128/410 [00:53<01:21,  3.46it/s] 31%|███▏      | 129/410 [00:53<01:20,  3.48it/s] 32%|███▏      | 130/410 [00:54<01:20,  3.50it/s] 32%|███▏      | 131/410 [00:54<01:19,  3.51it/s] 32%|███▏      | 132/410 [00:54<01:19,  3.52it/s] 32%|███▏      | 133/410 [00:54<01:18,  3.52it/s] 33%|███▎      | 134/410 [00:55<01:18,  3.53it/s] 33%|███▎      | 135/410 [00:55<01:17,  3.53it/s] 33%|███▎      | 136/410 [00:55<01:17,  3.53it/s] 33%|███▎      | 137/410 [00:56<01:17,  3.53it/s] 34%|███▎      | 138/410 [00:56<01:17,  3.53it/s] 34%|███▍      | 139/410 [00:56<01:17,  3.48it/s] 34%|███▍      | 140/410 [00:56<01:17,  3.49it/s] 34%|███▍      | 141/410 [00:57<01:16,  3.51it/s] 35%|███▍      | 142/410 [00:57<01:16,  3.52it/s] 35%|███▍      | 143/410 [00:57<01:15,  3.52it/s] 35%|███▌      | 144/410 [00:58<01:15,  3.53it/s] 35%|███▌      | 145/410 [00:58<01:15,  3.53it/s] 36%|███▌      | 146/410 [00:58<01:14,  3.53it/s] 36%|███▌      | 147/410 [00:58<01:14,  3.54it/s] 36%|███▌      | 148/410 [00:59<01:13,  3.54it/s] 36%|███▋      | 149/410 [00:59<01:13,  3.54it/s] 37%|███▋      | 150/410 [00:59<01:14,  3.49it/s] 37%|███▋      | 151/410 [01:00<01:13,  3.51it/s] 37%|███▋      | 152/410 [01:00<01:13,  3.52it/s] 37%|███▋      | 153/410 [01:00<01:12,  3.53it/s] 38%|███▊      | 154/410 [01:00<01:12,  3.53it/s] 38%|███▊      | 155/410 [01:01<01:12,  3.53it/s] 38%|███▊      | 156/410 [01:01<01:11,  3.54it/s] 38%|███▊      | 157/410 [01:01<01:11,  3.56it/s] 39%|███▊      | 158/410 [01:02<01:10,  3.58it/s] 39%|███▉      | 159/410 [01:02<01:09,  3.59it/s] 39%|███▉      | 160/410 [01:02<01:09,  3.60it/s] 39%|███▉      | 161/410 [01:02<01:11,  3.48it/s] 40%|███▉      | 162/410 [01:03<01:10,  3.52it/s] 40%|███▉      | 163/410 [01:03<01:09,  3.55it/s] 40%|████      | 164/410 [01:03<01:06,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 07:11:11,192 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:11:11,192 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:11:11,192 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.9141, 'eval_samples_per_second': 352.829, 'eval_steps_per_second': 44.179, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.20it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.86it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.96it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.89it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.31it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.89it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.74it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.60it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.67it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.64it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.73it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.53it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.44it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.35it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.36it/s][A
 21%|██        | 92/438 [00:02<00:07, 43.77it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.08it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 42.11it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.46it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.96it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.01it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.10it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.04it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.25it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.41it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.44it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.53it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.65it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.63it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.49it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.45it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.44it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.46it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.53it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.51it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.58it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.63it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.70it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.53it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.39it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.27it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 42.94it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 43.53it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 43.93it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.16it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.28it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.39it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.30it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.32it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.14it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.20it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.36it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.50it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.68it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.72it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.61it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.52it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.37it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.37it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.38it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.45it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.59it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.65it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.73it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.63it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.48it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.38it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.41it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.80it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.07it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.32it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.47it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.56it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.50it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.34it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.28it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.24it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.31it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.46it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.55it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.68it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.84it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.78it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.62it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:13<01:06,  3.72it/s]
100%|██████████| 438/438 [00:09<00:00, 44.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:11:21,530 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 07:11:21,705 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:11:24,488 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:11:24,571 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:11:24,620 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:18<18:27,  4.52s/it] 40%|████      | 166/410 [01:18<13:12,  3.25s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:18<09:33,  2.36s/it] 41%|████      | 168/410 [01:18<07:00,  1.74s/it] 41%|████      | 169/410 [01:19<05:13,  1.30s/it] 41%|████▏     | 170/410 [01:19<03:58,  1.00it/s] 42%|████▏     | 171/410 [01:19<03:06,  1.28it/s] 42%|████▏     | 172/410 [01:20<02:30,  1.58it/s] 42%|████▏     | 173/410 [01:20<02:07,  1.87it/s] 42%|████▏     | 174/410 [01:20<01:48,  2.17it/s] 43%|████▎     | 175/410 [01:20<01:35,  2.46it/s] 43%|████▎     | 176/410 [01:21<01:26,  2.71it/s] 43%|████▎     | 177/410 [01:21<01:20,  2.90it/s] 43%|████▎     | 178/410 [01:21<01:15,  3.06it/s] 44%|████▎     | 179/410 [01:22<01:12,  3.19it/s] 44%|████▍     | 180/410 [01:22<01:09,  3.29it/s] 44%|████▍     | 181/410 [01:22<01:08,  3.36it/s] 44%|████▍     | 182/410 [01:22<01:06,  3.41it/s] 45%|████▍     | 183/410 [01:23<01:05,  3.44it/s] 45%|████▍     | 184/410 [01:23<01:05,  3.46it/s] 45%|████▌     | 185/410 [01:23<01:04,  3.48it/s] 45%|████▌     | 186/410 [01:24<01:05,  3.43it/s] 46%|████▌     | 187/410 [01:24<01:04,  3.46it/s] 46%|████▌     | 188/410 [01:24<01:03,  3.48it/s] 46%|████▌     | 189/410 [01:24<01:03,  3.50it/s] 46%|████▋     | 190/410 [01:25<01:02,  3.51it/s] 47%|████▋     | 191/410 [01:25<01:02,  3.51it/s] 47%|████▋     | 192/410 [01:25<01:01,  3.52it/s] 47%|████▋     | 193/410 [01:26<01:01,  3.53it/s] 47%|████▋     | 194/410 [01:26<01:01,  3.53it/s] 48%|████▊     | 195/410 [01:26<01:00,  3.53it/s] 48%|████▊     | 196/410 [01:26<01:00,  3.52it/s] 48%|████▊     | 197/410 [01:27<01:04,  3.28it/s] 48%|████▊     | 198/410 [01:27<01:03,  3.35it/s] 49%|████▊     | 199/410 [01:27<01:01,  3.41it/s] 49%|████▉     | 200/410 [01:28<01:01,  3.44it/s] 49%|████▉     | 201/410 [01:28<01:00,  3.47it/s] 49%|████▉     | 202/410 [01:28<00:59,  3.49it/s] 50%|████▉     | 203/410 [01:29<00:59,  3.50it/s] 50%|████▉     | 204/410 [01:29<00:58,  3.51it/s] 50%|█████     | 205/410 [01:29<00:58,  3.52it/s] 50%|█████     | 206/410 [01:29<00:57,  3.52it/s] 50%|█████     | 207/410 [01:30<00:57,  3.53it/s] 51%|█████     | 208/410 [01:30<01:00,  3.32it/s] 51%|█████     | 209/410 [01:30<00:59,  3.38it/s] 51%|█████     | 210/410 [01:31<00:58,  3.42it/s] 51%|█████▏    | 211/410 [01:31<00:57,  3.45it/s] 52%|█████▏    | 212/410 [01:31<00:56,  3.48it/s] 52%|█████▏    | 213/410 [01:31<00:56,  3.49it/s] 52%|█████▏    | 214/410 [01:32<00:55,  3.51it/s] 52%|█████▏    | 215/410 [01:32<00:55,  3.52it/s] 53%|█████▎    | 216/410 [01:32<00:55,  3.52it/s] 53%|█████▎    | 217/410 [01:33<00:54,  3.53it/s] 53%|█████▎    | 218/410 [01:33<00:54,  3.53it/s] 53%|█████▎    | 219/410 [01:33<00:56,  3.37it/s] 54%|█████▎    | 220/410 [01:33<00:55,  3.42it/s] 54%|█████▍    | 221/410 [01:34<00:54,  3.46it/s] 54%|█████▍    | 222/410 [01:34<00:53,  3.48it/s] 54%|█████▍    | 223/410 [01:34<00:53,  3.50it/s] 55%|█████▍    | 224/410 [01:35<00:53,  3.51it/s] 55%|█████▍    | 225/410 [01:35<00:52,  3.52it/s] 55%|█████▌    | 226/410 [01:35<00:52,  3.53it/s] 55%|█████▌    | 227/410 [01:35<00:51,  3.53it/s] 56%|█████▌    | 228/410 [01:36<00:51,  3.54it/s] 56%|█████▌    | 229/410 [01:36<00:51,  3.54it/s] 56%|█████▌    | 230/410 [01:36<00:53,  3.36it/s] 56%|█████▋    | 231/410 [01:37<00:52,  3.41it/s] 57%|█████▋    | 232/410 [01:37<00:51,  3.45it/s] 57%|█████▋    | 233/410 [01:37<00:50,  3.48it/s] 57%|█████▋    | 234/410 [01:37<00:50,  3.49it/s] 57%|█████▋    | 235/410 [01:38<00:49,  3.50it/s] 58%|█████▊    | 236/410 [01:38<00:49,  3.51it/s] 58%|█████▊    | 237/410 [01:38<00:49,  3.52it/s] 58%|█████▊    | 238/410 [01:39<00:48,  3.52it/s] 58%|█████▊    | 239/410 [01:39<00:48,  3.53it/s] 59%|█████▊    | 240/410 [01:39<00:48,  3.53it/s] 59%|█████▉    | 241/410 [01:39<00:50,  3.35it/s] 59%|█████▉    | 242/410 [01:40<00:49,  3.41it/s] 59%|█████▉    | 243/410 [01:40<00:48,  3.45it/s] 60%|█████▉    | 244/410 [01:40<00:47,  3.48it/s] 60%|█████▉    | 245/410 [01:41<00:47,  3.49it/s] 60%|██████    | 246/410 [01:41<00:44,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 07:11:48,837 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:11:48,838 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:11:48,838 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8638, 'eval_samples_per_second': 354.631, 'eval_steps_per_second': 44.405, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.45it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.50it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.32it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.44it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.66it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.13it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.36it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.51it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.71it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.90it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.04it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.26it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.15it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.14it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.24it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.49it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.72it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.66it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.39it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.17it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.18it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.27it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.45it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.51it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.73it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.77it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.75it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.52it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.24it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.15it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.37it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.59it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.96it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.18it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.15it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 44.14it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.06it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.20it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.24it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.45it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.55it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.62it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.64it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.45it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.38it/s][A
 61%|██████    | 267/438 [00:06<00:04, 42.41it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.65it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.02it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.21it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.56it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.44it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.22it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.11it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.26it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.40it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.49it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.57it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 42.71it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 41.57it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 30.51it/s][A
 80%|████████  | 352/438 [00:08<00:02, 33.89it/s][A
 82%|████████▏ | 357/438 [00:08<00:02, 36.62it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 38.76it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 40.47it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 41.76it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 42.67it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 43.27it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.39it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.34it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 43.45it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.61it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.91it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.49it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.74it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.75it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.75it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.44it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.12it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [01:51<00:44,  3.65it/s]
100%|██████████| 438/438 [00:10<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:11:58,967 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 07:11:59,059 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:12:02,109 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:12:02,258 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:12:02,318 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:55<12:24,  4.57s/it] 60%|██████    | 248/410 [01:56<08:51,  3.28s/it] 61%|██████    | 249/410 [01:56<06:23,  2.38s/it] 61%|██████    | 250/410 [01:56<04:40,  1.75s/it] 61%|██████    | 251/410 [01:57<03:28,  1.31s/it] 61%|██████▏   | 252/410 [01:57<02:38,  1.00s/it] 62%|██████▏   | 253/410 [01:57<02:03,  1.27it/s] 62%|██████▏   | 254/410 [01:57<01:41,  1.53it/s] 62%|██████▏   | 255/410 [01:58<01:23,  1.85it/s] 62%|██████▏   | 256/410 [01:58<01:11,  2.15it/s] 63%|██████▎   | 257/410 [01:58<01:02,  2.44it/s] 63%|██████▎   | 258/410 [01:59<00:56,  2.69it/s] 63%|██████▎   | 259/410 [01:59<00:52,  2.89it/s] 63%|██████▎   | 260/410 [01:59<00:48,  3.06it/s] 64%|██████▎   | 261/410 [01:59<00:46,  3.19it/s] 64%|██████▍   | 262/410 [02:00<00:44,  3.29it/s] 64%|██████▍   | 263/410 [02:00<00:43,  3.36it/s] 64%|██████▍   | 264/410 [02:00<00:42,  3.41it/s] 65%|██████▍   | 265/410 [02:01<00:43,  3.32it/s] 65%|██████▍   | 266/410 [02:01<00:42,  3.38it/s] 65%|██████▌   | 267/410 [02:01<00:41,  3.42it/s] 65%|██████▌   | 268/410 [02:01<00:41,  3.46it/s] 66%|██████▌   | 269/410 [02:02<00:40,  3.48it/s] 66%|██████▌   | 270/410 [02:02<00:40,  3.50it/s] 66%|██████▌   | 271/410 [02:02<00:39,  3.51it/s] 66%|██████▋   | 272/410 [02:03<00:39,  3.51it/s] 67%|██████▋   | 273/410 [02:03<00:38,  3.52it/s] 67%|██████▋   | 274/410 [02:03<00:38,  3.53it/s] 67%|██████▋   | 275/410 [02:03<00:38,  3.53it/s] 67%|██████▋   | 276/410 [02:04<00:39,  3.41it/s] 68%|██████▊   | 277/410 [02:04<00:38,  3.44it/s] 68%|██████▊   | 278/410 [02:04<00:38,  3.47it/s] 68%|██████▊   | 279/410 [02:05<00:37,  3.49it/s] 68%|██████▊   | 280/410 [02:05<00:37,  3.50it/s] 69%|██████▊   | 281/410 [02:05<00:36,  3.51it/s] 69%|██████▉   | 282/410 [02:05<00:36,  3.51it/s] 69%|██████▉   | 283/410 [02:06<00:36,  3.52it/s] 69%|██████▉   | 284/410 [02:06<00:35,  3.52it/s] 70%|██████▉   | 285/410 [02:06<00:35,  3.52it/s] 70%|██████▉   | 286/410 [02:07<00:35,  3.52it/s] 70%|███████   | 287/410 [02:07<00:35,  3.44it/s] 70%|███████   | 288/410 [02:07<00:35,  3.47it/s] 70%|███████   | 289/410 [02:07<00:34,  3.49it/s] 71%|███████   | 290/410 [02:08<00:34,  3.51it/s] 71%|███████   | 291/410 [02:08<00:33,  3.51it/s] 71%|███████   | 292/410 [02:08<00:33,  3.51it/s] 71%|███████▏  | 293/410 [02:09<00:33,  3.52it/s] 72%|███████▏  | 294/410 [02:09<00:32,  3.52it/s] 72%|███████▏  | 295/410 [02:09<00:32,  3.53it/s] 72%|███████▏  | 296/410 [02:09<00:32,  3.53it/s] 72%|███████▏  | 297/410 [02:10<00:32,  3.53it/s] 73%|███████▎  | 298/410 [02:10<00:32,  3.46it/s] 73%|███████▎  | 299/410 [02:10<00:31,  3.48it/s] 73%|███████▎  | 300/410 [02:11<00:31,  3.50it/s] 73%|███████▎  | 301/410 [02:11<00:31,  3.51it/s] 74%|███████▎  | 302/410 [02:11<00:30,  3.52it/s] 74%|███████▍  | 303/410 [02:11<00:30,  3.53it/s] 74%|███████▍  | 304/410 [02:12<00:29,  3.54it/s] 74%|███████▍  | 305/410 [02:12<00:29,  3.56it/s] 75%|███████▍  | 306/410 [02:12<00:28,  3.59it/s] 75%|███████▍  | 307/410 [02:13<00:28,  3.59it/s] 75%|███████▌  | 308/410 [02:13<00:28,  3.60it/s] 75%|███████▌  | 309/410 [02:13<00:28,  3.54it/s] 76%|███████▌  | 310/410 [02:13<00:28,  3.57it/s] 76%|███████▌  | 311/410 [02:14<00:27,  3.59it/s] 76%|███████▌  | 312/410 [02:14<00:27,  3.60it/s] 76%|███████▋  | 313/410 [02:14<00:26,  3.60it/s] 77%|███████▋  | 314/410 [02:14<00:26,  3.60it/s] 77%|███████▋  | 315/410 [02:15<00:26,  3.61it/s] 77%|███████▋  | 316/410 [02:15<00:26,  3.61it/s] 77%|███████▋  | 317/410 [02:15<00:25,  3.61it/s] 78%|███████▊  | 318/410 [02:16<00:25,  3.61it/s] 78%|███████▊  | 319/410 [02:16<00:25,  3.61it/s] 78%|███████▊  | 320/410 [02:16<00:25,  3.50it/s] 78%|███████▊  | 321/410 [02:16<00:25,  3.53it/s] 79%|███████▊  | 322/410 [02:17<00:24,  3.55it/s] 79%|███████▉  | 323/410 [02:17<00:24,  3.57it/s] 79%|███████▉  | 324/410 [02:17<00:23,  3.59it/s] 79%|███████▉  | 325/410 [02:18<00:23,  3.60it/s] 80%|███████▉  | 326/410 [02:18<00:23,  3.60it/s] 80%|███████▉  | 327/410 [02:18<00:23,  3.60it/s] 80%|████████  | 328/410 [02:18<00:21,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 07:12:26,363 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:12:26,363 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:12:26,363 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 10.0423, 'eval_samples_per_second': 348.328, 'eval_steps_per_second': 43.616, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.00it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.25it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.19it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.48it/s][A
  7%|▋         | 32/438 [00:00<00:09, 42.29it/s][A
  8%|▊         | 37/438 [00:00<00:09, 42.80it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.30it/s][A
 11%|█         | 47/438 [00:01<00:08, 43.77it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.20it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.43it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.54it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.50it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.08it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.05it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.16it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.34it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.57it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.70it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.74it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.62it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.41it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.29it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.28it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.35it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.50it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.51it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.68it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.65it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.51it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.33it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.23it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.25it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.30it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.38it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.66it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.49it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.68it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.47it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.33it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.32it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.41it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.33it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.48it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.59it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.73it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.66it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.49it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.42it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.46it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.39it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.34it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.52it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.65it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.68it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.68it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 42.76it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.26it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.65it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.76it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.92it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.16it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.39it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.47it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.27it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.29it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.45it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.32it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.33it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.35it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.42it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.49it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.57it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.47it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.48it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.50it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.44it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.43it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.44it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.45it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.61it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.57it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.49it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 40.21it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 41.55it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 42.52it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 42.52it/s][A 80%|████████  | 328/410 [02:28<00:21,  3.76it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:12:36,679 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 07:12:37,059 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:12:40,876 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:12:41,147 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:12:41,281 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:36<07:26,  5.51s/it] 80%|████████  | 330/410 [02:36<05:16,  3.96s/it] 81%|████████  | 331/410 [02:37<03:45,  2.85s/it] 81%|████████  | 332/410 [02:37<02:42,  2.08s/it] 81%|████████  | 333/410 [02:37<01:58,  1.54s/it] 81%|████████▏ | 334/410 [02:38<01:28,  1.16s/it] 82%|████████▏ | 335/410 [02:38<01:07,  1.11it/s] 82%|████████▏ | 336/410 [02:38<00:52,  1.40it/s] 82%|████████▏ | 337/410 [02:38<00:42,  1.71it/s] 82%|████████▏ | 338/410 [02:39<00:35,  2.02it/s] 83%|████████▎ | 339/410 [02:39<00:30,  2.32it/s] 83%|████████▎ | 340/410 [02:39<00:27,  2.58it/s] 83%|████████▎ | 341/410 [02:40<00:25,  2.70it/s] 83%|████████▎ | 342/410 [02:40<00:23,  2.91it/s] 84%|████████▎ | 343/410 [02:40<00:21,  3.07it/s] 84%|████████▍ | 344/410 [02:40<00:20,  3.20it/s] 84%|████████▍ | 345/410 [02:41<00:19,  3.29it/s] 84%|████████▍ | 346/410 [02:41<00:19,  3.36it/s] 85%|████████▍ | 347/410 [02:41<00:18,  3.41it/s] 85%|████████▍ | 348/410 [02:42<00:17,  3.45it/s] 85%|████████▌ | 349/410 [02:42<00:17,  3.47it/s] 85%|████████▌ | 350/410 [02:42<00:17,  3.49it/s] 86%|████████▌ | 351/410 [02:42<00:16,  3.51it/s] 86%|████████▌ | 352/410 [02:43<00:17,  3.40it/s] 86%|████████▌ | 353/410 [02:43<00:16,  3.44it/s] 86%|████████▋ | 354/410 [02:43<00:16,  3.46it/s] 87%|████████▋ | 355/410 [02:44<00:15,  3.48it/s] 87%|████████▋ | 356/410 [02:44<00:15,  3.49it/s] 87%|████████▋ | 357/410 [02:44<00:15,  3.50it/s] 87%|████████▋ | 358/410 [02:44<00:14,  3.51it/s] 88%|████████▊ | 359/410 [02:45<00:14,  3.52it/s] 88%|████████▊ | 360/410 [02:45<00:14,  3.53it/s] 88%|████████▊ | 361/410 [02:45<00:13,  3.54it/s] 88%|████████▊ | 362/410 [02:46<00:13,  3.54it/s] 89%|████████▊ | 363/410 [02:46<00:13,  3.44it/s] 89%|████████▉ | 364/410 [02:46<00:13,  3.47it/s] 89%|████████▉ | 365/410 [02:46<00:12,  3.49it/s] 89%|████████▉ | 366/410 [02:47<00:12,  3.51it/s] 90%|████████▉ | 367/410 [02:47<00:12,  3.45it/s] 90%|████████▉ | 368/410 [02:47<00:12,  3.47it/s] 90%|█████████ | 369/410 [02:48<00:11,  3.49it/s] 90%|█████████ | 370/410 [02:48<00:11,  3.51it/s] 90%|█████████ | 371/410 [02:48<00:11,  3.54it/s] 91%|█████████ | 372/410 [02:48<00:10,  3.56it/s] 91%|█████████ | 373/410 [02:49<00:12,  3.00it/s] 91%|█████████ | 374/410 [02:49<00:11,  3.11it/s] 91%|█████████▏| 375/410 [02:49<00:10,  3.25it/s] 92%|█████████▏| 376/410 [02:50<00:10,  3.35it/s] 92%|█████████▏| 377/410 [02:50<00:09,  3.43it/s] 92%|█████████▏| 378/410 [02:50<00:09,  3.48it/s] 92%|█████████▏| 379/410 [02:51<00:08,  3.52it/s] 93%|█████████▎| 380/410 [02:51<00:08,  3.55it/s] 93%|█████████▎| 381/410 [02:51<00:08,  3.57it/s] 93%|█████████▎| 382/410 [02:51<00:07,  3.58it/s] 93%|█████████▎| 383/410 [02:52<00:07,  3.59it/s] 94%|█████████▎| 384/410 [02:52<00:07,  3.60it/s] 94%|█████████▍| 385/410 [02:52<00:07,  3.54it/s] 94%|█████████▍| 386/410 [02:53<00:06,  3.57it/s] 94%|█████████▍| 387/410 [02:53<00:06,  3.57it/s] 95%|█████████▍| 388/410 [02:53<00:06,  3.58it/s] 95%|█████████▍| 389/410 [02:53<00:05,  3.59it/s] 95%|█████████▌| 390/410 [02:54<00:05,  3.59it/s] 95%|█████████▌| 391/410 [02:54<00:05,  3.60it/s] 96%|█████████▌| 392/410 [02:54<00:04,  3.61it/s] 96%|█████████▌| 393/410 [02:54<00:04,  3.61it/s] 96%|█████████▌| 394/410 [02:55<00:04,  3.61it/s] 96%|█████████▋| 395/410 [02:55<00:04,  3.61it/s] 97%|█████████▋| 396/410 [02:55<00:03,  3.61it/s] 97%|█████████▋| 397/410 [02:56<00:03,  3.61it/s] 97%|█████████▋| 398/410 [02:56<00:03,  3.58it/s] 97%|█████████▋| 399/410 [02:56<00:03,  3.60it/s] 98%|█████████▊| 400/410 [02:56<00:02,  3.60it/s] 98%|█████████▊| 401/410 [02:57<00:02,  3.61it/s] 98%|█████████▊| 402/410 [02:57<00:02,  3.61it/s] 98%|█████████▊| 403/410 [02:57<00:01,  3.61it/s] 99%|█████████▊| 404/410 [02:57<00:01,  3.61it/s] 99%|█████████▉| 405/410 [02:58<00:01,  3.61it/s] 99%|█████████▉| 406/410 [02:58<00:01,  3.62it/s] 99%|█████████▉| 407/410 [02:58<00:00,  3.61it/s]100%|█████████▉| 408/410 [02:59<00:00,  3.61it/s]100%|█████████▉| 409/410 [02:59<00:00,  3.49it/s]100%|██████████| 410/410 [02:59<00:00,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 07:13:07,158 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:13:07,158 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:13:07,159 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.9089, 'eval_samples_per_second': 353.017, 'eval_steps_per_second': 44.203, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.94it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.96it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.13it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.94it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.13it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.80it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.58it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.42it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.61it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.74it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.85it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.73it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.36it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.35it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.25it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.37it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.55it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.67it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.79it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.72it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.97it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.33it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.57it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.78it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.05it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.33it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.42it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.42it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.42it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.34it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.33it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.34it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.42it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.50it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.67it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.66it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.62it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.45it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.48it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.40it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.33it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.44it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.48it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.64it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.62it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.57it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.18it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.29it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.28it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.31it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.47it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.88it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.43it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.37it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.49it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.43it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.42it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.35it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.31it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.51it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.50it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.53it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.49it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.50it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.55it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.42it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.40it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.47it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.64it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.57it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.53it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.57it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.88it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.08it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.05it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.13it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.31it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.43it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.48it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.53it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.45it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.48it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.49it/s][A                                                 
                                                 [A100%|██████████| 410/410 [03:09<00:00,  3.67it/s]
100%|██████████| 438/438 [00:09<00:00, 44.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:13:17,087 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 07:13:17,223 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:13:19,935 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:13:20,043 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:13:20,098 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 07:13:20,773 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 07:13:20,774 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82 (score: 1.0478343963623047).
                                                 100%|██████████| 410/410 [03:19<00:00,  3.67it/s]100%|██████████| 410/410 [03:19<00:00,  2.05it/s]
[INFO|trainer.py:1894] 2023-08-28 07:13:27,073 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 07:13:27,143 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:13:29,510 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:13:29,575 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:13:29,612 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:13:29,974 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   train_runtime            = 0:03:19.55
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   train_samples_per_second =     131.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:29,975 >>   train_steps_per_second   =      2.055
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8561, 'eval_samples_per_second': 354.905, 'eval_steps_per_second': 44.439, 'epoch': 5.0}
{'train_runtime': 199.5503, 'train_samples_per_second': 131.27, 'train_steps_per_second': 2.055, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 07:13:30 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 07:13:30,184 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:13:30,184 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 07:13:30,184 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.30it/s]  3%|▎         | 12/438 [00:00<00:08, 48.56it/s]  4%|▍         | 17/438 [00:00<00:08, 47.19it/s]  5%|▌         | 22/438 [00:00<00:08, 46.32it/s]  6%|▌         | 27/438 [00:00<00:09, 45.61it/s]  7%|▋         | 32/438 [00:00<00:08, 45.60it/s]  8%|▊         | 37/438 [00:00<00:08, 45.44it/s] 10%|▉         | 42/438 [00:00<00:08, 44.93it/s] 11%|█         | 47/438 [00:01<00:08, 44.54it/s] 12%|█▏        | 52/438 [00:01<00:09, 42.58it/s] 13%|█▎        | 57/438 [00:01<00:08, 43.23it/s] 14%|█▍        | 62/438 [00:01<00:08, 43.67it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.09it/s] 16%|█▋        | 72/438 [00:01<00:08, 44.36it/s] 18%|█▊        | 77/438 [00:01<00:08, 44.52it/s] 19%|█▊        | 82/438 [00:01<00:07, 44.68it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.45it/s] 21%|██        | 92/438 [00:02<00:07, 44.10it/s] 22%|██▏       | 97/438 [00:02<00:07, 43.90it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.16it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.33it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.50it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.69it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.81it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.77it/s] 30%|███       | 132/438 [00:02<00:06, 44.50it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.26it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.17it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.25it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.37it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.45it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.61it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.78it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.82it/s] 40%|████      | 177/438 [00:03<00:05, 44.62it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.43it/s] 43%|████▎     | 187/438 [00:04<00:06, 41.23it/s] 44%|████▍     | 192/438 [00:04<00:05, 42.36it/s] 45%|████▍     | 197/438 [00:04<00:05, 43.21it/s] 46%|████▌     | 202/438 [00:04<00:05, 43.81it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.22it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.38it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.57it/s] 51%|█████     | 222/438 [00:04<00:04, 44.53it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.23it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.09it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.31it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.57it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.63it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.72it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.88it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.89it/s] 61%|██████    | 267/438 [00:06<00:03, 44.87it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.54it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.36it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.44it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.51it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.63it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.74it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.93it/s] 70%|███████   | 307/438 [00:06<00:02, 44.88it/s] 71%|███████   | 312/438 [00:07<00:02, 44.81it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.60it/s] 74%|███████▎  | 322/438 [00:07<00:02, 39.96it/s] 75%|███████▍  | 327/438 [00:07<00:02, 41.44it/s] 76%|███████▌  | 332/438 [00:07<00:02, 42.49it/s] 77%|███████▋  | 337/438 [00:07<00:02, 43.31it/s] 78%|███████▊  | 342/438 [00:07<00:02, 43.85it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.13it/s] 80%|████████  | 352/438 [00:07<00:01, 44.33it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.29it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.06it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.03it/s] 85%|████████▍ | 372/438 [00:08<00:01, 44.17it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.46it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.68it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.84it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.85it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.85it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.68it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.48it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.37it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.28it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.46it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.39it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.82it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.89it/s]100%|██████████| 438/438 [00:09<00:00, 44.36it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:13:40,078 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   eval_loss               =     1.0478
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   eval_runtime            = 0:00:09.89
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   eval_samples_per_second =    353.552
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   eval_steps_per_second   =      44.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:13:40,078 >>   perplexity              =     2.8515
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:53,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:53,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:53,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:53,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:53,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:13:54,432 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:13:54,433 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:13:55,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:13:56,194 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:13:56,194 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:59,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:59,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:59,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:59,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:59,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:14:00,275 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:14:00,276 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:14:00,898 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:14:01,129 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:14:01,129 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.16it/s]Extractor Predicting: 4it [00:03,  1.28it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.45it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:07,  1.46it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.48it/s]Extractor Predicting: 20it [00:14,  1.48it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.47it/s]Extractor Predicting: 25it [00:17,  1.44it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:21,  1.37it/s]Extractor Predicting: 32it [00:22,  1.33it/s]Extractor Predicting: 33it [00:23,  1.33it/s]Extractor Predicting: 34it [00:24,  1.31it/s]Extractor Predicting: 35it [00:25,  1.31it/s]Extractor Predicting: 36it [00:25,  1.33it/s]Extractor Predicting: 37it [00:26,  1.29it/s]Extractor Predicting: 38it [00:27,  1.27it/s]Extractor Predicting: 39it [00:28,  1.29it/s]Extractor Predicting: 40it [00:28,  1.29it/s]Extractor Predicting: 41it [00:29,  1.29it/s]Extractor Predicting: 42it [00:30,  1.27it/s]Extractor Predicting: 43it [00:31,  1.27it/s]Extractor Predicting: 44it [00:32,  1.27it/s]Extractor Predicting: 45it [00:32,  1.26it/s]Extractor Predicting: 46it [00:33,  1.25it/s]Extractor Predicting: 47it [00:34,  1.27it/s]Extractor Predicting: 48it [00:35,  1.28it/s]Extractor Predicting: 49it [00:36,  1.26it/s]Extractor Predicting: 50it [00:36,  1.26it/s]Extractor Predicting: 51it [00:37,  1.27it/s]Extractor Predicting: 52it [00:38,  1.27it/s]Extractor Predicting: 53it [00:39,  1.24it/s]Extractor Predicting: 54it [00:39,  1.28it/s]Extractor Predicting: 55it [00:40,  1.29it/s]Extractor Predicting: 56it [00:41,  1.30it/s]Extractor Predicting: 57it [00:42,  1.27it/s]Extractor Predicting: 58it [00:43,  1.26it/s]Extractor Predicting: 59it [00:43,  1.25it/s]Extractor Predicting: 60it [00:44,  1.33it/s]Extractor Predicting: 61it [00:45,  1.33it/s]Extractor Predicting: 62it [00:46,  1.33it/s]Extractor Predicting: 63it [00:46,  1.32it/s]Extractor Predicting: 64it [00:47,  1.35it/s]Extractor Predicting: 65it [00:48,  1.38it/s]Extractor Predicting: 66it [00:48,  1.35it/s]Extractor Predicting: 67it [00:49,  1.36it/s]Extractor Predicting: 68it [00:50,  1.34it/s]Extractor Predicting: 69it [00:51,  1.33it/s]Extractor Predicting: 70it [00:51,  1.35it/s]Extractor Predicting: 71it [00:52,  1.36it/s]Extractor Predicting: 72it [00:53,  1.38it/s]Extractor Predicting: 73it [00:54,  1.40it/s]Extractor Predicting: 74it [00:54,  1.43it/s]Extractor Predicting: 75it [00:55,  1.43it/s]Extractor Predicting: 76it [00:56,  1.42it/s]Extractor Predicting: 77it [00:56,  1.42it/s]Extractor Predicting: 78it [00:57,  1.43it/s]Extractor Predicting: 79it [00:58,  1.42it/s]Extractor Predicting: 80it [00:59,  1.39it/s]Extractor Predicting: 81it [00:59,  1.38it/s]Extractor Predicting: 82it [01:00,  1.38it/s]Extractor Predicting: 83it [01:01,  1.41it/s]Extractor Predicting: 84it [01:01,  1.39it/s]Extractor Predicting: 85it [01:02,  1.40it/s]Extractor Predicting: 86it [01:03,  1.36it/s]Extractor Predicting: 87it [01:04,  1.36it/s]Extractor Predicting: 88it [01:04,  1.41it/s]Extractor Predicting: 89it [01:05,  1.46it/s]Extractor Predicting: 90it [01:06,  1.45it/s]Extractor Predicting: 91it [01:06,  1.47it/s]Extractor Predicting: 92it [01:07,  1.51it/s]Extractor Predicting: 93it [01:08,  1.52it/s]Extractor Predicting: 94it [01:08,  1.55it/s]Extractor Predicting: 95it [01:09,  1.54it/s]Extractor Predicting: 96it [01:10,  1.48it/s]Extractor Predicting: 97it [01:10,  1.50it/s]Extractor Predicting: 98it [01:11,  1.52it/s]Extractor Predicting: 99it [01:11,  1.51it/s]Extractor Predicting: 100it [01:12,  1.48it/s]Extractor Predicting: 101it [01:13,  1.46it/s]Extractor Predicting: 102it [01:14,  1.45it/s]Extractor Predicting: 103it [01:14,  1.47it/s]Extractor Predicting: 104it [01:15,  1.37it/s]Extractor Predicting: 105it [01:16,  1.38it/s]Extractor Predicting: 106it [01:16,  1.41it/s]Extractor Predicting: 107it [01:17,  1.40it/s]Extractor Predicting: 108it [01:18,  1.45it/s]Extractor Predicting: 109it [01:18,  1.47it/s]Extractor Predicting: 110it [01:19,  1.51it/s]Extractor Predicting: 111it [01:20,  1.49it/s]Extractor Predicting: 112it [01:20,  1.52it/s]Extractor Predicting: 113it [01:21,  1.51it/s]Extractor Predicting: 114it [01:22,  1.48it/s]Extractor Predicting: 115it [01:22,  1.48it/s]Extractor Predicting: 116it [01:23,  1.43it/s]Extractor Predicting: 117it [01:24,  1.40it/s]Extractor Predicting: 118it [01:25,  1.40it/s]Extractor Predicting: 119it [01:25,  1.38it/s]Extractor Predicting: 120it [01:26,  1.35it/s]Extractor Predicting: 121it [01:27,  1.32it/s]Extractor Predicting: 122it [01:28,  1.34it/s]Extractor Predicting: 123it [01:29,  1.32it/s]Extractor Predicting: 124it [01:29,  1.32it/s]Extractor Predicting: 125it [01:30,  1.22it/s]Extractor Predicting: 126it [01:31,  1.24it/s]Extractor Predicting: 127it [01:32,  1.25it/s]Extractor Predicting: 128it [01:33,  1.29it/s]Extractor Predicting: 129it [01:33,  1.30it/s]Extractor Predicting: 130it [01:34,  1.32it/s]Extractor Predicting: 131it [01:35,  1.31it/s]Extractor Predicting: 132it [01:36,  1.30it/s]Extractor Predicting: 133it [01:36,  1.30it/s]Extractor Predicting: 134it [01:37,  1.30it/s]Extractor Predicting: 135it [01:38,  1.31it/s]Extractor Predicting: 136it [01:39,  1.30it/s]Extractor Predicting: 137it [01:39,  1.30it/s]Extractor Predicting: 138it [01:40,  1.29it/s]Extractor Predicting: 139it [01:41,  1.32it/s]Extractor Predicting: 140it [01:42,  1.34it/s]Extractor Predicting: 141it [01:42,  1.32it/s]Extractor Predicting: 142it [01:43,  1.30it/s]Extractor Predicting: 143it [01:44,  1.32it/s]Extractor Predicting: 144it [01:45,  1.32it/s]Extractor Predicting: 145it [01:45,  1.76it/s]Extractor Predicting: 145it [01:45,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:15:57,637 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:15:57,667 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:15:57,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:15:57,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:15:57,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:15:58,436 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:15:58,437 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:15:59,032 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:16:00,142 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:16:00,142 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:16:03,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:16:03,265 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:16:03,265 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:16:03,265 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:16:03,265 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:16:04,034 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:16:04,035 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:16:04,645 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:16:04,835 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:16:04,839 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.35it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.37it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:08,  1.38it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:12,  1.39it/s]Extractor Predicting: 18it [00:13,  1.32it/s]Extractor Predicting: 19it [00:13,  1.32it/s]Extractor Predicting: 20it [00:14,  1.33it/s]Extractor Predicting: 21it [00:15,  1.34it/s]Extractor Predicting: 22it [00:16,  1.31it/s]Extractor Predicting: 23it [00:16,  1.34it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:18,  1.39it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.34it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.38it/s]Extractor Predicting: 33it [00:24,  1.39it/s]Extractor Predicting: 34it [00:24,  1.41it/s]Extractor Predicting: 35it [00:25,  1.43it/s]Extractor Predicting: 36it [00:26,  1.38it/s]Extractor Predicting: 37it [00:27,  1.40it/s]Extractor Predicting: 38it [00:27,  1.41it/s]Extractor Predicting: 39it [00:28,  1.40it/s]Extractor Predicting: 40it [00:29,  1.41it/s]Extractor Predicting: 41it [00:29,  1.42it/s]Extractor Predicting: 42it [00:30,  1.43it/s]Extractor Predicting: 43it [00:31,  1.43it/s]Extractor Predicting: 44it [00:31,  1.48it/s]Extractor Predicting: 45it [00:32,  1.44it/s]Extractor Predicting: 46it [00:33,  1.49it/s]Extractor Predicting: 47it [00:33,  1.45it/s]Extractor Predicting: 48it [00:34,  1.42it/s]Extractor Predicting: 49it [00:35,  1.41it/s]Extractor Predicting: 50it [00:36,  1.35it/s]Extractor Predicting: 51it [00:36,  1.41it/s]Extractor Predicting: 52it [00:37,  1.41it/s]Extractor Predicting: 53it [00:38,  1.43it/s]Extractor Predicting: 54it [00:38,  1.46it/s]Extractor Predicting: 55it [00:39,  1.40it/s]Extractor Predicting: 56it [00:40,  1.39it/s]Extractor Predicting: 57it [00:41,  1.39it/s]Extractor Predicting: 58it [00:41,  1.40it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:43,  1.38it/s]Extractor Predicting: 61it [00:43,  1.39it/s]Extractor Predicting: 62it [00:44,  1.40it/s]Extractor Predicting: 63it [00:45,  1.38it/s]Extractor Predicting: 64it [00:46,  1.35it/s]Extractor Predicting: 65it [00:46,  1.37it/s]Extractor Predicting: 66it [00:47,  1.38it/s]Extractor Predicting: 67it [00:48,  1.43it/s]Extractor Predicting: 68it [00:48,  1.44it/s]Extractor Predicting: 69it [00:49,  1.41it/s]Extractor Predicting: 70it [00:50,  1.39it/s]Extractor Predicting: 71it [00:51,  1.39it/s]Extractor Predicting: 72it [00:51,  1.38it/s]Extractor Predicting: 73it [00:52,  1.37it/s]Extractor Predicting: 74it [00:53,  1.40it/s]Extractor Predicting: 75it [00:54,  1.37it/s]Extractor Predicting: 76it [00:54,  1.35it/s]Extractor Predicting: 77it [00:55,  1.37it/s]Extractor Predicting: 78it [00:56,  1.38it/s]Extractor Predicting: 79it [00:56,  1.40it/s]Extractor Predicting: 80it [00:57,  1.41it/s]Extractor Predicting: 81it [00:58,  1.35it/s]Extractor Predicting: 82it [00:59,  1.35it/s]Extractor Predicting: 83it [00:59,  1.38it/s]Extractor Predicting: 84it [01:00,  1.40it/s]Extractor Predicting: 85it [01:01,  1.41it/s]Extractor Predicting: 86it [01:01,  1.42it/s]Extractor Predicting: 87it [01:02,  1.41it/s]Extractor Predicting: 88it [01:03,  1.46it/s]Extractor Predicting: 89it [01:04,  1.46it/s]Extractor Predicting: 90it [01:04,  1.48it/s]Extractor Predicting: 91it [01:05,  1.44it/s]Extractor Predicting: 92it [01:06,  1.42it/s]Extractor Predicting: 93it [01:06,  1.44it/s]Extractor Predicting: 94it [01:07,  1.31it/s]Extractor Predicting: 95it [01:08,  1.35it/s]Extractor Predicting: 96it [01:09,  1.33it/s]Extractor Predicting: 97it [01:09,  1.35it/s]Extractor Predicting: 98it [01:10,  1.35it/s]Extractor Predicting: 99it [01:11,  1.37it/s]Extractor Predicting: 100it [01:12,  1.39it/s]Extractor Predicting: 101it [01:12,  1.38it/s]Extractor Predicting: 102it [01:13,  1.36it/s]Extractor Predicting: 103it [01:14,  1.38it/s]Extractor Predicting: 104it [01:14,  1.38it/s]Extractor Predicting: 105it [01:15,  1.39it/s]Extractor Predicting: 106it [01:16,  1.39it/s]Extractor Predicting: 107it [01:17,  1.40it/s]Extractor Predicting: 108it [01:17,  1.41it/s]Extractor Predicting: 109it [01:18,  1.43it/s]Extractor Predicting: 110it [01:19,  1.41it/s]Extractor Predicting: 111it [01:19,  1.42it/s]Extractor Predicting: 112it [01:20,  1.43it/s]Extractor Predicting: 113it [01:21,  1.40it/s]Extractor Predicting: 114it [01:22,  1.37it/s]Extractor Predicting: 115it [01:22,  1.39it/s]Extractor Predicting: 116it [01:23,  1.38it/s]Extractor Predicting: 117it [01:24,  1.40it/s]Extractor Predicting: 118it [01:24,  1.40it/s]Extractor Predicting: 119it [01:25,  1.40it/s]Extractor Predicting: 120it [01:26,  1.40it/s]Extractor Predicting: 121it [01:27,  1.40it/s]Extractor Predicting: 122it [01:27,  1.41it/s]Extractor Predicting: 123it [01:28,  1.43it/s]Extractor Predicting: 124it [01:29,  1.42it/s]Extractor Predicting: 125it [01:29,  1.43it/s]Extractor Predicting: 126it [01:30,  1.41it/s]Extractor Predicting: 127it [01:31,  1.41it/s]Extractor Predicting: 128it [01:31,  1.44it/s]Extractor Predicting: 129it [01:32,  1.42it/s]Extractor Predicting: 130it [01:33,  1.45it/s]Extractor Predicting: 131it [01:34,  1.45it/s]Extractor Predicting: 132it [01:34,  1.43it/s]Extractor Predicting: 133it [01:35,  1.42it/s]Extractor Predicting: 134it [01:36,  1.41it/s]Extractor Predicting: 135it [01:36,  1.42it/s]Extractor Predicting: 136it [01:37,  1.40it/s]Extractor Predicting: 137it [01:38,  1.41it/s]Extractor Predicting: 138it [01:39,  1.43it/s]Extractor Predicting: 139it [01:39,  1.39it/s]Extractor Predicting: 140it [01:40,  1.43it/s]Extractor Predicting: 140it [01:40,  1.39it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:17:57,189 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:17:57,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:17:57,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:17:57,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:17:57,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:17:58,093 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:17:58,094 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:17:58,698 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:17:59,767 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:17:59,767 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:18:02,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:18:02,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:18:02,859 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:18:02,859 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:18:02,859 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:18:03,640 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:18:03,641 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:18:04,234 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:18:04,438 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:18:04,438 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 3it [00:02,  1.32it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:18:07,932 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:18:07,933 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:18:07,987 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:18:07,988 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:18:08,014 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:18:16,546 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:18:16,563 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:18:16,667 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:18:16,668 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:18:16,744 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:18:16,770 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:18:17,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:18,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:18,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:19,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:20,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:21,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:22,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:23,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:24,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:25,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:26,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:27,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:28,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:29,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:30,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:31,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:31,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:32,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:33,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:34,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:35,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:36,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:59, 19.91s/it][WARNING|generation_utils.py:914] 2023-08-28 07:18:37,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:38,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:38,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:39,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:40,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:41,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:42,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:43,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:44,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:45,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:46,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:46,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:47,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:48,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:49,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:50,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:51,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:51,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:52,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:53,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:54,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:55,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:56,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:57,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:58,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:42<02:49, 21.21s/it][WARNING|generation_utils.py:914] 2023-08-28 07:18:59,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:59,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:00,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:01,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:02,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:03,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:03,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:04,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:05,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:06,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:06,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:07,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:08,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:08,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:09,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:10,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:11,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:11,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:12,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:13,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:13,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:14,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:15,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:16,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:17,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:18,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:18,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:19,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:20,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:20,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:21,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:22,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:23,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:23,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:24,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:25,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:25,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:26,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:27,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:28,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:28,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:29,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:30,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:30,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:31,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:32,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:33,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<03:13, 27.59s/it][WARNING|generation_utils.py:914] 2023-08-28 07:19:34,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:34,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:35,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:36,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:37,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:38,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:39,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:40,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:41,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:42,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:42,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:43,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:44,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:44,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:45,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:46,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:47,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:48,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:49,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:49,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:50,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:51,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:52,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:53,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:53,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:38<02:29, 24.92s/it][WARNING|generation_utils.py:914] 2023-08-28 07:19:55,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:55,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:56,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:57,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:58,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:58,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:59,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:00,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:01,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:02,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:03,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:04,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:04,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:05,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:06,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:07,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:07,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:08,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:09,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:10,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:11,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:12,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:12,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:13,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:14,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:15,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:16,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:59<01:58, 23.80s/it][WARNING|generation_utils.py:914] 2023-08-28 07:20:16,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:17,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:18,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:19,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:20,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:21,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:21,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:22,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:23,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:24,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:25,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:25,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:26,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:27,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:28,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:28,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:29,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:30,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:31,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:31,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:32,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:33,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:34,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:35,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:36,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:37,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:21<01:31, 22.92s/it][WARNING|generation_utils.py:914] 2023-08-28 07:20:38,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:39,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:40,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:41,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:41,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:42,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:43,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:44,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:45,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:46,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:46,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:47,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:48,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:49,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:50,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:51,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:52,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:53,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:54,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:55,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:55,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:56,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:57,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:20:58,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:42<01:06, 22.29s/it][WARNING|generation_utils.py:914] 2023-08-28 07:20:59,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:00,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:00,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:01,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:02,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:02,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:03,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:04,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:05,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:05,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:06,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:07,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:07,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:08,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:09,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:09,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:10,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:11,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:12,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:12,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:13,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:14,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:57<00:40, 20.26s/it][WARNING|generation_utils.py:914] 2023-08-28 07:21:15,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:15,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:16,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:17,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:18,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:19,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:19,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:20,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:21,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:22,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:22,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:23,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:24,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:25,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:26,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:26,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:28,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:29,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:30,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:30,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:31,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:15<00:19, 19.54s/it][WARNING|generation_utils.py:914] 2023-08-28 07:21:33,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:33,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:34,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:36,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:37,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:38,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:39,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:39,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:40,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:41,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:42,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:43,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:44,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:44,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:45,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:46,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:47,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:48,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:48,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:49,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:50,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:51,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:52,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:53,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:54,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:55,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:56,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:21:57,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:40<00:00, 21.22s/it]Generating: 100%|██████████| 10/10 [03:40<00:00, 22.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:06,112 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:06,138 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:06,138 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:06,138 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:06,138 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:22:06,883 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:22:06,884 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:22:07,498 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:22:08,603 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:22:08,603 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:11,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:11,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:11,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:11,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:22:11,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:22:12,347 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:22:12,348 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:22:12,951 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:22:13,153 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:22:13,153 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.01s/it]Extractor Estimating: 2it [00:01,  1.10it/s]Extractor Estimating: 3it [00:02,  1.20it/s]Extractor Estimating: 4it [00:03,  1.10it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:05,  1.19it/s]Extractor Estimating: 7it [00:05,  1.21it/s]Extractor Estimating: 8it [00:06,  1.24it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.21it/s]Extractor Estimating: 11it [00:09,  1.17it/s]Extractor Estimating: 12it [00:10,  1.16it/s]Extractor Estimating: 13it [00:10,  1.20it/s]Extractor Estimating: 14it [00:11,  1.23it/s]Extractor Estimating: 15it [00:12,  1.21it/s]Extractor Estimating: 16it [00:13,  1.22it/s]Extractor Estimating: 17it [00:14,  1.22it/s]Extractor Estimating: 18it [00:15,  1.20it/s]Extractor Estimating: 19it [00:15,  1.19it/s]Extractor Estimating: 20it [00:16,  1.18it/s]Extractor Estimating: 21it [00:17,  1.13it/s]Extractor Estimating: 22it [00:18,  1.11it/s]Extractor Estimating: 23it [00:19,  1.19it/s]Extractor Estimating: 24it [00:20,  1.23it/s]Extractor Estimating: 25it [00:21,  1.21it/s]Extractor Estimating: 26it [00:21,  1.16it/s]Extractor Estimating: 27it [00:22,  1.20it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.16it/s]Extractor Estimating: 30it [00:25,  1.13it/s]Extractor Estimating: 31it [00:26,  1.16it/s]Extractor Estimating: 32it [00:26,  1.20it/s]Extractor Estimating: 33it [00:27,  1.23it/s]Extractor Estimating: 34it [00:28,  1.21it/s]Extractor Estimating: 35it [00:29,  1.14it/s]Extractor Estimating: 36it [00:30,  1.11it/s]Extractor Estimating: 37it [00:31,  1.16it/s]Extractor Estimating: 38it [00:32,  1.14it/s]Extractor Estimating: 39it [00:33,  1.12it/s]Extractor Estimating: 40it [00:34,  1.12it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:35,  1.15it/s]Extractor Estimating: 43it [00:36,  1.14it/s]Extractor Estimating: 44it [00:37,  1.11it/s]Extractor Estimating: 45it [00:38,  1.16it/s]Extractor Estimating: 46it [00:39,  1.16it/s]Extractor Estimating: 47it [00:40,  1.16it/s]Extractor Estimating: 48it [00:41,  1.15it/s]Extractor Estimating: 49it [00:41,  1.17it/s]Extractor Estimating: 50it [00:42,  1.13it/s]Extractor Estimating: 51it [00:43,  1.14it/s]Extractor Estimating: 52it [00:44,  1.17it/s]Extractor Estimating: 53it [00:45,  1.19it/s]Extractor Estimating: 54it [00:45,  1.23it/s]Extractor Estimating: 55it [00:46,  1.23it/s]Extractor Estimating: 56it [00:47,  1.24it/s]Extractor Estimating: 57it [00:48,  1.24it/s]Extractor Estimating: 58it [00:49,  1.25it/s]Extractor Estimating: 59it [00:49,  1.30it/s]Extractor Estimating: 60it [00:50,  1.29it/s]Extractor Estimating: 61it [00:51,  1.27it/s]Extractor Estimating: 62it [00:52,  1.26it/s]Extractor Estimating: 63it [00:53,  1.27it/s]Extractor Estimating: 64it [00:53,  1.28it/s]Extractor Estimating: 65it [00:54,  1.33it/s]Extractor Estimating: 66it [00:55,  1.32it/s]Extractor Estimating: 67it [00:56,  1.32it/s]Extractor Estimating: 68it [00:56,  1.28it/s]Extractor Estimating: 69it [00:57,  1.33it/s]Extractor Estimating: 70it [00:58,  1.28it/s]Extractor Estimating: 71it [00:59,  1.30it/s]Extractor Estimating: 72it [00:59,  1.32it/s]Extractor Estimating: 73it [01:00,  1.25it/s]Extractor Estimating: 74it [01:01,  1.29it/s]Extractor Estimating: 75it [01:02,  1.24it/s]Extractor Estimating: 76it [01:03,  1.28it/s]Extractor Estimating: 77it [01:03,  1.32it/s]Extractor Estimating: 78it [01:04,  1.26it/s]Extractor Estimating: 79it [01:05,  1.19it/s]Extractor Estimating: 80it [01:06,  1.23it/s]Extractor Estimating: 81it [01:07,  1.24it/s]Extractor Estimating: 82it [01:07,  1.26it/s]Extractor Estimating: 83it [01:08,  1.26it/s]Extractor Estimating: 84it [01:09,  1.29it/s]Extractor Estimating: 85it [01:10,  1.33it/s]Extractor Estimating: 86it [01:10,  1.34it/s]Extractor Estimating: 87it [01:11,  1.33it/s]Extractor Estimating: 88it [01:12,  1.37it/s]Extractor Estimating: 89it [01:13,  1.31it/s]Extractor Estimating: 90it [01:13,  1.33it/s]Extractor Estimating: 91it [01:14,  1.35it/s]Extractor Estimating: 92it [01:15,  1.30it/s]Extractor Estimating: 93it [01:16,  1.30it/s]Extractor Estimating: 94it [01:16,  1.32it/s]Extractor Estimating: 95it [01:17,  1.31it/s]Extractor Estimating: 96it [01:18,  1.34it/s]Extractor Estimating: 97it [01:19,  1.36it/s]Extractor Estimating: 98it [01:19,  1.33it/s]Extractor Estimating: 99it [01:20,  1.33it/s]Extractor Estimating: 100it [01:21,  1.30it/s]Extractor Estimating: 101it [01:22,  1.34it/s]Extractor Estimating: 102it [01:22,  1.36it/s]Extractor Estimating: 103it [01:23,  1.32it/s]Extractor Estimating: 104it [01:24,  1.32it/s]Extractor Estimating: 105it [01:25,  1.27it/s]Extractor Estimating: 106it [01:26,  1.30it/s]Extractor Estimating: 107it [01:26,  1.35it/s]Extractor Estimating: 108it [01:27,  1.40it/s]Extractor Estimating: 109it [01:28,  1.41it/s]Extractor Estimating: 110it [01:28,  1.44it/s]Extractor Estimating: 111it [01:29,  1.42it/s]Extractor Estimating: 112it [01:30,  1.41it/s]Extractor Estimating: 113it [01:30,  1.39it/s]Extractor Estimating: 114it [01:31,  1.43it/s]Extractor Estimating: 115it [01:32,  1.36it/s]Extractor Estimating: 116it [01:33,  1.37it/s]Extractor Estimating: 117it [01:33,  1.35it/s]Extractor Estimating: 118it [01:34,  1.40it/s]Extractor Estimating: 119it [01:35,  1.42it/s]Extractor Estimating: 120it [01:35,  1.39it/s]Extractor Estimating: 121it [01:36,  1.38it/s]Extractor Estimating: 122it [01:37,  1.26it/s]Extractor Estimating: 123it [01:38,  1.30it/s]Extractor Estimating: 124it [01:39,  1.30it/s]Extractor Estimating: 125it [01:39,  1.34it/s]Extractor Estimating: 126it [01:40,  1.33it/s]Extractor Estimating: 127it [01:41,  1.32it/s]Extractor Estimating: 128it [01:42,  1.32it/s]Extractor Estimating: 129it [01:42,  1.32it/s]Extractor Estimating: 130it [01:43,  1.28it/s]Extractor Estimating: 131it [01:44,  1.29it/s]Extractor Estimating: 132it [01:45,  1.36it/s]Extractor Estimating: 133it [01:45,  1.35it/s]Extractor Estimating: 134it [01:46,  1.34it/s]Extractor Estimating: 135it [01:47,  1.34it/s]Extractor Estimating: 136it [01:48,  1.31it/s]Extractor Estimating: 137it [01:48,  1.36it/s]Extractor Estimating: 138it [01:49,  1.38it/s]Extractor Estimating: 139it [01:50,  1.36it/s]Extractor Estimating: 140it [01:51,  1.33it/s]Extractor Estimating: 141it [01:52,  1.24it/s]Extractor Estimating: 142it [01:52,  1.28it/s]Extractor Estimating: 143it [01:53,  1.27it/s]Extractor Estimating: 144it [01:54,  1.22it/s]Extractor Estimating: 145it [01:55,  1.22it/s]Extractor Estimating: 146it [01:56,  1.25it/s]Extractor Estimating: 147it [01:56,  1.28it/s]Extractor Estimating: 148it [01:57,  1.31it/s]Extractor Estimating: 149it [01:58,  1.30it/s]Extractor Estimating: 150it [01:59,  1.29it/s]Extractor Estimating: 151it [01:59,  1.25it/s]Extractor Estimating: 152it [02:00,  1.25it/s]Extractor Estimating: 153it [02:01,  1.28it/s]Extractor Estimating: 154it [02:02,  1.27it/s]Extractor Estimating: 155it [02:03,  1.26it/s]Extractor Estimating: 156it [02:03,  1.20it/s]Extractor Estimating: 157it [02:04,  1.22it/s]Extractor Estimating: 158it [02:05,  1.22it/s]Extractor Estimating: 159it [02:06,  1.15it/s]Extractor Estimating: 160it [02:07,  1.20it/s]Extractor Estimating: 161it [02:08,  1.23it/s]Extractor Estimating: 162it [02:08,  1.22it/s]Extractor Estimating: 163it [02:09,  1.24it/s]Extractor Estimating: 164it [02:10,  1.25it/s]Extractor Estimating: 165it [02:11,  1.25it/s]Extractor Estimating: 166it [02:12,  1.27it/s]Extractor Estimating: 167it [02:12,  1.26it/s]Extractor Estimating: 168it [02:13,  1.27it/s]Extractor Estimating: 169it [02:14,  1.21it/s]Extractor Estimating: 170it [02:15,  1.20it/s]Extractor Estimating: 171it [02:16,  1.21it/s]Extractor Estimating: 172it [02:16,  1.27it/s]Extractor Estimating: 173it [02:17,  1.21it/s]Extractor Estimating: 174it [02:18,  1.22it/s]Extractor Estimating: 175it [02:19,  1.21it/s]Extractor Estimating: 176it [02:20,  1.19it/s]Extractor Estimating: 177it [02:21,  1.26it/s]Extractor Estimating: 178it [02:21,  1.30it/s]Extractor Estimating: 179it [02:22,  1.33it/s]Extractor Estimating: 180it [02:23,  1.37it/s]Extractor Estimating: 181it [02:23,  1.39it/s]Extractor Estimating: 182it [02:24,  1.36it/s]Extractor Estimating: 183it [02:25,  1.38it/s]Extractor Estimating: 184it [02:25,  1.39it/s]Extractor Estimating: 185it [02:26,  1.39it/s]Extractor Estimating: 186it [02:27,  1.36it/s]Extractor Estimating: 187it [02:28,  1.34it/s]Extractor Estimating: 188it [02:28,  1.39it/s]Extractor Estimating: 189it [02:29,  1.38it/s]Extractor Estimating: 190it [02:30,  1.38it/s]Extractor Estimating: 191it [02:31,  1.40it/s]Extractor Estimating: 192it [02:31,  1.32it/s]Extractor Estimating: 193it [02:32,  1.28it/s]Extractor Estimating: 194it [02:33,  1.30it/s]Extractor Estimating: 195it [02:34,  1.32it/s]Extractor Estimating: 196it [02:34,  1.32it/s]Extractor Estimating: 197it [02:35,  1.26it/s]Extractor Estimating: 198it [02:36,  1.33it/s]Extractor Estimating: 199it [02:37,  1.37it/s]Extractor Estimating: 200it [02:37,  1.34it/s]Extractor Estimating: 201it [02:38,  1.31it/s]Extractor Estimating: 202it [02:39,  1.27it/s]Extractor Estimating: 203it [02:40,  1.31it/s]Extractor Estimating: 204it [02:41,  1.29it/s]Extractor Estimating: 205it [02:41,  1.29it/s]Extractor Estimating: 206it [02:42,  1.32it/s]Extractor Estimating: 207it [02:43,  1.32it/s]Extractor Estimating: 208it [02:44,  1.19it/s]Extractor Estimating: 209it [02:45,  1.19it/s]Extractor Estimating: 210it [02:46,  1.19it/s]Extractor Estimating: 211it [02:46,  1.20it/s]Extractor Estimating: 212it [02:47,  1.23it/s]Extractor Estimating: 213it [02:49,  1.10s/it]Extractor Estimating: 214it [02:50,  1.01s/it]Extractor Estimating: 215it [02:51,  1.04it/s]Extractor Estimating: 216it [02:51,  1.10it/s]Extractor Estimating: 217it [02:52,  1.16it/s]Extractor Estimating: 218it [02:53,  1.20it/s]Extractor Estimating: 219it [02:54,  1.21it/s]Extractor Estimating: 220it [02:55,  1.18it/s]Extractor Estimating: 221it [02:55,  1.21it/s]Extractor Estimating: 222it [02:56,  1.23it/s]Extractor Estimating: 223it [02:57,  1.22it/s]Extractor Estimating: 224it [02:58,  1.21it/s]Extractor Estimating: 225it [02:59,  1.22it/s]Extractor Estimating: 226it [02:59,  1.26it/s]Extractor Estimating: 227it [03:00,  1.25it/s]Extractor Estimating: 228it [03:01,  1.26it/s]Extractor Estimating: 229it [03:02,  1.22it/s]Extractor Estimating: 230it [03:03,  1.18it/s]Extractor Estimating: 231it [03:04,  1.18it/s]Extractor Estimating: 232it [03:04,  1.19it/s]Extractor Estimating: 233it [03:05,  1.23it/s]Extractor Estimating: 234it [03:06,  1.16it/s]Extractor Estimating: 235it [03:07,  1.19it/s]Extractor Estimating: 236it [03:08,  1.22it/s]Extractor Estimating: 237it [03:09,  1.24it/s]Extractor Estimating: 238it [03:09,  1.23it/s]Extractor Estimating: 239it [03:10,  1.21it/s]Extractor Estimating: 240it [03:11,  1.23it/s]Extractor Estimating: 241it [03:12,  1.19it/s]Extractor Estimating: 242it [03:13,  1.16it/s]Extractor Estimating: 243it [03:14,  1.19it/s]Extractor Estimating: 244it [03:14,  1.19it/s]Extractor Estimating: 245it [03:15,  1.23it/s]Extractor Estimating: 246it [03:16,  1.24it/s]Extractor Estimating: 247it [03:17,  1.27it/s]Extractor Estimating: 248it [03:18,  1.27it/s]Extractor Estimating: 249it [03:18,  1.22it/s]Extractor Estimating: 250it [03:19,  1.23it/s]Extractor Estimating: 250it [03:19,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:47,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:47,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:47,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:47,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:47,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:25:48,644 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:25:48,645 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:25:48,979 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:25:50,092 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:25:50,092 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:52,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:52,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:52,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:52,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:25:52,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:25:53,694 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:25:53,695 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:25:54,006 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:25:54,200 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:25:54,200 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 09:18:14,193 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 09:18:14,454 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5100 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 17642
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17742, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17742, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.335, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.335, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.341, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.320, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.333, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.661, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.316, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.312, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.343, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.347, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.632, loss:nan
g_step 1200, step 135, avg_time 1.360, loss:nan
g_step 1300, step 22, avg_time 1.317, loss:nan
g_step 1400, step 122, avg_time 1.325, loss:nan
g_step 1500, step 9, avg_time 1.325, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.641, loss:nan
g_step 1700, step 209, avg_time 1.340, loss:nan
g_step 1800, step 96, avg_time 1.316, loss:nan
g_step 1900, step 196, avg_time 1.350, loss:nan
g_step 2000, step 83, avg_time 1.343, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.635, loss:nan
g_step 2200, step 70, avg_time 1.327, loss:nan
g_step 2300, step 170, avg_time 1.331, loss:nan
g_step 2400, step 57, avg_time 1.342, loss:nan
g_step 2500, step 157, avg_time 1.355, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.602, loss:nan
g_step 2700, step 144, avg_time 1.357, loss:nan
g_step 2800, step 31, avg_time 1.326, loss:nan
g_step 2900, step 131, avg_time 1.339, loss:nan
g_step 3000, step 18, avg_time 1.341, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.643, loss:nan
g_step 3200, step 5, avg_time 1.352, loss:nan
g_step 3300, step 105, avg_time 1.338, loss:nan
g_step 3400, step 205, avg_time 1.347, loss:nan
g_step 3500, step 92, avg_time 1.322, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.666, loss:nan
g_step 3700, step 79, avg_time 1.328, loss:nan
g_step 3800, step 179, avg_time 1.322, loss:nan
g_step 3900, step 66, avg_time 1.321, loss:nan
g_step 4000, step 166, avg_time 1.360, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.639, loss:nan
g_step 4200, step 153, avg_time 1.349, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:18:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:18:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-18-14_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:18:15 - WARNING - datasets.builder -   Using custom data configuration default-5e2a0cd232985ad9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5e2a0cd232985ad9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:18:17,387 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:18:17,422 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:18:17,423 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:18:17,424 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:18:17,541 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:18:17,595 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:18:18,000 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:18:21,086 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:18:21,103 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5e2a0cd232985ad9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.78ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.73ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.23ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.47ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.88ba/s]100%|██████████| 6/6 [00:01<00:00,  4.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.39ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.02ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.26ba/s]100%|██████████| 4/4 [00:00<00:00,  5.38ba/s]100%|██████████| 4/4 [00:00<00:00,  4.76ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.58ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.46ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.35ba/s]100%|██████████| 6/6 [00:00<00:00, 10.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.18ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.75ba/s]100%|██████████| 4/4 [00:00<00:00,  9.75ba/s]
[INFO|trainer.py:414] 2023-08-28 09:18:25,345 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:18:25,418 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:18:25,418 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 09:18:25,418 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:18:25,418 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:18:25,418 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:18:25,418 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:18:25,418 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<01:54,  3.48it/s]  0%|          | 2/400 [00:00<01:51,  3.57it/s]  1%|          | 3/400 [00:00<01:50,  3.59it/s]  1%|          | 4/400 [00:01<01:50,  3.58it/s]  1%|▏         | 5/400 [00:01<01:50,  3.56it/s]  2%|▏         | 6/400 [00:01<01:50,  3.55it/s]  2%|▏         | 7/400 [00:01<01:50,  3.55it/s]  2%|▏         | 8/400 [00:02<01:52,  3.47it/s]  2%|▏         | 9/400 [00:02<01:51,  3.50it/s]  2%|▎         | 10/400 [00:02<01:51,  3.51it/s]  3%|▎         | 11/400 [00:03<01:50,  3.52it/s]  3%|▎         | 12/400 [00:03<01:49,  3.53it/s]  3%|▎         | 13/400 [00:03<01:49,  3.53it/s]  4%|▎         | 14/400 [00:03<01:49,  3.53it/s]  4%|▍         | 15/400 [00:04<01:48,  3.54it/s]  4%|▍         | 16/400 [00:04<01:48,  3.54it/s]  4%|▍         | 17/400 [00:04<01:48,  3.54it/s]  4%|▍         | 18/400 [00:05<01:47,  3.54it/s]  5%|▍         | 19/400 [00:05<01:53,  3.35it/s]  5%|▌         | 20/400 [00:05<01:51,  3.41it/s]  5%|▌         | 21/400 [00:05<01:49,  3.45it/s]  6%|▌         | 22/400 [00:06<01:48,  3.47it/s]  6%|▌         | 23/400 [00:06<01:47,  3.49it/s]  6%|▌         | 24/400 [00:06<01:47,  3.50it/s]  6%|▋         | 25/400 [00:07<01:46,  3.51it/s]  6%|▋         | 26/400 [00:07<01:46,  3.51it/s]  7%|▋         | 27/400 [00:07<01:45,  3.52it/s]  7%|▋         | 28/400 [00:07<01:45,  3.53it/s]  7%|▋         | 29/400 [00:08<01:45,  3.53it/s]  8%|▊         | 30/400 [00:08<01:50,  3.35it/s]  8%|▊         | 31/400 [00:08<01:48,  3.41it/s]  8%|▊         | 32/400 [00:09<01:46,  3.45it/s]  8%|▊         | 33/400 [00:09<01:45,  3.47it/s]  8%|▊         | 34/400 [00:09<01:44,  3.49it/s]  9%|▉         | 35/400 [00:10<01:44,  3.50it/s]  9%|▉         | 36/400 [00:10<01:43,  3.51it/s]  9%|▉         | 37/400 [00:10<01:43,  3.52it/s] 10%|▉         | 38/400 [00:10<01:42,  3.53it/s] 10%|▉         | 39/400 [00:11<01:42,  3.53it/s] 10%|█         | 40/400 [00:11<01:41,  3.53it/s] 10%|█         | 41/400 [00:11<01:47,  3.33it/s] 10%|█         | 42/400 [00:12<01:45,  3.39it/s] 11%|█         | 43/400 [00:12<01:44,  3.43it/s] 11%|█         | 44/400 [00:12<01:42,  3.46it/s] 11%|█▏        | 45/400 [00:12<01:41,  3.49it/s] 12%|█▏        | 46/400 [00:13<01:41,  3.50it/s] 12%|█▏        | 47/400 [00:13<01:40,  3.51it/s] 12%|█▏        | 48/400 [00:13<01:39,  3.52it/s] 12%|█▏        | 49/400 [00:14<01:39,  3.53it/s] 12%|█▎        | 50/400 [00:14<01:39,  3.53it/s] 13%|█▎        | 51/400 [00:14<01:38,  3.53it/s] 13%|█▎        | 52/400 [00:14<01:43,  3.35it/s] 13%|█▎        | 53/400 [00:15<01:42,  3.40it/s] 14%|█▎        | 54/400 [00:15<01:40,  3.44it/s] 14%|█▍        | 55/400 [00:15<01:39,  3.47it/s] 14%|█▍        | 56/400 [00:16<01:38,  3.49it/s] 14%|█▍        | 57/400 [00:16<01:37,  3.51it/s] 14%|█▍        | 58/400 [00:16<01:37,  3.52it/s] 15%|█▍        | 59/400 [00:16<01:36,  3.52it/s] 15%|█▌        | 60/400 [00:17<01:36,  3.53it/s] 15%|█▌        | 61/400 [00:17<01:36,  3.53it/s] 16%|█▌        | 62/400 [00:17<01:35,  3.53it/s] 16%|█▌        | 63/400 [00:18<01:45,  3.20it/s] 16%|█▌        | 64/400 [00:18<01:41,  3.30it/s] 16%|█▋        | 65/400 [00:18<01:39,  3.37it/s] 16%|█▋        | 66/400 [00:18<01:37,  3.42it/s] 17%|█▋        | 67/400 [00:19<01:36,  3.45it/s] 17%|█▋        | 68/400 [00:19<01:35,  3.47it/s] 17%|█▋        | 69/400 [00:19<01:34,  3.49it/s] 18%|█▊        | 70/400 [00:20<01:34,  3.50it/s] 18%|█▊        | 71/400 [00:20<01:33,  3.51it/s] 18%|█▊        | 72/400 [00:20<01:33,  3.52it/s] 18%|█▊        | 73/400 [00:20<01:32,  3.52it/s] 18%|█▊        | 74/400 [00:21<01:32,  3.52it/s] 19%|█▉        | 75/400 [00:21<01:32,  3.52it/s] 19%|█▉        | 76/400 [00:21<01:31,  3.53it/s] 19%|█▉        | 77/400 [00:22<01:31,  3.53it/s] 20%|█▉        | 78/400 [00:22<01:31,  3.53it/s] 20%|█▉        | 79/400 [00:22<01:30,  3.53it/s] 20%|██        | 80/400 [00:22<01:24,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 09:18:48,294 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:18:48,295 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:18:48,295 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.77it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.61it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.96it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.43it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.99it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.38it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.75it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.41it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.39it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 43.87it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.31it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.46it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.74it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.73it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.53it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.26it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.31it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.51it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.79it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.81it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.78it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.60it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.44it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.74it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.99it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.21it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.74it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.73it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.41it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.52it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.93it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.13it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.35it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.58it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.68it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.67it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.61it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.32it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.28it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.32it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.58it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.56it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.85it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.78it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.60it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.40it/s][A
 61%|██████    | 267/438 [00:05<00:03, 43.30it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.75it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.01it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.27it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.48it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.66it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.65it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.28it/s][A
 70%|███████   | 307/438 [00:06<00:03, 43.49it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.84it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.03it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.45it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.63it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.76it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.59it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.43it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.40it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.36it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.39it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.50it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.56it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.70it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.70it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.55it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.45it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.39it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 41.22it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 42.34it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.12it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.69it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.07it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.33it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.41it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.24it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.24it/s][A 20%|██        | 80/400 [00:32<01:24,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:18:58,299 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 09:18:58,463 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:19:00,649 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:19:00,761 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:19:00,821 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:36<22:39,  4.26s/it] 20%|██        | 82/400 [00:36<16:15,  3.07s/it] 21%|██        | 83/400 [00:37<11:47,  2.23s/it] 21%|██        | 84/400 [00:37<08:40,  1.65s/it] 21%|██▏       | 85/400 [00:37<06:29,  1.24s/it] 22%|██▏       | 86/400 [00:37<04:58,  1.05it/s] 22%|██▏       | 87/400 [00:38<03:54,  1.33it/s] 22%|██▏       | 88/400 [00:38<03:10,  1.64it/s] 22%|██▏       | 89/400 [00:38<02:41,  1.92it/s] 22%|██▎       | 90/400 [00:39<02:19,  2.23it/s] 23%|██▎       | 91/400 [00:39<02:03,  2.51it/s] 23%|██▎       | 92/400 [00:39<01:52,  2.75it/s] 23%|██▎       | 93/400 [00:39<01:44,  2.94it/s] 24%|██▎       | 94/400 [00:40<01:38,  3.10it/s] 24%|██▍       | 95/400 [00:40<01:34,  3.22it/s] 24%|██▍       | 96/400 [00:40<01:31,  3.31it/s] 24%|██▍       | 97/400 [00:41<01:29,  3.37it/s] 24%|██▍       | 98/400 [00:41<01:28,  3.42it/s] 25%|██▍       | 99/400 [00:41<01:27,  3.45it/s] 25%|██▌       | 100/400 [00:41<01:29,  3.36it/s] 25%|██▌       | 101/400 [00:42<01:27,  3.41it/s] 26%|██▌       | 102/400 [00:42<01:26,  3.45it/s] 26%|██▌       | 103/400 [00:42<01:25,  3.47it/s] 26%|██▌       | 104/400 [00:43<01:24,  3.49it/s] 26%|██▋       | 105/400 [00:43<01:24,  3.50it/s] 26%|██▋       | 106/400 [00:43<01:23,  3.51it/s] 27%|██▋       | 107/400 [00:43<01:23,  3.52it/s] 27%|██▋       | 108/400 [00:44<01:22,  3.52it/s] 27%|██▋       | 109/400 [00:44<01:22,  3.53it/s] 28%|██▊       | 110/400 [00:44<01:22,  3.53it/s] 28%|██▊       | 111/400 [00:45<01:24,  3.44it/s] 28%|██▊       | 112/400 [00:45<01:23,  3.47it/s] 28%|██▊       | 113/400 [00:45<01:22,  3.49it/s] 28%|██▊       | 114/400 [00:45<01:21,  3.50it/s] 29%|██▉       | 115/400 [00:46<01:21,  3.51it/s] 29%|██▉       | 116/400 [00:46<01:20,  3.52it/s] 29%|██▉       | 117/400 [00:46<01:20,  3.52it/s] 30%|██▉       | 118/400 [00:47<01:20,  3.52it/s] 30%|██▉       | 119/400 [00:47<01:21,  3.47it/s] 30%|███       | 120/400 [00:47<01:20,  3.48it/s] 30%|███       | 121/400 [00:47<01:19,  3.49it/s] 30%|███       | 122/400 [00:48<01:20,  3.45it/s] 31%|███       | 123/400 [00:48<01:19,  3.47it/s] 31%|███       | 124/400 [00:48<01:19,  3.49it/s] 31%|███▏      | 125/400 [00:49<01:38,  2.78it/s] 32%|███▏      | 126/400 [00:49<01:32,  2.97it/s] 32%|███▏      | 127/400 [00:49<01:27,  3.11it/s] 32%|███▏      | 128/400 [00:50<01:24,  3.22it/s] 32%|███▏      | 129/400 [00:50<01:22,  3.27it/s] 32%|███▎      | 130/400 [00:50<01:20,  3.34it/s] 33%|███▎      | 131/400 [00:50<01:19,  3.39it/s] 33%|███▎      | 132/400 [00:51<01:18,  3.43it/s] 33%|███▎      | 133/400 [00:51<01:17,  3.46it/s] 34%|███▎      | 134/400 [00:51<01:16,  3.48it/s] 34%|███▍      | 135/400 [00:52<01:15,  3.49it/s] 34%|███▍      | 136/400 [00:52<01:15,  3.50it/s] 34%|███▍      | 137/400 [00:52<01:14,  3.51it/s] 34%|███▍      | 138/400 [00:52<01:14,  3.52it/s] 35%|███▍      | 139/400 [00:53<01:13,  3.53it/s] 35%|███▌      | 140/400 [00:53<01:15,  3.43it/s] 35%|███▌      | 141/400 [00:53<01:14,  3.46it/s] 36%|███▌      | 142/400 [00:54<01:13,  3.49it/s] 36%|███▌      | 143/400 [00:54<01:13,  3.51it/s] 36%|███▌      | 144/400 [00:54<01:12,  3.52it/s] 36%|███▋      | 145/400 [00:54<01:12,  3.52it/s] 36%|███▋      | 146/400 [00:55<01:11,  3.53it/s] 37%|███▋      | 147/400 [00:55<01:11,  3.53it/s] 37%|███▋      | 148/400 [00:55<01:11,  3.53it/s] 37%|███▋      | 149/400 [00:56<01:11,  3.53it/s] 38%|███▊      | 150/400 [00:56<01:10,  3.54it/s] 38%|███▊      | 151/400 [00:56<01:12,  3.46it/s] 38%|███▊      | 152/400 [00:56<01:11,  3.48it/s] 38%|███▊      | 153/400 [00:57<01:10,  3.49it/s] 38%|███▊      | 154/400 [00:57<01:10,  3.50it/s] 39%|███▉      | 155/400 [00:57<01:09,  3.51it/s] 39%|███▉      | 156/400 [00:58<01:09,  3.52it/s] 39%|███▉      | 157/400 [00:58<01:08,  3.53it/s] 40%|███▉      | 158/400 [00:58<01:08,  3.53it/s] 40%|███▉      | 159/400 [00:58<01:08,  3.53it/s] 40%|████      | 160/400 [00:59<01:02,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 09:19:24,591 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:19:24,591 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:19:24,591 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8854, 'eval_samples_per_second': 353.855, 'eval_steps_per_second': 44.308, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.63it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.58it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.15it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.84it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.38it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.05it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.80it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.33it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.66it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.71it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.79it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.76it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.75it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.58it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.30it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.19it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.33it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.64it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.77it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.79it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.72it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.46it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.27it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.19it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.36it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.53it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.29it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.82it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 43.99it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.03it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.95it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.06it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.19it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.42it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.40it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.59it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.70it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.65it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.41it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.21it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.22it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.35it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.55it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.65it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.69it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.58it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.32it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.26it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.27it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.22it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.47it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.60it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 42.30it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 43.15it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.68it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.90it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.98it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.12it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.25it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.33it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.25it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.32it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.49it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.63it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.58it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.50it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.38it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.35it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.35it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.44it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.50it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.52it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.61it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.64it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.49it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.43it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.42it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.42it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.30it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 42.98it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.54it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 43.96it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.15it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.15it/s][A 40%|████      | 160/400 [01:09<01:02,  3.85it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:19:34,751 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 09:19:34,947 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:19:38,952 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:19:39,256 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:19:39,409 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:16<20:55,  5.25s/it] 40%|████      | 162/400 [01:16<14:55,  3.76s/it] 41%|████      | 163/400 [01:16<10:44,  2.72s/it] 41%|████      | 164/400 [01:16<07:52,  2.00s/it] 41%|████▏     | 165/400 [01:17<05:49,  1.49s/it] 42%|████▏     | 166/400 [01:17<04:23,  1.12s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:17<03:23,  1.15it/s] 42%|████▏     | 168/400 [01:18<02:41,  1.44it/s] 42%|████▏     | 169/400 [01:18<02:12,  1.75it/s] 42%|████▎     | 170/400 [01:18<01:51,  2.06it/s] 43%|████▎     | 171/400 [01:18<01:37,  2.36it/s] 43%|████▎     | 172/400 [01:19<01:27,  2.62it/s] 43%|████▎     | 173/400 [01:19<01:19,  2.84it/s] 44%|████▎     | 174/400 [01:19<01:14,  3.02it/s] 44%|████▍     | 175/400 [01:20<01:13,  3.04it/s] 44%|████▍     | 176/400 [01:20<01:10,  3.17it/s] 44%|████▍     | 177/400 [01:20<01:08,  3.27it/s] 44%|████▍     | 178/400 [01:20<01:06,  3.35it/s] 45%|████▍     | 179/400 [01:21<01:04,  3.40it/s] 45%|████▌     | 180/400 [01:21<01:03,  3.44it/s] 45%|████▌     | 181/400 [01:21<01:03,  3.47it/s] 46%|████▌     | 182/400 [01:22<01:02,  3.48it/s] 46%|████▌     | 183/400 [01:22<01:02,  3.50it/s] 46%|████▌     | 184/400 [01:22<01:01,  3.51it/s] 46%|████▋     | 185/400 [01:22<01:01,  3.52it/s] 46%|████▋     | 186/400 [01:23<01:00,  3.52it/s] 47%|████▋     | 187/400 [01:23<01:00,  3.53it/s] 47%|████▋     | 188/400 [01:23<01:00,  3.53it/s] 47%|████▋     | 189/400 [01:24<00:59,  3.53it/s] 48%|████▊     | 190/400 [01:24<01:02,  3.36it/s] 48%|████▊     | 191/400 [01:24<01:01,  3.41it/s] 48%|████▊     | 192/400 [01:24<01:00,  3.45it/s] 48%|████▊     | 193/400 [01:25<00:59,  3.47it/s] 48%|████▊     | 194/400 [01:25<00:58,  3.49it/s] 49%|████▉     | 195/400 [01:25<00:58,  3.51it/s] 49%|████▉     | 196/400 [01:26<00:58,  3.52it/s] 49%|████▉     | 197/400 [01:26<00:57,  3.52it/s] 50%|████▉     | 198/400 [01:26<00:57,  3.52it/s] 50%|████▉     | 199/400 [01:26<00:57,  3.52it/s] 50%|█████     | 200/400 [01:27<00:56,  3.53it/s] 50%|█████     | 201/400 [01:27<00:57,  3.45it/s] 50%|█████     | 202/400 [01:27<00:56,  3.47it/s] 51%|█████     | 203/400 [01:28<00:56,  3.49it/s] 51%|█████     | 204/400 [01:28<00:55,  3.51it/s] 51%|█████▏    | 205/400 [01:28<00:55,  3.52it/s] 52%|█████▏    | 206/400 [01:28<00:55,  3.52it/s] 52%|█████▏    | 207/400 [01:29<00:54,  3.53it/s] 52%|█████▏    | 208/400 [01:29<00:54,  3.54it/s] 52%|█████▏    | 209/400 [01:29<00:53,  3.54it/s] 52%|█████▎    | 210/400 [01:30<00:53,  3.54it/s] 53%|█████▎    | 211/400 [01:30<00:53,  3.54it/s] 53%|█████▎    | 212/400 [01:30<00:54,  3.43it/s] 53%|█████▎    | 213/400 [01:30<00:53,  3.47it/s] 54%|█████▎    | 214/400 [01:31<00:53,  3.49it/s] 54%|█████▍    | 215/400 [01:31<00:52,  3.50it/s] 54%|█████▍    | 216/400 [01:31<00:52,  3.51it/s] 54%|█████▍    | 217/400 [01:32<00:52,  3.52it/s] 55%|█████▍    | 218/400 [01:32<00:51,  3.52it/s] 55%|█████▍    | 219/400 [01:32<00:51,  3.53it/s] 55%|█████▌    | 220/400 [01:32<00:50,  3.53it/s] 55%|█████▌    | 221/400 [01:33<00:50,  3.53it/s] 56%|█████▌    | 222/400 [01:33<00:50,  3.53it/s] 56%|█████▌    | 223/400 [01:33<00:53,  3.34it/s] 56%|█████▌    | 224/400 [01:34<00:51,  3.39it/s] 56%|█████▋    | 225/400 [01:34<00:51,  3.43it/s] 56%|█████▋    | 226/400 [01:34<00:50,  3.46it/s] 57%|█████▋    | 227/400 [01:34<00:49,  3.48it/s] 57%|█████▋    | 228/400 [01:35<00:49,  3.50it/s] 57%|█████▋    | 229/400 [01:35<00:48,  3.51it/s] 57%|█████▊    | 230/400 [01:35<00:48,  3.51it/s] 58%|█████▊    | 231/400 [01:36<00:48,  3.52it/s] 58%|█████▊    | 232/400 [01:36<00:47,  3.52it/s] 58%|█████▊    | 233/400 [01:36<00:47,  3.52it/s] 58%|█████▊    | 234/400 [01:36<00:48,  3.43it/s] 59%|█████▉    | 235/400 [01:37<00:47,  3.46it/s] 59%|█████▉    | 236/400 [01:37<00:47,  3.48it/s] 59%|█████▉    | 237/400 [01:37<00:46,  3.50it/s] 60%|█████▉    | 238/400 [01:38<00:46,  3.51it/s] 60%|█████▉    | 239/400 [01:38<00:45,  3.51it/s] 60%|██████    | 240/400 [01:38<00:41,  3.84it/s][INFO|trainer.py:2140] 2023-08-28 09:20:04,036 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:20:04,036 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:20:04,036 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8789, 'eval_samples_per_second': 354.087, 'eval_steps_per_second': 44.337, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.42it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.52it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.13it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.29it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.66it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.17it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.71it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.35it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.43it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.51it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 40.79it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 42.03it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 42.83it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.51it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 43.89it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.01it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.05it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.07it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.90it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.04it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.35it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.58it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.66it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.53it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.32it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.32it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.44it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.72it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.73it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.65it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.40it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.28it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.36it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.53it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.59it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.61it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.68it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.71it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.48it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.34it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.35it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.51it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.59it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.64it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.64it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.70it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.66it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.46it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.41it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.42it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.53it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.61it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.64it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.58it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.58it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.52it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.40it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.37it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.45it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.23it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.75it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.27it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.40it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.37it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.28it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.31it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.25it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.32it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.49it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.63it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.27it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.96it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.11it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.25it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.13it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.25it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.42it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.51it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.53it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.66it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.66it/s][A 60%|██████    | 240/400 [01:48<00:41,  3.84it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:20:14,041 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 09:20:14,256 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:20:16,910 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:20:16,996 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:20:17,026 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [01:52<11:50,  4.47s/it] 60%|██████    | 242/400 [01:53<08:28,  3.22s/it] 61%|██████    | 243/400 [01:53<06:06,  2.34s/it] 61%|██████    | 244/400 [01:53<04:28,  1.72s/it] 61%|██████▏   | 245/400 [01:54<03:19,  1.29s/it] 62%|██████▏   | 246/400 [01:54<02:32,  1.01it/s] 62%|██████▏   | 247/400 [01:54<01:58,  1.29it/s] 62%|██████▏   | 248/400 [01:54<01:36,  1.58it/s] 62%|██████▏   | 249/400 [01:55<01:19,  1.90it/s] 62%|██████▎   | 250/400 [01:55<01:08,  2.20it/s] 63%|██████▎   | 251/400 [01:55<00:59,  2.48it/s] 63%|██████▎   | 252/400 [01:56<00:54,  2.73it/s] 63%|██████▎   | 253/400 [01:56<00:50,  2.93it/s] 64%|██████▎   | 254/400 [01:56<00:47,  3.09it/s] 64%|██████▍   | 255/400 [01:56<00:45,  3.21it/s] 64%|██████▍   | 256/400 [01:57<00:43,  3.30it/s] 64%|██████▍   | 257/400 [01:57<00:42,  3.37it/s] 64%|██████▍   | 258/400 [01:57<00:41,  3.42it/s] 65%|██████▍   | 259/400 [01:58<00:41,  3.36it/s] 65%|██████▌   | 260/400 [01:58<00:41,  3.41it/s] 65%|██████▌   | 261/400 [01:58<00:40,  3.44it/s] 66%|██████▌   | 262/400 [01:58<00:39,  3.47it/s] 66%|██████▌   | 263/400 [01:59<00:39,  3.49it/s] 66%|██████▌   | 264/400 [01:59<00:38,  3.50it/s] 66%|██████▋   | 265/400 [01:59<00:38,  3.51it/s] 66%|██████▋   | 266/400 [02:00<00:38,  3.52it/s] 67%|██████▋   | 267/400 [02:00<00:37,  3.52it/s] 67%|██████▋   | 268/400 [02:00<00:37,  3.52it/s] 67%|██████▋   | 269/400 [02:00<00:37,  3.53it/s] 68%|██████▊   | 270/400 [02:01<00:37,  3.43it/s] 68%|██████▊   | 271/400 [02:01<00:37,  3.46it/s] 68%|██████▊   | 272/400 [02:01<00:36,  3.48it/s] 68%|██████▊   | 273/400 [02:02<00:36,  3.50it/s] 68%|██████▊   | 274/400 [02:02<00:35,  3.51it/s] 69%|██████▉   | 275/400 [02:02<00:35,  3.51it/s] 69%|██████▉   | 276/400 [02:02<00:35,  3.52it/s] 69%|██████▉   | 277/400 [02:03<00:34,  3.52it/s] 70%|██████▉   | 278/400 [02:03<00:34,  3.52it/s] 70%|██████▉   | 279/400 [02:03<00:34,  3.52it/s] 70%|███████   | 280/400 [02:04<00:34,  3.52it/s] 70%|███████   | 281/400 [02:04<00:34,  3.41it/s] 70%|███████   | 282/400 [02:04<00:34,  3.44it/s] 71%|███████   | 283/400 [02:04<00:33,  3.47it/s] 71%|███████   | 284/400 [02:05<00:33,  3.49it/s] 71%|███████▏  | 285/400 [02:05<00:32,  3.50it/s] 72%|███████▏  | 286/400 [02:05<00:32,  3.51it/s] 72%|███████▏  | 287/400 [02:06<00:32,  3.52it/s] 72%|███████▏  | 288/400 [02:06<00:31,  3.53it/s] 72%|███████▏  | 289/400 [02:06<00:31,  3.53it/s] 72%|███████▎  | 290/400 [02:06<00:31,  3.53it/s] 73%|███████▎  | 291/400 [02:07<00:30,  3.53it/s] 73%|███████▎  | 292/400 [02:07<00:32,  3.31it/s] 73%|███████▎  | 293/400 [02:07<00:31,  3.37it/s] 74%|███████▎  | 294/400 [02:08<00:30,  3.42it/s] 74%|███████▍  | 295/400 [02:08<00:30,  3.45it/s] 74%|███████▍  | 296/400 [02:08<00:29,  3.48it/s] 74%|███████▍  | 297/400 [02:08<00:29,  3.50it/s] 74%|███████▍  | 298/400 [02:09<00:29,  3.51it/s] 75%|███████▍  | 299/400 [02:09<00:28,  3.52it/s] 75%|███████▌  | 300/400 [02:09<00:28,  3.53it/s] 75%|███████▌  | 301/400 [02:10<00:28,  3.53it/s] 76%|███████▌  | 302/400 [02:10<00:27,  3.53it/s] 76%|███████▌  | 303/400 [02:10<00:28,  3.35it/s] 76%|███████▌  | 304/400 [02:10<00:28,  3.40it/s] 76%|███████▋  | 305/400 [02:11<00:27,  3.44it/s] 76%|███████▋  | 306/400 [02:11<00:27,  3.46it/s] 77%|███████▋  | 307/400 [02:11<00:26,  3.48it/s] 77%|███████▋  | 308/400 [02:12<00:26,  3.50it/s] 77%|███████▋  | 309/400 [02:12<00:25,  3.51it/s] 78%|███████▊  | 310/400 [02:12<00:25,  3.51it/s] 78%|███████▊  | 311/400 [02:12<00:25,  3.52it/s] 78%|███████▊  | 312/400 [02:13<00:25,  3.52it/s] 78%|███████▊  | 313/400 [02:13<00:24,  3.52it/s] 78%|███████▊  | 314/400 [02:13<00:25,  3.34it/s] 79%|███████▉  | 315/400 [02:14<00:25,  3.40it/s] 79%|███████▉  | 316/400 [02:14<00:24,  3.43it/s] 79%|███████▉  | 317/400 [02:14<00:23,  3.47it/s] 80%|███████▉  | 318/400 [02:14<00:23,  3.49it/s] 80%|███████▉  | 319/400 [02:15<00:23,  3.50it/s] 80%|████████  | 320/400 [02:15<00:20,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 09:20:40,884 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:20:40,884 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:20:40,884 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8808, 'eval_samples_per_second': 354.022, 'eval_steps_per_second': 44.329, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.47it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.58it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.15it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.30it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.77it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.33it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.73it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.40it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/438 [00:01<00:09, 40.11it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 41.54it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 42.49it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 43.22it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.80it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.20it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.23it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.21it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.02it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.93it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.10it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.39it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.75it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.79it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.81it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.47it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.27it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.20it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.66it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.79it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.87it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.73it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.43it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 38.38it/s][A
 44%|████▍     | 192/438 [00:04<00:06, 40.15it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 41.57it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 42.54it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.29it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 43.75it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 44.16it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.28it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 43.80it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 43.78it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 43.86it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.09it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.39it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.57it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.80it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.80it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.69it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.40it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.16it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.11it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.20it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.75it/s][A
 70%|███████   | 307/438 [00:06<00:03, 42.90it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.44it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 43.61it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.73it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.82it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.98it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.33it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.54it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.46it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.58it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.62it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.47it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.33it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.27it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.53it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.49it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.49it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.57it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.68it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.65it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.53it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.44it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.39it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.47it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.56it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.55it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 42.22it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 42.22it/s][A 80%|████████  | 320/400 [02:25<00:20,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:20:51,043 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 09:20:51,204 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:20:53,743 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:20:53,827 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:20:53,878 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [02:29<05:48,  4.42s/it] 80%|████████  | 322/400 [02:29<04:07,  3.18s/it] 81%|████████  | 323/400 [02:30<02:57,  2.31s/it] 81%|████████  | 324/400 [02:30<02:09,  1.70s/it] 81%|████████▏ | 325/400 [02:30<01:35,  1.27s/it] 82%|████████▏ | 326/400 [02:30<01:12,  1.02it/s] 82%|████████▏ | 327/400 [02:31<00:56,  1.30it/s] 82%|████████▏ | 328/400 [02:31<00:44,  1.61it/s] 82%|████████▏ | 329/400 [02:31<00:37,  1.90it/s] 82%|████████▎ | 330/400 [02:32<00:31,  2.21it/s] 83%|████████▎ | 331/400 [02:32<00:27,  2.49it/s] 83%|████████▎ | 332/400 [02:32<00:24,  2.73it/s] 83%|████████▎ | 333/400 [02:32<00:22,  2.92it/s] 84%|████████▎ | 334/400 [02:33<00:21,  3.09it/s] 84%|████████▍ | 335/400 [02:33<00:20,  3.21it/s] 84%|████████▍ | 336/400 [02:33<00:19,  3.30it/s] 84%|████████▍ | 337/400 [02:34<00:18,  3.37it/s] 84%|████████▍ | 338/400 [02:34<00:18,  3.41it/s] 85%|████████▍ | 339/400 [02:34<00:17,  3.44it/s] 85%|████████▌ | 340/400 [02:34<00:17,  3.39it/s] 85%|████████▌ | 341/400 [02:35<00:17,  3.43it/s] 86%|████████▌ | 342/400 [02:35<00:16,  3.46it/s] 86%|████████▌ | 343/400 [02:35<00:16,  3.47it/s] 86%|████████▌ | 344/400 [02:36<00:16,  3.49it/s] 86%|████████▋ | 345/400 [02:36<00:15,  3.50it/s] 86%|████████▋ | 346/400 [02:36<00:15,  3.52it/s] 87%|████████▋ | 347/400 [02:36<00:15,  3.53it/s] 87%|████████▋ | 348/400 [02:37<00:14,  3.53it/s] 87%|████████▋ | 349/400 [02:37<00:14,  3.53it/s] 88%|████████▊ | 350/400 [02:37<00:14,  3.53it/s] 88%|████████▊ | 351/400 [02:38<00:14,  3.30it/s] 88%|████████▊ | 352/400 [02:38<00:14,  3.37it/s] 88%|████████▊ | 353/400 [02:38<00:13,  3.41it/s] 88%|████████▊ | 354/400 [02:39<00:13,  3.44it/s] 89%|████████▉ | 355/400 [02:39<00:12,  3.47it/s] 89%|████████▉ | 356/400 [02:39<00:12,  3.49it/s] 89%|████████▉ | 357/400 [02:39<00:12,  3.50it/s] 90%|████████▉ | 358/400 [02:40<00:11,  3.51it/s] 90%|████████▉ | 359/400 [02:40<00:11,  3.52it/s] 90%|█████████ | 360/400 [02:40<00:11,  3.52it/s] 90%|█████████ | 361/400 [02:40<00:11,  3.53it/s] 90%|█████████ | 362/400 [02:41<00:11,  3.41it/s] 91%|█████████ | 363/400 [02:41<00:10,  3.44it/s] 91%|█████████ | 364/400 [02:41<00:10,  3.47it/s] 91%|█████████▏| 365/400 [02:42<00:10,  3.49it/s] 92%|█████████▏| 366/400 [02:42<00:09,  3.50it/s] 92%|█████████▏| 367/400 [02:42<00:09,  3.51it/s] 92%|█████████▏| 368/400 [02:43<00:09,  3.52it/s] 92%|█████████▏| 369/400 [02:43<00:08,  3.52it/s] 92%|█████████▎| 370/400 [02:43<00:08,  3.52it/s] 93%|█████████▎| 371/400 [02:43<00:08,  3.52it/s] 93%|█████████▎| 372/400 [02:44<00:07,  3.53it/s] 93%|█████████▎| 373/400 [02:44<00:07,  3.43it/s] 94%|█████████▎| 374/400 [02:44<00:07,  3.46it/s] 94%|█████████▍| 375/400 [02:45<00:07,  3.48it/s] 94%|█████████▍| 376/400 [02:45<00:06,  3.50it/s] 94%|█████████▍| 377/400 [02:45<00:06,  3.51it/s] 94%|█████████▍| 378/400 [02:45<00:06,  3.51it/s] 95%|█████████▍| 379/400 [02:46<00:05,  3.52it/s] 95%|█████████▌| 380/400 [02:46<00:05,  3.52it/s] 95%|█████████▌| 381/400 [02:46<00:05,  3.53it/s] 96%|█████████▌| 382/400 [02:47<00:05,  3.53it/s] 96%|█████████▌| 383/400 [02:47<00:04,  3.53it/s] 96%|█████████▌| 384/400 [02:47<00:04,  3.36it/s] 96%|█████████▋| 385/400 [02:47<00:04,  3.41it/s] 96%|█████████▋| 386/400 [02:48<00:04,  3.45it/s] 97%|█████████▋| 387/400 [02:48<00:03,  3.48it/s] 97%|█████████▋| 388/400 [02:48<00:03,  3.50it/s] 97%|█████████▋| 389/400 [02:49<00:03,  3.51it/s] 98%|█████████▊| 390/400 [02:49<00:02,  3.52it/s] 98%|█████████▊| 391/400 [02:49<00:02,  3.52it/s] 98%|█████████▊| 392/400 [02:49<00:02,  3.52it/s] 98%|█████████▊| 393/400 [02:50<00:01,  3.53it/s] 98%|█████████▊| 394/400 [02:50<00:01,  3.53it/s] 99%|█████████▉| 395/400 [02:50<00:01,  3.47it/s] 99%|█████████▉| 396/400 [02:51<00:01,  3.49it/s] 99%|█████████▉| 397/400 [02:51<00:00,  3.50it/s]100%|█████████▉| 398/400 [02:51<00:00,  3.51it/s]100%|█████████▉| 399/400 [02:51<00:00,  3.51it/s]100%|██████████| 400/400 [02:52<00:00,  3.01it/s][INFO|trainer.py:2140] 2023-08-28 09:21:17,745 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:21:17,745 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:21:17,745 >>   Batch size = 8
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.9585, 'eval_samples_per_second': 351.259, 'eval_steps_per_second': 43.983, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.41it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.82it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.45it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.61it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.05it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.68it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.47it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.86it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.38it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.22it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.50it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.66it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.83it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.91it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.82it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.43it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.18it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.07it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.18it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.38it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.61it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.72it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.83it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.75it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.45it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.31it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.27it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.39it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.57it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.73it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.80it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.66it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.45it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.35it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.28it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.29it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.49it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.64it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.75it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.70it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.51it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.35it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.38it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.40it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.39it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.57it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.57it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.69it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.72it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.48it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.40it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.34it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.38it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.48it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.50it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.69it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.80it/s][A
 71%|███████   | 312/438 [00:06<00:02, 43.95it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.07it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.04it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.38it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.52it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.66it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.52it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.49it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.46it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.40it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.44it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.49it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.55it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.62it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.68it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.56it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.50it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.50it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.48it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.47it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.48it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.55it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.60it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.65it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.52it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.52it/s][A100%|██████████| 400/400 [03:02<00:00,  3.01it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:21:27,685 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 09:21:27,812 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:21:30,256 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:21:30,350 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:21:30,411 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:21:31,517 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:21:31,533 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80 (score: 1.0478343963623047).
                                                 100%|██████████| 400/400 [03:16<00:00,  3.01it/s]100%|██████████| 400/400 [03:16<00:00,  2.03it/s]
[INFO|trainer.py:1894] 2023-08-28 09:21:42,070 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 09:21:42,346 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:21:46,252 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:21:46,566 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:21:46,715 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:21:47,560 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   train_runtime            = 0:03:16.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   train_samples_per_second =     129.74
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:47,560 >>   train_steps_per_second   =      2.035
{'eval_loss': 1.0478343963623047, 'eval_runtime': 9.8401, 'eval_samples_per_second': 355.483, 'eval_steps_per_second': 44.512, 'epoch': 5.0}
{'train_runtime': 196.5465, 'train_samples_per_second': 129.74, 'train_steps_per_second': 2.035, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 09:21:47 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:21:47,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:21:47,980 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 09:21:47,980 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.11it/s]  3%|▎         | 12/438 [00:00<00:08, 49.07it/s]  4%|▍         | 17/438 [00:00<00:08, 47.41it/s]  5%|▌         | 22/438 [00:00<00:08, 46.62it/s]  6%|▌         | 27/438 [00:00<00:08, 46.09it/s]  7%|▋         | 32/438 [00:00<00:08, 45.79it/s]  8%|▊         | 37/438 [00:00<00:08, 45.60it/s] 10%|▉         | 42/438 [00:00<00:08, 45.17it/s] 11%|█         | 47/438 [00:01<00:08, 44.71it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.38it/s] 13%|█▎        | 57/438 [00:01<00:09, 38.43it/s] 14%|█▍        | 62/438 [00:01<00:09, 40.33it/s] 15%|█▌        | 67/438 [00:01<00:08, 41.72it/s] 16%|█▋        | 72/438 [00:01<00:08, 42.66it/s] 18%|█▊        | 77/438 [00:01<00:08, 43.33it/s] 19%|█▊        | 82/438 [00:01<00:08, 43.85it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.27it/s] 21%|██        | 92/438 [00:02<00:07, 44.28it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.03it/s] 23%|██▎       | 102/438 [00:02<00:07, 43.89it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.05it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.26it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.53it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.64it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.82it/s] 30%|███       | 132/438 [00:02<00:06, 44.86it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.73it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.38it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.21it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.24it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.40it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.65it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.77it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.95it/s] 40%|████      | 177/438 [00:03<00:05, 44.86it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.70it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.45it/s] 44%|████▍     | 192/438 [00:04<00:05, 43.61it/s] 45%|████▍     | 197/438 [00:04<00:05, 43.84it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.03it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.37it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.57it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.75it/s] 51%|█████     | 222/438 [00:05<00:04, 44.71it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.54it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.13it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.24it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.29it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.42it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.50it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.74it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.74it/s] 61%|██████    | 267/438 [00:06<00:03, 44.66it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.46it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.36it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.41it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.37it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.36it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.63it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.69it/s] 70%|███████   | 307/438 [00:06<00:02, 44.79it/s] 71%|███████   | 312/438 [00:07<00:02, 44.62it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.42it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.37it/s] 75%|███████▍  | 327/438 [00:07<00:02, 43.12it/s] 76%|███████▌  | 332/438 [00:07<00:02, 43.67it/s] 77%|███████▋  | 337/438 [00:07<00:02, 43.93it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.26it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.47it/s] 80%|████████  | 352/438 [00:07<00:01, 44.56it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.49it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.40it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.12it/s] 85%|████████▍ | 372/438 [00:08<00:01, 44.27it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.36it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.52it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.58it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.70it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.86it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.89it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.68it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.57it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.53it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.66it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.66it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.82it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.82it/s]100%|██████████| 438/438 [00:09<00:00, 44.40it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:21:57,866 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   eval_loss               =     1.0478
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   eval_samples_per_second =    353.822
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   eval_steps_per_second   =     44.304
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:21:57,866 >>   perplexity              =     2.8515
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:04,908 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:04,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:04,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:04,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:04,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:22:05,736 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:22:05,737 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:22:06,358 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:22:07,474 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:22:07,474 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:10,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:10,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:10,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:10,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:22:10,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:22:11,251 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:22:11,252 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:22:11,965 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:22:12,217 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:22:12,218 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.49it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.48it/s]Extractor Predicting: 23it [00:15,  1.38it/s]Extractor Predicting: 24it [00:16,  1.42it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:17,  1.43it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:20,  1.36it/s]Extractor Predicting: 31it [00:21,  1.38it/s]Extractor Predicting: 32it [00:22,  1.34it/s]Extractor Predicting: 33it [00:23,  1.34it/s]Extractor Predicting: 34it [00:23,  1.32it/s]Extractor Predicting: 35it [00:24,  1.32it/s]Extractor Predicting: 36it [00:25,  1.34it/s]Extractor Predicting: 37it [00:26,  1.28it/s]Extractor Predicting: 38it [00:27,  1.28it/s]Extractor Predicting: 39it [00:27,  1.29it/s]Extractor Predicting: 40it [00:28,  1.29it/s]Extractor Predicting: 41it [00:29,  1.27it/s]Extractor Predicting: 42it [00:30,  1.25it/s]Extractor Predicting: 43it [00:31,  1.25it/s]Extractor Predicting: 44it [00:31,  1.27it/s]Extractor Predicting: 45it [00:32,  1.25it/s]Extractor Predicting: 46it [00:33,  1.24it/s]Extractor Predicting: 47it [00:34,  1.27it/s]Extractor Predicting: 48it [00:34,  1.28it/s]Extractor Predicting: 49it [00:35,  1.25it/s]Extractor Predicting: 50it [00:36,  1.25it/s]Extractor Predicting: 51it [00:37,  1.25it/s]Extractor Predicting: 52it [00:38,  1.25it/s]Extractor Predicting: 53it [00:39,  1.24it/s]Extractor Predicting: 54it [00:39,  1.27it/s]Extractor Predicting: 55it [00:40,  1.28it/s]Extractor Predicting: 56it [00:41,  1.30it/s]Extractor Predicting: 57it [00:42,  1.30it/s]Extractor Predicting: 58it [00:42,  1.26it/s]Extractor Predicting: 59it [00:43,  1.25it/s]Extractor Predicting: 60it [00:44,  1.33it/s]Extractor Predicting: 61it [00:45,  1.34it/s]Extractor Predicting: 62it [00:45,  1.33it/s]Extractor Predicting: 63it [00:46,  1.32it/s]Extractor Predicting: 64it [00:47,  1.35it/s]Extractor Predicting: 65it [00:47,  1.38it/s]Extractor Predicting: 66it [00:48,  1.35it/s]Extractor Predicting: 67it [00:49,  1.35it/s]Extractor Predicting: 68it [00:50,  1.34it/s]Extractor Predicting: 69it [00:51,  1.33it/s]Extractor Predicting: 70it [00:51,  1.36it/s]Extractor Predicting: 71it [00:52,  1.37it/s]Extractor Predicting: 72it [00:53,  1.38it/s]Extractor Predicting: 73it [00:53,  1.39it/s]Extractor Predicting: 74it [00:54,  1.43it/s]Extractor Predicting: 75it [00:55,  1.44it/s]Extractor Predicting: 76it [00:55,  1.44it/s]Extractor Predicting: 77it [00:56,  1.42it/s]Extractor Predicting: 78it [00:57,  1.42it/s]Extractor Predicting: 79it [00:58,  1.42it/s]Extractor Predicting: 80it [00:58,  1.41it/s]Extractor Predicting: 81it [00:59,  1.40it/s]Extractor Predicting: 82it [01:00,  1.38it/s]Extractor Predicting: 83it [01:00,  1.40it/s]Extractor Predicting: 84it [01:01,  1.39it/s]Extractor Predicting: 85it [01:02,  1.39it/s]Extractor Predicting: 86it [01:03,  1.36it/s]Extractor Predicting: 87it [01:03,  1.34it/s]Extractor Predicting: 88it [01:04,  1.39it/s]Extractor Predicting: 89it [01:05,  1.45it/s]Extractor Predicting: 90it [01:05,  1.45it/s]Extractor Predicting: 91it [01:06,  1.48it/s]Extractor Predicting: 92it [01:07,  1.51it/s]Extractor Predicting: 93it [01:07,  1.52it/s]Extractor Predicting: 94it [01:08,  1.55it/s]Extractor Predicting: 95it [01:09,  1.55it/s]Extractor Predicting: 96it [01:09,  1.51it/s]Extractor Predicting: 97it [01:10,  1.50it/s]Extractor Predicting: 98it [01:11,  1.52it/s]Extractor Predicting: 99it [01:11,  1.51it/s]Extractor Predicting: 100it [01:12,  1.49it/s]Extractor Predicting: 101it [01:13,  1.48it/s]Extractor Predicting: 102it [01:13,  1.47it/s]Extractor Predicting: 103it [01:14,  1.49it/s]Extractor Predicting: 104it [01:15,  1.49it/s]Extractor Predicting: 105it [01:15,  1.47it/s]Extractor Predicting: 106it [01:16,  1.48it/s]Extractor Predicting: 107it [01:17,  1.44it/s]Extractor Predicting: 108it [01:17,  1.48it/s]Extractor Predicting: 109it [01:18,  1.51it/s]Extractor Predicting: 110it [01:19,  1.54it/s]Extractor Predicting: 111it [01:19,  1.54it/s]Extractor Predicting: 112it [01:20,  1.54it/s]Extractor Predicting: 113it [01:21,  1.42it/s]Extractor Predicting: 114it [01:21,  1.42it/s]Extractor Predicting: 115it [01:22,  1.44it/s]Extractor Predicting: 116it [01:23,  1.43it/s]Extractor Predicting: 117it [01:24,  1.38it/s]Extractor Predicting: 118it [01:24,  1.39it/s]Extractor Predicting: 119it [01:25,  1.37it/s]Extractor Predicting: 120it [01:26,  1.35it/s]Extractor Predicting: 121it [01:27,  1.32it/s]Extractor Predicting: 122it [01:27,  1.34it/s]Extractor Predicting: 123it [01:28,  1.33it/s]Extractor Predicting: 124it [01:29,  1.32it/s]Extractor Predicting: 125it [01:30,  1.31it/s]Extractor Predicting: 126it [01:30,  1.30it/s]Extractor Predicting: 127it [01:31,  1.29it/s]Extractor Predicting: 128it [01:32,  1.32it/s]Extractor Predicting: 129it [01:33,  1.30it/s]Extractor Predicting: 130it [01:33,  1.34it/s]Extractor Predicting: 131it [01:34,  1.33it/s]Extractor Predicting: 132it [01:35,  1.32it/s]Extractor Predicting: 133it [01:36,  1.29it/s]Extractor Predicting: 134it [01:37,  1.32it/s]Extractor Predicting: 135it [01:37,  1.33it/s]Extractor Predicting: 136it [01:38,  1.31it/s]Extractor Predicting: 137it [01:39,  1.31it/s]Extractor Predicting: 138it [01:40,  1.32it/s]Extractor Predicting: 139it [01:40,  1.34it/s]Extractor Predicting: 140it [01:41,  1.36it/s]Extractor Predicting: 141it [01:42,  1.33it/s]Extractor Predicting: 142it [01:43,  1.33it/s]Extractor Predicting: 143it [01:43,  1.35it/s]Extractor Predicting: 144it [01:44,  1.32it/s]Extractor Predicting: 145it [01:44,  1.77it/s]Extractor Predicting: 145it [01:44,  1.39it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:06,465 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:06,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:06,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:06,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:06,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:24:07,266 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:24:07,267 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:24:07,868 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:24:08,986 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:24:08,986 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:11,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:11,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:11,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:11,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:24:11,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:24:12,791 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:24:12,792 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:24:13,416 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:24:13,625 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:24:13,625 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:08,  1.39it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:12,  1.39it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.36it/s]Extractor Predicting: 21it [00:15,  1.38it/s]Extractor Predicting: 22it [00:16,  1.26it/s]Extractor Predicting: 23it [00:16,  1.31it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.36it/s]Extractor Predicting: 27it [00:19,  1.36it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:21,  1.34it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:22,  1.37it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.42it/s]Extractor Predicting: 34it [00:24,  1.43it/s]Extractor Predicting: 35it [00:25,  1.46it/s]Extractor Predicting: 36it [00:26,  1.40it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:28,  1.38it/s]Extractor Predicting: 40it [00:28,  1.40it/s]Extractor Predicting: 41it [00:29,  1.40it/s]Extractor Predicting: 42it [00:30,  1.42it/s]Extractor Predicting: 43it [00:31,  1.43it/s]Extractor Predicting: 44it [00:31,  1.45it/s]Extractor Predicting: 45it [00:32,  1.45it/s]Extractor Predicting: 46it [00:33,  1.50it/s]Extractor Predicting: 47it [00:33,  1.46it/s]Extractor Predicting: 48it [00:34,  1.43it/s]Extractor Predicting: 49it [00:35,  1.40it/s]Extractor Predicting: 50it [00:36,  1.37it/s]Extractor Predicting: 51it [00:36,  1.41it/s]Extractor Predicting: 52it [00:37,  1.41it/s]Extractor Predicting: 53it [00:38,  1.43it/s]Extractor Predicting: 54it [00:38,  1.47it/s]Extractor Predicting: 55it [00:39,  1.44it/s]Extractor Predicting: 56it [00:40,  1.42it/s]Extractor Predicting: 57it [00:40,  1.41it/s]Extractor Predicting: 58it [00:41,  1.40it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:43,  1.41it/s]Extractor Predicting: 61it [00:43,  1.40it/s]Extractor Predicting: 62it [00:44,  1.41it/s]Extractor Predicting: 63it [00:45,  1.36it/s]Extractor Predicting: 64it [00:46,  1.33it/s]Extractor Predicting: 65it [00:46,  1.38it/s]Extractor Predicting: 66it [00:47,  1.38it/s]Extractor Predicting: 67it [00:48,  1.43it/s]Extractor Predicting: 68it [00:48,  1.41it/s]Extractor Predicting: 69it [00:49,  1.40it/s]Extractor Predicting: 70it [00:50,  1.41it/s]Extractor Predicting: 71it [00:50,  1.39it/s]Extractor Predicting: 72it [00:51,  1.38it/s]Extractor Predicting: 73it [00:52,  1.36it/s]Extractor Predicting: 74it [00:53,  1.39it/s]Extractor Predicting: 75it [00:53,  1.38it/s]Extractor Predicting: 76it [00:54,  1.36it/s]Extractor Predicting: 77it [00:55,  1.37it/s]Extractor Predicting: 78it [00:56,  1.37it/s]Extractor Predicting: 79it [00:56,  1.39it/s]Extractor Predicting: 80it [00:57,  1.43it/s]Extractor Predicting: 81it [00:58,  1.36it/s]Extractor Predicting: 82it [00:58,  1.36it/s]Extractor Predicting: 83it [00:59,  1.36it/s]Extractor Predicting: 84it [01:00,  1.38it/s]Extractor Predicting: 85it [01:01,  1.40it/s]Extractor Predicting: 86it [01:01,  1.40it/s]Extractor Predicting: 87it [01:02,  1.40it/s]Extractor Predicting: 88it [01:03,  1.42it/s]Extractor Predicting: 89it [01:03,  1.44it/s]Extractor Predicting: 90it [01:04,  1.46it/s]Extractor Predicting: 91it [01:05,  1.43it/s]Extractor Predicting: 92it [01:05,  1.42it/s]Extractor Predicting: 93it [01:06,  1.43it/s]Extractor Predicting: 94it [01:07,  1.42it/s]Extractor Predicting: 95it [01:08,  1.42it/s]Extractor Predicting: 96it [01:09,  1.30it/s]Extractor Predicting: 97it [01:09,  1.33it/s]Extractor Predicting: 98it [01:10,  1.34it/s]Extractor Predicting: 99it [01:11,  1.36it/s]Extractor Predicting: 100it [01:11,  1.38it/s]Extractor Predicting: 101it [01:12,  1.37it/s]Extractor Predicting: 102it [01:13,  1.37it/s]Extractor Predicting: 103it [01:14,  1.38it/s]Extractor Predicting: 104it [01:14,  1.39it/s]Extractor Predicting: 105it [01:15,  1.39it/s]Extractor Predicting: 106it [01:16,  1.38it/s]Extractor Predicting: 107it [01:16,  1.39it/s]Extractor Predicting: 108it [01:17,  1.41it/s]Extractor Predicting: 109it [01:18,  1.41it/s]Extractor Predicting: 110it [01:19,  1.39it/s]Extractor Predicting: 111it [01:19,  1.42it/s]Extractor Predicting: 112it [01:20,  1.43it/s]Extractor Predicting: 113it [01:21,  1.41it/s]Extractor Predicting: 114it [01:21,  1.39it/s]Extractor Predicting: 115it [01:22,  1.41it/s]Extractor Predicting: 116it [01:23,  1.39it/s]Extractor Predicting: 117it [01:24,  1.41it/s]Extractor Predicting: 118it [01:24,  1.42it/s]Extractor Predicting: 119it [01:25,  1.40it/s]Extractor Predicting: 120it [01:26,  1.39it/s]Extractor Predicting: 121it [01:26,  1.39it/s]Extractor Predicting: 122it [01:27,  1.41it/s]Extractor Predicting: 123it [01:28,  1.43it/s]Extractor Predicting: 124it [01:28,  1.42it/s]Extractor Predicting: 125it [01:29,  1.42it/s]Extractor Predicting: 126it [01:30,  1.37it/s]Extractor Predicting: 127it [01:31,  1.37it/s]Extractor Predicting: 128it [01:31,  1.42it/s]Extractor Predicting: 129it [01:32,  1.41it/s]Extractor Predicting: 130it [01:33,  1.44it/s]Extractor Predicting: 131it [01:33,  1.43it/s]Extractor Predicting: 132it [01:34,  1.42it/s]Extractor Predicting: 133it [01:35,  1.42it/s]Extractor Predicting: 134it [01:36,  1.41it/s]Extractor Predicting: 135it [01:36,  1.42it/s]Extractor Predicting: 136it [01:37,  1.39it/s]Extractor Predicting: 137it [01:38,  1.41it/s]Extractor Predicting: 138it [01:38,  1.43it/s]Extractor Predicting: 139it [01:39,  1.39it/s]Extractor Predicting: 140it [01:40,  1.42it/s]Extractor Predicting: 140it [01:40,  1.40it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:01,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:01,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:01,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:01,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:01,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:26:02,455 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:26:02,456 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:26:02,796 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:26:03,945 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:26:03,945 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:06,189 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:06,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:06,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:06,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:26:06,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:26:06,630 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:26:06,631 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:26:06,917 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:26:07,119 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:26:07,119 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 3it [00:02,  1.31it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_0', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:20<03:07, 20.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:42<02:51, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<03:14, 27.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:38<02:29, 24.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:00<01:58, 23.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:21<01:31, 22.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:41<01:06, 22.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:57<00:40, 20.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:15<00:19, 19.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:41<00:00, 21.27s/it]Generating: 100%|██████████| 10/10 [03:41<00:00, 22.11s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n']
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n', 'Relation : original language of film or TV show . Context : After the death of his son Richard , his elder son Henry entered the movie business , becoming famous for his role as the bad guy in James Dean \'s 1984 film " The Cabin in the Woods " . Head Entity : The Cabin in the Woods , Tail Entity : original language of film .\n', 'Relation : original language of film or TV show . Context : This film feature the first four seasons of the Showtime series " Shark " , adapted from the novel by William Pipes . Head Entity : Shark , Tail Entity : Italian .\n', 'Relation : original language of film or TV show . Context : In 1994 , , he starred in the Netflix film adaptation of the famous novel " The Book of Revelation " directed by John McEnroe . Head Entity : The Book of Revelation , Tail Entity : Portuguese .\n', 'Relation : original language of film or TV show . Context : She also starred in the TV show " The Big Bang Theory " where she played the role of Dr. Sheldon in the show \'s first season . Head Entity : The Big Bang Theory , Tail Entity : English .\n', 'Relation : original language of film or TV show . Context : The novel was first published by the book " The Big Bang Theory " in 1986 with David Frum and Bill Paxton . Head Entity : The Big Bang Theory , Tail Entity : English language .\n']
{'target': 600, 'success': 10, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 33, 'raw': 96}
{'target': 600, 'success': 45, 'raw': 128}
{'target': 600, 'success': 56, 'raw': 160}
{'target': 600, 'success': 68, 'raw': 192}
{'target': 600, 'success': 81, 'raw': 224}
{'target': 600, 'success': 90, 'raw': 256}
{'target': 600, 'success': 101, 'raw': 288}
{'target': 600, 'success': 116, 'raw': 320}
{'target': 600, 'success': 131, 'raw': 352}
{'target': 600, 'success': 145, 'raw': 384}
{'target': 600, 'success': 154, 'raw': 416}
{'target': 600, 'success': 170, 'raw': 448}
{'target': 600, 'success': 182, 'raw': 480}
{'target': 600, 'success': 197, 'raw': 512}
{'target': 600, 'success': 213, 'raw': 544}
{'target': 600, 'success': 227, 'raw': 576}
{'target': 600, 'success': 243, 'raw': 608}
{'target': 600, 'success': 260, 'raw': 640}
{'target': 600, 'success': 271, 'raw': 672}
{'target': 600, 'success': 285, 'raw': 704}
{'target': 600, 'success': 296, 'raw': 736}
{'target': 600, 'success': 309, 'raw': 768}
{'target': 600, 'success': 324, 'raw': 800}
{'target': 600, 'success': 336, 'raw': 832}
{'target': 600, 'success': 350, 'raw': 864}
{'target': 600, 'success': 362, 'raw': 896}
{'target': 600, 'success': 374, 'raw': 928}
{'target': 600, 'success': 388, 'raw': 960}
{'target': 600, 'success': 399, 'raw': 992}
{'target': 600, 'success': 417, 'raw': 1024}
{'target': 600, 'success': 427, 'raw': 1056}
{'target': 600, 'success': 441, 'raw': 1088}
{'target': 600, 'success': 453, 'raw': 1120}
{'target': 600, 'success': 465, 'raw': 1152}
{'target': 600, 'success': 481, 'raw': 1184}
{'target': 600, 'success': 490, 'raw': 1216}
{'target': 600, 'success': 494, 'raw': 1248}
{'target': 600, 'success': 513, 'raw': 1280}
{'target': 600, 'success': 529, 'raw': 1312}
{'target': 600, 'success': 547, 'raw': 1344}
{'target': 600, 'success': 557, 'raw': 1376}
{'target': 600, 'success': 571, 'raw': 1408}
{'target': 600, 'success': 584, 'raw': 1440}
{'target': 600, 'success': 594, 'raw': 1472}
{'target': 600, 'success': 607, 'raw': 1504}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4035904255319149, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Sire\', \'original language of film or TV show\', \'\', \'In 1993 , he was cast in the comedy drama " Sire " as a former " " Teflon " detective who has just escaped a serial killer .\')', '(\'The History of the Cinema\', \'original language of film or TV show\', \'\', \'There he studied as a journalist and wrote several books about the history of cinema and its influences , including " The History of the Cinema " on BBC Two Channel .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n']
['Relation : sport . Context : Later in the year , the Brazilian national squad , along with two of the stars at the Confederations Cup , were presented with a medal in the 1, 0 - 1 match , held on Sunday , April 4th , 1997 at Estadio Nacional Santiago . Head Entity : March 4th , 1997 , Tail Entity : Brazil national squad .\n', 'Relation : sport . Context : After he was drafted into the NBA under his elder sister , Charlotte Hornets general manager Michael Jordan , his team traded forward Anthony Davis to the Sacramento Kings . Head Entity : NBA , Tail Entity : basketball .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 520, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 565, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 610, 'raw': 896}
{'prompt': 'Relation : sport .', 'success_rate': 0.6808035714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : competition class . Context : Following his graduation in 1957 , his class graduated high school and met in the college at the end of the class 's seventh grade . Head Entity : college , Tail Entity : professional .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 398, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 472, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7025462962962963, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : location .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8536931818181818, 'errors': {'', '(\'Super Nintendo DS\', \'operating system\', \'\', \'The first " Super Nintendo DS " in Japan , released in Japan in March 2008 , was the DS " Pro DS " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n']
['Relation : religion . Context : Later in the year ( 1143–46 , " The Year of God " at the end of the Holy Book ) , " Theodolianus " led the church back to Egypt . Head Entity : Theodolianus , Tail Entity : Theodolianite .\n', 'Relation : religion . Context : After he had recovered from her miscarriage , his elder sister Travissi married Abigail Danczuk . Head Entity : Abigail Danczuk , Tail Entity : Buddhism .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 260, 'raw': 384}
{'target': 600, 'success': 283, 'raw': 416}
{'target': 600, 'success': 304, 'raw': 448}
{'target': 600, 'success': 329, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 503, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 559, 'raw': 864}
{'target': 600, 'success': 580, 'raw': 896}
{'target': 600, 'success': 602, 'raw': 928}
{'prompt': 'Relation : religion .', 'success_rate': 0.6487068965517241, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 11816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_5_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:19, 19.43s/it]Extractor Estimating: 2it [00:20,  8.84s/it]Extractor Estimating: 3it [00:21,  5.14s/it]Extractor Estimating: 4it [00:24,  4.36s/it]Extractor Estimating: 5it [00:25,  3.04s/it]Extractor Estimating: 6it [00:26,  2.29s/it]Extractor Estimating: 7it [00:27,  1.80s/it]Extractor Estimating: 8it [00:27,  1.47s/it]Extractor Estimating: 9it [00:28,  1.27s/it]Extractor Estimating: 10it [00:29,  1.14s/it]Extractor Estimating: 11it [00:30,  1.04s/it]Extractor Estimating: 12it [00:31,  1.01it/s]Extractor Estimating: 13it [00:31,  1.08it/s]Extractor Estimating: 14it [00:32,  1.16it/s]Extractor Estimating: 15it [00:33,  1.17it/s]Extractor Estimating: 16it [00:34,  1.20it/s]Extractor Estimating: 17it [00:35,  1.20it/s]Extractor Estimating: 18it [00:35,  1.21it/s]Extractor Estimating: 19it [00:36,  1.20it/s]Extractor Estimating: 20it [00:37,  1.20it/s]Extractor Estimating: 21it [00:38,  1.14it/s]Extractor Estimating: 22it [00:39,  1.13it/s]Extractor Estimating: 23it [00:40,  1.20it/s]Extractor Estimating: 24it [00:40,  1.24it/s]Extractor Estimating: 25it [00:41,  1.23it/s]Extractor Estimating: 26it [00:42,  1.19it/s]Extractor Estimating: 27it [00:43,  1.22it/s]Extractor Estimating: 28it [00:44,  1.22it/s]Extractor Estimating: 29it [00:45,  1.18it/s]Extractor Estimating: 30it [00:46,  1.16it/s]Extractor Estimating: 31it [00:46,  1.19it/s]Extractor Estimating: 32it [00:47,  1.21it/s]Extractor Estimating: 33it [00:48,  1.24it/s]Extractor Estimating: 34it [00:49,  1.24it/s]Extractor Estimating: 35it [00:50,  1.22it/s]Extractor Estimating: 36it [00:51,  1.16it/s]Extractor Estimating: 37it [00:51,  1.20it/s]Extractor Estimating: 38it [00:52,  1.17it/s]Extractor Estimating: 39it [00:53,  1.15it/s]Extractor Estimating: 40it [00:54,  1.14it/s]Extractor Estimating: 41it [00:55,  1.17it/s]Extractor Estimating: 42it [00:56,  1.11it/s]Extractor Estimating: 43it [00:57,  1.13it/s]Extractor Estimating: 44it [00:58,  1.10it/s]Extractor Estimating: 45it [00:58,  1.15it/s]Extractor Estimating: 46it [00:59,  1.16it/s]Extractor Estimating: 47it [01:00,  1.18it/s]Extractor Estimating: 48it [01:01,  1.16it/s]Extractor Estimating: 49it [01:02,  1.18it/s]Extractor Estimating: 50it [01:03,  1.15it/s]Extractor Estimating: 51it [01:04,  1.17it/s]Extractor Estimating: 52it [01:04,  1.19it/s]Extractor Estimating: 53it [01:05,  1.21it/s]Extractor Estimating: 54it [01:06,  1.23it/s]Extractor Estimating: 55it [01:07,  1.25it/s]Extractor Estimating: 56it [01:08,  1.25it/s]Extractor Estimating: 57it [01:08,  1.25it/s]Extractor Estimating: 58it [01:09,  1.27it/s]Extractor Estimating: 59it [01:10,  1.32it/s]Extractor Estimating: 60it [01:11,  1.30it/s]Extractor Estimating: 61it [01:11,  1.28it/s]Extractor Estimating: 62it [01:12,  1.28it/s]Extractor Estimating: 63it [01:13,  1.29it/s]Extractor Estimating: 64it [01:14,  1.31it/s]Extractor Estimating: 65it [01:14,  1.34it/s]Extractor Estimating: 66it [01:15,  1.33it/s]Extractor Estimating: 67it [01:16,  1.34it/s]Extractor Estimating: 68it [01:17,  1.31it/s]Extractor Estimating: 69it [01:17,  1.34it/s]Extractor Estimating: 70it [01:18,  1.29it/s]Extractor Estimating: 71it [01:19,  1.31it/s]Extractor Estimating: 72it [01:20,  1.35it/s]Extractor Estimating: 73it [01:20,  1.28it/s]Extractor Estimating: 74it [01:21,  1.29it/s]Extractor Estimating: 75it [01:22,  1.24it/s]Extractor Estimating: 76it [01:23,  1.30it/s]Extractor Estimating: 77it [01:24,  1.34it/s]Extractor Estimating: 78it [01:24,  1.25it/s]Extractor Estimating: 79it [01:25,  1.26it/s]Extractor Estimating: 80it [01:26,  1.31it/s]Extractor Estimating: 81it [01:27,  1.30it/s]Extractor Estimating: 82it [01:27,  1.29it/s]Extractor Estimating: 83it [01:28,  1.28it/s]Extractor Estimating: 84it [01:29,  1.34it/s]Extractor Estimating: 85it [01:30,  1.37it/s]Extractor Estimating: 86it [01:30,  1.39it/s]Extractor Estimating: 87it [01:31,  1.34it/s]Extractor Estimating: 88it [01:32,  1.38it/s]Extractor Estimating: 89it [01:33,  1.33it/s]Extractor Estimating: 90it [01:33,  1.35it/s]Extractor Estimating: 91it [01:34,  1.36it/s]Extractor Estimating: 92it [01:35,  1.31it/s]Extractor Estimating: 93it [01:36,  1.32it/s]Extractor Estimating: 94it [01:36,  1.34it/s]Extractor Estimating: 95it [01:37,  1.31it/s]Extractor Estimating: 96it [01:38,  1.34it/s]Extractor Estimating: 97it [01:39,  1.36it/s]Extractor Estimating: 98it [01:39,  1.34it/s]Extractor Estimating: 99it [01:40,  1.34it/s]Extractor Estimating: 100it [01:41,  1.31it/s]Extractor Estimating: 101it [01:42,  1.35it/s]Extractor Estimating: 102it [01:42,  1.39it/s]Extractor Estimating: 103it [01:43,  1.34it/s]Extractor Estimating: 104it [01:44,  1.34it/s]Extractor Estimating: 105it [01:45,  1.28it/s]Extractor Estimating: 106it [01:45,  1.32it/s]Extractor Estimating: 107it [01:46,  1.37it/s]Extractor Estimating: 108it [01:47,  1.42it/s]Extractor Estimating: 109it [01:47,  1.43it/s]Extractor Estimating: 110it [01:48,  1.44it/s]Extractor Estimating: 111it [01:49,  1.43it/s]Extractor Estimating: 112it [01:49,  1.42it/s]Extractor Estimating: 113it [01:50,  1.40it/s]Extractor Estimating: 114it [01:51,  1.44it/s]Extractor Estimating: 115it [01:52,  1.27it/s]Extractor Estimating: 116it [01:53,  1.31it/s]Extractor Estimating: 117it [01:53,  1.34it/s]Extractor Estimating: 118it [01:54,  1.40it/s]Extractor Estimating: 119it [01:55,  1.41it/s]Extractor Estimating: 120it [01:55,  1.39it/s]Extractor Estimating: 121it [01:56,  1.39it/s]Extractor Estimating: 122it [01:57,  1.33it/s]Extractor Estimating: 123it [01:58,  1.35it/s]Extractor Estimating: 124it [01:58,  1.33it/s]Extractor Estimating: 125it [01:59,  1.36it/s]Extractor Estimating: 126it [02:00,  1.38it/s]Extractor Estimating: 127it [02:01,  1.35it/s]Extractor Estimating: 128it [02:01,  1.35it/s]Extractor Estimating: 129it [02:02,  1.33it/s]Extractor Estimating: 130it [02:03,  1.31it/s]Extractor Estimating: 131it [02:04,  1.32it/s]Extractor Estimating: 132it [02:04,  1.39it/s]Extractor Estimating: 133it [02:05,  1.38it/s]Extractor Estimating: 134it [02:06,  1.37it/s]Extractor Estimating: 135it [02:06,  1.36it/s]Extractor Estimating: 136it [02:07,  1.33it/s]Extractor Estimating: 137it [02:08,  1.39it/s]Extractor Estimating: 138it [02:09,  1.42it/s]Extractor Estimating: 139it [02:09,  1.38it/s]Extractor Estimating: 140it [02:10,  1.36it/s]Extractor Estimating: 141it [02:11,  1.35it/s]Extractor Estimating: 142it [02:12,  1.37it/s]Extractor Estimating: 143it [02:12,  1.35it/s]Extractor Estimating: 144it [02:13,  1.30it/s]Extractor Estimating: 145it [02:14,  1.28it/s]Extractor Estimating: 146it [02:15,  1.30it/s]Extractor Estimating: 147it [02:15,  1.31it/s]Extractor Estimating: 148it [02:16,  1.33it/s]Extractor Estimating: 149it [02:17,  1.33it/s]Extractor Estimating: 150it [02:18,  1.33it/s]Extractor Estimating: 151it [02:19,  1.29it/s]Extractor Estimating: 152it [02:19,  1.27it/s]Extractor Estimating: 153it [02:20,  1.31it/s]Extractor Estimating: 154it [02:21,  1.29it/s]Extractor Estimating: 155it [02:22,  1.28it/s]Extractor Estimating: 156it [02:23,  1.21it/s]Extractor Estimating: 157it [02:23,  1.24it/s]Extractor Estimating: 158it [02:24,  1.23it/s]Extractor Estimating: 159it [02:25,  1.17it/s]Extractor Estimating: 160it [02:26,  1.20it/s]Extractor Estimating: 161it [02:27,  1.25it/s]Extractor Estimating: 162it [02:27,  1.24it/s]Extractor Estimating: 163it [02:28,  1.27it/s]Extractor Estimating: 164it [02:29,  1.25it/s]Extractor Estimating: 165it [02:30,  1.27it/s]Extractor Estimating: 166it [02:31,  1.29it/s]Extractor Estimating: 167it [02:31,  1.28it/s]Extractor Estimating: 168it [02:32,  1.26it/s]Extractor Estimating: 169it [02:33,  1.22it/s]Extractor Estimating: 170it [02:34,  1.22it/s]Extractor Estimating: 171it [02:35,  1.23it/s]Extractor Estimating: 172it [02:35,  1.28it/s]Extractor Estimating: 173it [02:36,  1.23it/s]Extractor Estimating: 174it [02:37,  1.25it/s]Extractor Estimating: 175it [02:38,  1.23it/s]Extractor Estimating: 176it [02:39,  1.20it/s]Extractor Estimating: 177it [02:39,  1.28it/s]Extractor Estimating: 178it [02:40,  1.32it/s]Extractor Estimating: 179it [02:41,  1.35it/s]Extractor Estimating: 180it [02:41,  1.38it/s]Extractor Estimating: 181it [02:42,  1.39it/s]Extractor Estimating: 182it [02:43,  1.38it/s]Extractor Estimating: 183it [02:44,  1.40it/s]Extractor Estimating: 184it [02:44,  1.40it/s]Extractor Estimating: 185it [02:45,  1.40it/s]Extractor Estimating: 186it [02:46,  1.37it/s]Extractor Estimating: 187it [02:47,  1.36it/s]Extractor Estimating: 188it [02:47,  1.39it/s]Extractor Estimating: 189it [02:48,  1.38it/s]Extractor Estimating: 190it [02:49,  1.40it/s]Extractor Estimating: 191it [02:49,  1.41it/s]Extractor Estimating: 192it [02:50,  1.34it/s]Extractor Estimating: 193it [02:51,  1.30it/s]Extractor Estimating: 194it [02:52,  1.32it/s]Extractor Estimating: 195it [02:52,  1.34it/s]Extractor Estimating: 196it [02:53,  1.33it/s]Extractor Estimating: 197it [02:54,  1.28it/s]Extractor Estimating: 198it [02:55,  1.34it/s]Extractor Estimating: 199it [02:55,  1.39it/s]Extractor Estimating: 200it [02:56,  1.37it/s]Extractor Estimating: 201it [02:57,  1.35it/s]Extractor Estimating: 202it [02:58,  1.30it/s]Extractor Estimating: 203it [02:58,  1.34it/s]Extractor Estimating: 204it [02:59,  1.23it/s]Extractor Estimating: 205it [03:00,  1.27it/s]Extractor Estimating: 206it [03:01,  1.30it/s]Extractor Estimating: 207it [03:02,  1.31it/s]Extractor Estimating: 208it [03:02,  1.27it/s]Extractor Estimating: 209it [03:03,  1.26it/s]Extractor Estimating: 210it [03:04,  1.25it/s]Extractor Estimating: 211it [03:05,  1.25it/s]Extractor Estimating: 212it [03:06,  1.26it/s]Extractor Estimating: 213it [03:07,  1.21it/s]Extractor Estimating: 214it [03:07,  1.23it/s]Extractor Estimating: 215it [03:08,  1.23it/s]Extractor Estimating: 216it [03:09,  1.26it/s]Extractor Estimating: 217it [03:10,  1.27it/s]Extractor Estimating: 218it [03:10,  1.29it/s]Extractor Estimating: 219it [03:11,  1.28it/s]Extractor Estimating: 220it [03:12,  1.24it/s]Extractor Estimating: 221it [03:13,  1.26it/s]Extractor Estimating: 222it [03:14,  1.26it/s]Extractor Estimating: 223it [03:14,  1.25it/s]Extractor Estimating: 224it [03:15,  1.25it/s]Extractor Estimating: 225it [03:16,  1.25it/s]Extractor Estimating: 226it [03:17,  1.28it/s]Extractor Estimating: 227it [03:18,  1.28it/s]Extractor Estimating: 228it [03:18,  1.30it/s]Extractor Estimating: 229it [03:19,  1.28it/s]Extractor Estimating: 230it [03:20,  1.24it/s]Extractor Estimating: 231it [03:21,  1.22it/s]Extractor Estimating: 232it [03:22,  1.22it/s]Extractor Estimating: 233it [03:22,  1.26it/s]Extractor Estimating: 234it [03:23,  1.18it/s]Extractor Estimating: 235it [03:24,  1.20it/s]Extractor Estimating: 236it [03:25,  1.23it/s]Extractor Estimating: 237it [03:26,  1.25it/s]Extractor Estimating: 238it [03:27,  1.23it/s]Extractor Estimating: 239it [03:27,  1.21it/s]Extractor Estimating: 240it [03:28,  1.24it/s]Extractor Estimating: 241it [03:29,  1.20it/s]Extractor Estimating: 242it [03:30,  1.17it/s]Extractor Estimating: 243it [03:31,  1.19it/s]Extractor Estimating: 244it [03:32,  1.19it/s]Extractor Estimating: 245it [03:32,  1.25it/s]Extractor Estimating: 246it [03:33,  1.22it/s]Extractor Estimating: 247it [03:34,  1.26it/s]Extractor Estimating: 248it [03:35,  1.25it/s]Extractor Estimating: 249it [03:36,  1.22it/s]Extractor Estimating: 250it [03:36,  1.23it/s]Extractor Estimating: 250it [03:36,  1.15it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5004 mean pseudo reward: 0.8798166204882047
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 22739
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22839, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22839, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.485, loss:1291.4705
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.164, loss:1275.1610
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.179, loss:1217.2430
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.161, loss:1207.3030
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.157, loss:1162.6842
>> valid entity prec:0.5515, rec:0.5588, f1:0.5551
>> valid relation prec:0.3571, rec:0.0630, f1:0.1071
>> valid relation with NER prec:0.3571, rec:0.0630, f1:0.1071
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.724, loss:1178.3718
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.164, loss:1082.6851
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.173, loss:1147.7507
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.172, loss:1087.9641
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.168, loss:1106.4082
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5071, rec:0.6530, f1:0.5709
>> valid relation prec:0.3218, rec:0.1119, f1:0.1661
>> valid relation with NER prec:0.3218, rec:0.1119, f1:0.1661
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 2.715, loss:1059.0660
g_step 1200, step 155, avg_time 1.179, loss:1066.2327
g_step 1300, step 46, avg_time 1.167, loss:992.2650
g_step 1400, step 146, avg_time 1.168, loss:1009.5588
g_step 1500, step 37, avg_time 1.173, loss:959.9033
>> valid entity prec:0.4988, rec:0.6570, f1:0.5671
>> valid relation prec:0.2503, rec:0.0707, f1:0.1103
>> valid relation with NER prec:0.2503, rec:0.0707, f1:0.1103
g_step 1600, step 137, avg_time 2.703, loss:956.7535
g_step 1700, step 28, avg_time 1.164, loss:925.4351
g_step 1800, step 128, avg_time 1.163, loss:909.3596
g_step 1900, step 19, avg_time 1.169, loss:907.2711
g_step 2000, step 119, avg_time 1.167, loss:851.7107
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6110, rec:0.4658, f1:0.5286
>> valid relation prec:0.3490, rec:0.0913, f1:0.1448
>> valid relation with NER prec:0.3490, rec:0.0913, f1:0.1448
g_step 2100, step 10, avg_time 2.701, loss:871.7440
g_step 2200, step 110, avg_time 1.162, loss:825.5650
g_step 2300, step 1, avg_time 1.170, loss:833.9981
g_step 2400, step 101, avg_time 1.171, loss:777.1338
g_step 2500, step 201, avg_time 1.168, loss:794.1703
>> valid entity prec:0.5639, rec:0.5437, f1:0.5536
>> valid relation prec:0.3083, rec:0.1048, f1:0.1564
>> valid relation with NER prec:0.3083, rec:0.1048, f1:0.1564
g_step 2600, step 92, avg_time 2.700, loss:788.7865
g_step 2700, step 192, avg_time 1.170, loss:744.4634
g_step 2800, step 83, avg_time 1.164, loss:707.1430
g_step 2900, step 183, avg_time 1.174, loss:745.9727
g_step 3000, step 74, avg_time 1.169, loss:682.3343
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5142, rec:0.6294, f1:0.5660
>> valid relation prec:0.2578, rec:0.1623, f1:0.1992
>> valid relation with NER prec:0.2578, rec:0.1623, f1:0.1992
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 174, avg_time 2.715, loss:714.7347
g_step 3200, step 65, avg_time 1.169, loss:666.0552
g_step 3300, step 165, avg_time 1.160, loss:649.4820
g_step 3400, step 56, avg_time 1.169, loss:648.9347
g_step 3500, step 156, avg_time 1.171, loss:625.9892
>> valid entity prec:0.5860, rec:0.4914, f1:0.5346
>> valid relation prec:0.2726, rec:0.1260, f1:0.1723
>> valid relation with NER prec:0.2726, rec:0.1260, f1:0.1723
g_step 3600, step 47, avg_time 2.683, loss:591.0648
g_step 3700, step 147, avg_time 1.172, loss:608.1780
g_step 3800, step 38, avg_time 1.162, loss:576.2613
g_step 3900, step 138, avg_time 1.170, loss:580.0989
g_step 4000, step 29, avg_time 1.172, loss:590.6661
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5915, rec:0.4661, f1:0.5214
>> valid relation prec:0.2540, rec:0.0950, f1:0.1383
>> valid relation with NER prec:0.2540, rec:0.0950, f1:0.1383
g_step 4100, step 129, avg_time 2.686, loss:535.2714
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 11:22:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 11:22:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_11-22-03_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 11:22:05 - WARNING - datasets.builder -   Using custom data configuration default-849692017e503b7b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-849692017e503b7b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 11:22:06,890 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:22:06,891 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:22:06,892 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:22:06,893 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:22:06,961 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,018 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,018 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,018 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,019 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,019 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:07,019 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 11:22:07,261 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:22:10,406 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 11:22:10,406 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-849692017e503b7b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 11:22:10 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14753bde05f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:03,  1.47ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.50ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.17ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.62ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.93ba/s]100%|██████████| 6/6 [00:01<00:00,  3.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.39ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.28ba/s]100%|██████████| 4/4 [00:00<00:00,  5.42ba/s]100%|██████████| 4/4 [00:00<00:00,  4.78ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.69ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.91ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.66ba/s]100%|██████████| 6/6 [00:00<00:00, 11.06ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.56ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.44ba/s]100%|██████████| 4/4 [00:00<00:00,  9.44ba/s]
[INFO|trainer.py:414] 2023-08-28 11:22:14,914 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 11:22:14,990 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 11:22:14,990 >>   Num examples = 5018
[INFO|trainer.py:1149] 2023-08-28 11:22:14,990 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 11:22:14,990 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 11:22:14,990 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 11:22:14,990 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 11:22:14,990 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<05:00,  1.30it/s]  1%|          | 2/390 [00:01<03:42,  1.74it/s]  1%|          | 3/390 [00:01<02:58,  2.16it/s]  1%|          | 4/390 [00:01<02:35,  2.48it/s]  1%|▏         | 5/390 [00:02<02:20,  2.73it/s]  2%|▏         | 6/390 [00:02<02:11,  2.92it/s]  2%|▏         | 7/390 [00:02<02:07,  3.00it/s]  2%|▏         | 8/390 [00:03<02:02,  3.11it/s]  2%|▏         | 9/390 [00:03<01:59,  3.19it/s]  3%|▎         | 10/390 [00:03<01:57,  3.24it/s]  3%|▎         | 11/390 [00:03<01:55,  3.27it/s]  3%|▎         | 12/390 [00:04<01:58,  3.20it/s]  3%|▎         | 13/390 [00:04<01:56,  3.25it/s]  4%|▎         | 14/390 [00:04<01:54,  3.29it/s]  4%|▍         | 15/390 [00:05<01:53,  3.31it/s]  4%|▍         | 16/390 [00:05<01:52,  3.32it/s]  4%|▍         | 17/390 [00:05<01:51,  3.33it/s]  5%|▍         | 18/390 [00:06<01:51,  3.34it/s]  5%|▍         | 19/390 [00:06<01:50,  3.35it/s]  5%|▌         | 20/390 [00:06<01:50,  3.35it/s]  5%|▌         | 21/390 [00:06<01:50,  3.35it/s]  6%|▌         | 22/390 [00:07<01:49,  3.35it/s]  6%|▌         | 23/390 [00:07<01:49,  3.35it/s]  6%|▌         | 24/390 [00:07<01:49,  3.35it/s]  6%|▋         | 25/390 [00:08<01:48,  3.35it/s]  7%|▋         | 26/390 [00:08<01:48,  3.35it/s]  7%|▋         | 27/390 [00:08<01:48,  3.35it/s]  7%|▋         | 28/390 [00:09<01:48,  3.35it/s]  7%|▋         | 29/390 [00:09<01:47,  3.35it/s]  8%|▊         | 30/390 [00:09<01:47,  3.35it/s]  8%|▊         | 31/390 [00:09<01:47,  3.35it/s]  8%|▊         | 32/390 [00:10<01:46,  3.35it/s]  8%|▊         | 33/390 [00:10<01:46,  3.35it/s]  9%|▊         | 34/390 [00:10<01:49,  3.26it/s]  9%|▉         | 35/390 [00:11<01:48,  3.29it/s]  9%|▉         | 36/390 [00:11<01:47,  3.31it/s]  9%|▉         | 37/390 [00:11<01:46,  3.32it/s] 10%|▉         | 38/390 [00:12<01:45,  3.32it/s] 10%|█         | 39/390 [00:12<01:45,  3.33it/s] 10%|█         | 40/390 [00:12<01:44,  3.34it/s] 11%|█         | 41/390 [00:12<01:44,  3.35it/s] 11%|█         | 42/390 [00:13<01:43,  3.35it/s] 11%|█         | 43/390 [00:13<01:43,  3.35it/s] 11%|█▏        | 44/390 [00:13<01:43,  3.35it/s] 12%|█▏        | 45/390 [00:14<01:42,  3.35it/s] 12%|█▏        | 46/390 [00:14<01:42,  3.35it/s] 12%|█▏        | 47/390 [00:14<01:42,  3.35it/s] 12%|█▏        | 48/390 [00:15<01:42,  3.35it/s] 13%|█▎        | 49/390 [00:15<01:41,  3.35it/s] 13%|█▎        | 50/390 [00:15<01:41,  3.34it/s] 13%|█▎        | 51/390 [00:15<01:41,  3.34it/s] 13%|█▎        | 52/390 [00:16<01:41,  3.34it/s] 14%|█▎        | 53/390 [00:16<01:40,  3.34it/s] 14%|█▍        | 54/390 [00:16<01:40,  3.34it/s] 14%|█▍        | 55/390 [00:17<01:40,  3.35it/s] 14%|█▍        | 56/390 [00:17<01:39,  3.35it/s] 15%|█▍        | 57/390 [00:17<01:39,  3.35it/s] 15%|█▍        | 58/390 [00:18<01:39,  3.35it/s] 15%|█▌        | 59/390 [00:18<01:38,  3.35it/s] 15%|█▌        | 60/390 [00:18<01:38,  3.35it/s] 16%|█▌        | 61/390 [00:18<01:38,  3.35it/s] 16%|█▌        | 62/390 [00:19<01:37,  3.35it/s] 16%|█▌        | 63/390 [00:19<01:37,  3.35it/s] 16%|█▋        | 64/390 [00:19<01:37,  3.34it/s] 17%|█▋        | 65/390 [00:20<01:37,  3.34it/s] 17%|█▋        | 66/390 [00:20<01:36,  3.34it/s] 17%|█▋        | 67/390 [00:20<01:36,  3.34it/s] 17%|█▋        | 68/390 [00:21<01:36,  3.34it/s] 18%|█▊        | 69/390 [00:21<01:36,  3.34it/s] 18%|█▊        | 70/390 [00:21<01:35,  3.34it/s] 18%|█▊        | 71/390 [00:21<01:35,  3.34it/s] 18%|█▊        | 72/390 [00:22<01:35,  3.34it/s] 19%|█▊        | 73/390 [00:22<01:34,  3.34it/s] 19%|█▉        | 74/390 [00:22<01:34,  3.35it/s] 19%|█▉        | 75/390 [00:23<01:34,  3.35it/s] 19%|█▉        | 76/390 [00:23<01:33,  3.35it/s] 20%|█▉        | 77/390 [00:23<01:33,  3.35it/s] 20%|██        | 78/390 [00:24<01:33,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 11:22:39,117 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:22:39,117 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:22:39,117 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.05it/s][A
  3%|▎         | 12/438 [00:00<00:08, 47.68it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.46it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.71it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.25it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.01it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.83it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.69it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.76it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.80it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.64it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.48it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.50it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.57it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.45it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.54it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.55it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.63it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.61it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.57it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.52it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.52it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.53it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.47it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.66it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.59it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.71it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.69it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.55it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.38it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.41it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.48it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.57it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.55it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.63it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.75it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.60it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.49it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.36it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.38it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.49it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.56it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.60it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.67it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.62it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.45it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.34it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.39it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.41it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.57it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.64it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.66it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.72it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.55it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.34it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.23it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.37it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.45it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.63it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.67it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.67it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.54it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.33it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.35it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.42it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.48it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.54it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.69it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.69it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.63it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.53it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.33it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.30it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.30it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.46it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.58it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.69it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.69it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.64it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.47it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.32it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.29it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.35it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.50it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:33,  3.35it/s]
100%|██████████| 438/438 [00:09<00:00, 44.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:22:49,059 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 11:22:49,149 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:22:52,503 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:22:52,632 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:22:52,693 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:44<33:28,  6.46s/it] 21%|██        | 80/390 [00:45<23:52,  4.62s/it] 21%|██        | 81/390 [00:45<17:07,  3.32s/it] 21%|██        | 82/390 [00:45<12:24,  2.42s/it] 21%|██▏       | 83/390 [00:46<09:07,  1.78s/it] 22%|██▏       | 84/390 [00:46<06:49,  1.34s/it] 22%|██▏       | 85/390 [00:46<05:12,  1.03s/it] 22%|██▏       | 86/390 [00:46<04:05,  1.24it/s] 22%|██▏       | 87/390 [00:47<03:18,  1.53it/s] 23%|██▎       | 88/390 [00:47<02:46,  1.81it/s] 23%|██▎       | 89/390 [00:47<02:23,  2.10it/s] 23%|██▎       | 90/390 [00:48<02:08,  2.33it/s] 23%|██▎       | 91/390 [00:48<01:56,  2.57it/s] 24%|██▎       | 92/390 [00:48<01:47,  2.76it/s] 24%|██▍       | 93/390 [00:49<02:02,  2.43it/s] 24%|██▍       | 94/390 [00:49<01:51,  2.65it/s] 24%|██▍       | 95/390 [00:49<01:44,  2.83it/s] 25%|██▍       | 96/390 [00:50<01:39,  2.96it/s] 25%|██▍       | 97/390 [00:50<01:35,  3.07it/s] 25%|██▌       | 98/390 [00:50<01:32,  3.15it/s] 25%|██▌       | 99/390 [00:51<01:30,  3.21it/s] 26%|██▌       | 100/390 [00:51<01:29,  3.25it/s] 26%|██▌       | 101/390 [00:51<01:28,  3.28it/s] 26%|██▌       | 102/390 [00:52<01:27,  3.30it/s] 26%|██▋       | 103/390 [00:52<01:26,  3.32it/s] 27%|██▋       | 104/390 [00:52<01:25,  3.33it/s] 27%|██▋       | 105/390 [00:52<01:25,  3.34it/s] 27%|██▋       | 106/390 [00:53<01:25,  3.34it/s] 27%|██▋       | 107/390 [00:53<01:27,  3.24it/s] 28%|██▊       | 108/390 [00:53<01:26,  3.27it/s] 28%|██▊       | 109/390 [00:54<01:25,  3.30it/s] 28%|██▊       | 110/390 [00:54<01:24,  3.31it/s] 28%|██▊       | 111/390 [00:54<01:23,  3.33it/s] 29%|██▊       | 112/390 [00:55<01:23,  3.33it/s] 29%|██▉       | 113/390 [00:55<01:22,  3.34it/s] 29%|██▉       | 114/390 [00:55<01:22,  3.34it/s] 29%|██▉       | 115/390 [00:55<01:22,  3.34it/s] 30%|██▉       | 116/390 [00:56<01:21,  3.34it/s] 30%|███       | 117/390 [00:56<01:24,  3.22it/s] 30%|███       | 118/390 [00:56<01:23,  3.26it/s] 31%|███       | 119/390 [00:57<01:22,  3.29it/s] 31%|███       | 120/390 [00:57<01:21,  3.31it/s] 31%|███       | 121/390 [00:57<01:20,  3.32it/s] 31%|███▏      | 122/390 [00:58<01:20,  3.35it/s] 32%|███▏      | 123/390 [00:58<01:19,  3.38it/s] 32%|███▏      | 124/390 [00:58<01:18,  3.40it/s] 32%|███▏      | 125/390 [00:58<01:17,  3.41it/s] 32%|███▏      | 126/390 [00:59<01:17,  3.42it/s] 33%|███▎      | 127/390 [00:59<01:16,  3.43it/s] 33%|███▎      | 128/390 [00:59<01:18,  3.34it/s] 33%|███▎      | 129/390 [01:00<01:17,  3.37it/s] 33%|███▎      | 130/390 [01:00<01:16,  3.39it/s] 34%|███▎      | 131/390 [01:00<01:16,  3.41it/s] 34%|███▍      | 132/390 [01:00<01:15,  3.41it/s] 34%|███▍      | 133/390 [01:01<01:15,  3.42it/s] 34%|███▍      | 134/390 [01:01<01:14,  3.43it/s] 35%|███▍      | 135/390 [01:01<01:14,  3.43it/s] 35%|███▍      | 136/390 [01:02<01:13,  3.44it/s] 35%|███▌      | 137/390 [01:02<01:13,  3.44it/s] 35%|███▌      | 138/390 [01:02<01:13,  3.44it/s] 36%|███▌      | 139/390 [01:03<01:13,  3.40it/s] 36%|███▌      | 140/390 [01:03<01:13,  3.41it/s] 36%|███▌      | 141/390 [01:03<01:12,  3.43it/s] 36%|███▋      | 142/390 [01:03<01:12,  3.43it/s] 37%|███▋      | 143/390 [01:04<01:11,  3.43it/s] 37%|███▋      | 144/390 [01:04<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:04<01:11,  3.44it/s] 37%|███▋      | 146/390 [01:05<01:10,  3.44it/s] 38%|███▊      | 147/390 [01:05<01:10,  3.44it/s] 38%|███▊      | 148/390 [01:05<01:10,  3.44it/s] 38%|███▊      | 149/390 [01:05<01:09,  3.44it/s] 38%|███▊      | 150/390 [01:06<01:11,  3.37it/s] 39%|███▊      | 151/390 [01:06<01:10,  3.39it/s] 39%|███▉      | 152/390 [01:06<01:09,  3.41it/s] 39%|███▉      | 153/390 [01:07<01:09,  3.42it/s] 39%|███▉      | 154/390 [01:07<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:07<01:08,  3.43it/s] 40%|████      | 156/390 [01:07<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 11:23:23,072 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:23:23,072 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:23:23,072 >>   Batch size = 8
{'eval_loss': 0.9477631449699402, 'eval_runtime': 9.8335, 'eval_samples_per_second': 355.724, 'eval_steps_per_second': 44.542, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.02it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.19it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.59it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.73it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.84it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.69it/s][A
 11%|█         | 47/438 [00:01<00:09, 42.80it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.49it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 43.91it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.17it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.31it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.32it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.27it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.30it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.15it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.40it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.51it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.60it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.63it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.55it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.41it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.29it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.25it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.40it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.47it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.67it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.68it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.74it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.61it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.51it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.33it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.22it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 42.42it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 43.14it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 43.67it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.12it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.31it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.48it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.40it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.21it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.00it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.11it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.55it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.70it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.77it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.65it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.48it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.36it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.15it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.20it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.32it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.52it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.69it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.71it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.54it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.33it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.17it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 40.44it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 41.72it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 42.71it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.39it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.72it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.24it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.11it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.93it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.95it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.09it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.36it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.59it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.72it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.81it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.77it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.41it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.21it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.10it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.22it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.42it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.54it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.72it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.82it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.72it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:17<01:08,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 44.72it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:23:33,519 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 11:23:33,990 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:23:37,845 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:23:38,060 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:23:38,188 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:30<27:20,  7.04s/it] 41%|████      | 158/390 [01:31<19:25,  5.02s/it] 41%|████      | 159/390 [01:31<13:53,  3.61s/it] 41%|████      | 160/390 [01:31<10:01,  2.61s/it] 41%|████▏     | 161/390 [01:31<07:19,  1.92s/it] 42%|████▏     | 162/390 [01:32<05:26,  1.43s/it] 42%|████▏     | 163/390 [01:32<04:08,  1.09s/it] 42%|████▏     | 164/390 [01:32<03:13,  1.17it/s] 42%|████▏     | 165/390 [01:33<02:34,  1.45it/s] 43%|████▎     | 166/390 [01:33<02:07,  1.75it/s] 43%|████▎     | 167/390 [01:33<01:49,  2.05it/s] 43%|████▎     | 168/390 [01:34<01:37,  2.28it/s] 43%|████▎     | 169/390 [01:34<01:27,  2.52it/s] 44%|████▎     | 170/390 [01:34<01:20,  2.72it/s] 44%|████▍     | 171/390 [01:34<01:15,  2.89it/s] 44%|████▍     | 172/390 [01:35<01:12,  3.01it/s] 44%|████▍     | 173/390 [01:35<01:09,  3.11it/s] 45%|████▍     | 174/390 [01:35<01:07,  3.18it/s] 45%|████▍     | 175/390 [01:36<01:06,  3.23it/s] 45%|████▌     | 176/390 [01:36<01:05,  3.27it/s] 45%|████▌     | 177/390 [01:36<01:04,  3.29it/s] 46%|████▌     | 178/390 [01:37<01:04,  3.27it/s] 46%|████▌     | 179/390 [01:37<01:03,  3.30it/s] 46%|████▌     | 180/390 [01:37<01:03,  3.31it/s] 46%|████▋     | 181/390 [01:37<01:02,  3.33it/s] 47%|████▋     | 182/390 [01:38<01:02,  3.34it/s] 47%|████▋     | 183/390 [01:38<01:01,  3.34it/s] 47%|████▋     | 184/390 [01:38<01:01,  3.35it/s] 47%|████▋     | 185/390 [01:39<01:01,  3.35it/s] 48%|████▊     | 186/390 [01:39<01:00,  3.36it/s] 48%|████▊     | 187/390 [01:39<01:00,  3.36it/s] 48%|████▊     | 188/390 [01:40<01:00,  3.36it/s] 48%|████▊     | 189/390 [01:40<01:00,  3.30it/s] 49%|████▊     | 190/390 [01:40<01:00,  3.32it/s] 49%|████▉     | 191/390 [01:40<00:59,  3.33it/s] 49%|████▉     | 192/390 [01:41<00:59,  3.34it/s] 49%|████▉     | 193/390 [01:41<00:58,  3.35it/s] 50%|████▉     | 194/390 [01:41<00:58,  3.35it/s] 50%|█████     | 195/390 [01:42<00:58,  3.35it/s] 50%|█████     | 196/390 [01:42<00:57,  3.35it/s] 51%|█████     | 197/390 [01:42<00:57,  3.35it/s] 51%|█████     | 198/390 [01:43<00:57,  3.35it/s] 51%|█████     | 199/390 [01:43<00:56,  3.35it/s] 51%|█████▏    | 200/390 [01:43<00:57,  3.28it/s] 52%|█████▏    | 201/390 [01:43<00:57,  3.30it/s] 52%|█████▏    | 202/390 [01:44<00:56,  3.32it/s] 52%|█████▏    | 203/390 [01:44<00:56,  3.33it/s] 52%|█████▏    | 204/390 [01:44<00:55,  3.34it/s] 53%|█████▎    | 205/390 [01:45<00:55,  3.35it/s] 53%|█████▎    | 206/390 [01:45<00:54,  3.35it/s] 53%|█████▎    | 207/390 [01:45<00:54,  3.35it/s] 53%|█████▎    | 208/390 [01:46<00:54,  3.35it/s] 54%|█████▎    | 209/390 [01:46<00:54,  3.35it/s] 54%|█████▍    | 210/390 [01:46<00:55,  3.24it/s] 54%|█████▍    | 211/390 [01:46<00:54,  3.27it/s] 54%|█████▍    | 212/390 [01:47<00:53,  3.30it/s] 55%|█████▍    | 213/390 [01:47<00:54,  3.23it/s] 55%|█████▍    | 214/390 [01:47<00:53,  3.27it/s] 55%|█████▌    | 215/390 [01:48<00:53,  3.30it/s] 55%|█████▌    | 216/390 [01:48<00:52,  3.31it/s] 56%|█████▌    | 217/390 [01:48<00:52,  3.32it/s] 56%|█████▌    | 218/390 [01:49<00:52,  3.28it/s] 56%|█████▌    | 219/390 [01:49<01:00,  2.81it/s] 56%|█████▋    | 220/390 [01:49<00:58,  2.90it/s] 57%|█████▋    | 221/390 [01:50<00:55,  3.02it/s] 57%|█████▋    | 222/390 [01:50<00:53,  3.11it/s] 57%|█████▋    | 223/390 [01:50<00:52,  3.18it/s] 57%|█████▋    | 224/390 [01:51<00:51,  3.23it/s] 58%|█████▊    | 225/390 [01:51<00:50,  3.26it/s] 58%|█████▊    | 226/390 [01:51<00:49,  3.29it/s] 58%|█████▊    | 227/390 [01:51<00:49,  3.30it/s] 58%|█████▊    | 228/390 [01:52<00:48,  3.32it/s] 59%|█████▊    | 229/390 [01:52<00:48,  3.32it/s] 59%|█████▉    | 230/390 [01:52<00:48,  3.33it/s] 59%|█████▉    | 231/390 [01:53<00:47,  3.33it/s] 59%|█████▉    | 232/390 [01:53<00:47,  3.34it/s] 60%|█████▉    | 233/390 [01:53<00:47,  3.34it/s] 60%|██████    | 234/390 [01:54<00:46,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 11:24:09,196 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:24:09,197 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:24:09,197 >>   Batch size = 8
{'eval_loss': 0.9357728958129883, 'eval_runtime': 9.8973, 'eval_samples_per_second': 353.431, 'eval_steps_per_second': 44.255, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/438 [00:00<00:08, 47.93it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.62it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.70it/s][A
  6%|▌         | 27/438 [00:00<00:09, 42.46it/s][A
  7%|▋         | 32/438 [00:00<00:09, 43.23it/s][A
  8%|▊         | 37/438 [00:00<00:09, 43.58it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.84it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.20it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.43it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.56it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.45it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.16it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.23it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.42it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.43it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.51it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.58it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.73it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.73it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.51it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.34it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.38it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.49it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.50it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.56it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.64it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.69it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.73it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.60it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.49it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 42.44it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.21it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.59it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.97it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.36it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.29it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.04it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.14it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.36it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.55it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.64it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.62it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.61it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.63it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.39it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.23it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.19it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.40it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.59it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.63it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.61it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.64it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.45it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.38it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.09it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.04it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.63it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.08it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.33it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.47it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.42it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.36it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.23it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.14it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.30it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.37it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.55it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.71it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.81it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.72it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.53it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.05it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.21it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.36it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.42it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.56it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.75it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.78it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.70it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.50it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.31it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.16it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 42.68it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.36it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [02:04<00:46,  3.34it/s]
100%|██████████| 438/438 [00:09<00:00, 43.36it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:24:19,215 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 11:24:19,410 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:24:22,836 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:24:22,988 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:24:23,107 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:14<16:43,  6.47s/it] 61%|██████    | 236/390 [02:15<11:53,  4.64s/it] 61%|██████    | 237/390 [02:15<08:30,  3.33s/it] 61%|██████    | 238/390 [02:15<06:08,  2.42s/it] 61%|██████▏   | 239/390 [02:16<04:29,  1.79s/it] 62%|██████▏   | 240/390 [02:16<03:20,  1.34s/it] 62%|██████▏   | 241/390 [02:16<02:32,  1.03s/it] 62%|██████▏   | 242/390 [02:17<01:59,  1.24it/s] 62%|██████▏   | 243/390 [02:17<01:36,  1.53it/s] 63%|██████▎   | 244/390 [02:17<01:19,  1.83it/s] 63%|██████▎   | 245/390 [02:17<01:08,  2.11it/s] 63%|██████▎   | 246/390 [02:18<01:03,  2.28it/s] 63%|██████▎   | 247/390 [02:18<00:56,  2.52it/s] 64%|██████▎   | 248/390 [02:18<00:52,  2.72it/s] 64%|██████▍   | 249/390 [02:19<00:48,  2.88it/s] 64%|██████▍   | 250/390 [02:19<00:46,  3.01it/s] 64%|██████▍   | 251/390 [02:19<00:44,  3.10it/s] 65%|██████▍   | 252/390 [02:20<00:43,  3.17it/s] 65%|██████▍   | 253/390 [02:20<00:42,  3.22it/s] 65%|██████▌   | 254/390 [02:20<00:41,  3.26it/s] 65%|██████▌   | 255/390 [02:21<00:41,  3.29it/s] 66%|██████▌   | 256/390 [02:21<00:42,  3.13it/s] 66%|██████▌   | 257/390 [02:21<00:41,  3.20it/s] 66%|██████▌   | 258/390 [02:21<00:40,  3.24it/s] 66%|██████▋   | 259/390 [02:22<00:39,  3.28it/s] 67%|██████▋   | 260/390 [02:22<00:39,  3.30it/s] 67%|██████▋   | 261/390 [02:22<00:38,  3.31it/s] 67%|██████▋   | 262/390 [02:23<00:38,  3.32it/s] 67%|██████▋   | 263/390 [02:23<00:38,  3.33it/s] 68%|██████▊   | 264/390 [02:23<00:37,  3.34it/s] 68%|██████▊   | 265/390 [02:24<00:37,  3.34it/s] 68%|██████▊   | 266/390 [02:24<00:37,  3.34it/s] 68%|██████▊   | 267/390 [02:24<00:36,  3.35it/s] 69%|██████▊   | 268/390 [02:24<00:36,  3.35it/s] 69%|██████▉   | 269/390 [02:25<00:36,  3.35it/s] 69%|██████▉   | 270/390 [02:25<00:35,  3.35it/s] 69%|██████▉   | 271/390 [02:25<00:38,  3.12it/s] 70%|██████▉   | 272/390 [02:26<00:37,  3.18it/s] 70%|███████   | 273/390 [02:26<00:36,  3.23it/s] 70%|███████   | 274/390 [02:26<00:35,  3.26it/s] 71%|███████   | 275/390 [02:27<00:34,  3.29it/s] 71%|███████   | 276/390 [02:27<00:34,  3.31it/s] 71%|███████   | 277/390 [02:27<00:34,  3.32it/s] 71%|███████▏  | 278/390 [02:28<00:33,  3.33it/s] 72%|███████▏  | 279/390 [02:28<00:33,  3.34it/s] 72%|███████▏  | 280/390 [02:28<00:32,  3.34it/s] 72%|███████▏  | 281/390 [02:29<00:35,  3.10it/s] 72%|███████▏  | 282/390 [02:29<00:34,  3.17it/s] 73%|███████▎  | 283/390 [02:29<00:33,  3.22it/s] 73%|███████▎  | 284/390 [02:29<00:32,  3.26it/s] 73%|███████▎  | 285/390 [02:30<00:31,  3.28it/s] 73%|███████▎  | 286/390 [02:30<00:31,  3.30it/s] 74%|███████▎  | 287/390 [02:30<00:31,  3.31it/s] 74%|███████▍  | 288/390 [02:31<00:30,  3.32it/s] 74%|███████▍  | 289/390 [02:31<00:30,  3.32it/s] 74%|███████▍  | 290/390 [02:31<00:30,  3.33it/s] 75%|███████▍  | 291/390 [02:32<00:30,  3.20it/s] 75%|███████▍  | 292/390 [02:32<00:30,  3.24it/s] 75%|███████▌  | 293/390 [02:32<00:29,  3.28it/s] 75%|███████▌  | 294/390 [02:32<00:29,  3.30it/s] 76%|███████▌  | 295/390 [02:33<00:28,  3.32it/s] 76%|███████▌  | 296/390 [02:33<00:28,  3.33it/s] 76%|███████▌  | 297/390 [02:33<00:27,  3.34it/s] 76%|███████▋  | 298/390 [02:34<00:27,  3.35it/s] 77%|███████▋  | 299/390 [02:34<00:27,  3.35it/s] 77%|███████▋  | 300/390 [02:34<00:26,  3.35it/s] 77%|███████▋  | 301/390 [02:35<00:27,  3.29it/s] 77%|███████▋  | 302/390 [02:35<00:26,  3.31it/s] 78%|███████▊  | 303/390 [02:35<00:26,  3.32it/s] 78%|███████▊  | 304/390 [02:35<00:25,  3.33it/s] 78%|███████▊  | 305/390 [02:36<00:25,  3.33it/s] 78%|███████▊  | 306/390 [02:36<00:25,  3.34it/s] 79%|███████▊  | 307/390 [02:36<00:24,  3.34it/s] 79%|███████▉  | 308/390 [02:37<00:24,  3.35it/s] 79%|███████▉  | 309/390 [02:37<00:24,  3.35it/s] 79%|███████▉  | 310/390 [02:37<00:23,  3.35it/s] 80%|███████▉  | 311/390 [02:38<00:23,  3.29it/s] 80%|████████  | 312/390 [02:38<00:23,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 11:24:53,450 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:24:53,450 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:24:53,450 >>   Batch size = 8
{'eval_loss': 0.9376245141029358, 'eval_runtime': 9.8972, 'eval_samples_per_second': 353.433, 'eval_steps_per_second': 44.255, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.68it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.10it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.57it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.88it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.10it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.87it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.67it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.52it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.60it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.76it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.83it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.73it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.66it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.49it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.46it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.39it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.35it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.56it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.65it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.70it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 42.03it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 42.77it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.29it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.50it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.76it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.94it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.26it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.39it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.30it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.42it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.62it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.40it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.40it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.45it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.54it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.44it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.32it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.53it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.57it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.60it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.57it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.51it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.57it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.58it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.52it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.53it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.20it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.01it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 43.57it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.92it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.22it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.27it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.28it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.18it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.35it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.58it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.64it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.58it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.57it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.47it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.22it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.22it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.22it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.26it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.13it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.67it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.80it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.75it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.56it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.39it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.39it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 42.34it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 43.07it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.68it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.07it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.16it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.39it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 41.33it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 42.37it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 42.80it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.34it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.78it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.21it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.42it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:48<00:23,  3.31it/s]
100%|██████████| 438/438 [00:09<00:00, 44.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:25:03,547 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 11:25:03,768 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:25:06,678 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:25:06,817 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:25:06,870 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:58<08:12,  6.40s/it] 81%|████████  | 314/390 [02:59<05:47,  4.57s/it] 81%|████████  | 315/390 [02:59<04:06,  3.29s/it] 81%|████████  | 316/390 [02:59<02:57,  2.39s/it] 81%|████████▏ | 317/390 [03:00<02:08,  1.76s/it] 82%|████████▏ | 318/390 [03:00<01:35,  1.32s/it] 82%|████████▏ | 319/390 [03:00<01:12,  1.02s/it] 82%|████████▏ | 320/390 [03:01<00:56,  1.25it/s] 82%|████████▏ | 321/390 [03:01<00:44,  1.54it/s] 83%|████████▎ | 322/390 [03:01<00:37,  1.84it/s] 83%|████████▎ | 323/390 [03:01<00:31,  2.12it/s] 83%|████████▎ | 324/390 [03:02<00:28,  2.32it/s] 83%|████████▎ | 325/390 [03:02<00:25,  2.56it/s] 84%|████████▎ | 326/390 [03:02<00:23,  2.75it/s] 84%|████████▍ | 327/390 [03:03<00:21,  2.90it/s] 84%|████████▍ | 328/390 [03:03<00:20,  3.01it/s] 84%|████████▍ | 329/390 [03:03<00:19,  3.10it/s] 85%|████████▍ | 330/390 [03:04<00:18,  3.16it/s] 85%|████████▍ | 331/390 [03:04<00:18,  3.21it/s] 85%|████████▌ | 332/390 [03:04<00:17,  3.25it/s] 85%|████████▌ | 333/390 [03:04<00:17,  3.29it/s] 86%|████████▌ | 334/390 [03:05<00:17,  3.25it/s] 86%|████████▌ | 335/390 [03:05<00:16,  3.31it/s] 86%|████████▌ | 336/390 [03:05<00:16,  3.35it/s] 86%|████████▋ | 337/390 [03:06<00:15,  3.38it/s] 87%|████████▋ | 338/390 [03:06<00:15,  3.40it/s] 87%|████████▋ | 339/390 [03:06<00:14,  3.41it/s] 87%|████████▋ | 340/390 [03:07<00:14,  3.42it/s] 87%|████████▋ | 341/390 [03:07<00:14,  3.43it/s] 88%|████████▊ | 342/390 [03:07<00:13,  3.44it/s] 88%|████████▊ | 343/390 [03:07<00:13,  3.44it/s] 88%|████████▊ | 344/390 [03:08<00:13,  3.45it/s] 88%|████████▊ | 345/390 [03:08<00:13,  3.31it/s] 89%|████████▊ | 346/390 [03:08<00:13,  3.35it/s] 89%|████████▉ | 347/390 [03:09<00:12,  3.38it/s] 89%|████████▉ | 348/390 [03:09<00:12,  3.40it/s] 89%|████████▉ | 349/390 [03:09<00:12,  3.41it/s] 90%|████████▉ | 350/390 [03:09<00:11,  3.42it/s] 90%|█████████ | 351/390 [03:10<00:11,  3.43it/s] 90%|█████████ | 352/390 [03:10<00:11,  3.43it/s] 91%|█████████ | 353/390 [03:10<00:10,  3.44it/s] 91%|█████████ | 354/390 [03:11<00:10,  3.44it/s] 91%|█████████ | 355/390 [03:11<00:10,  3.44it/s] 91%|█████████▏| 356/390 [03:11<00:10,  3.35it/s] 92%|█████████▏| 357/390 [03:12<00:09,  3.38it/s] 92%|█████████▏| 358/390 [03:12<00:09,  3.40it/s] 92%|█████████▏| 359/390 [03:12<00:09,  3.41it/s] 92%|█████████▏| 360/390 [03:12<00:08,  3.42it/s] 93%|█████████▎| 361/390 [03:13<00:08,  3.43it/s] 93%|█████████▎| 362/390 [03:13<00:08,  3.43it/s] 93%|█████████▎| 363/390 [03:13<00:07,  3.44it/s] 93%|█████████▎| 364/390 [03:14<00:07,  3.44it/s] 94%|█████████▎| 365/390 [03:14<00:07,  3.44it/s] 94%|█████████▍| 366/390 [03:14<00:06,  3.44it/s] 94%|█████████▍| 367/390 [03:14<00:06,  3.34it/s] 94%|█████████▍| 368/390 [03:15<00:06,  3.37it/s] 95%|█████████▍| 369/390 [03:15<00:06,  3.39it/s] 95%|█████████▍| 370/390 [03:15<00:05,  3.40it/s] 95%|█████████▌| 371/390 [03:16<00:05,  3.41it/s] 95%|█████████▌| 372/390 [03:16<00:05,  3.41it/s] 96%|█████████▌| 373/390 [03:16<00:04,  3.42it/s] 96%|█████████▌| 374/390 [03:17<00:04,  3.43it/s] 96%|█████████▌| 375/390 [03:17<00:04,  3.43it/s] 96%|█████████▋| 376/390 [03:17<00:04,  3.43it/s] 97%|█████████▋| 377/390 [03:17<00:03,  3.44it/s] 97%|█████████▋| 378/390 [03:18<00:03,  3.24it/s] 97%|█████████▋| 379/390 [03:18<00:03,  3.30it/s] 97%|█████████▋| 380/390 [03:18<00:02,  3.34it/s] 98%|█████████▊| 381/390 [03:19<00:02,  3.37it/s] 98%|█████████▊| 382/390 [03:19<00:02,  3.39it/s] 98%|█████████▊| 383/390 [03:19<00:02,  3.41it/s] 98%|█████████▊| 384/390 [03:19<00:01,  3.42it/s] 99%|█████████▊| 385/390 [03:20<00:01,  3.43it/s] 99%|█████████▉| 386/390 [03:20<00:01,  3.43it/s] 99%|█████████▉| 387/390 [03:20<00:00,  3.43it/s] 99%|█████████▉| 388/390 [03:21<00:00,  3.44it/s]100%|█████████▉| 389/390 [03:21<00:00,  3.30it/s]100%|██████████| 390/390 [03:21<00:00,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 11:25:36,765 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:25:36,765 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:25:36,765 >>   Batch size = 8
{'eval_loss': 0.9408739805221558, 'eval_runtime': 9.9158, 'eval_samples_per_second': 352.771, 'eval_steps_per_second': 44.172, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.87it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.03it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.88it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.64it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.19it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.91it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.71it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.59it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.63it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.77it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.85it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.72it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.50it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.38it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.37it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.37it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.43it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.40it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.68it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.04it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.24it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.34it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.37it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.26it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.30it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.31it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.39it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.45it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.56it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.71it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.65it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.49it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.37it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.31it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.42it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.44it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.58it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.71it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.66it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.64it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.59it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.44it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.45it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.37it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.43it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 43.61it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.00it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.30it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.47it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.47it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.34it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.34it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.34it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.36it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.53it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.68it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.68it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.63it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.50it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.34it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.40it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.35it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.45it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.59it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.70it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.81it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.70it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.50it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.36it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.33it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.39it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 42.33it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.11it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.67it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.03it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.23it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.22it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.06it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.27it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.09it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.25it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.45it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.57it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.74it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.65it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.57it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:31<00:00,  3.34it/s]
100%|██████████| 438/438 [00:09<00:00, 44.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:25:46,705 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 11:25:46,841 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:25:49,570 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:25:49,665 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:25:49,704 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 11:25:56,061 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 11:25:56,101 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156 (score: 0.9357728958129883).
                                                 100%|██████████| 390/390 [03:50<00:00,  3.34it/s]100%|██████████| 390/390 [03:50<00:00,  1.69it/s]
[INFO|trainer.py:1894] 2023-08-28 11:26:05,843 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 11:26:05,978 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:26:09,046 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:26:09,157 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:26:09,211 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:26:10,119 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   train_loss               =     0.8329
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   train_runtime            = 0:03:50.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   train_samples            =       5018
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   train_samples_per_second =    108.769
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:10,119 >>   train_steps_per_second   =      1.691
{'eval_loss': 0.9427183866500854, 'eval_runtime': 9.861, 'eval_samples_per_second': 354.731, 'eval_steps_per_second': 44.417, 'epoch': 4.99}
{'train_runtime': 230.6716, 'train_samples_per_second': 108.769, 'train_steps_per_second': 1.691, 'train_loss': 0.8328768999148638, 'epoch': 4.99}
08/28/2023 11:26:10 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 11:26:10,414 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:26:10,414 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 11:26:10,414 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.81it/s]  3%|▎         | 12/438 [00:00<00:08, 49.32it/s]  4%|▍         | 17/438 [00:00<00:08, 47.50it/s]  5%|▌         | 22/438 [00:00<00:08, 46.58it/s]  6%|▌         | 27/438 [00:00<00:08, 46.11it/s]  7%|▋         | 32/438 [00:00<00:08, 45.75it/s]  8%|▊         | 37/438 [00:00<00:08, 45.48it/s] 10%|▉         | 42/438 [00:00<00:08, 45.17it/s] 11%|█         | 47/438 [00:01<00:08, 44.60it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.52it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.64it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.76it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.83it/s] 16%|█▋        | 72/438 [00:01<00:08, 44.79it/s] 18%|█▊        | 77/438 [00:01<00:08, 44.85it/s] 19%|█▊        | 82/438 [00:01<00:07, 44.83it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.58it/s] 21%|██        | 92/438 [00:02<00:07, 44.42it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.34it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.00it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.35it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.49it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.62it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.78it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.63it/s] 30%|███       | 132/438 [00:02<00:06, 44.28it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.22it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.29it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.42it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.47it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.59it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.75it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.75it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.73it/s] 40%|████      | 177/438 [00:03<00:05, 44.53it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.36it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.46it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.58it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.54it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.62it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.67it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.75it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.68it/s] 51%|█████     | 222/438 [00:04<00:04, 44.57it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.46it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.47it/s] 54%|█████▍    | 237/438 [00:05<00:04, 41.10it/s] 55%|█████▌    | 242/438 [00:05<00:04, 42.15it/s] 56%|█████▋    | 247/438 [00:05<00:04, 43.03it/s] 58%|█████▊    | 252/438 [00:05<00:04, 43.59it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.08it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.36it/s] 61%|██████    | 267/438 [00:05<00:03, 44.35it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.43it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.05it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.12it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.42it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.52it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.73it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.74it/s] 70%|███████   | 307/438 [00:06<00:02, 44.69it/s] 71%|███████   | 312/438 [00:06<00:02, 44.62it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.43it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.25it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.25it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.36it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.56it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.65it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.81it/s] 80%|████████  | 352/438 [00:07<00:01, 44.80it/s] 82%|████████▏ | 357/438 [00:08<00:01, 44.59it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.41it/s] 84%|████████▍ | 367/438 [00:08<00:01, 44.30it/s] 85%|████████▍ | 372/438 [00:08<00:01, 41.57it/s] 86%|████████▌ | 377/438 [00:08<00:01, 42.52it/s] 87%|████████▋ | 382/438 [00:08<00:01, 43.26it/s] 88%|████████▊ | 387/438 [00:08<00:01, 43.80it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.16it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.34it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.18it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.20it/s] 94%|█████████▍| 412/438 [00:09<00:00, 43.89it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.03it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.18it/s] 97%|█████████▋| 427/438 [00:09<00:00, 43.03it/s] 99%|█████████▊| 432/438 [00:09<00:00, 43.68it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.09it/s]100%|██████████| 438/438 [00:09<00:00, 44.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:26:20,301 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   eval_loss               =     0.9358
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   eval_samples_per_second =    353.808
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   eval_steps_per_second   =     44.302
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:26:20,301 >>   perplexity              =     2.5492
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:31,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:31,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:31,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:31,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:31,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:26:32,468 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:26:32,469 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:26:33,125 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:26:34,306 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:26:34,306 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:37,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:37,514 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:37,514 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:37,514 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:26:37,514 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:26:38,432 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:26:38,433 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:26:39,117 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:26:39,399 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:26:39,399 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.26it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:08,  1.36it/s]Extractor Predicting: 12it [00:08,  1.34it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.33it/s]Extractor Predicting: 15it [00:11,  1.34it/s]Extractor Predicting: 16it [00:11,  1.33it/s]Extractor Predicting: 17it [00:12,  1.32it/s]Extractor Predicting: 18it [00:13,  1.34it/s]Extractor Predicting: 19it [00:14,  1.37it/s]Extractor Predicting: 20it [00:14,  1.35it/s]Extractor Predicting: 21it [00:15,  1.33it/s]Extractor Predicting: 22it [00:16,  1.34it/s]Extractor Predicting: 23it [00:17,  1.35it/s]Extractor Predicting: 24it [00:17,  1.36it/s]Extractor Predicting: 25it [00:18,  1.33it/s]Extractor Predicting: 26it [00:19,  1.33it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:20,  1.35it/s]Extractor Predicting: 29it [00:21,  1.30it/s]Extractor Predicting: 30it [00:22,  1.25it/s]Extractor Predicting: 31it [00:23,  1.27it/s]Extractor Predicting: 32it [00:24,  1.24it/s]Extractor Predicting: 33it [00:24,  1.24it/s]Extractor Predicting: 34it [00:25,  1.23it/s]Extractor Predicting: 35it [00:26,  1.23it/s]Extractor Predicting: 36it [00:27,  1.24it/s]Extractor Predicting: 37it [00:28,  1.22it/s]Extractor Predicting: 38it [00:29,  1.21it/s]Extractor Predicting: 39it [00:29,  1.21it/s]Extractor Predicting: 40it [00:30,  1.19it/s]Extractor Predicting: 41it [00:31,  1.19it/s]Extractor Predicting: 42it [00:32,  1.18it/s]Extractor Predicting: 43it [00:33,  1.18it/s]Extractor Predicting: 44it [00:34,  1.18it/s]Extractor Predicting: 45it [00:35,  1.19it/s]Extractor Predicting: 46it [00:35,  1.18it/s]Extractor Predicting: 47it [00:36,  1.19it/s]Extractor Predicting: 48it [00:37,  1.20it/s]Extractor Predicting: 49it [00:38,  1.20it/s]Extractor Predicting: 50it [00:39,  1.19it/s]Extractor Predicting: 51it [00:40,  1.18it/s]Extractor Predicting: 52it [00:41,  1.11it/s]Extractor Predicting: 53it [00:41,  1.12it/s]Extractor Predicting: 54it [00:42,  1.17it/s]Extractor Predicting: 55it [00:43,  1.18it/s]Extractor Predicting: 56it [00:44,  1.19it/s]Extractor Predicting: 57it [00:45,  1.20it/s]Extractor Predicting: 58it [00:46,  1.18it/s]Extractor Predicting: 59it [00:46,  1.17it/s]Extractor Predicting: 60it [00:47,  1.23it/s]Extractor Predicting: 61it [00:48,  1.25it/s]Extractor Predicting: 62it [00:49,  1.26it/s]Extractor Predicting: 63it [00:50,  1.24it/s]Extractor Predicting: 64it [00:50,  1.24it/s]Extractor Predicting: 65it [00:51,  1.28it/s]Extractor Predicting: 66it [00:52,  1.26it/s]Extractor Predicting: 67it [00:53,  1.27it/s]Extractor Predicting: 68it [00:54,  1.23it/s]Extractor Predicting: 69it [00:54,  1.23it/s]Extractor Predicting: 70it [00:55,  1.26it/s]Extractor Predicting: 71it [00:56,  1.27it/s]Extractor Predicting: 72it [00:57,  1.27it/s]Extractor Predicting: 73it [00:57,  1.28it/s]Extractor Predicting: 74it [00:58,  1.31it/s]Extractor Predicting: 75it [00:59,  1.32it/s]Extractor Predicting: 76it [01:00,  1.32it/s]Extractor Predicting: 77it [01:00,  1.30it/s]Extractor Predicting: 78it [01:01,  1.31it/s]Extractor Predicting: 79it [01:02,  1.31it/s]Extractor Predicting: 80it [01:03,  1.30it/s]Extractor Predicting: 81it [01:04,  1.28it/s]Extractor Predicting: 82it [01:04,  1.28it/s]Extractor Predicting: 83it [01:05,  1.30it/s]Extractor Predicting: 84it [01:06,  1.28it/s]Extractor Predicting: 85it [01:07,  1.28it/s]Extractor Predicting: 86it [01:08,  1.26it/s]Extractor Predicting: 87it [01:08,  1.27it/s]Extractor Predicting: 88it [01:09,  1.30it/s]Extractor Predicting: 89it [01:10,  1.35it/s]Extractor Predicting: 90it [01:10,  1.34it/s]Extractor Predicting: 91it [01:11,  1.37it/s]Extractor Predicting: 92it [01:12,  1.39it/s]Extractor Predicting: 93it [01:13,  1.39it/s]Extractor Predicting: 94it [01:13,  1.41it/s]Extractor Predicting: 95it [01:14,  1.40it/s]Extractor Predicting: 96it [01:15,  1.37it/s]Extractor Predicting: 97it [01:15,  1.38it/s]Extractor Predicting: 98it [01:16,  1.39it/s]Extractor Predicting: 99it [01:17,  1.38it/s]Extractor Predicting: 100it [01:18,  1.36it/s]Extractor Predicting: 101it [01:18,  1.35it/s]Extractor Predicting: 102it [01:19,  1.36it/s]Extractor Predicting: 103it [01:20,  1.37it/s]Extractor Predicting: 104it [01:21,  1.37it/s]Extractor Predicting: 105it [01:21,  1.34it/s]Extractor Predicting: 106it [01:22,  1.36it/s]Extractor Predicting: 107it [01:23,  1.33it/s]Extractor Predicting: 108it [01:24,  1.35it/s]Extractor Predicting: 109it [01:24,  1.36it/s]Extractor Predicting: 110it [01:25,  1.39it/s]Extractor Predicting: 111it [01:26,  1.39it/s]Extractor Predicting: 112it [01:26,  1.41it/s]Extractor Predicting: 113it [01:27,  1.39it/s]Extractor Predicting: 114it [01:28,  1.36it/s]Extractor Predicting: 115it [01:29,  1.36it/s]Extractor Predicting: 116it [01:29,  1.34it/s]Extractor Predicting: 117it [01:30,  1.30it/s]Extractor Predicting: 118it [01:31,  1.30it/s]Extractor Predicting: 119it [01:32,  1.25it/s]Extractor Predicting: 120it [01:33,  1.23it/s]Extractor Predicting: 121it [01:34,  1.23it/s]Extractor Predicting: 122it [01:34,  1.26it/s]Extractor Predicting: 123it [01:35,  1.24it/s]Extractor Predicting: 124it [01:36,  1.23it/s]Extractor Predicting: 125it [01:37,  1.23it/s]Extractor Predicting: 126it [01:38,  1.22it/s]Extractor Predicting: 127it [01:38,  1.20it/s]Extractor Predicting: 128it [01:39,  1.22it/s]Extractor Predicting: 129it [01:40,  1.23it/s]Extractor Predicting: 130it [01:41,  1.26it/s]Extractor Predicting: 131it [01:44,  1.38s/it]Extractor Predicting: 132it [01:44,  1.20s/it]Extractor Predicting: 133it [01:45,  1.16s/it]Extractor Predicting: 134it [01:46,  1.04s/it]Extractor Predicting: 135it [01:47,  1.04it/s]Extractor Predicting: 136it [01:48,  1.09it/s]Extractor Predicting: 137it [01:50,  1.17s/it]Extractor Predicting: 138it [01:50,  1.06s/it]Extractor Predicting: 139it [01:51,  1.02s/it]Extractor Predicting: 140it [01:52,  1.04it/s]Extractor Predicting: 141it [01:53,  1.08it/s]Extractor Predicting: 142it [01:54,  1.12it/s]Extractor Predicting: 143it [01:54,  1.16it/s]Extractor Predicting: 144it [01:55,  1.16it/s]Extractor Predicting: 145it [01:56,  1.53it/s]Extractor Predicting: 145it [01:56,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:50,700 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:50,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:50,731 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:50,731 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:50,731 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:28:51,645 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:28:51,646 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:28:52,291 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:28:53,399 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:28:53,399 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:56,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:56,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:56,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:56,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:56,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:28:57,102 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:28:57,103 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:28:57,748 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:28:57,966 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:28:57,966 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.36018411967779057,
  "recall": 0.17895940537449972,
  "score": 0.23911382734912148,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.25it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.31it/s]Extractor Predicting: 13it [00:10,  1.28it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.34it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:14,  1.28it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.27it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.23it/s]Extractor Predicting: 23it [00:18,  1.26it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:20,  1.27it/s]Extractor Predicting: 27it [00:21,  1.27it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.26it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:25,  1.29it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.31it/s]Extractor Predicting: 35it [00:27,  1.34it/s]Extractor Predicting: 36it [00:28,  1.29it/s]Extractor Predicting: 37it [00:28,  1.23it/s]Extractor Predicting: 38it [00:29,  1.25it/s]Extractor Predicting: 39it [00:30,  1.25it/s]Extractor Predicting: 40it [00:31,  1.28it/s]Extractor Predicting: 41it [00:32,  1.28it/s]Extractor Predicting: 42it [00:32,  1.29it/s]Extractor Predicting: 43it [00:33,  1.30it/s]Extractor Predicting: 44it [00:34,  1.35it/s]Extractor Predicting: 45it [00:34,  1.34it/s]Extractor Predicting: 46it [00:35,  1.39it/s]Extractor Predicting: 47it [00:36,  1.33it/s]Extractor Predicting: 48it [00:37,  1.31it/s]Extractor Predicting: 49it [00:38,  1.30it/s]Extractor Predicting: 50it [00:38,  1.28it/s]Extractor Predicting: 51it [00:39,  1.29it/s]Extractor Predicting: 52it [00:40,  1.29it/s]Extractor Predicting: 53it [00:41,  1.31it/s]Extractor Predicting: 54it [00:41,  1.33it/s]Extractor Predicting: 55it [00:42,  1.32it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:44,  1.28it/s]Extractor Predicting: 58it [00:45,  1.29it/s]Extractor Predicting: 59it [00:45,  1.29it/s]Extractor Predicting: 60it [00:46,  1.30it/s]Extractor Predicting: 61it [00:47,  1.30it/s]Extractor Predicting: 62it [00:48,  1.28it/s]Extractor Predicting: 63it [00:48,  1.26it/s]Extractor Predicting: 64it [00:49,  1.24it/s]Extractor Predicting: 65it [00:50,  1.28it/s]Extractor Predicting: 66it [00:51,  1.27it/s]Extractor Predicting: 67it [00:51,  1.31it/s]Extractor Predicting: 68it [00:52,  1.32it/s]Extractor Predicting: 69it [00:53,  1.30it/s]Extractor Predicting: 70it [00:54,  1.30it/s]Extractor Predicting: 71it [00:55,  1.29it/s]Extractor Predicting: 72it [00:55,  1.28it/s]Extractor Predicting: 73it [00:56,  1.27it/s]Extractor Predicting: 74it [00:57,  1.27it/s]Extractor Predicting: 75it [00:58,  1.27it/s]Extractor Predicting: 76it [00:59,  1.25it/s]Extractor Predicting: 77it [00:59,  1.26it/s]Extractor Predicting: 78it [01:00,  1.26it/s]Extractor Predicting: 79it [01:01,  1.28it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.25it/s]Extractor Predicting: 82it [01:03,  1.25it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.29it/s]Extractor Predicting: 85it [01:06,  1.30it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:07,  1.30it/s]Extractor Predicting: 88it [01:08,  1.34it/s]Extractor Predicting: 89it [01:09,  1.35it/s]Extractor Predicting: 90it [01:09,  1.36it/s]Extractor Predicting: 91it [01:10,  1.32it/s]Extractor Predicting: 92it [01:11,  1.31it/s]Extractor Predicting: 93it [01:12,  1.33it/s]Extractor Predicting: 94it [01:12,  1.30it/s]Extractor Predicting: 95it [01:13,  1.30it/s]Extractor Predicting: 96it [01:14,  1.29it/s]Extractor Predicting: 97it [01:15,  1.29it/s]Extractor Predicting: 98it [01:16,  1.28it/s]Extractor Predicting: 99it [01:16,  1.29it/s]Extractor Predicting: 100it [01:17,  1.29it/s]Extractor Predicting: 101it [01:18,  1.29it/s]Extractor Predicting: 102it [01:19,  1.27it/s]Extractor Predicting: 103it [01:19,  1.28it/s]Extractor Predicting: 104it [01:20,  1.28it/s]Extractor Predicting: 105it [01:21,  1.29it/s]Extractor Predicting: 106it [01:22,  1.29it/s]Extractor Predicting: 107it [01:23,  1.29it/s]Extractor Predicting: 108it [01:23,  1.30it/s]Extractor Predicting: 109it [01:24,  1.30it/s]Extractor Predicting: 110it [01:25,  1.28it/s]Extractor Predicting: 111it [01:26,  1.30it/s]Extractor Predicting: 112it [01:26,  1.31it/s]Extractor Predicting: 113it [01:27,  1.29it/s]Extractor Predicting: 114it [01:28,  1.26it/s]Extractor Predicting: 115it [01:29,  1.29it/s]Extractor Predicting: 116it [01:30,  1.29it/s]Extractor Predicting: 117it [01:30,  1.30it/s]Extractor Predicting: 118it [01:31,  1.30it/s]Extractor Predicting: 119it [01:32,  1.29it/s]Extractor Predicting: 120it [01:33,  1.28it/s]Extractor Predicting: 121it [01:33,  1.29it/s]Extractor Predicting: 122it [01:34,  1.22it/s]Extractor Predicting: 123it [01:35,  1.25it/s]Extractor Predicting: 124it [01:36,  1.26it/s]Extractor Predicting: 125it [01:37,  1.28it/s]Extractor Predicting: 126it [01:37,  1.26it/s]Extractor Predicting: 127it [01:38,  1.27it/s]Extractor Predicting: 128it [01:39,  1.31it/s]Extractor Predicting: 129it [01:40,  1.30it/s]Extractor Predicting: 130it [01:40,  1.31it/s]Extractor Predicting: 131it [01:41,  1.34it/s]Extractor Predicting: 132it [01:42,  1.31it/s]Extractor Predicting: 133it [01:43,  1.31it/s]Extractor Predicting: 134it [01:44,  1.29it/s]Extractor Predicting: 135it [01:44,  1.30it/s]Extractor Predicting: 136it [01:45,  1.31it/s]Extractor Predicting: 137it [01:46,  1.30it/s]Extractor Predicting: 138it [01:47,  1.32it/s]Extractor Predicting: 139it [01:47,  1.28it/s]Extractor Predicting: 140it [01:48,  1.33it/s]Extractor Predicting: 140it [01:48,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:30:54,684 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:30:54,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:30:54,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:30:54,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:30:54,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:30:55,507 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:30:55,508 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:30:56,104 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:30:57,215 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:30:57,215 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:31:00,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:31:00,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:31:00,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:31:00,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:31:00,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:31:00,958 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:31:00,959 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:31:01,574 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:31:01,775 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:31:01,775 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35731039874902265,
  "recall": 0.1361740166865316,
  "score": 0.19719525350593314,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 3it [00:02,  1.29it/s]
[INFO|configuration_utils.py:515] 2023-08-28 11:31:05,331 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:31:05,333 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:31:05,370 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:31:05,371 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 11:31:05,387 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:31:13,461 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 11:31:13,490 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 11:31:13,627 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:31:13,627 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:31:13,707 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:31:13,733 >> loading file outputs/wrapper/fewrel/unseen_5_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6363636363636364,
  "recall": 0.09722222222222222,
  "score": 0.16867469879518074,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 11:31:14,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:14,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:15,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:16,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:17,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:18,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:19,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:20,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:22,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:22,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:23,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:24,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:25,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:26,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:27,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:28,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:29,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:29,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:30,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:31,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:32,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:52, 19.16s/it][WARNING|generation_utils.py:914] 2023-08-28 11:31:33,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:33,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:34,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:35,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:36,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:37,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:38,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:39,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:40,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:41,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:42,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:43,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:44,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:45,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:46,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:46,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:47,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:48,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:49,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:50,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:51,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:52,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:52,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:53,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:40<02:42, 20.28s/it][WARNING|generation_utils.py:914] 2023-08-28 11:31:54,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:54,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:55,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:56,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:57,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:57,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:58,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:31:59,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:00,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:00,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:01,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:02,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:02,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:03,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:04,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:04,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:05,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:06,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:07,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:07,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:08,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:09,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:10,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:10,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:11,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:12,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:12,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:13,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:00<02:20, 20.12s/it][WARNING|generation_utils.py:914] 2023-08-28 11:32:14,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:14,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:15,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:16,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:17,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:18,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:18,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:19,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:20,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:21,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:22,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:22,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:23,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:24,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:25,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:25,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:26,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:27,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:27,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:28,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:29,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:30,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:16<01:52, 18.77s/it][WARNING|generation_utils.py:914] 2023-08-28 11:32:30,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:31,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:32,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:33,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:33,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:34,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:35,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:36,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:36,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:37,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:38,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:38,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:39,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:40,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:41,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:41,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:42,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:43,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:44,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:44,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:45,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:46,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:47,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:33<01:30, 18.18s/it][WARNING|generation_utils.py:914] 2023-08-28 11:32:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:48,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:49,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:50,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:51,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:51,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:53,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:54,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:54,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:55,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:56,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:56,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:57,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:58,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:58,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:32:59,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:00,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:00,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:01,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:02,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:03,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:04,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:04,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:51<01:11, 17.92s/it][WARNING|generation_utils.py:914] 2023-08-28 11:33:05,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:06,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:07,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:08,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:08,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:09,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:10,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:11,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:12,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:12,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:13,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:14,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:15,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:16,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:17,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:17,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:18,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:20,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:21,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:21,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:22,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:23,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:24,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:24,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:25,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:12<00:56, 18.90s/it][WARNING|generation_utils.py:914] 2023-08-28 11:33:26,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:26,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:27,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:28,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:28,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:29,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:30,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:30,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:31,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:32,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:33,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:33,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:34,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:35,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:35,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:36,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:36,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:37,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:38,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:39,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:39,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:40,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:27<00:35, 17.56s/it][WARNING|generation_utils.py:914] 2023-08-28 11:33:41,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:41,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:42,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:43,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:44,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:44,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:45,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:46,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:47,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:48,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:48,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:49,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:50,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:51,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:52,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:53,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:54,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:55,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:55,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:56,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:57,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:44<00:17, 17.46s/it][WARNING|generation_utils.py:914] 2023-08-28 11:33:58,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:59,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:33:59,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:00,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:01,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:02,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:03,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:04,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:04,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:05,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:06,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:07,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:08,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:09,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:09,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:11,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:11,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:12,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:13,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:14,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:15,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:16,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:17,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:18,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:34:18,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:05<00:00, 18.66s/it]Generating: 100%|██████████| 10/10 [03:05<00:00, 18.56s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:26,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:26,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:26,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:26,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:26,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:34:27,258 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:34:27,259 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:34:27,871 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:34:29,014 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:34:29,014 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:31,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:31,966 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:31,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:31,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:34:31,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:34:32,699 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:34:32,700 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:34:33,309 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:34:33,547 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:34:33,547 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 269, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 359, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 491, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 599, 'raw': 864}
{'target': 600, 'success': 613, 'raw': 896}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.6841517857142857, 'errors': {'', '(\'The Big Lebowski\', \'original language of film or TV show\', \'\', \'In 2010 , " The Big Lebowski " premiered in Spanish and Italian and was adapted into the television series " The Big Lebowski " .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : sport .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'2008 Beijing Olympics\', \'sport\', \'\', "The 2008 Beijing Olympics were won on the day by the Beijing Lions of the WBO , who competed in the men \'s 100 - m hurdles .")', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.796875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.77, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : operating system . Context : The Cintell ( also known under the trademark " Cintell " ) is a Linux based personal computer running on a " Windows 64 " operating system , running Linux kernel , OS X . Head Entity : Linux 64 , Tail Entity : Linux .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : owned by . Context : Later in the year , the property was listed on a real estate market that valued at $14.2 million . Head Entity : Real estate market , Tail Entity : Land .\n']
['Relation : owned by . Context : Later in the year , the property was listed on a real estate market that valued at $14.2 million . Head Entity : Real estate market , Tail Entity : Land .\n', 'Relation : owned by . Context : The first two of these are the Mankato Bridge , built by the Finnish government , and the Mokkivy Bridge , built by the Norwegian Railways . Head Entity : Mankato Bridge , Tail Entity : Scandinavian Railways .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : religion .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 10073
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10173, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.25it/s]Extractor Estimating: 2it [00:01,  1.11it/s]Extractor Estimating: 3it [00:02,  1.14it/s]Extractor Estimating: 4it [00:03,  1.17it/s]Extractor Estimating: 5it [00:04,  1.20it/s]Extractor Estimating: 6it [00:05,  1.20it/s]Extractor Estimating: 7it [00:05,  1.27it/s]Extractor Estimating: 8it [00:06,  1.30it/s]Extractor Estimating: 9it [00:07,  1.32it/s]Extractor Estimating: 10it [00:07,  1.35it/s]Extractor Estimating: 11it [00:08,  1.30it/s]Extractor Estimating: 12it [00:09,  1.29it/s]Extractor Estimating: 13it [00:10,  1.27it/s]Extractor Estimating: 14it [00:11,  1.32it/s]Extractor Estimating: 15it [00:11,  1.31it/s]Extractor Estimating: 16it [00:12,  1.35it/s]Extractor Estimating: 17it [00:13,  1.32it/s]Extractor Estimating: 18it [00:14,  1.32it/s]Extractor Estimating: 19it [00:14,  1.26it/s]Extractor Estimating: 20it [00:15,  1.25it/s]Extractor Estimating: 21it [00:16,  1.30it/s]Extractor Estimating: 22it [00:17,  1.31it/s]Extractor Estimating: 23it [00:18,  1.30it/s]Extractor Estimating: 24it [00:18,  1.31it/s]Extractor Estimating: 25it [00:19,  1.32it/s]Extractor Estimating: 26it [00:20,  1.28it/s]Extractor Estimating: 27it [00:21,  1.22it/s]Extractor Estimating: 28it [00:22,  1.23it/s]Extractor Estimating: 29it [00:22,  1.21it/s]Extractor Estimating: 30it [00:23,  1.21it/s]Extractor Estimating: 31it [00:24,  1.20it/s]Extractor Estimating: 32it [00:25,  1.23it/s]Extractor Estimating: 33it [00:26,  1.16it/s]Extractor Estimating: 34it [00:27,  1.18it/s]Extractor Estimating: 35it [00:27,  1.19it/s]Extractor Estimating: 36it [00:28,  1.24it/s]Extractor Estimating: 37it [00:29,  1.19it/s]Extractor Estimating: 38it [00:30,  1.13it/s]Extractor Estimating: 39it [00:31,  1.19it/s]Extractor Estimating: 40it [00:32,  1.19it/s]Extractor Estimating: 41it [00:33,  1.14it/s]Extractor Estimating: 42it [00:33,  1.18it/s]Extractor Estimating: 43it [00:34,  1.18it/s]Extractor Estimating: 44it [00:35,  1.16it/s]Extractor Estimating: 45it [00:36,  1.17it/s]Extractor Estimating: 46it [00:37,  1.18it/s]Extractor Estimating: 47it [00:38,  1.25it/s]Extractor Estimating: 48it [00:38,  1.27it/s]Extractor Estimating: 49it [00:39,  1.22it/s]Extractor Estimating: 50it [00:40,  1.22it/s]Extractor Estimating: 51it [00:41,  1.26it/s]Extractor Estimating: 52it [00:41,  1.31it/s]Extractor Estimating: 53it [00:42,  1.25it/s]Extractor Estimating: 54it [00:43,  1.27it/s]Extractor Estimating: 55it [00:44,  1.32it/s]Extractor Estimating: 56it [00:45,  1.31it/s]Extractor Estimating: 57it [00:45,  1.31it/s]Extractor Estimating: 58it [00:46,  1.33it/s]Extractor Estimating: 59it [00:47,  1.34it/s]Extractor Estimating: 60it [00:47,  1.34it/s]Extractor Estimating: 61it [00:48,  1.34it/s]Extractor Estimating: 62it [00:49,  1.39it/s]Extractor Estimating: 63it [00:50,  1.37it/s]Extractor Estimating: 64it [00:50,  1.39it/s]Extractor Estimating: 65it [00:51,  1.38it/s]Extractor Estimating: 66it [00:52,  1.35it/s]Extractor Estimating: 67it [00:53,  1.36it/s]Extractor Estimating: 68it [00:53,  1.35it/s]Extractor Estimating: 69it [00:54,  1.33it/s]Extractor Estimating: 70it [00:55,  1.31it/s]Extractor Estimating: 71it [00:56,  1.31it/s]Extractor Estimating: 72it [00:57,  1.27it/s]Extractor Estimating: 73it [00:57,  1.30it/s]Extractor Estimating: 74it [00:58,  1.31it/s]Extractor Estimating: 75it [00:59,  1.33it/s]Extractor Estimating: 76it [00:59,  1.34it/s]Extractor Estimating: 77it [01:00,  1.35it/s]Extractor Estimating: 78it [01:01,  1.39it/s]Extractor Estimating: 79it [01:02,  1.37it/s]Extractor Estimating: 80it [01:02,  1.42it/s]Extractor Estimating: 81it [01:03,  1.34it/s]Extractor Estimating: 82it [01:04,  1.41it/s]Extractor Estimating: 83it [01:04,  1.39it/s]Extractor Estimating: 84it [01:05,  1.36it/s]Extractor Estimating: 85it [01:06,  1.41it/s]Extractor Estimating: 86it [01:07,  1.46it/s]Extractor Estimating: 87it [01:07,  1.43it/s]Extractor Estimating: 88it [01:08,  1.41it/s]Extractor Estimating: 89it [01:09,  1.44it/s]Extractor Estimating: 90it [01:09,  1.45it/s]Extractor Estimating: 91it [01:10,  1.41it/s]Extractor Estimating: 92it [01:11,  1.46it/s]Extractor Estimating: 93it [01:11,  1.45it/s]Extractor Estimating: 94it [01:12,  1.48it/s]Extractor Estimating: 95it [01:13,  1.43it/s]Extractor Estimating: 96it [01:14,  1.35it/s]Extractor Estimating: 97it [01:14,  1.38it/s]Extractor Estimating: 98it [01:15,  1.30it/s]Extractor Estimating: 99it [01:16,  1.29it/s]Extractor Estimating: 100it [01:17,  1.30it/s]Extractor Estimating: 101it [01:17,  1.34it/s]Extractor Estimating: 102it [01:18,  1.39it/s]Extractor Estimating: 103it [01:19,  1.39it/s]Extractor Estimating: 104it [01:19,  1.42it/s]Extractor Estimating: 105it [01:20,  1.42it/s]Extractor Estimating: 106it [01:21,  1.39it/s]Extractor Estimating: 107it [01:22,  1.43it/s]Extractor Estimating: 108it [01:22,  1.48it/s]Extractor Estimating: 109it [01:23,  1.47it/s]Extractor Estimating: 110it [01:24,  1.50it/s]Extractor Estimating: 111it [01:24,  1.45it/s]Extractor Estimating: 112it [01:25,  1.45it/s]Extractor Estimating: 113it [01:26,  1.49it/s]Extractor Estimating: 114it [01:26,  1.48it/s]Extractor Estimating: 115it [01:27,  1.51it/s]Extractor Estimating: 116it [01:28,  1.49it/s]Extractor Estimating: 117it [01:28,  1.53it/s]Extractor Estimating: 118it [01:29,  1.41it/s]Extractor Estimating: 119it [01:30,  1.42it/s]Extractor Estimating: 120it [01:30,  1.43it/s]Extractor Estimating: 121it [01:31,  1.44it/s]Extractor Estimating: 122it [01:32,  1.45it/s]Extractor Estimating: 123it [01:32,  1.44it/s]Extractor Estimating: 124it [01:33,  1.43it/s]Extractor Estimating: 125it [01:34,  1.42it/s]Extractor Estimating: 126it [01:35,  1.42it/s]Extractor Estimating: 127it [01:35,  1.42it/s]Extractor Estimating: 128it [01:36,  1.40it/s]Extractor Estimating: 129it [01:37,  1.46it/s]Extractor Estimating: 130it [01:37,  1.43it/s]Extractor Estimating: 131it [01:38,  1.42it/s]Extractor Estimating: 132it [01:39,  1.43it/s]Extractor Estimating: 133it [01:40,  1.40it/s]Extractor Estimating: 134it [01:40,  1.38it/s]Extractor Estimating: 135it [01:41,  1.45it/s]Extractor Estimating: 136it [01:42,  1.43it/s]Extractor Estimating: 137it [01:42,  1.45it/s]Extractor Estimating: 138it [01:43,  1.49it/s]Extractor Estimating: 139it [01:44,  1.50it/s]Extractor Estimating: 140it [01:44,  1.53it/s]Extractor Estimating: 141it [01:45,  1.51it/s]Extractor Estimating: 142it [01:46,  1.53it/s]Extractor Estimating: 143it [01:46,  1.52it/s]Extractor Estimating: 144it [01:47,  1.55it/s]Extractor Estimating: 145it [01:48,  1.51it/s]Extractor Estimating: 146it [01:48,  1.52it/s]Extractor Estimating: 147it [01:49,  1.55it/s]Extractor Estimating: 148it [01:49,  1.53it/s]Extractor Estimating: 149it [01:50,  1.48it/s]Extractor Estimating: 150it [01:51,  1.46it/s]Extractor Estimating: 151it [01:52,  1.43it/s]Extractor Estimating: 152it [01:52,  1.42it/s]Extractor Estimating: 153it [01:53,  1.37it/s]Extractor Estimating: 154it [01:54,  1.36it/s]Extractor Estimating: 155it [01:55,  1.33it/s]Extractor Estimating: 156it [01:56,  1.29it/s]Extractor Estimating: 157it [01:56,  1.30it/s]Extractor Estimating: 158it [01:57,  1.28it/s]Extractor Estimating: 159it [01:58,  1.27it/s]Extractor Estimating: 160it [01:59,  1.25it/s]Extractor Estimating: 161it [01:59,  1.27it/s]Extractor Estimating: 162it [02:00,  1.29it/s]Extractor Estimating: 163it [02:01,  1.31it/s]Extractor Estimating: 164it [02:02,  1.27it/s]Extractor Estimating: 165it [02:03,  1.27it/s]Extractor Estimating: 166it [02:03,  1.29it/s]Extractor Estimating: 167it [02:04,  1.27it/s]Extractor Estimating: 168it [02:05,  1.26it/s]Extractor Estimating: 169it [02:06,  1.25it/s]Extractor Estimating: 170it [02:07,  1.19it/s]Extractor Estimating: 171it [02:07,  1.23it/s]Extractor Estimating: 172it [02:08,  1.24it/s]Extractor Estimating: 173it [02:09,  1.26it/s]Extractor Estimating: 174it [02:10,  1.26it/s]Extractor Estimating: 175it [02:11,  1.28it/s]Extractor Estimating: 176it [02:11,  1.33it/s]Extractor Estimating: 177it [02:12,  1.38it/s]Extractor Estimating: 178it [02:13,  1.44it/s]Extractor Estimating: 179it [02:13,  1.52it/s]Extractor Estimating: 180it [02:14,  1.48it/s]Extractor Estimating: 181it [02:15,  1.34it/s]Extractor Estimating: 182it [02:15,  1.43it/s]Extractor Estimating: 183it [02:16,  1.47it/s]Extractor Estimating: 184it [02:17,  1.52it/s]Extractor Estimating: 185it [02:17,  1.48it/s]Extractor Estimating: 186it [02:18,  1.42it/s]Extractor Estimating: 187it [02:19,  1.41it/s]Extractor Estimating: 188it [02:19,  1.44it/s]Extractor Estimating: 189it [02:20,  1.45it/s]Extractor Estimating: 190it [02:21,  1.44it/s]Extractor Estimating: 191it [02:21,  1.45it/s]Extractor Estimating: 192it [02:22,  1.42it/s]Extractor Estimating: 193it [02:23,  1.44it/s]Extractor Estimating: 194it [02:24,  1.47it/s]Extractor Estimating: 195it [02:24,  1.53it/s]Extractor Estimating: 196it [02:25,  1.53it/s]Extractor Estimating: 197it [02:26,  1.49it/s]Extractor Estimating: 198it [02:26,  1.48it/s]Extractor Estimating: 199it [02:27,  1.52it/s]Extractor Estimating: 200it [02:27,  1.52it/s]Extractor Estimating: 201it [02:28,  1.52it/s]Extractor Estimating: 202it [02:29,  1.44it/s]Extractor Estimating: 203it [02:30,  1.41it/s]Extractor Estimating: 204it [02:30,  1.37it/s]Extractor Estimating: 205it [02:31,  1.31it/s]Extractor Estimating: 206it [02:32,  1.31it/s]Extractor Estimating: 207it [02:33,  1.27it/s]Extractor Estimating: 208it [02:34,  1.27it/s]Extractor Estimating: 209it [02:34,  1.34it/s]Extractor Estimating: 210it [02:35,  1.33it/s]Extractor Estimating: 211it [02:36,  1.35it/s]Extractor Estimating: 212it [02:37,  1.34it/s]Extractor Estimating: 213it [02:37,  1.33it/s]Extractor Estimating: 214it [02:38,  1.29it/s]Extractor Estimating: 215it [02:39,  1.38it/s]Extractor Estimating: 216it [02:39,  1.40it/s]Extractor Estimating: 217it [02:40,  1.40it/s]Extractor Estimating: 218it [02:41,  1.33it/s]Extractor Estimating: 219it [02:42,  1.32it/s]Extractor Estimating: 220it [02:43,  1.30it/s]Extractor Estimating: 221it [02:43,  1.34it/s]Extractor Estimating: 222it [02:44,  1.32it/s]Extractor Estimating: 223it [02:45,  1.31it/s]Extractor Estimating: 224it [02:45,  1.35it/s]Extractor Estimating: 225it [02:46,  1.36it/s]Extractor Estimating: 226it [02:47,  1.38it/s]Extractor Estimating: 227it [02:48,  1.31it/s]Extractor Estimating: 228it [02:49,  1.26it/s]Extractor Estimating: 229it [02:49,  1.28it/s]Extractor Estimating: 230it [02:50,  1.35it/s]Extractor Estimating: 231it [02:51,  1.33it/s]Extractor Estimating: 232it [02:52,  1.31it/s]Extractor Estimating: 233it [02:52,  1.26it/s]Extractor Estimating: 234it [02:53,  1.25it/s]Extractor Estimating: 235it [02:54,  1.23it/s]Extractor Estimating: 236it [02:55,  1.24it/s]Extractor Estimating: 237it [02:56,  1.28it/s]Extractor Estimating: 238it [02:56,  1.30it/s]Extractor Estimating: 239it [02:57,  1.29it/s]Extractor Estimating: 240it [02:58,  1.31it/s]Extractor Estimating: 241it [02:59,  1.34it/s]Extractor Estimating: 242it [02:59,  1.37it/s]Extractor Estimating: 243it [03:00,  1.33it/s]Extractor Estimating: 244it [03:01,  1.33it/s]Extractor Estimating: 245it [03:02,  1.30it/s]Extractor Estimating: 246it [03:02,  1.31it/s]Extractor Estimating: 247it [03:03,  1.30it/s]Extractor Estimating: 248it [03:04,  1.32it/s]Extractor Estimating: 249it [03:05,  1.32it/s]Extractor Estimating: 250it [03:05,  1.30it/s]Extractor Estimating: 250it [03:05,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:37:59,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:37:59,196 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:37:59,196 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:37:59,196 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:37:59,196 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:38:00,276 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:38:00,277 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:38:00,931 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:38:02,153 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:38:02,153 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:38:05,095 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:38:05,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:38:05,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:38:05,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:38:05,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:38:05,889 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:38:05,891 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:38:06,467 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:38:06,693 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:38:06,693 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:18:25,096 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:18:25,390 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4996 mean pseudo reward: 0.9614821695559642
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 20987
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21087, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21087, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.148, loss:683.1271
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.152, loss:663.6492
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.135, loss:626.3849
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.160, loss:618.2125
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.141, loss:586.9502
>> valid entity prec:0.6069, rec:0.5294, f1:0.5655
>> valid relation prec:0.3428, rec:0.1457, f1:0.2045
>> valid relation with NER prec:0.3428, rec:0.1457, f1:0.2045
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.687, loss:601.0246
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.155, loss:588.9274
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.140, loss:609.0879
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.151, loss:606.5053
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.148, loss:628.3809
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5894, rec:0.5579, f1:0.5732
>> valid relation prec:0.2780, rec:0.1294, f1:0.1766
>> valid relation with NER prec:0.2780, rec:0.1294, f1:0.1766
new max entity f1 on valid!
g_step 1100, step 55, avg_time 2.670, loss:592.3485
g_step 1200, step 155, avg_time 1.150, loss:590.7909
g_step 1300, step 46, avg_time 1.132, loss:556.8291
g_step 1400, step 146, avg_time 1.147, loss:565.2634
g_step 1500, step 37, avg_time 1.156, loss:569.2377
>> valid entity prec:0.5608, rec:0.5767, f1:0.5686
>> valid relation prec:0.2719, rec:0.1388, f1:0.1838
>> valid relation with NER prec:0.2719, rec:0.1388, f1:0.1838
g_step 1600, step 137, avg_time 2.667, loss:526.9793
g_step 1700, step 28, avg_time 1.140, loss:546.7605
g_step 1800, step 128, avg_time 1.152, loss:518.6473
g_step 1900, step 19, avg_time 1.141, loss:525.3812
g_step 2000, step 119, avg_time 1.150, loss:485.0863
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5825, rec:0.5156, f1:0.5470
>> valid relation prec:0.2622, rec:0.1042, f1:0.1491
>> valid relation with NER prec:0.2622, rec:0.1042, f1:0.1491
g_step 2100, step 10, avg_time 2.667, loss:484.9767
g_step 2200, step 110, avg_time 1.148, loss:455.8694
g_step 2300, step 1, avg_time 1.146, loss:491.0105
g_step 2400, step 101, avg_time 1.160, loss:424.9616
g_step 2500, step 201, avg_time 1.146, loss:466.2185
>> valid entity prec:0.5385, rec:0.5800, f1:0.5585
>> valid relation prec:0.2516, rec:0.1454, f1:0.1843
>> valid relation with NER prec:0.2516, rec:0.1454, f1:0.1843
g_step 2600, step 92, avg_time 2.664, loss:422.1123
g_step 2700, step 192, avg_time 1.149, loss:440.0893
g_step 2800, step 83, avg_time 1.137, loss:389.5864
g_step 2900, step 183, avg_time 1.150, loss:422.4254
g_step 3000, step 74, avg_time 1.146, loss:399.4766
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5979, rec:0.5275, f1:0.5605
>> valid relation prec:0.3086, rec:0.1489, f1:0.2008
>> valid relation with NER prec:0.3086, rec:0.1489, f1:0.2008
g_step 3100, step 174, avg_time 2.652, loss:388.1824
g_step 3200, step 65, avg_time 1.147, loss:358.6282
g_step 3300, step 165, avg_time 1.156, loss:371.8112
g_step 3400, step 56, avg_time 1.145, loss:370.2341
g_step 3500, step 156, avg_time 1.163, loss:368.2227
>> valid entity prec:0.5814, rec:0.5319, f1:0.5555
>> valid relation prec:0.3095, rec:0.1675, f1:0.2174
>> valid relation with NER prec:0.3095, rec:0.1675, f1:0.2174
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 47, avg_time 2.680, loss:351.4525
g_step 3700, step 147, avg_time 1.153, loss:344.6226
g_step 3800, step 38, avg_time 1.130, loss:341.5939
g_step 3900, step 138, avg_time 1.152, loss:319.1257
g_step 4000, step 29, avg_time 1.140, loss:326.4120
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5728, rec:0.5357, f1:0.5537
>> valid relation prec:0.2350, rec:0.1326, f1:0.1695
>> valid relation with NER prec:0.2350, rec:0.1326, f1:0.1695
g_step 4100, step 129, avg_time 2.681, loss:315.0291
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:18:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:18:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-18-25_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:18:26 - WARNING - datasets.builder -   Using custom data configuration default-4f8dc2188defb16b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4f8dc2188defb16b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:18:28,483 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:18:28,511 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:18:28,512 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:18:28,513 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:18:28,651 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:18:28,709 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:18:29,103 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:18:32,276 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:18:32,276 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4f8dc2188defb16b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.71ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.67ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.11ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.35ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.48ba/s]100%|██████████| 6/6 [00:01<00:00,  4.96ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.84ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.14ba/s]100%|██████████| 4/4 [00:00<00:00,  5.04ba/s]100%|██████████| 4/4 [00:00<00:00,  4.49ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.41ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.75ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.95ba/s]100%|██████████| 6/6 [00:00<00:00,  9.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.07ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.82ba/s]100%|██████████| 4/4 [00:00<00:00,  9.92ba/s]
[INFO|trainer.py:414] 2023-08-28 13:18:36,526 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:18:36,660 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:18:36,661 >>   Num examples = 5005
[INFO|trainer.py:1149] 2023-08-28 13:18:36,661 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:18:36,661 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:18:36,661 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:18:36,661 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:18:36,661 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.31it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:00<01:53,  3.42it/s]  1%|          | 4/390 [00:01<01:52,  3.43it/s]  1%|▏         | 5/390 [00:01<01:51,  3.44it/s]  2%|▏         | 6/390 [00:01<01:51,  3.45it/s]  2%|▏         | 7/390 [00:02<01:50,  3.45it/s]  2%|▏         | 8/390 [00:02<01:50,  3.45it/s]  2%|▏         | 9/390 [00:02<01:50,  3.46it/s]  3%|▎         | 10/390 [00:02<01:49,  3.45it/s]  3%|▎         | 11/390 [00:03<01:49,  3.45it/s]  3%|▎         | 12/390 [00:03<01:49,  3.45it/s]  3%|▎         | 13/390 [00:03<01:49,  3.45it/s]  4%|▎         | 14/390 [00:04<01:48,  3.46it/s]  4%|▍         | 15/390 [00:04<01:48,  3.46it/s]  4%|▍         | 16/390 [00:04<01:48,  3.46it/s]  4%|▍         | 17/390 [00:04<01:47,  3.46it/s]  5%|▍         | 18/390 [00:05<01:47,  3.46it/s]  5%|▍         | 19/390 [00:05<01:47,  3.46it/s]  5%|▌         | 20/390 [00:05<01:47,  3.46it/s]  5%|▌         | 21/390 [00:06<01:46,  3.46it/s]  6%|▌         | 22/390 [00:06<01:46,  3.46it/s]  6%|▌         | 23/390 [00:06<01:46,  3.46it/s]  6%|▌         | 24/390 [00:06<01:45,  3.46it/s]  6%|▋         | 25/390 [00:07<01:45,  3.46it/s]  7%|▋         | 26/390 [00:07<01:45,  3.46it/s]  7%|▋         | 27/390 [00:07<01:45,  3.46it/s]  7%|▋         | 28/390 [00:08<01:44,  3.45it/s]  7%|▋         | 29/390 [00:08<01:44,  3.46it/s]  8%|▊         | 30/390 [00:08<01:44,  3.45it/s]  8%|▊         | 31/390 [00:08<01:43,  3.45it/s]  8%|▊         | 32/390 [00:09<01:43,  3.45it/s]  8%|▊         | 33/390 [00:09<01:43,  3.45it/s]  9%|▊         | 34/390 [00:09<01:43,  3.45it/s]  9%|▉         | 35/390 [00:10<01:42,  3.45it/s]  9%|▉         | 36/390 [00:10<01:47,  3.30it/s]  9%|▉         | 37/390 [00:10<01:45,  3.34it/s] 10%|▉         | 38/390 [00:11<01:44,  3.37it/s] 10%|█         | 39/390 [00:11<01:43,  3.39it/s] 10%|█         | 40/390 [00:11<01:42,  3.41it/s] 11%|█         | 41/390 [00:11<01:41,  3.42it/s] 11%|█         | 42/390 [00:12<01:41,  3.43it/s] 11%|█         | 43/390 [00:12<01:41,  3.43it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 48/390 [00:13<01:39,  3.45it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.45it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 55/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.45it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.45it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.45it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.45it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.45it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.41it/s] 17%|█▋        | 65/390 [00:18<01:35,  3.39it/s] 17%|█▋        | 66/390 [00:19<01:40,  3.21it/s] 17%|█▋        | 67/390 [00:19<01:39,  3.25it/s] 17%|█▋        | 68/390 [00:19<01:37,  3.29it/s] 18%|█▊        | 69/390 [00:20<01:36,  3.34it/s] 18%|█▊        | 70/390 [00:20<01:35,  3.37it/s] 18%|█▊        | 71/390 [00:20<01:34,  3.39it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.41it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.43it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.43it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.44it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.44it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 13:18:59,460 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:18:59,460 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:18:59,460 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.44it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.22it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.38it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.73it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.29it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.95it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.66it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.66it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.74it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.64it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.52it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.44it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.62it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.46it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.49it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.60it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.60it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.66it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.58it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.56it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.54it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.50it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.37it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.49it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.57it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.65it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.62it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.60it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.59it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.63it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.55it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.50it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.49it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.51it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.56it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.61it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.60it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.70it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.56it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.54it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.52it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.51it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.52it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.60it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.34it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.49it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.53it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.53it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.50it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.44it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.44it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.58it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.56it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.53it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.59it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.54it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.51it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.50it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.52it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.52it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.61it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.56it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.53it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.60it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.48it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.53it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.53it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.53it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.45it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.22it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.49it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.49it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.49it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.47it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.50it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.45it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.54it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.54it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.50it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.54it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.56it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.61it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.54it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.58it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:32<01:30,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 44.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:19:09,520 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 13:19:09,750 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:19:13,882 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:19:14,125 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:19:14,287 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:48<40:26,  7.80s/it] 21%|██        | 80/390 [00:48<28:46,  5.57s/it] 21%|██        | 81/390 [00:49<21:20,  4.14s/it] 21%|██        | 82/390 [00:49<15:20,  2.99s/it] 21%|██▏       | 83/390 [00:49<11:09,  2.18s/it] 22%|██▏       | 84/390 [00:50<08:14,  1.62s/it] 22%|██▏       | 85/390 [00:50<06:12,  1.22s/it] 22%|██▏       | 86/390 [00:50<04:46,  1.06it/s] 22%|██▏       | 87/390 [00:51<03:45,  1.34it/s] 23%|██▎       | 88/390 [00:51<03:03,  1.64it/s] 23%|██▎       | 89/390 [00:51<02:34,  1.95it/s] 23%|██▎       | 90/390 [00:51<02:13,  2.24it/s] 23%|██▎       | 91/390 [00:52<01:59,  2.51it/s] 24%|██▎       | 92/390 [00:52<01:49,  2.73it/s] 24%|██▍       | 93/390 [00:52<01:41,  2.91it/s] 24%|██▍       | 94/390 [00:53<01:40,  2.95it/s] 24%|██▍       | 95/390 [00:53<01:35,  3.08it/s] 25%|██▍       | 96/390 [00:53<01:32,  3.19it/s] 25%|██▍       | 97/390 [00:53<01:29,  3.26it/s] 25%|██▌       | 98/390 [00:54<01:28,  3.32it/s] 25%|██▌       | 99/390 [00:54<01:26,  3.35it/s] 26%|██▌       | 100/390 [00:54<01:25,  3.38it/s] 26%|██▌       | 101/390 [00:55<01:24,  3.40it/s] 26%|██▌       | 102/390 [00:55<01:24,  3.42it/s] 26%|██▋       | 103/390 [00:55<01:23,  3.43it/s] 27%|██▋       | 104/390 [00:55<01:23,  3.44it/s] 27%|██▋       | 105/390 [00:56<01:24,  3.36it/s] 27%|██▋       | 106/390 [00:56<01:23,  3.39it/s] 27%|██▋       | 107/390 [00:56<01:23,  3.41it/s] 28%|██▊       | 108/390 [00:57<01:22,  3.42it/s] 28%|██▊       | 109/390 [00:57<01:21,  3.43it/s] 28%|██▊       | 110/390 [00:57<01:21,  3.43it/s] 28%|██▊       | 111/390 [00:58<01:21,  3.44it/s] 29%|██▊       | 112/390 [00:58<01:20,  3.44it/s] 29%|██▉       | 113/390 [00:58<01:20,  3.44it/s] 29%|██▉       | 114/390 [00:58<01:20,  3.45it/s] 29%|██▉       | 115/390 [00:59<01:19,  3.45it/s] 30%|██▉       | 116/390 [00:59<01:22,  3.32it/s] 30%|███       | 117/390 [00:59<01:21,  3.36it/s] 30%|███       | 118/390 [01:00<01:20,  3.39it/s] 31%|███       | 119/390 [01:00<01:19,  3.40it/s] 31%|███       | 120/390 [01:00<01:18,  3.42it/s] 31%|███       | 121/390 [01:00<01:18,  3.43it/s] 31%|███▏      | 122/390 [01:01<01:18,  3.44it/s] 32%|███▏      | 123/390 [01:01<01:17,  3.44it/s] 32%|███▏      | 124/390 [01:01<01:17,  3.44it/s] 32%|███▏      | 125/390 [01:02<01:16,  3.44it/s] 32%|███▏      | 126/390 [01:02<01:16,  3.45it/s] 33%|███▎      | 127/390 [01:02<01:19,  3.31it/s] 33%|███▎      | 128/390 [01:03<01:18,  3.35it/s] 33%|███▎      | 129/390 [01:03<01:17,  3.38it/s] 33%|███▎      | 130/390 [01:03<01:16,  3.40it/s] 34%|███▎      | 131/390 [01:03<01:15,  3.41it/s] 34%|███▍      | 132/390 [01:04<01:15,  3.42it/s] 34%|███▍      | 133/390 [01:04<01:14,  3.43it/s] 34%|███▍      | 134/390 [01:04<01:14,  3.44it/s] 35%|███▍      | 135/390 [01:05<01:14,  3.44it/s] 35%|███▍      | 136/390 [01:05<01:13,  3.44it/s] 35%|███▌      | 137/390 [01:05<01:13,  3.44it/s] 35%|███▌      | 138/390 [01:05<01:13,  3.43it/s] 36%|███▌      | 139/390 [01:06<01:13,  3.43it/s] 36%|███▌      | 140/390 [01:06<01:12,  3.44it/s] 36%|███▌      | 141/390 [01:06<01:12,  3.44it/s] 36%|███▋      | 142/390 [01:07<01:12,  3.44it/s] 37%|███▋      | 143/390 [01:07<01:11,  3.44it/s] 37%|███▋      | 144/390 [01:07<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:07<01:11,  3.44it/s] 37%|███▋      | 146/390 [01:08<01:10,  3.44it/s] 38%|███▊      | 147/390 [01:08<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:08<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:09<01:12,  3.33it/s] 38%|███▊      | 150/390 [01:09<01:11,  3.37it/s] 39%|███▊      | 151/390 [01:09<01:10,  3.39it/s] 39%|███▉      | 152/390 [01:10<01:09,  3.41it/s] 39%|███▉      | 153/390 [01:10<01:09,  3.42it/s] 39%|███▉      | 154/390 [01:10<01:08,  3.42it/s] 40%|███▉      | 155/390 [01:10<01:08,  3.43it/s] 40%|████      | 156/390 [01:11<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:19:47,910 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:19:47,910 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:19:47,910 >>   Batch size = 8
{'eval_loss': 0.9406272172927856, 'eval_runtime': 9.8425, 'eval_samples_per_second': 355.398, 'eval_steps_per_second': 44.501, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.50it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.10it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.48it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.20it/s][A
  7%|▋         | 32/438 [00:00<00:09, 42.50it/s][A
  8%|▊         | 37/438 [00:00<00:09, 43.21it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.65it/s][A
 11%|█         | 47/438 [00:01<00:08, 43.98it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.29it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.47it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.51it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.27it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.22it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.43it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.50it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.63it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.63it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.64it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.44it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.28it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.22it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.38it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.57it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.68it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.71it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.64it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.57it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.46it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.39it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 42.08it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 42.93it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.51it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.98it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.29it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.42it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.40it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.31it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.18it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.14it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.30it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.57it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.68it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.74it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.69it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.56it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.34it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.27it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.28it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.41it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.54it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.73it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.77it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.85it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.58it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.38it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.42it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 42.63it/s][A
 70%|███████   | 307/438 [00:06<00:03, 43.35it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.82it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.17it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.46it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.28it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.33it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.08it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.19it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.44it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.62it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.78it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.75it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.71it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.52it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.11it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.02it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.11it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.32it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.49it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 41.74it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 42.65it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.35it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.78it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.96it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 43.99it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.10it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:21<01:08,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:19:57,996 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 13:19:58,147 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:20:03,876 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:20:04,181 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:20:04,303 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:36<29:55,  7.70s/it] 41%|████      | 158/390 [01:36<21:15,  5.50s/it] 41%|████      | 159/390 [01:36<15:09,  3.94s/it] 41%|████      | 160/390 [01:37<10:54,  2.85s/it] 41%|████▏     | 161/390 [01:37<07:56,  2.08s/it] 42%|████▏     | 162/390 [01:37<05:52,  1.55s/it] 42%|████▏     | 163/390 [01:38<04:26,  1.17s/it] 42%|████▏     | 164/390 [01:38<03:25,  1.10it/s] 42%|████▏     | 165/390 [01:38<02:43,  1.38it/s] 43%|████▎     | 166/390 [01:38<02:13,  1.67it/s] 43%|████▎     | 167/390 [01:39<01:53,  1.97it/s] 43%|████▎     | 168/390 [01:39<01:43,  2.15it/s] 43%|████▎     | 169/390 [01:39<01:31,  2.41it/s] 44%|████▎     | 170/390 [01:40<01:23,  2.63it/s] 44%|████▍     | 171/390 [01:40<01:17,  2.81it/s] 44%|████▍     | 172/390 [01:40<01:13,  2.95it/s] 44%|████▍     | 173/390 [01:41<01:10,  3.06it/s] 45%|████▍     | 174/390 [01:41<01:08,  3.13it/s] 45%|████▍     | 175/390 [01:41<01:07,  3.18it/s] 45%|████▌     | 176/390 [01:42<01:06,  3.23it/s] 45%|████▌     | 177/390 [01:42<01:05,  3.26it/s] 46%|████▌     | 178/390 [01:42<01:10,  3.01it/s] 46%|████▌     | 179/390 [01:42<01:07,  3.11it/s] 46%|████▌     | 180/390 [01:43<01:06,  3.18it/s] 46%|████▋     | 181/390 [01:43<01:04,  3.23it/s] 47%|████▋     | 182/390 [01:43<01:03,  3.27it/s] 47%|████▋     | 183/390 [01:44<01:02,  3.30it/s] 47%|████▋     | 184/390 [01:44<01:02,  3.32it/s] 47%|████▋     | 185/390 [01:44<01:01,  3.34it/s] 48%|████▊     | 186/390 [01:45<01:00,  3.35it/s] 48%|████▊     | 187/390 [01:45<01:00,  3.35it/s] 48%|████▊     | 188/390 [01:45<01:04,  3.12it/s] 48%|████▊     | 189/390 [01:46<01:03,  3.19it/s] 49%|████▊     | 190/390 [01:46<01:01,  3.24it/s] 49%|████▉     | 191/390 [01:46<01:00,  3.28it/s] 49%|████▉     | 192/390 [01:47<01:07,  2.91it/s] 49%|████▉     | 193/390 [01:47<01:04,  3.06it/s] 50%|████▉     | 194/390 [01:47<01:01,  3.17it/s] 50%|█████     | 195/390 [01:47<01:00,  3.24it/s] 50%|█████     | 196/390 [01:48<00:58,  3.30it/s] 51%|█████     | 197/390 [01:48<00:57,  3.34it/s] 51%|█████     | 198/390 [01:49<01:29,  2.14it/s] 51%|█████     | 199/390 [01:49<01:19,  2.41it/s] 51%|█████▏    | 200/390 [01:49<01:11,  2.65it/s] 52%|█████▏    | 201/390 [01:50<01:06,  2.85it/s] 52%|█████▏    | 202/390 [01:50<01:02,  3.00it/s] 52%|█████▏    | 203/390 [01:50<00:59,  3.12it/s] 52%|█████▏    | 204/390 [01:51<00:57,  3.21it/s] 53%|█████▎    | 205/390 [01:51<00:56,  3.28it/s] 53%|█████▎    | 206/390 [01:51<00:55,  3.32it/s] 53%|█████▎    | 207/390 [01:51<00:54,  3.36it/s] 53%|█████▎    | 208/390 [01:52<00:53,  3.38it/s] 54%|█████▎    | 209/390 [01:52<00:53,  3.40it/s] 54%|█████▍    | 210/390 [01:52<00:52,  3.41it/s] 54%|█████▍    | 211/390 [01:53<00:52,  3.42it/s] 54%|█████▍    | 212/390 [01:53<00:51,  3.43it/s] 55%|█████▍    | 213/390 [01:53<00:51,  3.43it/s] 55%|█████▍    | 214/390 [01:54<00:51,  3.44it/s] 55%|█████▌    | 215/390 [01:54<00:50,  3.44it/s] 55%|█████▌    | 216/390 [01:54<00:52,  3.31it/s] 56%|█████▌    | 217/390 [01:54<00:51,  3.35it/s] 56%|█████▌    | 218/390 [01:55<00:50,  3.38it/s] 56%|█████▌    | 219/390 [01:55<00:50,  3.40it/s] 56%|█████▋    | 220/390 [01:55<00:49,  3.41it/s] 57%|█████▋    | 221/390 [01:56<00:49,  3.42it/s] 57%|█████▋    | 222/390 [01:56<00:49,  3.43it/s] 57%|█████▋    | 223/390 [01:56<00:48,  3.43it/s] 57%|█████▋    | 224/390 [01:56<00:48,  3.44it/s] 58%|█████▊    | 225/390 [01:57<00:48,  3.44it/s] 58%|█████▊    | 226/390 [01:57<00:47,  3.44it/s] 58%|█████▊    | 227/390 [01:57<00:49,  3.30it/s] 58%|█████▊    | 228/390 [01:58<00:48,  3.34it/s] 59%|█████▊    | 229/390 [01:58<00:47,  3.37it/s] 59%|█████▉    | 230/390 [01:58<00:47,  3.39it/s] 59%|█████▉    | 231/390 [01:59<00:46,  3.41it/s] 59%|█████▉    | 232/390 [01:59<00:46,  3.42it/s] 60%|█████▉    | 233/390 [01:59<00:45,  3.43it/s] 60%|██████    | 234/390 [01:59<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:20:36,619 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:20:36,619 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:20:36,619 >>   Batch size = 8
{'eval_loss': 0.9531099200248718, 'eval_runtime': 9.9213, 'eval_samples_per_second': 352.574, 'eval_steps_per_second': 44.147, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.53it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.73it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.70it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.91it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.23it/s][A
  7%|▋         | 32/438 [00:00<00:09, 41.73it/s][A
  8%|▊         | 37/438 [00:00<00:09, 42.76it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.19it/s][A
 11%|█         | 47/438 [00:01<00:08, 43.74it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.45it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.50it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.46it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.15it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.12it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.22it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.38it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.53it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.65it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.77it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.81it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.65it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.35it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.28it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.35it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.40it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.45it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.60it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.77it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.84it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.61it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.53it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.20it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.58it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.88it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.53it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.63it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.57it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.38it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.39it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.38it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.47it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.59it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.67it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.62it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.65it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.52it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.45it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.45it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.43it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.49it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.54it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.60it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.67it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.63it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.50it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.47it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.21it/s][A
 70%|███████   | 307/438 [00:06<00:03, 43.64it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.97it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.20it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.36it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.46it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.48it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.47it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.29it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.26it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.43it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.58it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.56it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.64it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.61it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.62it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.58it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.34it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.26it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.44it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.43it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.66it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.68it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.64it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.63it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.45it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.45it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 42.71it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [02:09<00:45,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 42.71it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:20:46,755 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:20:46,978 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:20:50,833 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:20:50,986 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:20:51,067 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:21<16:59,  6.58s/it] 61%|██████    | 236/390 [02:21<12:03,  4.70s/it] 61%|██████    | 237/390 [02:21<08:36,  3.38s/it] 61%|██████    | 238/390 [02:22<06:12,  2.45s/it] 61%|██████▏   | 239/390 [02:22<04:32,  1.81s/it] 62%|██████▏   | 240/390 [02:22<03:23,  1.35s/it] 62%|██████▏   | 241/390 [02:22<02:34,  1.04s/it] 62%|██████▏   | 242/390 [02:23<02:00,  1.23it/s] 62%|██████▏   | 243/390 [02:23<01:37,  1.51it/s] 63%|██████▎   | 244/390 [02:23<01:20,  1.81it/s] 63%|██████▎   | 245/390 [02:24<01:09,  2.10it/s] 63%|██████▎   | 246/390 [02:24<01:00,  2.37it/s] 63%|██████▎   | 247/390 [02:24<00:55,  2.59it/s] 64%|██████▎   | 248/390 [02:25<00:50,  2.78it/s] 64%|██████▍   | 249/390 [02:25<00:49,  2.87it/s] 64%|██████▍   | 250/390 [02:25<00:46,  3.00it/s] 64%|██████▍   | 251/390 [02:25<00:44,  3.10it/s] 65%|██████▍   | 252/390 [02:26<00:43,  3.17it/s] 65%|██████▍   | 253/390 [02:26<00:42,  3.22it/s] 65%|██████▌   | 254/390 [02:26<00:41,  3.27it/s] 65%|██████▌   | 255/390 [02:27<00:41,  3.29it/s] 66%|██████▌   | 256/390 [02:27<00:40,  3.31it/s] 66%|██████▌   | 257/390 [02:27<00:39,  3.33it/s] 66%|██████▌   | 258/390 [02:28<00:39,  3.34it/s] 66%|██████▋   | 259/390 [02:28<00:40,  3.23it/s] 67%|██████▋   | 260/390 [02:28<00:39,  3.27it/s] 67%|██████▋   | 261/390 [02:28<00:39,  3.30it/s] 67%|██████▋   | 262/390 [02:29<00:38,  3.31it/s] 67%|██████▋   | 263/390 [02:29<00:38,  3.32it/s] 68%|██████▊   | 264/390 [02:29<00:37,  3.33it/s] 68%|██████▊   | 265/390 [02:30<00:37,  3.34it/s] 68%|██████▊   | 266/390 [02:30<00:37,  3.34it/s] 68%|██████▊   | 267/390 [02:30<00:36,  3.34it/s] 69%|██████▊   | 268/390 [02:31<00:36,  3.35it/s] 69%|██████▉   | 269/390 [02:31<00:37,  3.20it/s] 69%|██████▉   | 270/390 [02:31<00:36,  3.27it/s] 69%|██████▉   | 271/390 [02:31<00:35,  3.32it/s] 70%|██████▉   | 272/390 [02:32<00:35,  3.36it/s] 70%|███████   | 273/390 [02:32<00:34,  3.39it/s] 70%|███████   | 274/390 [02:32<00:34,  3.41it/s] 71%|███████   | 275/390 [02:33<00:33,  3.42it/s] 71%|███████   | 276/390 [02:33<00:33,  3.43it/s] 71%|███████   | 277/390 [02:33<00:32,  3.43it/s] 71%|███████▏  | 278/390 [02:34<00:32,  3.44it/s] 72%|███████▏  | 279/390 [02:34<00:32,  3.44it/s] 72%|███████▏  | 280/390 [02:34<00:33,  3.31it/s] 72%|███████▏  | 281/390 [02:34<00:32,  3.35it/s] 72%|███████▏  | 282/390 [02:35<00:31,  3.38it/s] 73%|███████▎  | 283/390 [02:35<00:31,  3.40it/s] 73%|███████▎  | 284/390 [02:35<00:31,  3.42it/s] 73%|███████▎  | 285/390 [02:36<00:30,  3.43it/s] 73%|███████▎  | 286/390 [02:36<00:30,  3.43it/s] 74%|███████▎  | 287/390 [02:36<00:29,  3.43it/s] 74%|███████▍  | 288/390 [02:36<00:29,  3.44it/s] 74%|███████▍  | 289/390 [02:37<00:29,  3.44it/s] 74%|███████▍  | 290/390 [02:37<00:29,  3.44it/s] 75%|███████▍  | 291/390 [02:37<00:32,  3.08it/s] 75%|███████▍  | 292/390 [02:38<00:30,  3.19it/s] 75%|███████▌  | 293/390 [02:38<00:29,  3.26it/s] 75%|███████▌  | 294/390 [02:38<00:28,  3.32it/s] 76%|███████▌  | 295/390 [02:39<00:28,  3.35it/s] 76%|███████▌  | 296/390 [02:39<00:27,  3.38it/s] 76%|███████▌  | 297/390 [02:39<00:27,  3.40it/s] 76%|███████▋  | 298/390 [02:39<00:26,  3.41it/s] 77%|███████▋  | 299/390 [02:40<00:26,  3.42it/s] 77%|███████▋  | 300/390 [02:40<00:26,  3.43it/s] 77%|███████▋  | 301/390 [02:40<00:28,  3.14it/s] 77%|███████▋  | 302/390 [02:41<00:27,  3.23it/s] 78%|███████▊  | 303/390 [02:41<00:26,  3.29it/s] 78%|███████▊  | 304/390 [02:41<00:25,  3.34it/s] 78%|███████▊  | 305/390 [02:42<00:25,  3.37it/s] 78%|███████▊  | 306/390 [02:42<00:24,  3.39it/s] 79%|███████▊  | 307/390 [02:42<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:42<00:23,  3.42it/s] 79%|███████▉  | 309/390 [02:43<00:23,  3.43it/s] 79%|███████▉  | 310/390 [02:43<00:23,  3.44it/s] 80%|███████▉  | 311/390 [02:43<00:22,  3.44it/s] 80%|████████  | 312/390 [02:44<00:24,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 13:21:20,921 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:21:20,921 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:21:20,921 >>   Batch size = 8
{'eval_loss': 0.9632771611213684, 'eval_runtime': 9.8992, 'eval_samples_per_second': 353.364, 'eval_steps_per_second': 44.246, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.38it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.83it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.24it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.47it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.53it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/438 [00:00<00:09, 44.38it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.41it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.61it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.90it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.95it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.72it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.46it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.14it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.10it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.27it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.53it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.70it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.76it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.81it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.77it/s][A
 28%|██▊       | 122/438 [00:02<00:10, 31.17it/s][A
 29%|██▉       | 127/438 [00:02<00:09, 34.44it/s][A
 30%|███       | 132/438 [00:03<00:08, 37.07it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 39.14it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 40.78it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 41.99it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 42.88it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.46it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 43.38it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.42it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.52it/s][A
 40%|████      | 177/438 [00:04<00:05, 43.84it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.46it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.72it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.75it/s][A
 46%|████▌     | 202/438 [00:05<00:14, 16.16it/s][A
 47%|████▋     | 207/438 [00:05<00:11, 20.05it/s][A
 48%|████▊     | 212/438 [00:05<00:09, 24.08it/s][A
 50%|████▉     | 217/438 [00:05<00:07, 28.00it/s][A
 51%|█████     | 222/438 [00:05<00:08, 24.92it/s][A
 52%|█████▏    | 227/438 [00:06<00:07, 28.81it/s][A
 53%|█████▎    | 232/438 [00:06<00:06, 32.26it/s][A
 54%|█████▍    | 237/438 [00:06<00:05, 35.31it/s][A
 55%|█████▌    | 242/438 [00:06<00:05, 37.76it/s][A
 56%|█████▋    | 247/438 [00:06<00:04, 39.69it/s][A
 58%|█████▊    | 252/438 [00:06<00:04, 41.21it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 42.14it/s][A
 60%|█████▉    | 262/438 [00:06<00:04, 42.50it/s][A
 61%|██████    | 267/438 [00:06<00:03, 42.77it/s][A
 62%|██████▏   | 272/438 [00:07<00:03, 43.10it/s][A
 63%|██████▎   | 277/438 [00:07<00:03, 43.60it/s][A
 64%|██████▍   | 282/438 [00:07<00:03, 44.09it/s][A
 66%|██████▌   | 287/438 [00:07<00:03, 44.42it/s][A
 67%|██████▋   | 292/438 [00:07<00:03, 44.57it/s][A
 68%|██████▊   | 297/438 [00:07<00:03, 44.66it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 44.59it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.22it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.05it/s][A
 72%|███████▏  | 317/438 [00:08<00:02, 44.08it/s][A
 74%|███████▎  | 322/438 [00:08<00:02, 44.27it/s][A
 75%|███████▍  | 327/438 [00:08<00:02, 44.57it/s][A
 76%|███████▌  | 332/438 [00:08<00:02, 38.79it/s][A
 77%|███████▋  | 337/438 [00:08<00:02, 40.52it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 41.78it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 42.73it/s][A
 80%|████████  | 352/438 [00:08<00:01, 43.41it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.76it/s][A
 83%|████████▎ | 362/438 [00:09<00:01, 43.99it/s][A
 84%|████████▍ | 367/438 [00:09<00:01, 44.04it/s][A
 85%|████████▍ | 372/438 [00:09<00:01, 43.90it/s][A
 86%|████████▌ | 377/438 [00:09<00:01, 43.82it/s][A
 87%|████████▋ | 382/438 [00:09<00:01, 44.10it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.42it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.48it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.72it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.70it/s][A
 93%|█████████▎| 407/438 [00:10<00:00, 44.56it/s][A
 94%|█████████▍| 412/438 [00:10<00:00, 44.37it/s][A
 95%|█████████▌| 417/438 [00:10<00:00, 44.26it/s][A
 96%|█████████▋| 422/438 [00:10<00:00, 44.24it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 44.39it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.46it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.67it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:55<00:24,  3.13it/s]
100%|██████████| 438/438 [00:10<00:00, 44.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:21:31,843 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 13:21:31,992 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:21:35,366 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:21:35,549 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:21:35,629 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:06<08:58,  6.99s/it] 81%|████████  | 314/390 [03:07<06:19,  5.00s/it] 81%|████████  | 315/390 [03:07<04:28,  3.59s/it] 81%|████████  | 316/390 [03:07<03:12,  2.60s/it] 81%|████████▏ | 317/390 [03:08<02:19,  1.91s/it] 82%|████████▏ | 318/390 [03:08<01:42,  1.43s/it] 82%|████████▏ | 319/390 [03:08<01:17,  1.09s/it] 82%|████████▏ | 320/390 [03:08<00:59,  1.18it/s] 82%|████████▏ | 321/390 [03:09<00:47,  1.46it/s] 83%|████████▎ | 322/390 [03:09<00:38,  1.76it/s] 83%|████████▎ | 323/390 [03:09<00:32,  2.05it/s] 83%|████████▎ | 324/390 [03:10<00:29,  2.27it/s] 83%|████████▎ | 325/390 [03:10<00:25,  2.51it/s] 84%|████████▎ | 326/390 [03:10<00:23,  2.72it/s] 84%|████████▍ | 327/390 [03:11<00:21,  2.88it/s] 84%|████████▍ | 328/390 [03:11<00:20,  3.00it/s] 84%|████████▍ | 329/390 [03:11<00:19,  3.10it/s] 85%|████████▍ | 330/390 [03:11<00:18,  3.17it/s] 85%|████████▍ | 331/390 [03:12<00:18,  3.22it/s] 85%|████████▌ | 332/390 [03:12<00:17,  3.26it/s] 85%|████████▌ | 333/390 [03:12<00:17,  3.29it/s] 86%|████████▌ | 334/390 [03:13<00:17,  3.16it/s] 86%|████████▌ | 335/390 [03:13<00:17,  3.22it/s] 86%|████████▌ | 336/390 [03:13<00:16,  3.26it/s] 86%|████████▋ | 337/390 [03:14<00:16,  3.29it/s] 87%|████████▋ | 338/390 [03:14<00:15,  3.31it/s] 87%|████████▋ | 339/390 [03:14<00:15,  3.32it/s] 87%|████████▋ | 340/390 [03:14<00:14,  3.33it/s] 87%|████████▋ | 341/390 [03:15<00:14,  3.34it/s] 88%|████████▊ | 342/390 [03:15<00:14,  3.35it/s] 88%|████████▊ | 343/390 [03:15<00:14,  3.35it/s] 88%|████████▊ | 344/390 [03:16<00:14,  3.26it/s] 88%|████████▊ | 345/390 [03:16<00:13,  3.29it/s] 89%|████████▊ | 346/390 [03:16<00:13,  3.30it/s] 89%|████████▉ | 347/390 [03:17<00:12,  3.32it/s] 89%|████████▉ | 348/390 [03:17<00:12,  3.32it/s] 89%|████████▉ | 349/390 [03:17<00:12,  3.33it/s] 90%|████████▉ | 350/390 [03:17<00:11,  3.34it/s] 90%|█████████ | 351/390 [03:18<00:11,  3.34it/s] 90%|█████████ | 352/390 [03:18<00:11,  3.35it/s] 91%|█████████ | 353/390 [03:18<00:11,  3.35it/s] 91%|█████████ | 354/390 [03:19<00:11,  3.25it/s] 91%|█████████ | 355/390 [03:19<00:10,  3.28it/s] 91%|█████████▏| 356/390 [03:19<00:10,  3.30it/s] 92%|█████████▏| 357/390 [03:20<00:09,  3.31it/s] 92%|█████████▏| 358/390 [03:20<00:09,  3.32it/s] 92%|█████████▏| 359/390 [03:20<00:09,  3.33it/s] 92%|█████████▏| 360/390 [03:20<00:08,  3.34it/s] 93%|█████████▎| 361/390 [03:21<00:08,  3.34it/s] 93%|█████████▎| 362/390 [03:21<00:08,  3.35it/s] 93%|█████████▎| 363/390 [03:21<00:08,  3.35it/s] 93%|█████████▎| 364/390 [03:22<00:08,  3.22it/s] 94%|█████████▎| 365/390 [03:22<00:07,  3.26it/s] 94%|█████████▍| 366/390 [03:22<00:07,  3.28it/s] 94%|█████████▍| 367/390 [03:23<00:06,  3.30it/s] 94%|█████████▍| 368/390 [03:23<00:06,  3.31it/s] 95%|█████████▍| 369/390 [03:23<00:06,  3.32it/s] 95%|█████████▍| 370/390 [03:23<00:06,  3.33it/s] 95%|█████████▌| 371/390 [03:24<00:05,  3.33it/s] 95%|█████████▌| 372/390 [03:24<00:05,  3.33it/s] 96%|█████████▌| 373/390 [03:24<00:05,  3.33it/s] 96%|█████████▌| 374/390 [03:25<00:04,  3.34it/s] 96%|█████████▌| 375/390 [03:25<00:04,  3.34it/s] 96%|█████████▋| 376/390 [03:25<00:04,  3.34it/s] 97%|█████████▋| 377/390 [03:26<00:03,  3.34it/s] 97%|█████████▋| 378/390 [03:26<00:03,  3.34it/s] 97%|█████████▋| 379/390 [03:26<00:03,  3.27it/s] 97%|█████████▋| 380/390 [03:27<00:03,  3.29it/s] 98%|█████████▊| 381/390 [03:27<00:02,  3.31it/s] 98%|█████████▊| 382/390 [03:27<00:02,  3.32it/s] 98%|█████████▊| 383/390 [03:27<00:02,  3.33it/s] 98%|█████████▊| 384/390 [03:28<00:01,  3.34it/s] 99%|█████████▊| 385/390 [03:28<00:01,  3.34it/s] 99%|█████████▉| 386/390 [03:28<00:01,  3.35it/s] 99%|█████████▉| 387/390 [03:29<00:00,  3.35it/s] 99%|█████████▉| 388/390 [03:29<00:00,  3.35it/s]100%|█████████▉| 389/390 [03:29<00:00,  3.19it/s]100%|██████████| 390/390 [03:30<00:00,  3.24it/s][INFO|trainer.py:2140] 2023-08-28 13:22:06,711 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:22:06,711 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:22:06,711 >>   Batch size = 8
{'eval_loss': 0.9708857536315918, 'eval_runtime': 10.8353, 'eval_samples_per_second': 322.833, 'eval_steps_per_second': 40.423, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.70it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.60it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.90it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.98it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.32it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.96it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.82it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.61it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.67it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.77it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.87it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.82it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.74it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.44it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.45it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.42it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.36it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.43it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.70it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.72it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.66it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 42.66it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.27it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.63it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.88it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.08it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.34it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.38it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.50it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.32it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.33it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.53it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.53it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.53it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.47it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.58it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.59it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.41it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.46it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.56it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.73it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.64it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.60it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.60it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.62it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.44it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.46it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 41.73it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 42.71it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.36it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 43.73it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.23it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.30it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.25it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.03it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.17it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.41it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.64it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.67it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.67it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.66it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.50it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.34it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.27it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.37it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.61it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.73it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.75it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.58it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.57it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.39it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.15it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 41.69it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 42.76it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.41it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 43.94it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.30it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.22it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.16it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.93it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.06it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.29it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.54it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:39<00:00,  3.24it/s]
100%|██████████| 438/438 [00:09<00:00, 44.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:22:17,138 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 13:22:17,691 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:22:22,644 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:22:22,914 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:22:23,064 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:22:32,857 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:22:32,857 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78 (score: 0.9406272172927856).
                                                 100%|██████████| 390/390 [04:07<00:00,  3.24it/s]100%|██████████| 390/390 [04:07<00:00,  1.57it/s]
[INFO|trainer.py:1894] 2023-08-28 13:22:44,594 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 13:22:44,845 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:22:48,434 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:22:48,628 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:22:48,696 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:22:49,170 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   train_loss               =     0.7191
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   train_runtime            = 0:04:07.83
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   train_samples            =       5005
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   train_samples_per_second =    100.973
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:49,170 >>   train_steps_per_second   =      1.574
{'eval_loss': 0.9761196374893188, 'eval_runtime': 9.8891, 'eval_samples_per_second': 353.724, 'eval_steps_per_second': 44.291, 'epoch': 4.99}
{'train_runtime': 247.8385, 'train_samples_per_second': 100.973, 'train_steps_per_second': 1.574, 'train_loss': 0.7191468067658253, 'epoch': 4.99}
08/28/2023 13:22:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:22:49,413 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:22:49,413 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 13:22:49,413 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.58it/s]  3%|▎         | 12/438 [00:00<00:08, 48.82it/s]  4%|▍         | 17/438 [00:00<00:08, 47.56it/s]  5%|▌         | 22/438 [00:00<00:08, 46.63it/s]  6%|▌         | 27/438 [00:00<00:08, 46.13it/s]  7%|▋         | 32/438 [00:00<00:08, 45.76it/s]  8%|▊         | 37/438 [00:00<00:08, 45.56it/s] 10%|▉         | 42/438 [00:00<00:08, 45.22it/s] 11%|█         | 47/438 [00:01<00:08, 44.77it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.74it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.76it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.85it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.89it/s] 16%|█▋        | 72/438 [00:01<00:08, 45.00it/s] 18%|█▊        | 77/438 [00:01<00:08, 45.06it/s] 19%|█▊        | 82/438 [00:01<00:07, 44.96it/s] 20%|█▉        | 87/438 [00:01<00:07, 44.65it/s] 21%|██        | 92/438 [00:02<00:07, 44.51it/s] 22%|██▏       | 97/438 [00:02<00:08, 42.24it/s] 23%|██▎       | 102/438 [00:02<00:07, 43.16it/s] 24%|██▍       | 107/438 [00:02<00:07, 43.70it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.18it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.47it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.60it/s] 29%|██▉       | 127/438 [00:02<00:07, 44.28it/s] 30%|███       | 132/438 [00:02<00:06, 44.47it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.18it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.23it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.45it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.68it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.80it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.93it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.87it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.76it/s] 40%|████      | 177/438 [00:03<00:05, 44.53it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.33it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.30it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.41it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.61it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.71it/s] 47%|████▋     | 207/438 [00:04<00:05, 44.85it/s] 48%|████▊     | 212/438 [00:04<00:05, 44.88it/s] 50%|████▉     | 217/438 [00:04<00:04, 44.79it/s] 51%|█████     | 222/438 [00:04<00:04, 44.55it/s] 52%|█████▏    | 227/438 [00:05<00:04, 44.40it/s] 53%|█████▎    | 232/438 [00:05<00:04, 43.31it/s] 54%|█████▍    | 237/438 [00:05<00:04, 43.77it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.04it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.31it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.53it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.73it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.62it/s] 61%|██████    | 267/438 [00:05<00:03, 44.55it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.25it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.27it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.35it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.47it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.58it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.74it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.80it/s] 70%|███████   | 307/438 [00:06<00:02, 44.85it/s] 71%|███████   | 312/438 [00:06<00:02, 44.60it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.43it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.39it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.48it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.42it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.61it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.68it/s] 79%|███████▉  | 347/438 [00:07<00:02, 44.82it/s] 80%|████████  | 352/438 [00:07<00:01, 44.74it/s] 82%|████████▏ | 357/438 [00:07<00:01, 44.58it/s] 83%|████████▎ | 362/438 [00:08<00:01, 44.41it/s] 84%|████████▍ | 367/438 [00:08<00:01, 42.91it/s] 85%|████████▍ | 372/438 [00:08<00:01, 43.51it/s] 86%|████████▌ | 377/438 [00:08<00:01, 43.81it/s] 87%|████████▋ | 382/438 [00:08<00:01, 44.02it/s] 88%|████████▊ | 387/438 [00:08<00:01, 44.26it/s] 89%|████████▉ | 392/438 [00:08<00:01, 44.54it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.54it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.49it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.26it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.29it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.37it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.51it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.54it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.68it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.74it/s]100%|██████████| 438/438 [00:09<00:00, 44.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:22:59,259 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   eval_loss               =     0.9406
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   eval_samples_per_second =    355.268
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   eval_steps_per_second   =     44.485
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:22:59,259 >>   perplexity              =     2.5616
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:11,719 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:11,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:11,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:11,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:11,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:23:12,813 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:23:12,814 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:23:13,476 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:23:14,627 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:23:14,627 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:17,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:17,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:17,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:17,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:17,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:23:18,863 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:23:18,864 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:23:19,537 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:23:19,842 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:23:19,842 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.19it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.26it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.38it/s]Extractor Predicting: 11it [00:08,  1.38it/s]Extractor Predicting: 12it [00:08,  1.36it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.34it/s]Extractor Predicting: 15it [00:11,  1.36it/s]Extractor Predicting: 16it [00:11,  1.35it/s]Extractor Predicting: 17it [00:12,  1.34it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:14,  1.38it/s]Extractor Predicting: 20it [00:14,  1.38it/s]Extractor Predicting: 21it [00:15,  1.35it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.36it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:19,  1.36it/s]Extractor Predicting: 28it [00:20,  1.34it/s]Extractor Predicting: 29it [00:21,  1.31it/s]Extractor Predicting: 30it [00:22,  1.27it/s]Extractor Predicting: 31it [00:23,  1.29it/s]Extractor Predicting: 32it [00:24,  1.24it/s]Extractor Predicting: 33it [00:24,  1.25it/s]Extractor Predicting: 34it [00:25,  1.24it/s]Extractor Predicting: 35it [00:26,  1.24it/s]Extractor Predicting: 36it [00:27,  1.24it/s]Extractor Predicting: 37it [00:28,  1.22it/s]Extractor Predicting: 38it [00:28,  1.21it/s]Extractor Predicting: 39it [00:29,  1.21it/s]Extractor Predicting: 40it [00:30,  1.20it/s]Extractor Predicting: 41it [00:31,  1.20it/s]Extractor Predicting: 42it [00:32,  1.19it/s]Extractor Predicting: 43it [00:33,  1.19it/s]Extractor Predicting: 44it [00:33,  1.19it/s]Extractor Predicting: 45it [00:34,  1.13it/s]Extractor Predicting: 46it [00:35,  1.14it/s]Extractor Predicting: 47it [00:36,  1.17it/s]Extractor Predicting: 48it [00:37,  1.19it/s]Extractor Predicting: 49it [00:38,  1.19it/s]Extractor Predicting: 50it [00:39,  1.19it/s]Extractor Predicting: 51it [00:39,  1.18it/s]Extractor Predicting: 52it [00:40,  1.18it/s]Extractor Predicting: 53it [00:41,  1.17it/s]Extractor Predicting: 54it [00:42,  1.21it/s]Extractor Predicting: 55it [00:43,  1.20it/s]Extractor Predicting: 56it [00:44,  1.22it/s]Extractor Predicting: 57it [00:44,  1.22it/s]Extractor Predicting: 58it [00:45,  1.20it/s]Extractor Predicting: 59it [00:46,  1.17it/s]Extractor Predicting: 60it [00:47,  1.24it/s]Extractor Predicting: 61it [00:48,  1.25it/s]Extractor Predicting: 62it [00:48,  1.25it/s]Extractor Predicting: 63it [00:49,  1.23it/s]Extractor Predicting: 64it [00:50,  1.25it/s]Extractor Predicting: 65it [00:51,  1.27it/s]Extractor Predicting: 66it [00:52,  1.26it/s]Extractor Predicting: 67it [00:52,  1.24it/s]Extractor Predicting: 68it [00:53,  1.24it/s]Extractor Predicting: 69it [00:54,  1.23it/s]Extractor Predicting: 70it [00:55,  1.27it/s]Extractor Predicting: 71it [00:56,  1.25it/s]Extractor Predicting: 72it [00:56,  1.28it/s]Extractor Predicting: 73it [00:57,  1.30it/s]Extractor Predicting: 74it [00:58,  1.33it/s]Extractor Predicting: 75it [00:59,  1.32it/s]Extractor Predicting: 76it [00:59,  1.32it/s]Extractor Predicting: 77it [01:00,  1.32it/s]Extractor Predicting: 78it [01:01,  1.33it/s]Extractor Predicting: 79it [01:02,  1.31it/s]Extractor Predicting: 80it [01:02,  1.31it/s]Extractor Predicting: 81it [01:03,  1.30it/s]Extractor Predicting: 82it [01:04,  1.30it/s]Extractor Predicting: 83it [01:05,  1.30it/s]Extractor Predicting: 84it [01:06,  1.29it/s]Extractor Predicting: 85it [01:06,  1.30it/s]Extractor Predicting: 86it [01:07,  1.28it/s]Extractor Predicting: 87it [01:08,  1.28it/s]Extractor Predicting: 88it [01:09,  1.32it/s]Extractor Predicting: 89it [01:09,  1.37it/s]Extractor Predicting: 90it [01:10,  1.34it/s]Extractor Predicting: 91it [01:11,  1.38it/s]Extractor Predicting: 92it [01:11,  1.40it/s]Extractor Predicting: 93it [01:12,  1.41it/s]Extractor Predicting: 94it [01:13,  1.43it/s]Extractor Predicting: 95it [01:13,  1.41it/s]Extractor Predicting: 96it [01:14,  1.38it/s]Extractor Predicting: 97it [01:15,  1.39it/s]Extractor Predicting: 98it [01:16,  1.42it/s]Extractor Predicting: 99it [01:16,  1.42it/s]Extractor Predicting: 100it [01:17,  1.38it/s]Extractor Predicting: 101it [01:18,  1.37it/s]Extractor Predicting: 102it [01:19,  1.37it/s]Extractor Predicting: 103it [01:19,  1.39it/s]Extractor Predicting: 104it [01:20,  1.39it/s]Extractor Predicting: 105it [01:21,  1.36it/s]Extractor Predicting: 106it [01:21,  1.37it/s]Extractor Predicting: 107it [01:22,  1.35it/s]Extractor Predicting: 108it [01:23,  1.38it/s]Extractor Predicting: 109it [01:24,  1.39it/s]Extractor Predicting: 110it [01:24,  1.40it/s]Extractor Predicting: 111it [01:25,  1.40it/s]Extractor Predicting: 112it [01:26,  1.43it/s]Extractor Predicting: 113it [01:26,  1.41it/s]Extractor Predicting: 114it [01:27,  1.37it/s]Extractor Predicting: 115it [01:28,  1.36it/s]Extractor Predicting: 116it [01:29,  1.34it/s]Extractor Predicting: 117it [01:30,  1.31it/s]Extractor Predicting: 118it [01:30,  1.31it/s]Extractor Predicting: 119it [01:31,  1.28it/s]Extractor Predicting: 120it [01:32,  1.26it/s]Extractor Predicting: 121it [01:33,  1.25it/s]Extractor Predicting: 122it [01:34,  1.27it/s]Extractor Predicting: 123it [01:34,  1.23it/s]Extractor Predicting: 124it [01:35,  1.23it/s]Extractor Predicting: 125it [01:36,  1.23it/s]Extractor Predicting: 126it [01:37,  1.23it/s]Extractor Predicting: 127it [01:38,  1.21it/s]Extractor Predicting: 128it [01:38,  1.23it/s]Extractor Predicting: 129it [01:39,  1.24it/s]Extractor Predicting: 130it [01:40,  1.17it/s]Extractor Predicting: 131it [01:41,  1.17it/s]Extractor Predicting: 132it [01:42,  1.18it/s]Extractor Predicting: 133it [01:43,  1.19it/s]Extractor Predicting: 134it [01:44,  1.22it/s]Extractor Predicting: 135it [01:44,  1.22it/s]Extractor Predicting: 136it [01:45,  1.21it/s]Extractor Predicting: 137it [01:46,  1.22it/s]Extractor Predicting: 138it [01:47,  1.23it/s]Extractor Predicting: 139it [01:48,  1.24it/s]Extractor Predicting: 140it [01:48,  1.26it/s]Extractor Predicting: 141it [01:49,  1.24it/s]Extractor Predicting: 142it [01:50,  1.24it/s]Extractor Predicting: 143it [01:51,  1.23it/s]Extractor Predicting: 144it [01:52,  1.22it/s]Extractor Predicting: 145it [01:52,  1.61it/s]Extractor Predicting: 145it [01:52,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:30,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:30,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:30,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:30,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:30,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:25:31,913 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:25:31,914 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:32,690 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:33,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:33,805 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:36,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:36,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:36,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:36,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:36,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:25:37,833 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:25:37,834 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:38,487 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:38,741 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:38,742 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.45541635961680177,
  "recall": 0.17667238421955403,
  "score": 0.25458290422245106,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.25it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 6it [00:04,  1.27it/s]Extractor Predicting: 7it [00:05,  1.29it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.32it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.34it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.28it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.30it/s]Extractor Predicting: 22it [00:17,  1.26it/s]Extractor Predicting: 23it [00:17,  1.26it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:20,  1.29it/s]Extractor Predicting: 27it [00:21,  1.26it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:24,  1.26it/s]Extractor Predicting: 32it [00:24,  1.29it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:26,  1.33it/s]Extractor Predicting: 35it [00:27,  1.35it/s]Extractor Predicting: 36it [00:27,  1.29it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.32it/s]Extractor Predicting: 39it [00:30,  1.31it/s]Extractor Predicting: 40it [00:30,  1.31it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:32,  1.33it/s]Extractor Predicting: 43it [00:33,  1.34it/s]Extractor Predicting: 44it [00:33,  1.38it/s]Extractor Predicting: 45it [00:34,  1.35it/s]Extractor Predicting: 46it [00:35,  1.40it/s]Extractor Predicting: 47it [00:36,  1.37it/s]Extractor Predicting: 48it [00:36,  1.34it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.28it/s]Extractor Predicting: 51it [00:39,  1.32it/s]Extractor Predicting: 52it [00:39,  1.31it/s]Extractor Predicting: 53it [00:40,  1.33it/s]Extractor Predicting: 54it [00:41,  1.33it/s]Extractor Predicting: 55it [00:42,  1.32it/s]Extractor Predicting: 56it [00:43,  1.24it/s]Extractor Predicting: 57it [00:43,  1.26it/s]Extractor Predicting: 58it [00:44,  1.25it/s]Extractor Predicting: 59it [00:45,  1.27it/s]Extractor Predicting: 60it [00:46,  1.28it/s]Extractor Predicting: 61it [00:46,  1.29it/s]Extractor Predicting: 62it [00:47,  1.30it/s]Extractor Predicting: 63it [00:48,  1.28it/s]Extractor Predicting: 64it [00:49,  1.22it/s]Extractor Predicting: 65it [00:50,  1.27it/s]Extractor Predicting: 66it [00:50,  1.28it/s]Extractor Predicting: 67it [00:51,  1.32it/s]Extractor Predicting: 68it [00:52,  1.30it/s]Extractor Predicting: 69it [00:53,  1.29it/s]Extractor Predicting: 70it [00:53,  1.30it/s]Extractor Predicting: 71it [00:54,  1.30it/s]Extractor Predicting: 72it [00:55,  1.27it/s]Extractor Predicting: 73it [00:56,  1.27it/s]Extractor Predicting: 74it [00:57,  1.29it/s]Extractor Predicting: 75it [00:57,  1.29it/s]Extractor Predicting: 76it [00:58,  1.26it/s]Extractor Predicting: 77it [00:59,  1.27it/s]Extractor Predicting: 78it [01:00,  1.28it/s]Extractor Predicting: 79it [01:00,  1.30it/s]Extractor Predicting: 80it [01:01,  1.32it/s]Extractor Predicting: 81it [01:02,  1.27it/s]Extractor Predicting: 82it [01:03,  1.27it/s]Extractor Predicting: 83it [01:04,  1.29it/s]Extractor Predicting: 84it [01:04,  1.29it/s]Extractor Predicting: 85it [01:05,  1.30it/s]Extractor Predicting: 86it [01:06,  1.30it/s]Extractor Predicting: 87it [01:07,  1.32it/s]Extractor Predicting: 88it [01:07,  1.34it/s]Extractor Predicting: 89it [01:08,  1.35it/s]Extractor Predicting: 90it [01:09,  1.37it/s]Extractor Predicting: 91it [01:10,  1.34it/s]Extractor Predicting: 92it [01:10,  1.32it/s]Extractor Predicting: 93it [01:11,  1.33it/s]Extractor Predicting: 94it [01:12,  1.31it/s]Extractor Predicting: 95it [01:13,  1.32it/s]Extractor Predicting: 96it [01:13,  1.31it/s]Extractor Predicting: 97it [01:14,  1.31it/s]Extractor Predicting: 98it [01:15,  1.30it/s]Extractor Predicting: 99it [01:16,  1.30it/s]Extractor Predicting: 100it [01:16,  1.31it/s]Extractor Predicting: 101it [01:17,  1.31it/s]Extractor Predicting: 102it [01:18,  1.29it/s]Extractor Predicting: 103it [01:19,  1.29it/s]Extractor Predicting: 104it [01:20,  1.30it/s]Extractor Predicting: 105it [01:20,  1.30it/s]Extractor Predicting: 106it [01:21,  1.30it/s]Extractor Predicting: 107it [01:22,  1.30it/s]Extractor Predicting: 108it [01:23,  1.31it/s]Extractor Predicting: 109it [01:23,  1.31it/s]Extractor Predicting: 110it [01:24,  1.29it/s]Extractor Predicting: 111it [01:25,  1.31it/s]Extractor Predicting: 112it [01:26,  1.32it/s]Extractor Predicting: 113it [01:26,  1.31it/s]Extractor Predicting: 114it [01:27,  1.28it/s]Extractor Predicting: 115it [01:28,  1.30it/s]Extractor Predicting: 116it [01:29,  1.30it/s]Extractor Predicting: 117it [01:30,  1.32it/s]Extractor Predicting: 118it [01:30,  1.32it/s]Extractor Predicting: 119it [01:31,  1.31it/s]Extractor Predicting: 120it [01:32,  1.28it/s]Extractor Predicting: 121it [01:33,  1.30it/s]Extractor Predicting: 122it [01:33,  1.31it/s]Extractor Predicting: 123it [01:34,  1.33it/s]Extractor Predicting: 124it [01:35,  1.30it/s]Extractor Predicting: 125it [01:36,  1.31it/s]Extractor Predicting: 126it [01:36,  1.29it/s]Extractor Predicting: 127it [01:37,  1.30it/s]Extractor Predicting: 128it [01:38,  1.32it/s]Extractor Predicting: 129it [01:39,  1.32it/s]Extractor Predicting: 130it [01:39,  1.34it/s]Extractor Predicting: 131it [01:40,  1.36it/s]Extractor Predicting: 132it [01:41,  1.34it/s]Extractor Predicting: 133it [01:42,  1.30it/s]Extractor Predicting: 134it [01:42,  1.30it/s]Extractor Predicting: 135it [01:43,  1.32it/s]Extractor Predicting: 136it [01:44,  1.33it/s]Extractor Predicting: 137it [01:45,  1.31it/s]Extractor Predicting: 138it [01:45,  1.33it/s]Extractor Predicting: 139it [01:46,  1.29it/s]Extractor Predicting: 140it [01:47,  1.25it/s]Extractor Predicting: 140it [01:47,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:38,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:38,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:38,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:38,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:38,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:27:38,909 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:27:38,910 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:27:39,514 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:40,614 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:40,614 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:43,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:43,590 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:43,590 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:43,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:43,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:27:44,386 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:27:44,387 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:27:44,996 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:45,215 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:45,215 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.43601895734597157,
  "recall": 0.13706793802145412,
  "score": 0.20856948537746542,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 3it [00:02,  1.31it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:27:49,645 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:27:49,646 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:27:49,712 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:27:49,713 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:27:49,735 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:27:58,460 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:27:58,460 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:27:58,589 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:27:58,590 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:27:58,661 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:27:58,718 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.041666666666666664,
  "score": 0.07692307692307693,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:27:59,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:59,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:00,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:01,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:02,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:02,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:03,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:04,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:05,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:06,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:06,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:07,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:08,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:09,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:10,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:10,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:11,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:12,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:13,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:13,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:14,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:29, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:15,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:16,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:17,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:18,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:18,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:19,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:20,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:21,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:21,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:22,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:23,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:24,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:25,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:25,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:26,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:27,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:28,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:29,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:30,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:31,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:32,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:32,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:33,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:35<02:23, 17.92s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:34,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:35,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:35,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:36,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:37,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:38,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:38,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:39,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:40,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:40,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:41,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:42,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:42,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:43,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:44,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:45,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:45,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:46,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:47,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:48,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:49,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:49,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:50,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:51,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:52<02:03, 17.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:51,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:52,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:53,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:53,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:54,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:55,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:56,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:56,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:57,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:58,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:58,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:59,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:00,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:00,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:01,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:02,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:02,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:03,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:04,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:04,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:05,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:06,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:07<01:39, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:06,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:07,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:08,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:08,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:09,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:10,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:10,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:11,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:12,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:13,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:14,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:15,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:16,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:16,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:17,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:17,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:18,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:20,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:21,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:21,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:22,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:22, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:23,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:25,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:26,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:26,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:27,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:28,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:29,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:29,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:30,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:31,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:31,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:32,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:33,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:34,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:35,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:36,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:37,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:38,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:38,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:39,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:41<01:06, 16.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:40,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:41,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:41,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:42,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:43,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:44,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:45,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:45,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:46,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:47,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:48,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:48,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:49,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:50,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:51,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:52,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:52,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:53,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:54,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:55,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:55,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:56,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:57,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:59<00:51, 17.11s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:58,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:58,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:59,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:00,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:00,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:01,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:02,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:03,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:03,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:04,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:05,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:05,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:06,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:07,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:08,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:09,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:10,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:11,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:11,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:12,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:13<00:32, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:13,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:14,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:15,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:15,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:16,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:17,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:18,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:19,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:19,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:20,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:21,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:22,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:22,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:23,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:24,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:25,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:25,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:26,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:27,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:28,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:29,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:30<00:16, 16.49s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:29,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:30,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:31,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:31,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:32,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:33,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:34,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:34,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:35,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:36,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:37,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:37,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:38,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:39,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:40,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:41,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:42,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:42,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:44,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:44,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:46,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:47,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:48,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:49<00:00, 17.31s/it]Generating: 100%|██████████| 10/10 [02:49<00:00, 16.99s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:54,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:54,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:54,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:54,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:54,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:30:55,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:30:55,189 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:30:55,512 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:30:56,705 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:30:56,706 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:59,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:59,790 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:59,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:59,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:59,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:31:00,602 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:31:00,603 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:31:01,239 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:31:01,493 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:31:01,493 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : composer . Context : Later in the year ( 11 July 1931 ) , a score by Italian composer Giovanni Botti was commissioned into the main recording of " The Pianist " by Italian composer Giuseppe Sorrentini . Head Entity : The Pianist , Tail Entity : Gian Giovanni Botti .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : United States .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participating team . Context : Following his promotion to the U.S. Open , he played in the 2002 Masters Finals in Los Angeles , California against Michael Johnson . Head Entity : Michael Johnson , Tail Entity : Marines .\n']
['Relation : participating team . Context : Following his promotion to the U.S. Open , he played in the 2002 Masters Finals in Los Angeles , California against Michael Johnson . Head Entity : Michael Johnson , Tail Entity : Marines .\n', 'Relation : participating team . Context : After the 2008 NBA Finals , the Rockets offered Houston the No. 1 spot in the Eastern Conference for the 2011 NBA Draft . Head Entity : 2011 NBA Draft , Tail Entity : Houston Rockets .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : sport .', 'success_rate': 0.8505434782608695, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year , the school built a high school for children of French ancestry at the end of the 17th century in Marseille , located just north of the Bourbons , and opened for students from around the world . Head Entity : Paris , Tail Entity : Bourbons .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : owned by . Context : Later in the year , the property was listed on a real estate market that valued at $14.2 million . Head Entity : Real estate market , Tail Entity : Land .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : religion .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 8936
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9036, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.31it/s]Extractor Estimating: 2it [00:01,  1.27it/s]Extractor Estimating: 3it [00:02,  1.32it/s]Extractor Estimating: 4it [00:03,  1.32it/s]Extractor Estimating: 5it [00:03,  1.31it/s]Extractor Estimating: 6it [00:04,  1.34it/s]Extractor Estimating: 7it [00:05,  1.32it/s]Extractor Estimating: 8it [00:06,  1.32it/s]Extractor Estimating: 9it [00:06,  1.33it/s]Extractor Estimating: 10it [00:07,  1.31it/s]Extractor Estimating: 11it [00:08,  1.32it/s]Extractor Estimating: 12it [00:09,  1.34it/s]Extractor Estimating: 13it [00:09,  1.38it/s]Extractor Estimating: 14it [00:10,  1.37it/s]Extractor Estimating: 15it [00:11,  1.35it/s]Extractor Estimating: 16it [00:11,  1.36it/s]Extractor Estimating: 17it [00:12,  1.40it/s]Extractor Estimating: 18it [00:13,  1.36it/s]Extractor Estimating: 19it [00:14,  1.39it/s]Extractor Estimating: 20it [00:14,  1.37it/s]Extractor Estimating: 21it [00:15,  1.39it/s]Extractor Estimating: 22it [00:16,  1.38it/s]Extractor Estimating: 23it [00:17,  1.37it/s]Extractor Estimating: 24it [00:17,  1.42it/s]Extractor Estimating: 25it [00:18,  1.42it/s]Extractor Estimating: 26it [00:19,  1.36it/s]Extractor Estimating: 27it [00:20,  1.31it/s]Extractor Estimating: 28it [00:20,  1.31it/s]Extractor Estimating: 29it [00:21,  1.26it/s]Extractor Estimating: 30it [00:22,  1.19it/s]Extractor Estimating: 31it [00:23,  1.17it/s]Extractor Estimating: 32it [00:24,  1.23it/s]Extractor Estimating: 33it [00:24,  1.26it/s]Extractor Estimating: 34it [00:25,  1.25it/s]Extractor Estimating: 35it [00:26,  1.25it/s]Extractor Estimating: 36it [00:27,  1.24it/s]Extractor Estimating: 37it [00:28,  1.18it/s]Extractor Estimating: 38it [00:29,  1.22it/s]Extractor Estimating: 39it [00:30,  1.16it/s]Extractor Estimating: 40it [00:30,  1.18it/s]Extractor Estimating: 41it [00:31,  1.21it/s]Extractor Estimating: 42it [00:32,  1.13it/s]Extractor Estimating: 43it [00:33,  1.18it/s]Extractor Estimating: 44it [00:34,  1.19it/s]Extractor Estimating: 45it [00:34,  1.24it/s]Extractor Estimating: 46it [00:35,  1.24it/s]Extractor Estimating: 47it [00:36,  1.20it/s]Extractor Estimating: 48it [00:37,  1.20it/s]Extractor Estimating: 49it [00:38,  1.19it/s]Extractor Estimating: 50it [00:39,  1.19it/s]Extractor Estimating: 51it [00:39,  1.24it/s]Extractor Estimating: 52it [00:40,  1.28it/s]Extractor Estimating: 53it [00:41,  1.32it/s]Extractor Estimating: 54it [00:42,  1.32it/s]Extractor Estimating: 55it [00:42,  1.34it/s]Extractor Estimating: 56it [00:43,  1.30it/s]Extractor Estimating: 57it [00:44,  1.37it/s]Extractor Estimating: 58it [00:44,  1.39it/s]Extractor Estimating: 59it [00:45,  1.38it/s]Extractor Estimating: 60it [00:46,  1.42it/s]Extractor Estimating: 61it [00:47,  1.38it/s]Extractor Estimating: 62it [00:47,  1.41it/s]Extractor Estimating: 63it [00:48,  1.42it/s]Extractor Estimating: 64it [00:49,  1.38it/s]Extractor Estimating: 65it [00:49,  1.43it/s]Extractor Estimating: 66it [00:50,  1.40it/s]Extractor Estimating: 67it [00:51,  1.41it/s]Extractor Estimating: 68it [00:52,  1.31it/s]Extractor Estimating: 69it [00:53,  1.27it/s]Extractor Estimating: 70it [00:53,  1.30it/s]Extractor Estimating: 71it [00:54,  1.27it/s]Extractor Estimating: 72it [00:55,  1.34it/s]Extractor Estimating: 73it [00:55,  1.40it/s]Extractor Estimating: 74it [00:56,  1.42it/s]Extractor Estimating: 75it [00:57,  1.40it/s]Extractor Estimating: 76it [00:58,  1.42it/s]Extractor Estimating: 77it [00:58,  1.41it/s]Extractor Estimating: 78it [00:59,  1.44it/s]Extractor Estimating: 79it [01:00,  1.44it/s]Extractor Estimating: 80it [01:00,  1.48it/s]Extractor Estimating: 81it [01:01,  1.48it/s]Extractor Estimating: 82it [01:02,  1.47it/s]Extractor Estimating: 83it [01:02,  1.50it/s]Extractor Estimating: 84it [01:03,  1.44it/s]Extractor Estimating: 85it [01:04,  1.43it/s]Extractor Estimating: 86it [01:04,  1.48it/s]Extractor Estimating: 87it [01:05,  1.46it/s]Extractor Estimating: 88it [01:06,  1.46it/s]Extractor Estimating: 89it [01:06,  1.46it/s]Extractor Estimating: 90it [01:07,  1.37it/s]Extractor Estimating: 91it [01:08,  1.40it/s]Extractor Estimating: 92it [01:09,  1.43it/s]Extractor Estimating: 93it [01:09,  1.47it/s]Extractor Estimating: 94it [01:10,  1.50it/s]Extractor Estimating: 95it [01:11,  1.47it/s]Extractor Estimating: 96it [01:11,  1.44it/s]Extractor Estimating: 97it [01:12,  1.46it/s]Extractor Estimating: 98it [01:13,  1.43it/s]Extractor Estimating: 99it [01:13,  1.46it/s]Extractor Estimating: 100it [01:14,  1.44it/s]Extractor Estimating: 101it [01:15,  1.50it/s]Extractor Estimating: 102it [01:15,  1.51it/s]Extractor Estimating: 103it [01:16,  1.50it/s]Extractor Estimating: 104it [01:17,  1.52it/s]Extractor Estimating: 105it [01:17,  1.48it/s]Extractor Estimating: 106it [01:18,  1.54it/s]Extractor Estimating: 107it [01:19,  1.46it/s]Extractor Estimating: 108it [01:19,  1.51it/s]Extractor Estimating: 109it [01:20,  1.59it/s]Extractor Estimating: 110it [01:21,  1.55it/s]Extractor Estimating: 111it [01:21,  1.53it/s]Extractor Estimating: 112it [01:22,  1.52it/s]Extractor Estimating: 113it [01:23,  1.49it/s]Extractor Estimating: 114it [01:23,  1.47it/s]Extractor Estimating: 115it [01:24,  1.49it/s]Extractor Estimating: 116it [01:25,  1.52it/s]Extractor Estimating: 117it [01:25,  1.54it/s]Extractor Estimating: 118it [01:26,  1.43it/s]Extractor Estimating: 119it [01:27,  1.48it/s]Extractor Estimating: 120it [01:27,  1.43it/s]Extractor Estimating: 121it [01:28,  1.44it/s]Extractor Estimating: 122it [01:29,  1.49it/s]Extractor Estimating: 123it [01:29,  1.46it/s]Extractor Estimating: 124it [01:30,  1.53it/s]Extractor Estimating: 125it [01:31,  1.50it/s]Extractor Estimating: 126it [01:31,  1.54it/s]Extractor Estimating: 127it [01:32,  1.57it/s]Extractor Estimating: 128it [01:33,  1.58it/s]Extractor Estimating: 129it [01:33,  1.59it/s]Extractor Estimating: 130it [01:34,  1.57it/s]Extractor Estimating: 131it [01:35,  1.53it/s]Extractor Estimating: 132it [01:35,  1.54it/s]Extractor Estimating: 133it [01:36,  1.49it/s]Extractor Estimating: 134it [01:36,  1.53it/s]Extractor Estimating: 135it [01:37,  1.50it/s]Extractor Estimating: 136it [01:38,  1.41it/s]Extractor Estimating: 137it [01:39,  1.44it/s]Extractor Estimating: 138it [01:39,  1.47it/s]Extractor Estimating: 139it [01:40,  1.56it/s]Extractor Estimating: 140it [01:41,  1.50it/s]Extractor Estimating: 141it [01:41,  1.47it/s]Extractor Estimating: 142it [01:42,  1.49it/s]Extractor Estimating: 143it [01:43,  1.46it/s]Extractor Estimating: 144it [01:43,  1.52it/s]Extractor Estimating: 145it [01:44,  1.52it/s]Extractor Estimating: 146it [01:45,  1.47it/s]Extractor Estimating: 147it [01:45,  1.43it/s]Extractor Estimating: 148it [01:46,  1.46it/s]Extractor Estimating: 149it [01:47,  1.48it/s]Extractor Estimating: 150it [01:47,  1.44it/s]Extractor Estimating: 151it [01:48,  1.35it/s]Extractor Estimating: 152it [01:49,  1.28it/s]Extractor Estimating: 153it [01:50,  1.33it/s]Extractor Estimating: 154it [01:51,  1.31it/s]Extractor Estimating: 155it [01:51,  1.29it/s]Extractor Estimating: 156it [01:52,  1.27it/s]Extractor Estimating: 157it [01:53,  1.29it/s]Extractor Estimating: 158it [01:54,  1.30it/s]Extractor Estimating: 159it [01:55,  1.28it/s]Extractor Estimating: 160it [01:55,  1.27it/s]Extractor Estimating: 161it [01:56,  1.27it/s]Extractor Estimating: 162it [01:57,  1.30it/s]Extractor Estimating: 163it [01:58,  1.27it/s]Extractor Estimating: 164it [01:58,  1.27it/s]Extractor Estimating: 165it [01:59,  1.24it/s]Extractor Estimating: 166it [02:00,  1.24it/s]Extractor Estimating: 167it [02:01,  1.30it/s]Extractor Estimating: 168it [02:02,  1.31it/s]Extractor Estimating: 169it [02:02,  1.28it/s]Extractor Estimating: 170it [02:03,  1.29it/s]Extractor Estimating: 171it [02:04,  1.32it/s]Extractor Estimating: 172it [02:05,  1.32it/s]Extractor Estimating: 173it [02:05,  1.30it/s]Extractor Estimating: 174it [02:06,  1.29it/s]Extractor Estimating: 175it [02:07,  1.29it/s]Extractor Estimating: 176it [02:08,  1.38it/s]Extractor Estimating: 177it [02:08,  1.44it/s]Extractor Estimating: 178it [02:09,  1.49it/s]Extractor Estimating: 179it [02:10,  1.48it/s]Extractor Estimating: 180it [02:10,  1.49it/s]Extractor Estimating: 181it [02:11,  1.56it/s]Extractor Estimating: 182it [02:11,  1.50it/s]Extractor Estimating: 183it [02:12,  1.51it/s]Extractor Estimating: 184it [02:13,  1.55it/s]Extractor Estimating: 185it [02:13,  1.52it/s]Extractor Estimating: 186it [02:14,  1.51it/s]Extractor Estimating: 187it [02:15,  1.48it/s]Extractor Estimating: 188it [02:15,  1.50it/s]Extractor Estimating: 189it [02:16,  1.49it/s]Extractor Estimating: 190it [02:17,  1.42it/s]Extractor Estimating: 191it [02:18,  1.44it/s]Extractor Estimating: 192it [02:18,  1.45it/s]Extractor Estimating: 193it [02:19,  1.44it/s]Extractor Estimating: 194it [02:20,  1.46it/s]Extractor Estimating: 195it [02:20,  1.47it/s]Extractor Estimating: 196it [02:21,  1.51it/s]Extractor Estimating: 197it [02:22,  1.46it/s]Extractor Estimating: 198it [02:22,  1.43it/s]Extractor Estimating: 199it [02:23,  1.48it/s]Extractor Estimating: 200it [02:24,  1.45it/s]Extractor Estimating: 201it [02:24,  1.42it/s]Extractor Estimating: 202it [02:25,  1.40it/s]Extractor Estimating: 203it [02:26,  1.39it/s]Extractor Estimating: 204it [02:27,  1.40it/s]Extractor Estimating: 205it [02:27,  1.36it/s]Extractor Estimating: 206it [02:28,  1.35it/s]Extractor Estimating: 207it [02:29,  1.39it/s]Extractor Estimating: 208it [02:30,  1.35it/s]Extractor Estimating: 209it [02:30,  1.36it/s]Extractor Estimating: 210it [02:31,  1.29it/s]Extractor Estimating: 211it [02:32,  1.28it/s]Extractor Estimating: 212it [02:33,  1.37it/s]Extractor Estimating: 213it [02:33,  1.34it/s]Extractor Estimating: 214it [02:34,  1.37it/s]Extractor Estimating: 215it [02:35,  1.40it/s]Extractor Estimating: 216it [02:36,  1.37it/s]Extractor Estimating: 217it [02:36,  1.40it/s]Extractor Estimating: 218it [02:37,  1.43it/s]Extractor Estimating: 219it [02:38,  1.44it/s]Extractor Estimating: 220it [02:38,  1.51it/s]Extractor Estimating: 221it [02:39,  1.46it/s]Extractor Estimating: 222it [02:40,  1.49it/s]Extractor Estimating: 223it [02:40,  1.47it/s]Extractor Estimating: 224it [02:41,  1.44it/s]Extractor Estimating: 225it [02:42,  1.44it/s]Extractor Estimating: 226it [02:42,  1.37it/s]Extractor Estimating: 227it [02:43,  1.35it/s]Extractor Estimating: 228it [02:44,  1.38it/s]Extractor Estimating: 229it [02:45,  1.43it/s]Extractor Estimating: 230it [02:45,  1.44it/s]Extractor Estimating: 231it [02:46,  1.34it/s]Extractor Estimating: 232it [02:47,  1.38it/s]Extractor Estimating: 233it [02:48,  1.38it/s]Extractor Estimating: 234it [02:48,  1.36it/s]Extractor Estimating: 235it [02:49,  1.36it/s]Extractor Estimating: 236it [02:50,  1.39it/s]Extractor Estimating: 237it [02:50,  1.40it/s]Extractor Estimating: 238it [02:51,  1.37it/s]Extractor Estimating: 239it [02:52,  1.37it/s]Extractor Estimating: 240it [02:53,  1.36it/s]Extractor Estimating: 241it [02:53,  1.33it/s]Extractor Estimating: 242it [02:54,  1.36it/s]Extractor Estimating: 243it [02:55,  1.36it/s]Extractor Estimating: 244it [02:56,  1.34it/s]Extractor Estimating: 245it [02:56,  1.32it/s]Extractor Estimating: 246it [02:57,  1.34it/s]Extractor Estimating: 247it [02:58,  1.32it/s]Extractor Estimating: 248it [02:59,  1.29it/s]Extractor Estimating: 249it [03:00,  1.28it/s]Extractor Estimating: 250it [03:00,  1.31it/s]Extractor Estimating: 250it [03:00,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:17,598 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:17,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:17,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:17,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:17,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:34:18,461 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:34:18,462 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:19,045 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:20,134 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:20,134 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:23,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:23,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:23,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:23,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:23,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:34:23,991 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:34:23,992 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:24,639 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:24,870 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:24,870 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:14:14,638 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:14:14,956 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4996 mean pseudo reward: 0.9519373411247185
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 19269
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19369, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19369, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.134, loss:546.5560
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.156, loss:489.4634
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.149, loss:457.8830
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.145, loss:454.9005
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.148, loss:429.9518
>> valid entity prec:0.5628, rec:0.5823, f1:0.5724
>> valid relation prec:0.3030, rec:0.1603, f1:0.2097
>> valid relation with NER prec:0.3030, rec:0.1603, f1:0.2097
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.683, loss:438.0323
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.142, loss:410.5643
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.137, loss:434.0137
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.149, loss:431.4915
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.145, loss:447.7447
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6098, rec:0.5042, f1:0.5520
>> valid relation prec:0.2942, rec:0.1225, f1:0.1730
>> valid relation with NER prec:0.2942, rec:0.1225, f1:0.1730
g_step 1100, step 55, avg_time 2.658, loss:436.2264
g_step 1200, step 155, avg_time 1.150, loss:437.9512
g_step 1300, step 46, avg_time 1.129, loss:428.1760
g_step 1400, step 146, avg_time 1.159, loss:412.8582
g_step 1500, step 37, avg_time 1.136, loss:419.3910
>> valid entity prec:0.6063, rec:0.5438, f1:0.5734
>> valid relation prec:0.3473, rec:0.1732, f1:0.2311
>> valid relation with NER prec:0.3473, rec:0.1732, f1:0.2311
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 2.674, loss:393.2099
g_step 1700, step 28, avg_time 1.128, loss:388.1726
g_step 1800, step 128, avg_time 1.138, loss:371.8264
g_step 1900, step 19, avg_time 1.139, loss:376.0674
g_step 2000, step 119, avg_time 1.139, loss:352.0020
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5610, rec:0.5350, f1:0.5477
>> valid relation prec:0.2831, rec:0.1308, f1:0.1790
>> valid relation with NER prec:0.2831, rec:0.1308, f1:0.1790
g_step 2100, step 10, avg_time 2.652, loss:357.1523
g_step 2200, step 110, avg_time 1.144, loss:342.6349
g_step 2300, step 1, avg_time 1.142, loss:353.5924
g_step 2400, step 101, avg_time 1.146, loss:312.6620
g_step 2500, step 201, avg_time 1.141, loss:340.2700
>> valid entity prec:0.6053, rec:0.5566, f1:0.5799
>> valid relation prec:0.3062, rec:0.1812, f1:0.2277
>> valid relation with NER prec:0.3062, rec:0.1812, f1:0.2277
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 2.661, loss:304.0365
g_step 2700, step 192, avg_time 1.143, loss:325.1278
g_step 2800, step 83, avg_time 1.134, loss:292.2721
g_step 2900, step 183, avg_time 1.136, loss:332.1064
g_step 3000, step 74, avg_time 1.147, loss:305.4758
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6067, rec:0.5179, f1:0.5588
>> valid relation prec:0.3113, rec:0.1537, f1:0.2058
>> valid relation with NER prec:0.3113, rec:0.1537, f1:0.2058
g_step 3100, step 174, avg_time 2.651, loss:291.2901
g_step 3200, step 65, avg_time 1.136, loss:264.2892
g_step 3300, step 165, avg_time 1.148, loss:307.1171
g_step 3400, step 56, avg_time 1.133, loss:280.1261
g_step 3500, step 156, avg_time 1.132, loss:281.1106
>> valid entity prec:0.5584, rec:0.5766, f1:0.5674
>> valid relation prec:0.2431, rec:0.1457, f1:0.1822
>> valid relation with NER prec:0.2431, rec:0.1457, f1:0.1822
g_step 3600, step 47, avg_time 2.662, loss:266.3183
g_step 3700, step 147, avg_time 1.140, loss:277.4967
g_step 3800, step 38, avg_time 1.138, loss:253.9232
g_step 3900, step 138, avg_time 1.138, loss:263.4825
g_step 4000, step 29, avg_time 1.139, loss:249.3916
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5988, rec:0.5281, f1:0.5613
>> valid relation prec:0.2642, rec:0.1492, f1:0.1907
>> valid relation with NER prec:0.2642, rec:0.1492, f1:0.1907
g_step 4100, step 129, avg_time 2.635, loss:239.0147
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:14:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:14:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-14-14_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:14:16 - WARNING - datasets.builder -   Using custom data configuration default-769bdb6492a2dca7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-769bdb6492a2dca7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:14:19,102 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:14:19,103 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:14:19,104 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:14:19,105 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:14:19,217 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:14:19,289 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:14:19,743 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:14:23,019 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:14:23,127 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-769bdb6492a2dca7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.72ba/s] 40%|████      | 2/5 [00:00<00:00,  3.72ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.24ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.55ba/s]100%|██████████| 5/5 [00:01<00:00,  4.67ba/s]100%|██████████| 5/5 [00:01<00:00,  4.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.86ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.96ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.55ba/s]100%|██████████| 4/4 [00:01<00:00,  4.68ba/s]100%|██████████| 4/4 [00:01<00:00,  3.99ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  4.00ba/s] 40%|████      | 2/5 [00:00<00:00,  5.56ba/s] 80%|████████  | 4/5 [00:00<00:00,  7.96ba/s]100%|██████████| 5/5 [00:00<00:00,  7.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.97ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.39ba/s]100%|██████████| 4/4 [00:00<00:00,  8.20ba/s]
[INFO|trainer.py:414] 2023-08-28 15:14:28,092 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:14:28,183 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:14:28,183 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 15:14:28,183 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:14:28,183 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:14:28,183 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:14:28,183 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:14:28,183 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:00,  3.23it/s]  1%|          | 2/390 [00:00<01:55,  3.35it/s]  1%|          | 3/390 [00:00<01:53,  3.40it/s]  1%|          | 4/390 [00:01<01:52,  3.42it/s]  1%|▏         | 5/390 [00:01<01:57,  3.28it/s]  2%|▏         | 6/390 [00:01<01:56,  3.30it/s]  2%|▏         | 7/390 [00:02<01:55,  3.32it/s]  2%|▏         | 8/390 [00:02<01:54,  3.33it/s]  2%|▏         | 9/390 [00:02<01:54,  3.34it/s]  3%|▎         | 10/390 [00:02<01:53,  3.35it/s]  3%|▎         | 11/390 [00:03<01:53,  3.35it/s]  3%|▎         | 12/390 [00:03<01:52,  3.35it/s]  3%|▎         | 13/390 [00:03<01:52,  3.35it/s]  4%|▎         | 14/390 [00:04<01:52,  3.35it/s]  4%|▍         | 15/390 [00:04<01:57,  3.19it/s]  4%|▍         | 16/390 [00:04<01:55,  3.24it/s]  4%|▍         | 17/390 [00:05<01:53,  3.27it/s]  5%|▍         | 18/390 [00:05<01:52,  3.30it/s]  5%|▍         | 19/390 [00:05<01:51,  3.31it/s]  5%|▌         | 20/390 [00:06<01:51,  3.32it/s]  5%|▌         | 21/390 [00:06<01:50,  3.33it/s]  6%|▌         | 22/390 [00:06<01:50,  3.33it/s]  6%|▌         | 23/390 [00:06<01:49,  3.34it/s]  6%|▌         | 24/390 [00:07<01:49,  3.34it/s]  6%|▋         | 25/390 [00:07<01:52,  3.24it/s]  7%|▋         | 26/390 [00:07<01:51,  3.27it/s]  7%|▋         | 27/390 [00:08<01:50,  3.30it/s]  7%|▋         | 28/390 [00:08<01:49,  3.31it/s]  7%|▋         | 29/390 [00:08<01:48,  3.32it/s]  8%|▊         | 30/390 [00:09<01:48,  3.33it/s]  8%|▊         | 31/390 [00:09<01:47,  3.34it/s]  8%|▊         | 32/390 [00:09<01:47,  3.34it/s]  8%|▊         | 33/390 [00:09<01:46,  3.34it/s]  9%|▊         | 34/390 [00:10<01:46,  3.35it/s]  9%|▉         | 35/390 [00:10<01:51,  3.18it/s]  9%|▉         | 36/390 [00:10<01:49,  3.23it/s]  9%|▉         | 37/390 [00:11<01:48,  3.26it/s] 10%|▉         | 38/390 [00:11<01:46,  3.29it/s] 10%|█         | 39/390 [00:11<01:46,  3.31it/s] 10%|█         | 40/390 [00:12<01:45,  3.32it/s] 11%|█         | 41/390 [00:12<01:44,  3.33it/s] 11%|█         | 42/390 [00:12<01:44,  3.33it/s] 11%|█         | 43/390 [00:12<01:43,  3.34it/s] 11%|█▏        | 44/390 [00:13<01:43,  3.34it/s] 12%|█▏        | 45/390 [00:13<01:50,  3.13it/s] 12%|█▏        | 46/390 [00:13<01:47,  3.19it/s] 12%|█▏        | 47/390 [00:14<01:46,  3.23it/s] 12%|█▏        | 48/390 [00:14<01:44,  3.27it/s] 13%|█▎        | 49/390 [00:14<01:43,  3.29it/s] 13%|█▎        | 50/390 [00:15<01:42,  3.31it/s] 13%|█▎        | 51/390 [00:15<01:42,  3.32it/s] 13%|█▎        | 52/390 [00:15<01:41,  3.33it/s] 14%|█▎        | 53/390 [00:16<01:40,  3.34it/s] 14%|█▍        | 54/390 [00:16<01:40,  3.35it/s] 14%|█▍        | 55/390 [00:16<01:44,  3.19it/s] 14%|█▍        | 56/390 [00:16<01:43,  3.24it/s] 15%|█▍        | 57/390 [00:17<01:41,  3.28it/s] 15%|█▍        | 58/390 [00:17<01:40,  3.30it/s] 15%|█▌        | 59/390 [00:17<01:39,  3.32it/s] 15%|█▌        | 60/390 [00:18<01:39,  3.33it/s] 16%|█▌        | 61/390 [00:18<01:38,  3.34it/s] 16%|█▌        | 62/390 [00:18<01:38,  3.34it/s] 16%|█▌        | 63/390 [00:19<01:37,  3.35it/s] 16%|█▋        | 64/390 [00:19<01:37,  3.35it/s] 17%|█▋        | 65/390 [00:19<01:36,  3.36it/s] 17%|█▋        | 66/390 [00:19<01:36,  3.36it/s] 17%|█▋        | 67/390 [00:20<01:39,  3.23it/s] 17%|█▋        | 68/390 [00:20<01:38,  3.27it/s] 18%|█▊        | 69/390 [00:20<01:37,  3.30it/s] 18%|█▊        | 70/390 [00:21<01:36,  3.31it/s] 18%|█▊        | 71/390 [00:21<01:35,  3.33it/s] 18%|█▊        | 72/390 [00:21<01:35,  3.34it/s] 19%|█▊        | 73/390 [00:22<01:34,  3.35it/s] 19%|█▉        | 74/390 [00:22<01:34,  3.35it/s] 19%|█▉        | 75/390 [00:22<01:33,  3.36it/s] 19%|█▉        | 76/390 [00:22<01:33,  3.36it/s] 20%|█▉        | 77/390 [00:23<01:38,  3.17it/s] 20%|██        | 78/390 [00:23<01:36,  3.23it/s][INFO|trainer.py:2140] 2023-08-28 15:14:51,852 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:14:51,852 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:14:51,852 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.30it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.86it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.16it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.11it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.52it/s][A
  7%|▋         | 32/438 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.93it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.63it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.68it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.78it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.86it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.88it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.68it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.60it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.45it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.46it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.56it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.65it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.78it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.90it/s][A
 26%|██▌       | 112/438 [00:02<00:08, 36.55it/s][A
 27%|██▋       | 117/438 [00:02<00:08, 38.80it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 40.49it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 41.72it/s][A
 30%|███       | 132/438 [00:03<00:07, 42.70it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.46it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.89it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.18it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.00it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.97it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.28it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.44it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.64it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.82it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.88it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.74it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.53it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.39it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.40it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.46it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.51it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.79it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.79it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.83it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.81it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.70it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 38.62it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 40.32it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 41.66it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 42.66it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.40it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.90it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.27it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.41it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.11it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 43.90it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.88it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.17it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.31it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.68it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.83it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.85it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.74it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.51it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.30it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.28it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.41it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.38it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.75it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.80it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.89it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.79it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.48it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 39.40it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 40.78it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 42.08it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 42.88it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 43.68it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.11it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.40it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.47it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.12it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.92it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.06it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.29it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.29it/s][A 20%|██        | 78/390 [00:33<01:36,  3.23it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:15:02,161 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 15:15:02,633 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:15:07,273 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:15:07,456 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:15:07,557 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:48<40:28,  7.81s/it] 21%|██        | 80/390 [00:49<28:42,  5.56s/it] 21%|██        | 81/390 [00:49<20:29,  3.98s/it] 21%|██        | 82/390 [00:49<14:45,  2.87s/it] 21%|██▏       | 83/390 [00:50<10:45,  2.10s/it] 22%|██▏       | 84/390 [00:50<07:58,  1.56s/it] 22%|██▏       | 85/390 [00:50<06:01,  1.18s/it] 22%|██▏       | 86/390 [00:51<04:45,  1.06it/s] 22%|██▏       | 87/390 [00:51<03:46,  1.34it/s] 23%|██▎       | 88/390 [00:51<03:05,  1.63it/s] 23%|██▎       | 89/390 [00:51<02:36,  1.93it/s] 23%|██▎       | 90/390 [00:52<02:15,  2.21it/s] 23%|██▎       | 91/390 [00:52<02:01,  2.46it/s] 24%|██▎       | 92/390 [00:52<01:51,  2.67it/s] 24%|██▍       | 93/390 [00:53<01:44,  2.85it/s] 24%|██▍       | 94/390 [00:53<01:39,  2.98it/s] 24%|██▍       | 95/390 [00:53<01:35,  3.09it/s] 25%|██▍       | 96/390 [00:54<01:36,  3.04it/s] 25%|██▍       | 97/390 [00:54<01:33,  3.13it/s] 25%|██▌       | 98/390 [00:54<01:31,  3.20it/s] 25%|██▌       | 99/390 [00:55<01:29,  3.24it/s] 26%|██▌       | 100/390 [00:55<01:28,  3.27it/s] 26%|██▌       | 101/390 [00:55<01:27,  3.29it/s] 26%|██▌       | 102/390 [00:55<01:27,  3.31it/s] 26%|██▋       | 103/390 [00:56<01:26,  3.32it/s] 27%|██▋       | 104/390 [00:56<01:25,  3.33it/s] 27%|██▋       | 105/390 [00:56<01:25,  3.34it/s] 27%|██▋       | 106/390 [00:57<01:29,  3.18it/s] 27%|██▋       | 107/390 [00:57<01:27,  3.23it/s] 28%|██▊       | 108/390 [00:57<01:26,  3.27it/s] 28%|██▊       | 109/390 [00:58<01:25,  3.29it/s] 28%|██▊       | 110/390 [00:58<01:24,  3.31it/s] 28%|██▊       | 111/390 [00:58<01:24,  3.32it/s] 29%|██▊       | 112/390 [00:58<01:23,  3.33it/s] 29%|██▉       | 113/390 [00:59<01:23,  3.34it/s] 29%|██▉       | 114/390 [00:59<01:22,  3.34it/s] 29%|██▉       | 115/390 [00:59<01:22,  3.35it/s] 30%|██▉       | 116/390 [01:00<01:24,  3.26it/s] 30%|███       | 117/390 [01:00<01:23,  3.28it/s] 30%|███       | 118/390 [01:00<01:22,  3.30it/s] 31%|███       | 119/390 [01:01<01:21,  3.31it/s] 31%|███       | 120/390 [01:01<01:21,  3.33it/s] 31%|███       | 121/390 [01:01<01:20,  3.33it/s] 31%|███▏      | 122/390 [01:01<01:20,  3.33it/s] 32%|███▏      | 123/390 [01:02<01:19,  3.34it/s] 32%|███▏      | 124/390 [01:02<01:19,  3.34it/s] 32%|███▏      | 125/390 [01:02<01:19,  3.34it/s] 32%|███▏      | 126/390 [01:03<01:22,  3.20it/s] 33%|███▎      | 127/390 [01:03<01:21,  3.25it/s] 33%|███▎      | 128/390 [01:03<01:19,  3.28it/s] 33%|███▎      | 129/390 [01:04<01:19,  3.30it/s] 33%|███▎      | 130/390 [01:04<01:18,  3.31it/s] 34%|███▎      | 131/390 [01:04<01:17,  3.32it/s] 34%|███▍      | 132/390 [01:04<01:17,  3.33it/s] 34%|███▍      | 133/390 [01:05<01:17,  3.33it/s] 34%|███▍      | 134/390 [01:05<01:16,  3.33it/s] 35%|███▍      | 135/390 [01:05<01:16,  3.34it/s] 35%|███▍      | 136/390 [01:06<01:19,  3.21it/s] 35%|███▌      | 137/390 [01:06<01:17,  3.25it/s] 35%|███▌      | 138/390 [01:06<01:16,  3.27it/s] 36%|███▌      | 139/390 [01:07<01:16,  3.30it/s] 36%|███▌      | 140/390 [01:07<01:15,  3.31it/s] 36%|███▌      | 141/390 [01:07<01:14,  3.32it/s] 36%|███▋      | 142/390 [01:08<01:14,  3.33it/s] 37%|███▋      | 143/390 [01:08<01:14,  3.33it/s] 37%|███▋      | 144/390 [01:08<01:13,  3.34it/s] 37%|███▋      | 145/390 [01:08<01:13,  3.34it/s] 37%|███▋      | 146/390 [01:09<01:16,  3.20it/s] 38%|███▊      | 147/390 [01:09<01:14,  3.24it/s] 38%|███▊      | 148/390 [01:09<01:14,  3.26it/s] 38%|███▊      | 149/390 [01:10<01:13,  3.29it/s] 38%|███▊      | 150/390 [01:10<01:12,  3.30it/s] 39%|███▊      | 151/390 [01:10<01:12,  3.32it/s] 39%|███▉      | 152/390 [01:11<01:11,  3.33it/s] 39%|███▉      | 153/390 [01:11<01:11,  3.34it/s] 39%|███▉      | 154/390 [01:11<01:10,  3.34it/s] 40%|███▉      | 155/390 [01:11<01:10,  3.35it/s] 40%|████      | 156/390 [01:12<01:12,  3.25it/s][INFO|trainer.py:2140] 2023-08-28 15:15:40,518 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:15:40,519 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:15:40,519 >>   Batch size = 8
{'eval_loss': 0.952972412109375, 'eval_runtime': 9.9879, 'eval_samples_per_second': 350.225, 'eval_steps_per_second': 43.853, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.52it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.70it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.16it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.51it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.69it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.04it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.78it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.53it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.57it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.79it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.82it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.91it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.99it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.88it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.74it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.63it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.52it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.47it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.58it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.70it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.85it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.90it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.57it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.41it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.76it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.00it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.08it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.39it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.55it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.66it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.67it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.54it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.54it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.58it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.57it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.53it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.63it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.71it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.84it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.75it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.64it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.59it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.67it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.59it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.58it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.63it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.76it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.84it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.59it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.72it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 41.86it/s][A
 61%|██████    | 267/438 [00:06<00:04, 39.45it/s][A
 62%|██████▏   | 272/438 [00:06<00:04, 40.99it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 42.17it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 42.98it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 43.67it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.00it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.17it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/438 [00:06<00:02, 43.95it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.04it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.32it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.54it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.72it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.86it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.80it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.68it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.38it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.18it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.28it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.44it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.62it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.75it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.79it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.93it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.75it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.54it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.41it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 42.16it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 42.92it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.61it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.06it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.38it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.56it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.54it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.48it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.48it/s][A 40%|████      | 156/390 [01:22<01:12,  3.25it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:15:50,603 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 15:15:50,814 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:15:54,140 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:15:54,267 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:15:54,305 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:36<28:39,  7.38s/it] 41%|████      | 158/390 [01:36<20:23,  5.27s/it] 41%|████      | 159/390 [01:36<14:33,  3.78s/it] 41%|████      | 160/390 [01:37<10:29,  2.74s/it] 41%|████▏     | 161/390 [01:37<07:39,  2.01s/it] 42%|████▏     | 162/390 [01:37<05:40,  1.49s/it] 42%|████▏     | 163/390 [01:38<04:17,  1.14s/it] 42%|████▏     | 164/390 [01:38<03:19,  1.13it/s] 42%|████▏     | 165/390 [01:38<02:39,  1.41it/s] 43%|████▎     | 166/390 [01:38<02:11,  1.71it/s] 43%|████▎     | 167/390 [01:39<01:51,  2.00it/s] 43%|████▎     | 168/390 [01:39<01:39,  2.24it/s] 43%|████▎     | 169/390 [01:39<01:28,  2.49it/s] 44%|████▎     | 170/390 [01:40<01:21,  2.70it/s] 44%|████▍     | 171/390 [01:40<01:16,  2.87it/s] 44%|████▍     | 172/390 [01:40<01:12,  3.00it/s] 44%|████▍     | 173/390 [01:41<01:09,  3.10it/s] 45%|████▍     | 174/390 [01:41<01:08,  3.17it/s] 45%|████▍     | 175/390 [01:41<01:06,  3.23it/s] 45%|████▌     | 176/390 [01:41<01:05,  3.26it/s] 45%|████▌     | 177/390 [01:42<01:04,  3.29it/s] 46%|████▌     | 178/390 [01:42<01:05,  3.23it/s] 46%|████▌     | 179/390 [01:42<01:04,  3.26it/s] 46%|████▌     | 180/390 [01:43<01:03,  3.29it/s] 46%|████▋     | 181/390 [01:43<01:03,  3.31it/s] 47%|████▋     | 182/390 [01:43<01:02,  3.33it/s] 47%|████▋     | 183/390 [01:44<01:02,  3.34it/s] 47%|████▋     | 184/390 [01:44<01:01,  3.34it/s] 47%|████▋     | 185/390 [01:44<01:01,  3.35it/s] 48%|████▊     | 186/390 [01:44<01:00,  3.35it/s] 48%|████▊     | 187/390 [01:45<01:00,  3.35it/s] 48%|████▊     | 188/390 [01:45<01:02,  3.22it/s] 48%|████▊     | 189/390 [01:45<01:01,  3.26it/s] 49%|████▊     | 190/390 [01:46<01:00,  3.29it/s] 49%|████▉     | 191/390 [01:46<01:00,  3.31it/s] 49%|████▉     | 192/390 [01:46<00:59,  3.32it/s] 49%|████▉     | 193/390 [01:47<00:59,  3.34it/s] 50%|████▉     | 194/390 [01:47<01:00,  3.25it/s] 50%|█████     | 195/390 [01:47<00:59,  3.28it/s] 50%|█████     | 196/390 [01:48<01:06,  2.90it/s] 51%|█████     | 197/390 [01:48<01:17,  2.48it/s] 51%|█████     | 198/390 [01:48<01:14,  2.59it/s] 51%|█████     | 199/390 [01:49<01:08,  2.77it/s] 51%|█████▏    | 200/390 [01:49<01:05,  2.92it/s] 52%|█████▏    | 201/390 [01:49<01:02,  3.03it/s] 52%|█████▏    | 202/390 [01:50<01:00,  3.12it/s] 52%|█████▏    | 203/390 [01:50<00:58,  3.19it/s] 52%|█████▏    | 204/390 [01:50<00:57,  3.23it/s] 53%|█████▎    | 205/390 [01:51<00:56,  3.27it/s] 53%|█████▎    | 206/390 [01:51<00:55,  3.29it/s] 53%|█████▎    | 207/390 [01:51<00:55,  3.31it/s] 53%|█████▎    | 208/390 [01:51<00:54,  3.33it/s] 54%|█████▎    | 209/390 [01:52<00:56,  3.23it/s] 54%|█████▍    | 210/390 [01:52<00:55,  3.26it/s] 54%|█████▍    | 211/390 [01:52<00:54,  3.29it/s] 54%|█████▍    | 212/390 [01:53<00:53,  3.31it/s] 55%|█████▍    | 213/390 [01:53<00:53,  3.32it/s] 55%|█████▍    | 214/390 [01:53<00:52,  3.33it/s] 55%|█████▌    | 215/390 [01:54<00:52,  3.34it/s] 55%|█████▌    | 216/390 [01:54<00:52,  3.34it/s] 56%|█████▌    | 217/390 [01:54<00:51,  3.35it/s] 56%|█████▌    | 218/390 [01:54<00:51,  3.35it/s] 56%|█████▌    | 219/390 [01:55<00:54,  3.16it/s] 56%|█████▋    | 220/390 [01:55<00:52,  3.21it/s] 57%|█████▋    | 221/390 [01:55<00:51,  3.25it/s] 57%|█████▋    | 222/390 [01:56<00:51,  3.28it/s] 57%|█████▋    | 223/390 [01:56<00:50,  3.30it/s] 57%|█████▋    | 224/390 [01:56<00:50,  3.31it/s] 58%|█████▊    | 225/390 [01:57<00:49,  3.32it/s] 58%|█████▊    | 226/390 [01:57<00:49,  3.33it/s] 58%|█████▊    | 227/390 [01:57<00:48,  3.33it/s] 58%|█████▊    | 228/390 [01:58<00:48,  3.34it/s] 59%|█████▊    | 229/390 [01:58<00:50,  3.19it/s] 59%|█████▉    | 230/390 [01:58<00:49,  3.24it/s] 59%|█████▉    | 231/390 [01:58<00:48,  3.27it/s] 59%|█████▉    | 232/390 [01:59<00:48,  3.29it/s] 60%|█████▉    | 233/390 [01:59<00:47,  3.31it/s] 60%|██████    | 234/390 [01:59<00:47,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 15:16:28,166 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:16:28,166 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:16:28,166 >>   Batch size = 8
{'eval_loss': 0.9722132086753845, 'eval_runtime': 9.8844, 'eval_samples_per_second': 353.893, 'eval_steps_per_second': 44.312, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.97it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.46it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.68it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.06it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.68it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.43it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.92it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.60it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 42.18it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 43.11it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 43.76it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.04it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.38it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.37it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.18it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.10it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.15it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.34it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.53it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.54it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.77it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.90it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.75it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.55it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.37it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.30it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.53it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.54it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.74it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.88it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.89it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.82it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.67it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.45it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.47it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 43.97it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.26it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.53it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.68it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.78it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.71it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.59it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.46it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.49it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.53it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.63it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.68it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.77it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.82it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.71it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.59it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.55it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.52it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.45it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.65it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.75it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.79it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.76it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.64it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.57it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.49it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.52it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 42.29it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.12it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.76it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.10it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.38it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.41it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.30it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.32it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.11it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.29it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.54it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.64it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.79it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.85it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.89it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.70it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.48it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.38it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.44it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.55it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.67it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.79it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.85it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.85it/s][A 60%|██████    | 234/390 [02:09<00:47,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:16:38,276 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:16:38,553 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:16:41,581 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:16:41,860 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:16:41,922 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:21<17:30,  6.78s/it] 61%|██████    | 236/390 [02:22<12:24,  4.83s/it] 61%|██████    | 237/390 [02:22<08:51,  3.47s/it] 61%|██████    | 238/390 [02:22<06:22,  2.52s/it] 61%|██████▏   | 239/390 [02:22<04:41,  1.86s/it] 62%|██████▏   | 240/390 [02:23<03:29,  1.39s/it] 62%|██████▏   | 241/390 [02:23<02:38,  1.06s/it] 62%|██████▏   | 242/390 [02:23<02:03,  1.20it/s] 62%|██████▏   | 243/390 [02:24<01:38,  1.49it/s] 63%|██████▎   | 244/390 [02:24<01:21,  1.78it/s] 63%|██████▎   | 245/390 [02:24<01:09,  2.08it/s] 63%|██████▎   | 246/390 [02:25<01:01,  2.35it/s] 63%|██████▎   | 247/390 [02:25<00:55,  2.58it/s] 64%|██████▎   | 248/390 [02:25<00:51,  2.77it/s] 64%|██████▍   | 249/390 [02:26<00:49,  2.83it/s] 64%|██████▍   | 250/390 [02:26<00:47,  2.97it/s] 64%|██████▍   | 251/390 [02:26<00:45,  3.07it/s] 65%|██████▍   | 252/390 [02:26<00:43,  3.15it/s] 65%|██████▍   | 253/390 [02:27<00:42,  3.21it/s] 65%|██████▌   | 254/390 [02:27<00:41,  3.25it/s] 65%|██████▌   | 255/390 [02:27<00:41,  3.28it/s] 66%|██████▌   | 256/390 [02:28<00:40,  3.30it/s] 66%|██████▌   | 257/390 [02:28<00:40,  3.31it/s] 66%|██████▌   | 258/390 [02:28<00:39,  3.32it/s] 66%|██████▋   | 259/390 [02:29<00:42,  3.12it/s] 67%|██████▋   | 260/390 [02:29<00:40,  3.18it/s] 67%|██████▋   | 261/390 [02:29<00:39,  3.23it/s] 67%|██████▋   | 262/390 [02:29<00:39,  3.26it/s] 67%|██████▋   | 263/390 [02:30<00:38,  3.29it/s] 68%|██████▊   | 264/390 [02:30<00:38,  3.30it/s] 68%|██████▊   | 265/390 [02:30<00:37,  3.31it/s] 68%|██████▊   | 266/390 [02:31<00:37,  3.32it/s] 68%|██████▊   | 267/390 [02:31<00:36,  3.33it/s] 69%|██████▊   | 268/390 [02:31<00:36,  3.34it/s] 69%|██████▉   | 269/390 [02:32<00:39,  3.09it/s] 69%|██████▉   | 270/390 [02:32<00:37,  3.16it/s] 69%|██████▉   | 271/390 [02:32<00:37,  3.21it/s] 70%|██████▉   | 272/390 [02:33<00:36,  3.25it/s] 70%|███████   | 273/390 [02:33<00:35,  3.28it/s] 70%|███████   | 274/390 [02:33<00:35,  3.30it/s] 71%|███████   | 275/390 [02:33<00:34,  3.32it/s] 71%|███████   | 276/390 [02:34<00:34,  3.32it/s] 71%|███████   | 277/390 [02:34<00:33,  3.33it/s] 71%|███████▏  | 278/390 [02:34<00:33,  3.34it/s] 72%|███████▏  | 279/390 [02:35<00:36,  3.02it/s] 72%|███████▏  | 280/390 [02:35<00:35,  3.12it/s] 72%|███████▏  | 281/390 [02:35<00:34,  3.19it/s] 72%|███████▏  | 282/390 [02:36<00:33,  3.23it/s] 73%|███████▎  | 283/390 [02:36<00:32,  3.27it/s] 73%|███████▎  | 284/390 [02:36<00:32,  3.29it/s] 73%|███████▎  | 285/390 [02:37<00:31,  3.31it/s] 73%|███████▎  | 286/390 [02:37<00:31,  3.32it/s] 74%|███████▎  | 287/390 [02:37<00:30,  3.33it/s] 74%|███████▍  | 288/390 [02:37<00:30,  3.32it/s] 74%|███████▍  | 289/390 [02:38<00:32,  3.10it/s] 74%|███████▍  | 290/390 [02:38<00:31,  3.17it/s] 75%|███████▍  | 291/390 [02:38<00:30,  3.22it/s] 75%|███████▍  | 292/390 [02:39<00:30,  3.26it/s] 75%|███████▌  | 293/390 [02:39<00:29,  3.29it/s] 75%|███████▌  | 294/390 [02:39<00:29,  3.31it/s] 76%|███████▌  | 295/390 [02:40<00:28,  3.32it/s] 76%|███████▌  | 296/390 [02:40<00:28,  3.33it/s] 76%|███████▌  | 297/390 [02:40<00:27,  3.33it/s] 76%|███████▋  | 298/390 [02:40<00:27,  3.34it/s] 77%|███████▋  | 299/390 [02:41<00:28,  3.21it/s] 77%|███████▋  | 300/390 [02:41<00:27,  3.25it/s] 77%|███████▋  | 301/390 [02:41<00:27,  3.28it/s] 77%|███████▋  | 302/390 [02:42<00:26,  3.30it/s] 78%|███████▊  | 303/390 [02:42<00:26,  3.32it/s] 78%|███████▊  | 304/390 [02:42<00:25,  3.33it/s] 78%|███████▊  | 305/390 [02:43<00:25,  3.34it/s] 78%|███████▊  | 306/390 [02:43<00:25,  3.35it/s] 79%|███████▊  | 307/390 [02:43<00:24,  3.35it/s] 79%|███████▉  | 308/390 [02:44<00:24,  3.35it/s] 79%|███████▉  | 309/390 [02:44<00:25,  3.23it/s] 79%|███████▉  | 310/390 [02:44<00:24,  3.26it/s] 80%|███████▉  | 311/390 [02:44<00:24,  3.29it/s] 80%|████████  | 312/390 [02:45<00:23,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 15:17:13,468 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:17:13,468 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:17:13,468 >>   Batch size = 8
{'eval_loss': 0.9832088947296143, 'eval_runtime': 9.8469, 'eval_samples_per_second': 355.239, 'eval_steps_per_second': 44.481, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.57it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.63it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.05it/s][A
  5%|▌         | 22/438 [00:00<00:09, 46.01it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.41it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.01it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.82it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.51it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.66it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.70it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.91it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.87it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.95it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.80it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.78it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.49it/s][A
 20%|█▉        | 87/438 [00:01<00:08, 41.58it/s][A
 21%|██        | 92/438 [00:02<00:08, 40.33it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 42.12it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 42.91it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.50it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.09it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.31it/s][A
 28%|██▊       | 122/438 [00:02<00:09, 33.04it/s][A
 29%|██▉       | 127/438 [00:02<00:08, 36.16it/s][A
 30%|███       | 132/438 [00:03<00:10, 29.69it/s][A
 31%|███       | 136/438 [00:03<00:10, 28.13it/s][A
 32%|███▏      | 141/438 [00:03<00:09, 32.03it/s][A
 33%|███▎      | 146/438 [00:03<00:08, 35.18it/s][A
 34%|███▍      | 151/438 [00:03<00:07, 37.74it/s][A
 36%|███▌      | 156/438 [00:03<00:07, 39.72it/s][A
 37%|███▋      | 161/438 [00:03<00:06, 41.17it/s][A
 38%|███▊      | 166/438 [00:04<00:06, 42.33it/s][A
 39%|███▉      | 171/438 [00:04<00:06, 43.12it/s][A
 40%|████      | 176/438 [00:04<00:06, 43.29it/s][A
 41%|████▏     | 181/438 [00:04<00:05, 43.38it/s][A
 42%|████▏     | 186/438 [00:04<00:05, 43.62it/s][A
 44%|████▎     | 191/438 [00:04<00:05, 43.96it/s][A
 45%|████▍     | 196/438 [00:04<00:05, 44.27it/s][A
 46%|████▌     | 201/438 [00:04<00:05, 44.51it/s][A
 47%|████▋     | 206/438 [00:04<00:05, 43.39it/s][A
 48%|████▊     | 211/438 [00:05<00:05, 43.86it/s][A
 49%|████▉     | 216/438 [00:05<00:05, 40.50it/s][A
 50%|█████     | 221/438 [00:05<00:05, 41.70it/s][A
 52%|█████▏    | 226/438 [00:05<00:04, 42.56it/s][A
 53%|█████▎    | 231/438 [00:05<00:04, 43.17it/s][A
 54%|█████▍    | 236/438 [00:05<00:04, 43.65it/s][A
 55%|█████▌    | 241/438 [00:05<00:04, 43.94it/s][A
 56%|█████▌    | 246/438 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 251/438 [00:05<00:04, 44.41it/s][A
 58%|█████▊    | 256/438 [00:06<00:04, 44.17it/s][A
 60%|█████▉    | 261/438 [00:06<00:03, 44.34it/s][A
 61%|██████    | 266/438 [00:06<00:03, 44.50it/s][A
 62%|██████▏   | 271/438 [00:06<00:03, 44.55it/s][A
 63%|██████▎   | 276/438 [00:06<00:03, 44.71it/s][A
 64%|██████▍   | 281/438 [00:06<00:03, 44.74it/s][A
 65%|██████▌   | 286/438 [00:06<00:03, 44.77it/s][A
 66%|██████▋   | 291/438 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 296/438 [00:07<00:03, 44.63it/s][A
 69%|██████▊   | 301/438 [00:07<00:03, 44.39it/s][A
 70%|██████▉   | 306/438 [00:07<00:02, 44.42it/s][A
 71%|███████   | 311/438 [00:07<00:02, 44.43it/s][A
 72%|███████▏  | 316/438 [00:07<00:02, 44.59it/s][A
 73%|███████▎  | 321/438 [00:07<00:02, 44.67it/s][A
 74%|███████▍  | 326/438 [00:07<00:02, 44.72it/s][A
 76%|███████▌  | 331/438 [00:07<00:02, 44.74it/s][A
 77%|███████▋  | 336/438 [00:07<00:02, 44.78it/s][A
 78%|███████▊  | 341/438 [00:08<00:02, 44.65it/s][A
 79%|███████▉  | 346/438 [00:08<00:02, 44.50it/s][A
 80%|████████  | 351/438 [00:08<00:02, 43.48it/s][A
 81%|████████▏ | 356/438 [00:08<00:01, 43.93it/s][A
 82%|████████▏ | 361/438 [00:08<00:01, 44.18it/s][A
 84%|████████▎ | 366/438 [00:08<00:01, 44.37it/s][A
 85%|████████▍ | 371/438 [00:08<00:01, 44.53it/s][A
 86%|████████▌ | 376/438 [00:08<00:01, 44.59it/s][A
 87%|████████▋ | 381/438 [00:08<00:01, 44.62it/s][A
 88%|████████▊ | 386/438 [00:09<00:01, 44.53it/s][A
 89%|████████▉ | 391/438 [00:09<00:01, 44.27it/s][A
 90%|█████████ | 396/438 [00:09<00:00, 44.38it/s][A
 92%|█████████▏| 401/438 [00:09<00:00, 44.57it/s][A
 93%|█████████▎| 406/438 [00:09<00:00, 44.72it/s][A
 94%|█████████▍| 411/438 [00:09<00:00, 44.76it/s][A
 95%|█████████▍| 416/438 [00:09<00:00, 44.71it/s][A
 96%|█████████▌| 421/438 [00:09<00:00, 44.77it/s][A
 97%|█████████▋| 426/438 [00:09<00:00, 44.62it/s][A
 98%|█████████▊| 431/438 [00:10<00:00, 44.50it/s][A
100%|█████████▉| 436/438 [00:10<00:00, 44.35it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.35it/s][A 80%|████████  | 312/390 [02:55<00:23,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:17:23,798 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 15:17:23,972 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:17:27,452 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:17:27,618 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:17:27,718 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:07<08:43,  6.80s/it] 81%|████████  | 314/390 [03:07<06:09,  4.86s/it] 81%|████████  | 315/390 [03:07<04:21,  3.49s/it] 81%|████████  | 316/390 [03:08<03:07,  2.53s/it] 81%|████████▏ | 317/390 [03:08<02:15,  1.86s/it] 82%|████████▏ | 318/390 [03:08<01:40,  1.39s/it] 82%|████████▏ | 319/390 [03:09<01:15,  1.06s/it] 82%|████████▏ | 320/390 [03:09<00:58,  1.20it/s] 82%|████████▏ | 321/390 [03:09<00:46,  1.49it/s] 83%|████████▎ | 322/390 [03:09<00:38,  1.79it/s] 83%|████████▎ | 323/390 [03:10<00:32,  2.08it/s] 83%|████████▎ | 324/390 [03:10<00:29,  2.27it/s] 83%|████████▎ | 325/390 [03:10<00:25,  2.52it/s] 84%|████████▎ | 326/390 [03:11<00:23,  2.72it/s] 84%|████████▍ | 327/390 [03:11<00:21,  2.89it/s] 84%|████████▍ | 328/390 [03:11<00:20,  3.02it/s] 84%|████████▍ | 329/390 [03:12<00:19,  3.11it/s] 85%|████████▍ | 330/390 [03:12<00:18,  3.19it/s] 85%|████████▍ | 331/390 [03:12<00:18,  3.24it/s] 85%|████████▌ | 332/390 [03:12<00:17,  3.28it/s] 85%|████████▌ | 333/390 [03:13<00:17,  3.30it/s] 86%|████████▌ | 334/390 [03:13<00:17,  3.25it/s] 86%|████████▌ | 335/390 [03:13<00:16,  3.28it/s] 86%|████████▌ | 336/390 [03:14<00:16,  3.31it/s] 86%|████████▋ | 337/390 [03:14<00:15,  3.32it/s] 87%|████████▋ | 338/390 [03:14<00:15,  3.34it/s] 87%|████████▋ | 339/390 [03:15<00:15,  3.35it/s] 87%|████████▋ | 340/390 [03:15<00:14,  3.35it/s] 87%|████████▋ | 341/390 [03:15<00:14,  3.35it/s] 88%|████████▊ | 342/390 [03:15<00:14,  3.36it/s] 88%|████████▊ | 343/390 [03:16<00:14,  3.36it/s] 88%|████████▊ | 344/390 [03:16<00:13,  3.36it/s] 88%|████████▊ | 345/390 [03:16<00:13,  3.26it/s] 89%|████████▊ | 346/390 [03:17<00:13,  3.28it/s] 89%|████████▉ | 347/390 [03:17<00:13,  3.30it/s] 89%|████████▉ | 348/390 [03:17<00:12,  3.31it/s] 89%|████████▉ | 349/390 [03:18<00:12,  3.32it/s] 90%|████████▉ | 350/390 [03:18<00:12,  3.32it/s] 90%|█████████ | 351/390 [03:18<00:11,  3.33it/s] 90%|█████████ | 352/390 [03:18<00:11,  3.33it/s] 91%|█████████ | 353/390 [03:19<00:11,  3.34it/s] 91%|█████████ | 354/390 [03:19<00:10,  3.34it/s] 91%|█████████ | 355/390 [03:19<00:10,  3.19it/s] 91%|█████████▏| 356/390 [03:20<00:10,  3.23it/s] 92%|█████████▏| 357/390 [03:20<00:10,  3.27it/s] 92%|█████████▏| 358/390 [03:20<00:09,  3.29it/s] 92%|█████████▏| 359/390 [03:21<00:09,  3.31it/s] 92%|█████████▏| 360/390 [03:21<00:09,  3.06it/s] 93%|█████████▎| 361/390 [03:21<00:09,  3.14it/s] 93%|█████████▎| 362/390 [03:22<00:08,  3.20it/s] 93%|█████████▎| 363/390 [03:22<00:08,  3.25it/s] 93%|█████████▎| 364/390 [03:22<00:07,  3.28it/s] 94%|█████████▎| 365/390 [03:22<00:07,  3.30it/s] 94%|█████████▍| 366/390 [03:23<00:07,  3.32it/s] 94%|█████████▍| 367/390 [03:23<00:06,  3.33it/s] 94%|█████████▍| 368/390 [03:23<00:06,  3.34it/s] 95%|█████████▍| 369/390 [03:24<00:06,  3.34it/s] 95%|█████████▍| 370/390 [03:24<00:06,  3.19it/s] 95%|█████████▌| 371/390 [03:24<00:05,  3.24it/s] 95%|█████████▌| 372/390 [03:25<00:05,  3.27it/s] 96%|█████████▌| 373/390 [03:25<00:05,  3.30it/s] 96%|█████████▌| 374/390 [03:25<00:04,  3.31it/s] 96%|█████████▌| 375/390 [03:25<00:04,  3.32it/s] 96%|█████████▋| 376/390 [03:26<00:04,  3.33it/s] 97%|█████████▋| 377/390 [03:26<00:03,  3.33it/s] 97%|█████████▋| 378/390 [03:26<00:03,  3.34it/s] 97%|█████████▋| 379/390 [03:27<00:03,  3.34it/s] 97%|█████████▋| 380/390 [03:27<00:03,  3.15it/s] 98%|█████████▊| 381/390 [03:27<00:02,  3.21it/s] 98%|█████████▊| 382/390 [03:28<00:02,  3.25it/s] 98%|█████████▊| 383/390 [03:28<00:02,  3.28it/s] 98%|█████████▊| 384/390 [03:28<00:01,  3.30it/s] 99%|█████████▊| 385/390 [03:29<00:01,  3.31it/s] 99%|█████████▉| 386/390 [03:29<00:01,  3.33it/s] 99%|█████████▉| 387/390 [03:29<00:00,  3.33it/s] 99%|█████████▉| 388/390 [03:29<00:00,  3.34it/s]100%|█████████▉| 389/390 [03:30<00:00,  3.34it/s]100%|██████████| 390/390 [03:30<00:00,  3.04it/s][INFO|trainer.py:2140] 2023-08-28 15:17:58,796 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:17:58,796 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:17:58,796 >>   Batch size = 8
{'eval_loss': 0.9896055459976196, 'eval_runtime': 10.2163, 'eval_samples_per_second': 342.393, 'eval_steps_per_second': 42.873, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.43it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.55it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.40it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.45it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.88it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.18it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.77it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.52it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.55it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.72it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.86it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.02it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.01it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.83it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.67it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.40it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.20it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.37it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.46it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.69it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.83it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.88it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.74it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 40.28it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 41.62it/s][A
 30%|███       | 132/438 [00:02<00:07, 42.55it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.26it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.84it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.17it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.52it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.41it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.20it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.09it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.19it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.26it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.57it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.69it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.86it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.94it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.90it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.63it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.52it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.38it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.40it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.54it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.62it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.76it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.84it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.86it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.77it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 39.44it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 41.03it/s][A
 61%|██████    | 267/438 [00:06<00:04, 42.22it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.95it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.56it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.02it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.27it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.12it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.90it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.14it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.39it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.61it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.74it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.79it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.80it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.65it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.45it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.24it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.36it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.42it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.54it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.74it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.85it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.82it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.80it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.63it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 39.76it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 41.29it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 42.31it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.10it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.63it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.13it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.47it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.30it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.08it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 43.93it/s][A100%|██████████| 390/390 [03:40<00:00,  3.04it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:18:09,269 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 15:18:09,757 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:18:13,550 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:18:13,667 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:18:13,755 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:18:22,725 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:18:22,726 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78 (score: 0.952972412109375).
                                                 100%|██████████| 390/390 [04:07<00:00,  3.04it/s]100%|██████████| 390/390 [04:07<00:00,  1.57it/s]
[INFO|trainer.py:1894] 2023-08-28 15:18:36,278 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 15:18:36,502 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:18:41,018 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:18:41,228 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:18:41,342 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:18:42,142 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   train_loss               =     0.6184
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   train_runtime            = 0:04:07.90
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   train_samples_per_second =    100.844
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:42,142 >>   train_steps_per_second   =      1.573
{'eval_loss': 0.9923932552337646, 'eval_runtime': 9.9319, 'eval_samples_per_second': 352.198, 'eval_steps_per_second': 44.1, 'epoch': 4.99}
{'train_runtime': 247.9086, 'train_samples_per_second': 100.844, 'train_steps_per_second': 1.573, 'train_loss': 0.6183845324394031, 'epoch': 4.99}
08/28/2023 15:18:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:18:42,562 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:18:42,562 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 15:18:42,562 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.98it/s]  3%|▎         | 12/438 [00:00<00:08, 49.46it/s]  4%|▍         | 17/438 [00:00<00:08, 47.72it/s]  5%|▌         | 22/438 [00:00<00:08, 46.72it/s]  6%|▌         | 27/438 [00:00<00:08, 46.17it/s]  7%|▋         | 32/438 [00:00<00:08, 45.93it/s]  8%|▊         | 37/438 [00:00<00:08, 45.66it/s] 10%|▉         | 42/438 [00:00<00:08, 45.38it/s] 11%|█         | 47/438 [00:01<00:08, 44.87it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.71it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.64it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.79it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.91it/s] 16%|█▋        | 72/438 [00:01<00:08, 41.19it/s] 18%|█▊        | 77/438 [00:01<00:08, 42.39it/s] 19%|█▊        | 82/438 [00:01<00:08, 43.24it/s] 20%|█▉        | 87/438 [00:01<00:08, 43.80it/s] 21%|██        | 92/438 [00:02<00:07, 43.99it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.17it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.38it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.51it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.31it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.44it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.64it/s] 29%|██▉       | 127/438 [00:02<00:06, 44.83it/s] 30%|███       | 132/438 [00:02<00:06, 44.93it/s] 31%|███▏      | 137/438 [00:03<00:06, 44.78it/s] 32%|███▏      | 142/438 [00:03<00:06, 44.77it/s] 34%|███▎      | 147/438 [00:03<00:06, 44.72it/s] 35%|███▍      | 152/438 [00:03<00:06, 44.62it/s] 36%|███▌      | 157/438 [00:03<00:06, 44.55it/s] 37%|███▋      | 162/438 [00:03<00:06, 44.61it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.74it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.82it/s] 40%|████      | 177/438 [00:03<00:05, 44.85it/s] 42%|████▏     | 182/438 [00:04<00:05, 44.79it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.77it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.73it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.69it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.61it/s] 47%|████▋     | 207/438 [00:04<00:05, 40.21it/s] 48%|████▊     | 212/438 [00:04<00:05, 41.56it/s] 50%|████▉     | 217/438 [00:04<00:05, 42.62it/s] 51%|█████     | 222/438 [00:04<00:04, 43.37it/s] 52%|█████▏    | 227/438 [00:05<00:04, 43.92it/s] 53%|█████▎    | 232/438 [00:05<00:04, 44.30it/s] 54%|█████▍    | 237/438 [00:05<00:04, 44.51it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.44it/s] 56%|█████▋    | 247/438 [00:05<00:04, 44.16it/s] 58%|█████▊    | 252/438 [00:05<00:04, 44.06it/s] 59%|█████▊    | 257/438 [00:05<00:04, 44.21it/s] 60%|█████▉    | 262/438 [00:05<00:03, 44.44it/s] 61%|██████    | 267/438 [00:06<00:03, 44.73it/s] 62%|██████▏   | 272/438 [00:06<00:03, 44.87it/s] 63%|██████▎   | 277/438 [00:06<00:03, 44.95it/s] 64%|██████▍   | 282/438 [00:06<00:03, 44.95it/s] 66%|██████▌   | 287/438 [00:06<00:03, 44.90it/s] 67%|██████▋   | 292/438 [00:06<00:03, 44.57it/s] 68%|██████▊   | 297/438 [00:06<00:03, 44.36it/s] 69%|██████▉   | 302/438 [00:06<00:03, 44.32it/s] 70%|███████   | 307/438 [00:06<00:02, 44.52it/s] 71%|███████   | 312/438 [00:07<00:02, 44.61it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.84it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.95it/s] 75%|███████▍  | 327/438 [00:07<00:02, 45.07it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.80it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.66it/s] 78%|███████▊  | 342/438 [00:07<00:02, 39.66it/s] 79%|███████▉  | 347/438 [00:07<00:02, 41.23it/s] 80%|████████  | 352/438 [00:07<00:02, 42.29it/s] 82%|████████▏ | 357/438 [00:08<00:01, 43.09it/s] 83%|████████▎ | 362/438 [00:08<00:01, 43.76it/s] 84%|████████▍ | 367/438 [00:08<00:01, 42.80it/s] 85%|████████▍ | 372/438 [00:08<00:01, 43.50it/s] 86%|████████▌ | 377/438 [00:08<00:01, 43.78it/s] 87%|████████▋ | 382/438 [00:08<00:01, 43.65it/s] 88%|████████▊ | 387/438 [00:08<00:01, 43.78it/s] 89%|████████▉ | 392/438 [00:08<00:01, 43.98it/s] 91%|█████████ | 397/438 [00:08<00:00, 44.35it/s] 92%|█████████▏| 402/438 [00:09<00:00, 44.58it/s] 93%|█████████▎| 407/438 [00:09<00:00, 44.56it/s] 94%|█████████▍| 412/438 [00:09<00:00, 44.79it/s] 95%|█████████▌| 417/438 [00:09<00:00, 44.87it/s] 96%|█████████▋| 422/438 [00:09<00:00, 44.77it/s] 97%|█████████▋| 427/438 [00:09<00:00, 44.41it/s] 99%|█████████▊| 432/438 [00:09<00:00, 44.41it/s]100%|█████████▉| 437/438 [00:09<00:00, 44.32it/s]100%|██████████| 438/438 [00:09<00:00, 44.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:18:52,475 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   eval_loss               =      0.953
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   eval_samples_per_second =    352.878
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   eval_steps_per_second   =     44.185
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:18:52,475 >>   perplexity              =     2.5934
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:09,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:10,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:10,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:10,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:10,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:19:11,497 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:19:11,498 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:19:12,238 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:19:13,538 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:19:13,623 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:17,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:17,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:17,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:17,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:19:17,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:19:18,428 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:19:18,429 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:19:19,127 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:19:19,422 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:19:19,423 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.19it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.37it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:08,  1.37it/s]Extractor Predicting: 12it [00:08,  1.36it/s]Extractor Predicting: 13it [00:09,  1.36it/s]Extractor Predicting: 14it [00:10,  1.34it/s]Extractor Predicting: 15it [00:11,  1.33it/s]Extractor Predicting: 16it [00:11,  1.32it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:14,  1.38it/s]Extractor Predicting: 20it [00:14,  1.36it/s]Extractor Predicting: 21it [00:15,  1.34it/s]Extractor Predicting: 22it [00:16,  1.36it/s]Extractor Predicting: 23it [00:17,  1.38it/s]Extractor Predicting: 24it [00:17,  1.38it/s]Extractor Predicting: 25it [00:18,  1.34it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:20,  1.35it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.32it/s]Extractor Predicting: 30it [00:22,  1.27it/s]Extractor Predicting: 31it [00:23,  1.28it/s]Extractor Predicting: 32it [00:24,  1.19it/s]Extractor Predicting: 33it [00:24,  1.22it/s]Extractor Predicting: 34it [00:25,  1.21it/s]Extractor Predicting: 35it [00:26,  1.22it/s]Extractor Predicting: 36it [00:27,  1.24it/s]Extractor Predicting: 37it [00:28,  1.22it/s]Extractor Predicting: 38it [00:29,  1.21it/s]Extractor Predicting: 39it [00:29,  1.21it/s]Extractor Predicting: 40it [00:30,  1.21it/s]Extractor Predicting: 41it [00:31,  1.21it/s]Extractor Predicting: 42it [00:32,  1.19it/s]Extractor Predicting: 43it [00:33,  1.18it/s]Extractor Predicting: 44it [00:34,  1.19it/s]Extractor Predicting: 45it [00:34,  1.20it/s]Extractor Predicting: 46it [00:35,  1.18it/s]Extractor Predicting: 47it [00:36,  1.17it/s]Extractor Predicting: 48it [00:37,  1.19it/s]Extractor Predicting: 49it [00:38,  1.20it/s]Extractor Predicting: 50it [00:39,  1.21it/s]Extractor Predicting: 51it [00:40,  1.16it/s]Extractor Predicting: 52it [00:40,  1.17it/s]Extractor Predicting: 53it [00:41,  1.17it/s]Extractor Predicting: 54it [00:42,  1.22it/s]Extractor Predicting: 55it [00:43,  1.18it/s]Extractor Predicting: 56it [00:44,  1.21it/s]Extractor Predicting: 57it [00:44,  1.21it/s]Extractor Predicting: 58it [00:45,  1.20it/s]Extractor Predicting: 59it [00:46,  1.14it/s]Extractor Predicting: 60it [00:47,  1.22it/s]Extractor Predicting: 61it [00:48,  1.24it/s]Extractor Predicting: 62it [00:49,  1.25it/s]Extractor Predicting: 63it [00:49,  1.23it/s]Extractor Predicting: 64it [00:50,  1.26it/s]Extractor Predicting: 65it [00:51,  1.29it/s]Extractor Predicting: 66it [00:52,  1.28it/s]Extractor Predicting: 67it [00:52,  1.27it/s]Extractor Predicting: 68it [00:53,  1.25it/s]Extractor Predicting: 69it [00:54,  1.24it/s]Extractor Predicting: 70it [00:55,  1.27it/s]Extractor Predicting: 71it [00:56,  1.27it/s]Extractor Predicting: 72it [00:56,  1.29it/s]Extractor Predicting: 73it [00:57,  1.31it/s]Extractor Predicting: 74it [00:58,  1.34it/s]Extractor Predicting: 75it [00:59,  1.35it/s]Extractor Predicting: 76it [00:59,  1.34it/s]Extractor Predicting: 77it [01:00,  1.31it/s]Extractor Predicting: 78it [01:01,  1.33it/s]Extractor Predicting: 79it [01:02,  1.33it/s]Extractor Predicting: 80it [01:02,  1.33it/s]Extractor Predicting: 81it [01:03,  1.28it/s]Extractor Predicting: 82it [01:04,  1.29it/s]Extractor Predicting: 83it [01:05,  1.31it/s]Extractor Predicting: 84it [01:06,  1.30it/s]Extractor Predicting: 85it [01:06,  1.29it/s]Extractor Predicting: 86it [01:07,  1.27it/s]Extractor Predicting: 87it [01:08,  1.28it/s]Extractor Predicting: 88it [01:09,  1.32it/s]Extractor Predicting: 89it [01:09,  1.36it/s]Extractor Predicting: 90it [01:10,  1.35it/s]Extractor Predicting: 91it [01:11,  1.37it/s]Extractor Predicting: 92it [01:11,  1.40it/s]Extractor Predicting: 93it [01:12,  1.41it/s]Extractor Predicting: 94it [01:13,  1.41it/s]Extractor Predicting: 95it [01:14,  1.41it/s]Extractor Predicting: 96it [01:14,  1.38it/s]Extractor Predicting: 97it [01:15,  1.39it/s]Extractor Predicting: 98it [01:16,  1.42it/s]Extractor Predicting: 99it [01:16,  1.39it/s]Extractor Predicting: 100it [01:17,  1.38it/s]Extractor Predicting: 101it [01:18,  1.37it/s]Extractor Predicting: 102it [01:19,  1.38it/s]Extractor Predicting: 103it [01:19,  1.39it/s]Extractor Predicting: 104it [01:20,  1.35it/s]Extractor Predicting: 105it [01:21,  1.34it/s]Extractor Predicting: 106it [01:22,  1.37it/s]Extractor Predicting: 107it [01:22,  1.35it/s]Extractor Predicting: 108it [01:23,  1.37it/s]Extractor Predicting: 109it [01:24,  1.38it/s]Extractor Predicting: 110it [01:25,  1.31it/s]Extractor Predicting: 111it [01:25,  1.34it/s]Extractor Predicting: 112it [01:26,  1.38it/s]Extractor Predicting: 113it [01:27,  1.36it/s]Extractor Predicting: 114it [01:27,  1.35it/s]Extractor Predicting: 115it [01:28,  1.35it/s]Extractor Predicting: 116it [01:29,  1.33it/s]Extractor Predicting: 117it [01:30,  1.31it/s]Extractor Predicting: 118it [01:31,  1.28it/s]Extractor Predicting: 119it [01:31,  1.27it/s]Extractor Predicting: 120it [01:32,  1.26it/s]Extractor Predicting: 121it [01:33,  1.26it/s]Extractor Predicting: 122it [01:34,  1.25it/s]Extractor Predicting: 123it [01:35,  1.24it/s]Extractor Predicting: 124it [01:35,  1.23it/s]Extractor Predicting: 125it [01:36,  1.24it/s]Extractor Predicting: 126it [01:37,  1.19it/s]Extractor Predicting: 127it [01:38,  1.19it/s]Extractor Predicting: 128it [01:39,  1.22it/s]Extractor Predicting: 129it [01:40,  1.24it/s]Extractor Predicting: 130it [01:40,  1.22it/s]Extractor Predicting: 131it [01:41,  1.22it/s]Extractor Predicting: 132it [01:42,  1.23it/s]Extractor Predicting: 133it [01:43,  1.23it/s]Extractor Predicting: 134it [01:44,  1.22it/s]Extractor Predicting: 135it [01:44,  1.23it/s]Extractor Predicting: 136it [01:45,  1.22it/s]Extractor Predicting: 137it [01:46,  1.23it/s]Extractor Predicting: 138it [01:47,  1.22it/s]Extractor Predicting: 139it [01:48,  1.25it/s]Extractor Predicting: 140it [01:48,  1.27it/s]Extractor Predicting: 141it [01:49,  1.25it/s]Extractor Predicting: 142it [01:50,  1.22it/s]Extractor Predicting: 143it [01:51,  1.25it/s]Extractor Predicting: 144it [01:52,  1.24it/s]Extractor Predicting: 145it [01:52,  1.64it/s]Extractor Predicting: 145it [01:52,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:29,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:29,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:29,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:29,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:29,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:21:30,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:21:30,664 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:21:31,332 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:21:32,462 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:21:32,462 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:34,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:34,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:34,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:34,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:34,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:21:35,012 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:21:35,013 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:21:35,337 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:21:35,578 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:21:35,578 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4532650448143406,
  "recall": 0.20240137221269297,
  "score": 0.2798418972332016,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.29it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.32it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.34it/s]Extractor Predicting: 16it [00:12,  1.36it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:13,  1.24it/s]Extractor Predicting: 19it [00:14,  1.24it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.28it/s]Extractor Predicting: 22it [00:17,  1.24it/s]Extractor Predicting: 23it [00:17,  1.27it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:20,  1.29it/s]Extractor Predicting: 27it [00:20,  1.29it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.28it/s]Extractor Predicting: 31it [00:24,  1.28it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.32it/s]Extractor Predicting: 35it [00:27,  1.35it/s]Extractor Predicting: 36it [00:27,  1.31it/s]Extractor Predicting: 37it [00:29,  1.02it/s]Extractor Predicting: 38it [00:30,  1.10it/s]Extractor Predicting: 39it [00:30,  1.15it/s]Extractor Predicting: 40it [00:31,  1.17it/s]Extractor Predicting: 41it [00:32,  1.21it/s]Extractor Predicting: 42it [00:33,  1.26it/s]Extractor Predicting: 43it [00:33,  1.30it/s]Extractor Predicting: 44it [00:34,  1.30it/s]Extractor Predicting: 45it [00:35,  1.32it/s]Extractor Predicting: 46it [00:36,  1.37it/s]Extractor Predicting: 47it [00:36,  1.35it/s]Extractor Predicting: 48it [00:37,  1.34it/s]Extractor Predicting: 49it [00:38,  1.08it/s]Extractor Predicting: 50it [00:39,  1.13it/s]Extractor Predicting: 51it [00:40,  1.21it/s]Extractor Predicting: 52it [00:41,  1.24it/s]Extractor Predicting: 53it [00:41,  1.26it/s]Extractor Predicting: 54it [00:42,  1.31it/s]Extractor Predicting: 55it [00:43,  1.32it/s]Extractor Predicting: 56it [00:44,  1.30it/s]Extractor Predicting: 57it [00:44,  1.31it/s]Extractor Predicting: 58it [00:45,  1.29it/s]Extractor Predicting: 59it [00:46,  1.30it/s]Extractor Predicting: 60it [00:47,  1.31it/s]Extractor Predicting: 61it [00:47,  1.32it/s]Extractor Predicting: 62it [00:48,  1.33it/s]Extractor Predicting: 63it [00:49,  1.30it/s]Extractor Predicting: 64it [00:50,  1.26it/s]Extractor Predicting: 65it [00:51,  1.30it/s]Extractor Predicting: 66it [00:51,  1.30it/s]Extractor Predicting: 67it [00:52,  1.34it/s]Extractor Predicting: 68it [00:53,  1.33it/s]Extractor Predicting: 69it [00:54,  1.31it/s]Extractor Predicting: 70it [00:54,  1.32it/s]Extractor Predicting: 71it [00:55,  1.31it/s]Extractor Predicting: 72it [00:56,  1.28it/s]Extractor Predicting: 73it [00:57,  1.27it/s]Extractor Predicting: 74it [00:57,  1.30it/s]Extractor Predicting: 75it [00:58,  1.30it/s]Extractor Predicting: 76it [00:59,  1.26it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.29it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.33it/s]Extractor Predicting: 81it [01:03,  1.27it/s]Extractor Predicting: 82it [01:04,  1.27it/s]Extractor Predicting: 83it [01:04,  1.30it/s]Extractor Predicting: 84it [01:05,  1.29it/s]Extractor Predicting: 85it [01:06,  1.30it/s]Extractor Predicting: 86it [01:07,  1.31it/s]Extractor Predicting: 87it [01:07,  1.34it/s]Extractor Predicting: 88it [01:08,  1.32it/s]Extractor Predicting: 89it [01:09,  1.34it/s]Extractor Predicting: 90it [01:10,  1.36it/s]Extractor Predicting: 91it [01:10,  1.33it/s]Extractor Predicting: 92it [01:11,  1.31it/s]Extractor Predicting: 93it [01:12,  1.33it/s]Extractor Predicting: 94it [01:13,  1.31it/s]Extractor Predicting: 95it [01:14,  1.32it/s]Extractor Predicting: 96it [01:14,  1.29it/s]Extractor Predicting: 97it [01:15,  1.31it/s]Extractor Predicting: 98it [01:16,  1.30it/s]Extractor Predicting: 99it [01:17,  1.30it/s]Extractor Predicting: 100it [01:17,  1.29it/s]Extractor Predicting: 101it [01:18,  1.29it/s]Extractor Predicting: 102it [01:19,  1.28it/s]Extractor Predicting: 103it [01:20,  1.30it/s]Extractor Predicting: 104it [01:21,  1.28it/s]Extractor Predicting: 105it [01:21,  1.29it/s]Extractor Predicting: 106it [01:22,  1.22it/s]Extractor Predicting: 107it [01:23,  1.24it/s]Extractor Predicting: 108it [01:24,  1.25it/s]Extractor Predicting: 109it [01:25,  1.27it/s]Extractor Predicting: 110it [01:25,  1.27it/s]Extractor Predicting: 111it [01:26,  1.30it/s]Extractor Predicting: 112it [01:27,  1.27it/s]Extractor Predicting: 113it [01:28,  1.28it/s]Extractor Predicting: 114it [01:28,  1.26it/s]Extractor Predicting: 115it [01:29,  1.29it/s]Extractor Predicting: 116it [01:30,  1.26it/s]Extractor Predicting: 117it [01:31,  1.29it/s]Extractor Predicting: 118it [01:31,  1.30it/s]Extractor Predicting: 119it [01:32,  1.31it/s]Extractor Predicting: 120it [01:33,  1.25it/s]Extractor Predicting: 121it [01:34,  1.28it/s]Extractor Predicting: 122it [01:35,  1.30it/s]Extractor Predicting: 123it [01:35,  1.32it/s]Extractor Predicting: 124it [01:36,  1.28it/s]Extractor Predicting: 125it [01:37,  1.30it/s]Extractor Predicting: 126it [01:38,  1.30it/s]Extractor Predicting: 127it [01:38,  1.31it/s]Extractor Predicting: 128it [01:39,  1.32it/s]Extractor Predicting: 129it [01:40,  1.32it/s]Extractor Predicting: 130it [01:41,  1.35it/s]Extractor Predicting: 131it [01:41,  1.38it/s]Extractor Predicting: 132it [01:42,  1.35it/s]Extractor Predicting: 133it [01:43,  1.33it/s]Extractor Predicting: 134it [01:44,  1.32it/s]Extractor Predicting: 135it [01:44,  1.33it/s]Extractor Predicting: 136it [01:45,  1.34it/s]Extractor Predicting: 137it [01:47,  1.05it/s]Extractor Predicting: 138it [01:47,  1.13it/s]Extractor Predicting: 139it [01:48,  1.14it/s]Extractor Predicting: 140it [01:49,  1.21it/s]Extractor Predicting: 140it [01:49,  1.28it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:36,082 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:36,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:36,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:36,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:36,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:23:36,937 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:23:36,938 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:23:37,581 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:23:39,288 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:23:39,331 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:42,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:42,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:42,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:42,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:23:42,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:23:43,632 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:23:43,633 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:23:44,281 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:23:44,583 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:23:44,583 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4279690794096978,
  "recall": 0.18146603098927294,
  "score": 0.2548650345260514,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 3it [00:02,  1.31it/s]
[INFO|configuration_utils.py:515] 2023-08-28 15:23:50,525 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:23:50,571 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:23:50,657 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:23:50,658 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 15:23:50,701 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:24:14,564 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 15:24:14,619 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 15:24:14,987 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:24:14,988 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:24:15,184 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:24:15,317 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42105263157894735,
  "recall": 0.05555555555555555,
  "score": 0.09815950920245398,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 15:24:15,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:16,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:17,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:18,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:19,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:20,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:20,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:21,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:22,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:23,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:24,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:25,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:25,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:26,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:27,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:28,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:29,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:29,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:31,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:31,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:32,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:39, 17.68s/it][WARNING|generation_utils.py:914] 2023-08-28 15:24:33,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:34,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:35,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:36,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:37,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:38,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:39,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:40,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:41,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:41,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:42,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:43,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:44,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:45,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:46,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:46,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:47,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:48,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:49,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:50,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:51,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:36<02:25, 18.16s/it][WARNING|generation_utils.py:914] 2023-08-28 15:24:52,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:52,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:53,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:54,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:55,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:56,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:56,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:57,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:58,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:58,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:24:59,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:00,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:00,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:01,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:02,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:02,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:03,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:04,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:05,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:05,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:06,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:51<01:57, 16.79s/it][WARNING|generation_utils.py:914] 2023-08-28 15:25:07,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:07,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:08,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:09,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:09,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:10,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:11,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:12,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:12,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:13,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:14,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:14,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:15,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:16,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:16,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:17,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:18,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:18,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:19,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:20,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:21,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:06<01:36, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-28 15:25:22,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:22,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:24,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:24,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:25,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:26,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:27,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:27,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:28,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:29,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:29,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:30,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:30,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:31,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:32,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:33,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:33,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:34,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:34,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:35,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:36,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:21<01:18, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-28 15:25:37,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:37,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:38,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:39,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:40,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:40,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:41,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:42,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:43,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:43,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:44,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:45,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:45,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:46,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:47,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:47,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:48,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:49,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:49,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:50,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:51,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:52,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:36<01:02, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-28 15:25:52,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:53,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:54,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:55,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:56,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:56,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:57,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:58,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:25:59,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:00,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:01,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:02,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:03,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:04,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:05,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:06,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:06,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:07,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:08,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:09,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:10,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:11,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:11,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:56<00:51, 17.02s/it][WARNING|generation_utils.py:914] 2023-08-28 15:26:12,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:13,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:13,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:15,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:16,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:16,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:17,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:18,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:19,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:19,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:20,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:21,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:21,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:22,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:23,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:23,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:24,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:25,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:27,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:27,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:28,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:13<00:33, 16.82s/it][WARNING|generation_utils.py:914] 2023-08-28 15:26:29,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:29,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:30,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:30,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:31,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:32,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:32,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:33,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:34,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:34,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:35,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:36,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:37,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:37,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:38,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:39,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:39,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:40,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:41,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:42,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:42,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:27<00:16, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-28 15:26:43,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:44,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:44,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:46,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:47,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:47,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:48,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:49,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:50,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:51,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:52,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:53,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:54,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:54,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:55,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:56,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:57,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:57,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:58,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:59,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:26:59,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:44<00:00, 16.38s/it]Generating: 100%|██████████| 10/10 [02:44<00:00, 16.46s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:06,959 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:06,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:06,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:06,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:06,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:27:07,905 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:27:07,906 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:27:08,545 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:27:09,713 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:27:09,713 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:13,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:13,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:13,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:13,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:27:13,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:27:14,288 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:27:14,289 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:27:14,972 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:27:15,290 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:27:15,290 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : main subject . Context : Later in the year ( 1141–1230 ) he married Brigadier John B. Stoughton , sister of Henry VIII 's fourth wife , Elizabeth of Bohemia , and daughters of King George II of Bohemia , Duke of Warwick and Lady Jane of Warwick ( born 13 March 1913 in Wrexham ) . Head Entity : Elizabeth of Bohemia , Duke of Warwick , Tail Entity : Henry VIII 's fourth wife , Elizabeth of Bohemia .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : main subject .', 'success_rate': 0.9017857142857143, 'errors': {''}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : United States .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : sport .', 'success_rate': 0.8892045454545454, 'errors': {'', 'too many values to unpack (expected 2)', "('2008', 'sport', '', 'In 2008 , the team won their third and final championship .')"}}
['Relation : competition class . Context : Later in the year he won his class - leading second - place match win over a team from the Belgian club Bontrager Bontrager , in a game against Bontrager Rheinmetall . Head Entity : Bontrager Bontrager , Tail Entity : Belgian football .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8806818181818182, 'errors': {'', "('1976 Summer Olympics', 'competition class', '', 'It was beaten by Germany at the 1976 Summer Olympics .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : location .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9017857142857143, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : religion . Context : Later in the year ( 1141–1230 ) he married the fourth of his six sons , John , who married Queen Elizabeth II , the Queen Elizabeth of Great Britain . Head Entity : John , Tail Entity : Christian .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 7846
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7946, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.12it/s]Extractor Estimating: 2it [00:01,  1.28it/s]Extractor Estimating: 3it [00:02,  1.32it/s]Extractor Estimating: 4it [00:03,  1.33it/s]Extractor Estimating: 5it [00:03,  1.34it/s]Extractor Estimating: 6it [00:04,  1.30it/s]Extractor Estimating: 7it [00:05,  1.31it/s]Extractor Estimating: 8it [00:06,  1.33it/s]Extractor Estimating: 9it [00:06,  1.29it/s]Extractor Estimating: 10it [00:07,  1.35it/s]Extractor Estimating: 11it [00:08,  1.30it/s]Extractor Estimating: 12it [00:09,  1.32it/s]Extractor Estimating: 13it [00:10,  1.26it/s]Extractor Estimating: 14it [00:10,  1.30it/s]Extractor Estimating: 15it [00:11,  1.35it/s]Extractor Estimating: 16it [00:12,  1.29it/s]Extractor Estimating: 17it [00:13,  1.26it/s]Extractor Estimating: 18it [00:13,  1.29it/s]Extractor Estimating: 19it [00:14,  1.32it/s]Extractor Estimating: 20it [00:15,  1.32it/s]Extractor Estimating: 21it [00:16,  1.30it/s]Extractor Estimating: 22it [00:16,  1.33it/s]Extractor Estimating: 23it [00:17,  1.38it/s]Extractor Estimating: 24it [00:18,  1.35it/s]Extractor Estimating: 25it [00:19,  1.31it/s]Extractor Estimating: 26it [00:19,  1.30it/s]Extractor Estimating: 27it [00:20,  1.29it/s]Extractor Estimating: 28it [00:21,  1.21it/s]Extractor Estimating: 29it [00:22,  1.17it/s]Extractor Estimating: 30it [00:23,  1.18it/s]Extractor Estimating: 31it [00:24,  1.18it/s]Extractor Estimating: 32it [00:25,  1.19it/s]Extractor Estimating: 33it [00:26,  1.11it/s]Extractor Estimating: 34it [00:26,  1.13it/s]Extractor Estimating: 35it [00:27,  1.18it/s]Extractor Estimating: 36it [00:28,  1.20it/s]Extractor Estimating: 37it [00:29,  1.24it/s]Extractor Estimating: 38it [00:30,  1.23it/s]Extractor Estimating: 39it [00:30,  1.26it/s]Extractor Estimating: 40it [00:31,  1.24it/s]Extractor Estimating: 41it [00:32,  1.19it/s]Extractor Estimating: 42it [00:33,  1.19it/s]Extractor Estimating: 43it [00:34,  1.22it/s]Extractor Estimating: 44it [00:34,  1.24it/s]Extractor Estimating: 45it [00:35,  1.17it/s]Extractor Estimating: 46it [00:36,  1.21it/s]Extractor Estimating: 47it [00:37,  1.21it/s]Extractor Estimating: 48it [00:38,  1.21it/s]Extractor Estimating: 49it [00:39,  1.22it/s]Extractor Estimating: 50it [00:39,  1.25it/s]Extractor Estimating: 51it [00:40,  1.32it/s]Extractor Estimating: 52it [00:41,  1.36it/s]Extractor Estimating: 53it [00:41,  1.36it/s]Extractor Estimating: 54it [00:42,  1.36it/s]Extractor Estimating: 55it [00:43,  1.42it/s]Extractor Estimating: 56it [00:43,  1.47it/s]Extractor Estimating: 57it [00:44,  1.42it/s]Extractor Estimating: 58it [00:45,  1.42it/s]Extractor Estimating: 59it [00:46,  1.45it/s]Extractor Estimating: 60it [00:46,  1.45it/s]Extractor Estimating: 61it [00:47,  1.43it/s]Extractor Estimating: 62it [00:48,  1.41it/s]Extractor Estimating: 63it [00:48,  1.40it/s]Extractor Estimating: 64it [00:49,  1.40it/s]Extractor Estimating: 65it [00:50,  1.39it/s]Extractor Estimating: 66it [00:51,  1.41it/s]Extractor Estimating: 67it [00:51,  1.44it/s]Extractor Estimating: 68it [00:52,  1.41it/s]Extractor Estimating: 69it [00:53,  1.40it/s]Extractor Estimating: 70it [00:53,  1.39it/s]Extractor Estimating: 71it [00:54,  1.43it/s]Extractor Estimating: 72it [00:55,  1.41it/s]Extractor Estimating: 73it [00:56,  1.39it/s]Extractor Estimating: 74it [00:56,  1.39it/s]Extractor Estimating: 75it [00:57,  1.37it/s]Extractor Estimating: 76it [00:58,  1.41it/s]Extractor Estimating: 77it [00:58,  1.44it/s]Extractor Estimating: 78it [00:59,  1.49it/s]Extractor Estimating: 79it [01:00,  1.44it/s]Extractor Estimating: 80it [01:00,  1.44it/s]Extractor Estimating: 81it [01:01,  1.46it/s]Extractor Estimating: 82it [01:02,  1.47it/s]Extractor Estimating: 83it [01:02,  1.48it/s]Extractor Estimating: 84it [01:03,  1.33it/s]Extractor Estimating: 85it [01:04,  1.38it/s]Extractor Estimating: 86it [01:05,  1.40it/s]Extractor Estimating: 87it [01:05,  1.43it/s]Extractor Estimating: 88it [01:06,  1.45it/s]Extractor Estimating: 89it [01:07,  1.45it/s]Extractor Estimating: 90it [01:07,  1.44it/s]Extractor Estimating: 91it [01:08,  1.49it/s]Extractor Estimating: 92it [01:09,  1.52it/s]Extractor Estimating: 93it [01:09,  1.56it/s]Extractor Estimating: 94it [01:10,  1.50it/s]Extractor Estimating: 95it [01:11,  1.48it/s]Extractor Estimating: 96it [01:11,  1.52it/s]Extractor Estimating: 97it [01:12,  1.50it/s]Extractor Estimating: 98it [01:13,  1.48it/s]Extractor Estimating: 99it [01:13,  1.41it/s]Extractor Estimating: 100it [01:14,  1.45it/s]Extractor Estimating: 101it [01:15,  1.53it/s]Extractor Estimating: 102it [01:15,  1.49it/s]Extractor Estimating: 103it [01:16,  1.51it/s]Extractor Estimating: 104it [01:17,  1.48it/s]Extractor Estimating: 105it [01:17,  1.52it/s]Extractor Estimating: 106it [01:18,  1.48it/s]Extractor Estimating: 107it [01:19,  1.54it/s]Extractor Estimating: 108it [01:19,  1.50it/s]Extractor Estimating: 109it [01:20,  1.55it/s]Extractor Estimating: 110it [01:21,  1.58it/s]Extractor Estimating: 111it [01:21,  1.64it/s]Extractor Estimating: 112it [01:22,  1.68it/s]Extractor Estimating: 113it [01:22,  1.70it/s]Extractor Estimating: 114it [01:23,  1.73it/s]Extractor Estimating: 115it [01:23,  1.65it/s]Extractor Estimating: 116it [01:24,  1.66it/s]Extractor Estimating: 117it [01:25,  1.67it/s]Extractor Estimating: 118it [01:25,  1.65it/s]Extractor Estimating: 119it [01:26,  1.63it/s]Extractor Estimating: 120it [01:27,  1.61it/s]Extractor Estimating: 121it [01:27,  1.66it/s]Extractor Estimating: 122it [01:28,  1.67it/s]Extractor Estimating: 123it [01:28,  1.66it/s]Extractor Estimating: 124it [01:29,  1.60it/s]Extractor Estimating: 125it [01:30,  1.61it/s]Extractor Estimating: 126it [01:30,  1.56it/s]Extractor Estimating: 127it [01:31,  1.52it/s]Extractor Estimating: 128it [01:32,  1.50it/s]Extractor Estimating: 129it [01:32,  1.50it/s]Extractor Estimating: 130it [01:33,  1.52it/s]Extractor Estimating: 131it [01:34,  1.54it/s]Extractor Estimating: 132it [01:34,  1.53it/s]Extractor Estimating: 133it [01:35,  1.57it/s]Extractor Estimating: 134it [01:36,  1.49it/s]Extractor Estimating: 135it [01:36,  1.47it/s]Extractor Estimating: 136it [01:37,  1.52it/s]Extractor Estimating: 137it [01:38,  1.53it/s]Extractor Estimating: 138it [01:38,  1.51it/s]Extractor Estimating: 139it [01:39,  1.51it/s]Extractor Estimating: 140it [01:40,  1.54it/s]Extractor Estimating: 141it [01:40,  1.54it/s]Extractor Estimating: 142it [01:41,  1.60it/s]Extractor Estimating: 143it [01:41,  1.61it/s]Extractor Estimating: 144it [01:42,  1.61it/s]Extractor Estimating: 145it [01:43,  1.64it/s]Extractor Estimating: 146it [01:43,  1.60it/s]Extractor Estimating: 147it [01:44,  1.63it/s]Extractor Estimating: 148it [01:45,  1.54it/s]Extractor Estimating: 149it [01:45,  1.55it/s]Extractor Estimating: 150it [01:46,  1.50it/s]Extractor Estimating: 151it [01:47,  1.41it/s]Extractor Estimating: 152it [01:48,  1.32it/s]Extractor Estimating: 153it [01:48,  1.33it/s]Extractor Estimating: 154it [01:49,  1.31it/s]Extractor Estimating: 155it [01:50,  1.37it/s]Extractor Estimating: 156it [01:51,  1.33it/s]Extractor Estimating: 157it [01:51,  1.35it/s]Extractor Estimating: 158it [01:52,  1.35it/s]Extractor Estimating: 159it [01:53,  1.36it/s]Extractor Estimating: 160it [01:53,  1.37it/s]Extractor Estimating: 161it [01:54,  1.32it/s]Extractor Estimating: 162it [01:55,  1.35it/s]Extractor Estimating: 163it [01:56,  1.36it/s]Extractor Estimating: 164it [01:56,  1.36it/s]Extractor Estimating: 165it [01:57,  1.34it/s]Extractor Estimating: 166it [01:58,  1.31it/s]Extractor Estimating: 167it [01:59,  1.30it/s]Extractor Estimating: 168it [02:00,  1.21it/s]Extractor Estimating: 169it [02:01,  1.23it/s]Extractor Estimating: 170it [02:01,  1.24it/s]Extractor Estimating: 171it [02:02,  1.28it/s]Extractor Estimating: 172it [02:03,  1.32it/s]Extractor Estimating: 173it [02:03,  1.33it/s]Extractor Estimating: 174it [02:04,  1.33it/s]Extractor Estimating: 175it [02:05,  1.27it/s]Extractor Estimating: 176it [02:06,  1.36it/s]Extractor Estimating: 177it [02:06,  1.44it/s]Extractor Estimating: 178it [02:07,  1.55it/s]Extractor Estimating: 179it [02:08,  1.37it/s]Extractor Estimating: 180it [02:08,  1.44it/s]Extractor Estimating: 181it [02:09,  1.45it/s]Extractor Estimating: 182it [02:10,  1.44it/s]Extractor Estimating: 183it [02:11,  1.43it/s]Extractor Estimating: 184it [02:11,  1.48it/s]Extractor Estimating: 185it [02:12,  1.51it/s]Extractor Estimating: 186it [02:12,  1.50it/s]Extractor Estimating: 187it [02:13,  1.55it/s]Extractor Estimating: 188it [02:14,  1.59it/s]Extractor Estimating: 189it [02:14,  1.55it/s]Extractor Estimating: 190it [02:15,  1.50it/s]Extractor Estimating: 191it [02:16,  1.61it/s]Extractor Estimating: 192it [02:16,  1.64it/s]Extractor Estimating: 193it [02:17,  1.67it/s]Extractor Estimating: 194it [02:17,  1.68it/s]Extractor Estimating: 195it [02:18,  1.67it/s]Extractor Estimating: 196it [02:19,  1.58it/s]Extractor Estimating: 197it [02:19,  1.60it/s]Extractor Estimating: 198it [02:20,  1.59it/s]Extractor Estimating: 199it [02:20,  1.63it/s]Extractor Estimating: 200it [02:21,  1.47it/s]Extractor Estimating: 201it [02:22,  1.52it/s]Extractor Estimating: 202it [02:22,  1.55it/s]Extractor Estimating: 203it [02:23,  1.56it/s]Extractor Estimating: 204it [02:24,  1.55it/s]Extractor Estimating: 205it [02:24,  1.54it/s]Extractor Estimating: 206it [02:25,  1.51it/s]Extractor Estimating: 207it [02:26,  1.55it/s]Extractor Estimating: 208it [02:26,  1.62it/s]Extractor Estimating: 209it [02:27,  1.52it/s]Extractor Estimating: 210it [02:28,  1.49it/s]Extractor Estimating: 211it [02:28,  1.49it/s]Extractor Estimating: 212it [02:29,  1.52it/s]Extractor Estimating: 213it [02:30,  1.52it/s]Extractor Estimating: 214it [02:30,  1.49it/s]Extractor Estimating: 215it [02:31,  1.43it/s]Extractor Estimating: 216it [02:32,  1.50it/s]Extractor Estimating: 217it [02:32,  1.55it/s]Extractor Estimating: 218it [02:33,  1.54it/s]Extractor Estimating: 219it [02:34,  1.51it/s]Extractor Estimating: 220it [02:34,  1.45it/s]Extractor Estimating: 221it [02:35,  1.40it/s]Extractor Estimating: 222it [02:36,  1.43it/s]Extractor Estimating: 223it [02:36,  1.48it/s]Extractor Estimating: 224it [02:37,  1.49it/s]Extractor Estimating: 225it [02:38,  1.45it/s]Extractor Estimating: 226it [02:39,  1.45it/s]Extractor Estimating: 227it [02:39,  1.43it/s]Extractor Estimating: 228it [02:40,  1.46it/s]Extractor Estimating: 229it [02:41,  1.45it/s]Extractor Estimating: 230it [02:42,  1.35it/s]Extractor Estimating: 231it [02:42,  1.40it/s]Extractor Estimating: 232it [02:43,  1.46it/s]Extractor Estimating: 233it [02:44,  1.35it/s]Extractor Estimating: 234it [02:44,  1.34it/s]Extractor Estimating: 235it [02:45,  1.41it/s]Extractor Estimating: 236it [02:46,  1.41it/s]Extractor Estimating: 237it [02:46,  1.43it/s]Extractor Estimating: 238it [02:47,  1.40it/s]Extractor Estimating: 239it [02:48,  1.33it/s]Extractor Estimating: 240it [02:49,  1.39it/s]Extractor Estimating: 241it [02:49,  1.40it/s]Extractor Estimating: 242it [02:50,  1.42it/s]Extractor Estimating: 243it [02:51,  1.48it/s]Extractor Estimating: 244it [02:51,  1.48it/s]Extractor Estimating: 245it [02:52,  1.46it/s]Extractor Estimating: 246it [02:53,  1.44it/s]Extractor Estimating: 247it [02:53,  1.43it/s]Extractor Estimating: 248it [02:54,  1.43it/s]Extractor Estimating: 249it [02:55,  1.41it/s]Extractor Estimating: 250it [02:55,  1.63it/s]Extractor Estimating: 250it [02:55,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:30,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:30,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:30,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:30,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:30,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:30:31,681 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:30:31,682 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:30:32,301 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:30:33,424 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:30:33,425 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:36,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:36,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:36,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:36,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:30:36,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:30:37,140 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:30:37,141 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:30:37,771 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:30:38,009 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:30:38,009 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:08:55,600 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:08:55,937 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4984 mean pseudo reward: 0.9541691701468226
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 17080
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17180, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17180, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.151, loss:587.3586
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.154, loss:512.8720
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.129, loss:450.0441
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.140, loss:482.6534
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.144, loss:434.8162
>> valid entity prec:0.5744, rec:0.5819, f1:0.5781
>> valid relation prec:0.3142, rec:0.1772, f1:0.2266
>> valid relation with NER prec:0.3142, rec:0.1772, f1:0.2266
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.670, loss:457.9274
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.156, loss:407.6308
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.134, loss:421.2362
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.141, loss:430.9676
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.143, loss:425.1468
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5708, rec:0.5579, f1:0.5643
>> valid relation prec:0.2892, rec:0.1446, f1:0.1928
>> valid relation with NER prec:0.2892, rec:0.1446, f1:0.1928
g_step 1100, step 60, avg_time 2.644, loss:410.1558
g_step 1200, step 160, avg_time 1.132, loss:419.6815
g_step 1300, step 52, avg_time 1.136, loss:402.2544
g_step 1400, step 152, avg_time 1.133, loss:416.8199
g_step 1500, step 44, avg_time 1.142, loss:393.0294
>> valid entity prec:0.5669, rec:0.5396, f1:0.5529
>> valid relation prec:0.2796, rec:0.1506, f1:0.1958
>> valid relation with NER prec:0.2796, rec:0.1506, f1:0.1958
g_step 1600, step 144, avg_time 2.651, loss:381.4133
g_step 1700, step 36, avg_time 1.147, loss:363.2707
g_step 1800, step 136, avg_time 1.137, loss:369.7815
g_step 1900, step 28, avg_time 1.139, loss:337.1781
g_step 2000, step 128, avg_time 1.144, loss:327.3007
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5522, rec:0.4948, f1:0.5219
>> valid relation prec:0.2834, rec:0.1371, f1:0.1848
>> valid relation with NER prec:0.2834, rec:0.1371, f1:0.1848
g_step 2100, step 20, avg_time 2.662, loss:352.0511
g_step 2200, step 120, avg_time 1.122, loss:320.0754
g_step 2300, step 12, avg_time 1.161, loss:326.4684
g_step 2400, step 112, avg_time 1.126, loss:313.5464
g_step 2500, step 4, avg_time 1.149, loss:303.4072
>> valid entity prec:0.5333, rec:0.5810, f1:0.5562
>> valid relation prec:0.2832, rec:0.1618, f1:0.2059
>> valid relation with NER prec:0.2832, rec:0.1618, f1:0.2059
g_step 2600, step 104, avg_time 2.627, loss:285.9150
g_step 2700, step 204, avg_time 1.110, loss:314.8320
g_step 2800, step 96, avg_time 1.093, loss:273.3094
g_step 2900, step 196, avg_time 1.114, loss:312.4715
g_step 3000, step 88, avg_time 1.131, loss:268.1619
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5411, rec:0.5914, f1:0.5651
>> valid relation prec:0.2544, rec:0.1623, f1:0.1982
>> valid relation with NER prec:0.2544, rec:0.1623, f1:0.1982
g_step 3100, step 188, avg_time 2.569, loss:279.9171
g_step 3200, step 80, avg_time 1.096, loss:257.2056
g_step 3300, step 180, avg_time 1.106, loss:285.6907
g_step 3400, step 72, avg_time 1.103, loss:257.4757
g_step 3500, step 172, avg_time 1.115, loss:265.1876
>> valid entity prec:0.5451, rec:0.5204, f1:0.5325
>> valid relation prec:0.3062, rec:0.1583, f1:0.2087
>> valid relation with NER prec:0.3062, rec:0.1583, f1:0.2087
g_step 3600, step 64, avg_time 2.574, loss:242.6425
g_step 3700, step 164, avg_time 1.104, loss:255.0346
g_step 3800, step 56, avg_time 1.126, loss:250.2961
g_step 3900, step 156, avg_time 1.095, loss:252.7266
g_step 4000, step 48, avg_time 1.110, loss:228.7161
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5766, rec:0.5521, f1:0.5641
>> valid relation prec:0.3007, rec:0.1589, f1:0.2079
>> valid relation with NER prec:0.3007, rec:0.1589, f1:0.2079
g_step 4100, step 148, avg_time 2.584, loss:231.3817
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:08:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:08:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-08-55_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:08:57 - WARNING - datasets.builder -   Using custom data configuration default-890ca102e93b88f7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-890ca102e93b88f7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:09:03,987 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:09:04,047 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:09:04,047 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:09:04,048 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:09:04,191 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,287 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,287 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,287 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,287 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,287 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:09:04,288 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:09:04,906 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:09:08,139 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:09:08,279 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-890ca102e93b88f7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.78ba/s] 40%|████      | 2/5 [00:00<00:01,  2.89ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.68ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.22ba/s]100%|██████████| 5/5 [00:01<00:00,  4.56ba/s]100%|██████████| 5/5 [00:01<00:00,  3.81ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.52ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.22ba/s]100%|██████████| 4/4 [00:01<00:00,  3.18ba/s]100%|██████████| 4/4 [00:01<00:00,  2.26ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.84ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.75ba/s]100%|██████████| 5/5 [00:00<00:00,  6.71ba/s]100%|██████████| 5/5 [00:00<00:00,  5.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.89ba/s] 50%|█████     | 2/4 [00:00<00:00,  5.63ba/s]100%|██████████| 4/4 [00:00<00:00,  8.90ba/s]100%|██████████| 4/4 [00:00<00:00,  7.60ba/s]
[INFO|trainer.py:414] 2023-08-28 17:09:18,438 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:09:18,562 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:09:18,562 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 17:09:18,562 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:09:18,562 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:09:18,562 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:09:18,563 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:09:18,563 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:02<17:20,  2.68s/it]  1%|          | 2/390 [00:04<13:37,  2.11s/it]  1%|          | 3/390 [00:04<08:46,  1.36s/it]  1%|          | 4/390 [00:05<07:25,  1.15s/it]  1%|▏         | 5/390 [00:06<06:22,  1.01it/s]  2%|▏         | 6/390 [00:06<05:00,  1.28it/s]  2%|▏         | 7/390 [00:07<04:30,  1.42it/s]  2%|▏         | 8/390 [00:07<03:54,  1.63it/s]  2%|▏         | 9/390 [00:08<03:16,  1.94it/s]  3%|▎         | 10/390 [00:08<02:50,  2.23it/s]  3%|▎         | 11/390 [00:08<02:32,  2.49it/s]  3%|▎         | 12/390 [00:08<02:20,  2.70it/s]  3%|▎         | 13/390 [00:09<02:11,  2.87it/s]  4%|▎         | 14/390 [00:09<02:05,  3.00it/s]  4%|▍         | 15/390 [00:09<02:00,  3.10it/s]  4%|▍         | 16/390 [00:10<01:57,  3.18it/s]  4%|▍         | 17/390 [00:10<01:55,  3.23it/s]  5%|▍         | 18/390 [00:11<02:49,  2.20it/s]  5%|▍         | 19/390 [00:11<02:31,  2.45it/s]  5%|▌         | 20/390 [00:11<02:18,  2.66it/s]  5%|▌         | 21/390 [00:12<02:10,  2.84it/s]  6%|▌         | 22/390 [00:12<02:03,  2.97it/s]  6%|▌         | 23/390 [00:12<01:59,  3.08it/s]  6%|▌         | 24/390 [00:13<01:56,  3.15it/s]  6%|▋         | 25/390 [00:13<01:53,  3.21it/s]  7%|▋         | 26/390 [00:13<01:51,  3.26it/s]  7%|▋         | 27/390 [00:14<02:11,  2.77it/s]  7%|▋         | 28/390 [00:14<02:03,  2.92it/s]  7%|▋         | 29/390 [00:14<01:58,  3.04it/s]  8%|▊         | 30/390 [00:14<01:55,  3.13it/s]  8%|▊         | 31/390 [00:15<01:52,  3.19it/s]  8%|▊         | 32/390 [00:15<01:50,  3.24it/s]  8%|▊         | 33/390 [00:15<01:48,  3.28it/s]  9%|▊         | 34/390 [00:16<01:47,  3.30it/s]  9%|▉         | 35/390 [00:16<01:47,  3.31it/s]  9%|▉         | 36/390 [00:16<01:46,  3.32it/s]  9%|▉         | 37/390 [00:17<01:56,  3.02it/s] 10%|▉         | 38/390 [00:17<01:53,  3.11it/s] 10%|█         | 39/390 [00:17<01:50,  3.18it/s] 10%|█         | 40/390 [00:18<01:48,  3.23it/s] 11%|█         | 41/390 [00:18<01:46,  3.28it/s] 11%|█         | 42/390 [00:18<01:44,  3.33it/s] 11%|█         | 43/390 [00:18<01:43,  3.37it/s] 11%|█▏        | 44/390 [00:19<01:42,  3.39it/s] 12%|█▏        | 45/390 [00:19<01:41,  3.41it/s] 12%|█▏        | 46/390 [00:19<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:20<02:12,  2.59it/s] 12%|█▏        | 48/390 [00:20<02:02,  2.80it/s] 13%|█▎        | 49/390 [00:20<01:54,  2.97it/s] 13%|█▎        | 50/390 [00:21<01:49,  3.10it/s] 13%|█▎        | 51/390 [00:21<01:45,  3.20it/s] 13%|█▎        | 52/390 [00:21<01:43,  3.27it/s] 14%|█▎        | 53/390 [00:22<01:41,  3.32it/s] 14%|█▍        | 54/390 [00:22<01:39,  3.36it/s] 14%|█▍        | 55/390 [00:22<01:38,  3.39it/s] 14%|█▍        | 56/390 [00:23<01:38,  3.40it/s] 15%|█▍        | 57/390 [00:23<01:56,  2.86it/s] 15%|█▍        | 58/390 [00:23<01:50,  3.02it/s] 15%|█▌        | 59/390 [00:24<01:45,  3.14it/s] 15%|█▌        | 60/390 [00:24<01:42,  3.23it/s] 16%|█▌        | 61/390 [00:24<01:39,  3.30it/s] 16%|█▌        | 62/390 [00:24<01:37,  3.35it/s] 16%|█▌        | 63/390 [00:25<01:36,  3.38it/s] 16%|█▋        | 64/390 [00:25<01:35,  3.40it/s] 17%|█▋        | 65/390 [00:25<01:34,  3.42it/s] 17%|█▋        | 66/390 [00:26<01:34,  3.43it/s] 17%|█▋        | 67/390 [00:27<03:05,  1.74it/s] 17%|█▋        | 68/390 [00:27<02:37,  2.05it/s] 18%|█▊        | 69/390 [00:27<02:17,  2.33it/s] 18%|█▊        | 70/390 [00:28<02:03,  2.58it/s] 18%|█▊        | 71/390 [00:28<01:53,  2.80it/s] 18%|█▊        | 72/390 [00:28<01:47,  2.97it/s] 19%|█▊        | 73/390 [00:29<01:42,  3.10it/s] 19%|█▉        | 74/390 [00:29<01:38,  3.20it/s] 19%|█▉        | 75/390 [00:30<02:14,  2.33it/s] 19%|█▉        | 76/390 [00:30<02:01,  2.59it/s] 20%|█▉        | 77/390 [00:30<01:52,  2.79it/s] 20%|██        | 78/390 [00:30<01:45,  2.96it/s][INFO|trainer.py:2140] 2023-08-28 17:09:49,589 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:09:49,589 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:09:49,589 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.43it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.45it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.60it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.09it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.66it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.31it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.76it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.48it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 58/438 [00:01<00:11, 34.32it/s][A
 14%|█▍        | 62/438 [00:01<00:12, 30.64it/s][A
 15%|█▌        | 67/438 [00:01<00:10, 34.11it/s][A
 16%|█▋        | 72/438 [00:01<00:09, 36.97it/s][A
 18%|█▊        | 77/438 [00:01<00:09, 39.18it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 40.77it/s][A
 20%|█▉        | 87/438 [00:02<00:08, 41.97it/s][A
 21%|██        | 92/438 [00:02<00:08, 42.88it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.45it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.41it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.39it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.65it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.86it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.33it/s][A
 29%|██▉       | 127/438 [00:03<00:06, 44.54it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.76it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.87it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.74it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.54it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.20it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.12it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.28it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.47it/s][A
 39%|███▉      | 172/438 [00:04<00:06, 38.00it/s][A
 40%|████      | 177/438 [00:04<00:06, 40.03it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 41.42it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 42.51it/s][A
 44%|████▍     | 192/438 [00:04<00:10, 24.47it/s][A
 45%|████▍     | 197/438 [00:04<00:08, 28.41it/s][A
 46%|████▌     | 202/438 [00:05<00:07, 31.95it/s][A
 47%|████▋     | 207/438 [00:05<00:06, 34.96it/s][A
 48%|████▊     | 212/438 [00:05<00:06, 37.52it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 39.53it/s][A
 51%|█████     | 222/438 [00:05<00:05, 41.15it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 42.07it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 42.50it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 42.93it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.45it/s][A
 56%|█████▋    | 247/438 [00:07<00:16, 11.57it/s][A
 58%|█████▊    | 252/438 [00:07<00:12, 14.97it/s][A
 59%|█████▊    | 257/438 [00:07<00:09, 18.75it/s][A
 60%|█████▉    | 262/438 [00:07<00:07, 22.72it/s][A
 61%|██████    | 267/438 [00:07<00:06, 26.69it/s][A
 62%|██████▏   | 272/438 [00:07<00:05, 30.39it/s][A
 63%|██████▎   | 277/438 [00:07<00:04, 33.70it/s][A
 64%|██████▍   | 282/438 [00:07<00:04, 36.47it/s][A
 66%|██████▌   | 287/438 [00:07<00:03, 38.63it/s][A
 67%|██████▋   | 292/438 [00:08<00:03, 39.93it/s][A
 68%|██████▊   | 297/438 [00:08<00:03, 41.04it/s][A
 69%|██████▉   | 302/438 [00:08<00:03, 41.94it/s][A
 70%|███████   | 307/438 [00:08<00:03, 42.74it/s][A
 71%|███████   | 312/438 [00:08<00:02, 43.45it/s][A
 72%|███████▏  | 317/438 [00:08<00:02, 43.92it/s][A
 74%|███████▎  | 322/438 [00:08<00:02, 44.30it/s][A
 75%|███████▍  | 327/438 [00:08<00:02, 44.51it/s][A
 76%|███████▌  | 332/438 [00:08<00:02, 44.38it/s][A
 77%|███████▋  | 337/438 [00:09<00:02, 44.29it/s][A
 78%|███████▊  | 342/438 [00:09<00:02, 44.12it/s][A
 79%|███████▉  | 347/438 [00:09<00:02, 44.13it/s][A
 80%|████████  | 352/438 [00:09<00:01, 44.32it/s][A
 82%|████████▏ | 357/438 [00:09<00:01, 44.54it/s][A
 83%|████████▎ | 362/438 [00:09<00:01, 44.67it/s][A
 84%|████████▍ | 367/438 [00:09<00:01, 44.84it/s][A
 85%|████████▍ | 372/438 [00:09<00:01, 44.85it/s][A
 86%|████████▌ | 377/438 [00:10<00:01, 44.78it/s][A
 87%|████████▋ | 382/438 [00:10<00:01, 44.56it/s][A
 88%|████████▊ | 387/438 [00:10<00:01, 44.42it/s][A
 89%|████████▉ | 392/438 [00:10<00:01, 44.40it/s][A
 91%|█████████ | 397/438 [00:10<00:01, 26.42it/s][A
 92%|█████████▏| 402/438 [00:10<00:01, 30.22it/s][A
 93%|█████████▎| 407/438 [00:10<00:00, 33.54it/s][A
 94%|█████████▍| 412/438 [00:11<00:00, 36.30it/s][A
 95%|█████████▌| 417/438 [00:11<00:00, 38.44it/s][A
 96%|█████████▋| 422/438 [00:11<00:00, 40.26it/s][A
 97%|█████████▋| 427/438 [00:11<00:00, 41.63it/s][A
 99%|█████████▊| 432/438 [00:11<00:00, 42.40it/s][A
100%|█████████▉| 437/438 [00:11<00:00, 42.77it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:11<00:00, 42.77it/s][A 20%|██        | 78/390 [00:42<01:45,  2.96it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:10:03,025 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 17:10:03,411 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:10:13,858 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:10:14,450 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:10:14,524 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:15<1:11:18, 13.76s/it] 21%|██        | 80/390 [01:16<50:20,  9.74s/it]   21%|██        | 81/390 [01:16<35:34,  6.91s/it] 21%|██        | 82/390 [01:16<25:17,  4.93s/it] 21%|██▏       | 83/390 [01:17<18:05,  3.54s/it] 22%|██▏       | 84/390 [01:17<13:04,  2.56s/it] 22%|██▏       | 85/390 [01:17<09:34,  1.88s/it] 22%|██▏       | 86/390 [01:18<07:08,  1.41s/it] 22%|██▏       | 87/390 [01:18<05:25,  1.08s/it] 23%|██▎       | 88/390 [01:18<04:14,  1.19it/s] 23%|██▎       | 89/390 [01:19<03:24,  1.47it/s] 23%|██▎       | 90/390 [01:19<03:03,  1.64it/s] 23%|██▎       | 91/390 [01:19<02:34,  1.94it/s] 24%|██▎       | 92/390 [01:20<02:14,  2.22it/s] 24%|██▍       | 93/390 [01:20<02:00,  2.47it/s] 24%|██▍       | 94/390 [01:20<01:50,  2.68it/s] 24%|██▍       | 95/390 [01:20<01:43,  2.85it/s] 25%|██▍       | 96/390 [01:21<01:38,  2.99it/s] 25%|██▍       | 97/390 [01:21<01:34,  3.09it/s] 25%|██▌       | 98/390 [01:21<01:32,  3.16it/s] 25%|██▌       | 99/390 [01:22<01:30,  3.22it/s] 26%|██▌       | 100/390 [01:22<01:49,  2.65it/s] 26%|██▌       | 101/390 [01:23<01:42,  2.83it/s] 26%|██▌       | 102/390 [01:23<01:37,  2.97it/s] 26%|██▋       | 103/390 [01:23<01:33,  3.07it/s] 27%|██▋       | 104/390 [01:23<01:30,  3.15it/s] 27%|██▋       | 105/390 [01:24<01:28,  3.21it/s] 27%|██▋       | 106/390 [01:24<01:27,  3.25it/s] 27%|██▋       | 107/390 [01:24<01:26,  3.28it/s] 28%|██▊       | 108/390 [01:25<01:25,  3.30it/s] 28%|██▊       | 109/390 [01:25<01:24,  3.31it/s] 28%|██▊       | 110/390 [01:26<02:07,  2.20it/s] 28%|██▊       | 111/390 [01:26<01:53,  2.45it/s] 29%|██▊       | 112/390 [01:26<01:44,  2.67it/s] 29%|██▉       | 113/390 [01:27<01:37,  2.84it/s] 29%|██▉       | 114/390 [01:27<01:32,  2.98it/s] 29%|██▉       | 115/390 [01:27<01:29,  3.08it/s] 30%|██▉       | 116/390 [01:28<01:26,  3.16it/s] 30%|███       | 117/390 [01:28<01:24,  3.21it/s] 30%|███       | 118/390 [01:28<01:23,  3.26it/s] 31%|███       | 119/390 [01:29<01:55,  2.35it/s] 31%|███       | 120/390 [01:29<01:44,  2.58it/s] 31%|███       | 121/390 [01:29<01:37,  2.77it/s] 31%|███▏      | 122/390 [01:30<01:31,  2.92it/s] 32%|███▏      | 123/390 [01:30<01:27,  3.04it/s] 32%|███▏      | 124/390 [01:30<01:24,  3.13it/s] 32%|███▏      | 125/390 [01:31<01:22,  3.19it/s] 32%|███▏      | 126/390 [01:31<01:21,  3.24it/s] 33%|███▎      | 127/390 [01:31<01:20,  3.27it/s] 33%|███▎      | 128/390 [01:32<01:37,  2.68it/s] 33%|███▎      | 129/390 [01:32<01:31,  2.86it/s] 33%|███▎      | 130/390 [01:32<01:26,  2.99it/s] 34%|███▎      | 131/390 [01:33<01:23,  3.09it/s] 34%|███▍      | 132/390 [01:33<01:21,  3.17it/s] 34%|███▍      | 133/390 [01:33<01:19,  3.22it/s] 34%|███▍      | 134/390 [01:34<01:18,  3.26it/s] 35%|███▍      | 135/390 [01:34<01:17,  3.28it/s] 35%|███▍      | 136/390 [01:34<01:16,  3.30it/s] 35%|███▌      | 137/390 [01:34<01:16,  3.32it/s] 35%|███▌      | 138/390 [01:35<01:42,  2.45it/s] 36%|███▌      | 139/390 [01:35<01:34,  2.67it/s] 36%|███▌      | 140/390 [01:36<01:27,  2.84it/s] 36%|███▌      | 141/390 [01:36<01:23,  2.98it/s] 36%|███▋      | 142/390 [01:36<01:20,  3.08it/s] 37%|███▋      | 143/390 [01:37<01:18,  3.16it/s] 37%|███▋      | 144/390 [01:38<03:08,  1.31it/s] 37%|███▋      | 145/390 [01:39<02:43,  1.50it/s] 37%|███▋      | 146/390 [01:39<02:15,  1.80it/s] 38%|███▊      | 147/390 [01:39<01:56,  2.09it/s] 38%|███▊      | 148/390 [01:40<02:02,  1.98it/s] 38%|███▊      | 149/390 [01:40<01:46,  2.26it/s] 38%|███▊      | 150/390 [01:41<01:35,  2.50it/s] 39%|███▊      | 151/390 [01:41<01:28,  2.71it/s] 39%|███▉      | 152/390 [01:41<01:22,  2.87it/s] 39%|███▉      | 153/390 [01:41<01:18,  3.00it/s] 39%|███▉      | 154/390 [01:42<01:16,  3.10it/s] 40%|███▉      | 155/390 [01:42<01:14,  3.17it/s] 40%|████      | 156/390 [01:42<01:12,  3.22it/s][INFO|trainer.py:2140] 2023-08-28 17:11:01,494 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:11:01,494 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:11:01,494 >>   Batch size = 8
{'eval_loss': 0.9780838489532471, 'eval_runtime': 11.6437, 'eval_samples_per_second': 300.421, 'eval_steps_per_second': 37.617, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.79it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.26it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.53it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.98it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.74it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.48it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.55it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.79it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.79it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.90it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.95it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.84it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.41it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.47it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.70it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.79it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.83it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.88it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.92it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.64it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 41.25it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 42.32it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.15it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.69it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.19it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.36it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.58it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.48it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.24it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.29it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.65it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.82it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.87it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.94it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.68it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.30it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.33it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.44it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.55it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.75it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.79it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.88it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.80it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.58it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.48it/s][A
 62%|██████▏   | 272/438 [00:06<00:05, 30.40it/s][A
 63%|██████▎   | 277/438 [00:06<00:04, 33.69it/s][A
 64%|██████▍   | 282/438 [00:06<00:04, 36.51it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 38.70it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 40.37it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 41.65it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 42.67it/s][A
 70%|███████   | 307/438 [00:07<00:03, 43.22it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.32it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 43.25it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.69it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.14it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.60it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.74it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.85it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.50it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.32it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.24it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.32it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.49it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.68it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.80it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.89it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.81it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.65it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 37.90it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 39.83it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 41.21it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 42.24it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.14it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.74it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.10it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.31it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.31it/s][A 40%|████      | 156/390 [01:52<01:12,  3.22it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:11:12,250 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:11:12,802 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:11:21,531 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:11:21,765 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:11:21,856 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:26<52:09, 13.43s/it] 41%|████      | 158/390 [02:27<36:47,  9.51s/it] 41%|████      | 159/390 [02:27<25:59,  6.75s/it] 41%|████      | 160/390 [02:27<18:27,  4.81s/it] 41%|████▏     | 161/390 [02:28<13:12,  3.46s/it] 42%|████▏     | 162/390 [02:28<09:32,  2.51s/it] 42%|████▏     | 163/390 [02:28<06:59,  1.85s/it] 42%|████▏     | 164/390 [02:29<05:12,  1.38s/it] 42%|████▏     | 165/390 [02:29<03:57,  1.06s/it] 43%|████▎     | 166/390 [02:29<03:05,  1.21it/s] 43%|████▎     | 167/390 [02:29<02:29,  1.49it/s] 43%|████▎     | 168/390 [02:30<02:09,  1.71it/s] 43%|████▎     | 169/390 [02:30<01:50,  2.01it/s] 44%|████▎     | 170/390 [02:30<01:36,  2.28it/s] 44%|████▍     | 171/390 [02:31<01:26,  2.52it/s] 44%|████▍     | 172/390 [02:31<01:19,  2.73it/s] 44%|████▍     | 173/390 [02:31<01:15,  2.89it/s] 45%|████▍     | 174/390 [02:32<01:11,  3.01it/s] 45%|████▍     | 175/390 [02:32<01:09,  3.11it/s] 45%|████▌     | 176/390 [02:32<01:07,  3.18it/s] 45%|████▌     | 177/390 [02:33<01:05,  3.23it/s] 46%|████▌     | 178/390 [02:34<01:55,  1.83it/s] 46%|████▌     | 179/390 [02:34<01:39,  2.12it/s] 46%|████▌     | 180/390 [02:34<01:28,  2.38it/s] 46%|████▋     | 181/390 [02:35<01:20,  2.61it/s] 47%|████▋     | 182/390 [02:35<01:16,  2.72it/s] 47%|████▋     | 183/390 [02:35<01:11,  2.89it/s] 47%|████▋     | 184/390 [02:35<01:08,  3.01it/s] 47%|████▋     | 185/390 [02:36<01:05,  3.11it/s] 48%|████▊     | 186/390 [02:36<01:07,  3.02it/s] 48%|████▊     | 187/390 [02:36<01:05,  3.11it/s] 48%|████▊     | 188/390 [02:37<01:03,  3.18it/s] 48%|████▊     | 189/390 [02:37<01:14,  2.71it/s] 49%|████▊     | 190/390 [02:38<01:52,  1.78it/s] 49%|████▉     | 191/390 [02:39<01:37,  2.03it/s] 49%|████▉     | 192/390 [02:39<01:25,  2.30it/s] 49%|████▉     | 193/390 [02:39<01:20,  2.43it/s] 50%|████▉     | 194/390 [02:39<01:13,  2.65it/s] 50%|█████     | 195/390 [02:40<01:08,  2.83it/s] 50%|█████     | 196/390 [02:40<01:05,  2.97it/s] 51%|█████     | 197/390 [02:40<01:02,  3.07it/s] 51%|█████     | 198/390 [02:41<01:00,  3.15it/s] 51%|█████     | 199/390 [02:41<00:59,  3.21it/s] 51%|█████▏    | 200/390 [02:41<01:03,  3.00it/s] 52%|█████▏    | 201/390 [02:42<01:01,  3.09it/s] 52%|█████▏    | 202/390 [02:42<00:59,  3.17it/s] 52%|█████▏    | 203/390 [02:42<00:58,  3.22it/s] 52%|█████▏    | 204/390 [02:43<00:57,  3.25it/s] 53%|█████▎    | 205/390 [02:43<00:56,  3.28it/s] 53%|█████▎    | 206/390 [02:43<00:55,  3.30it/s] 53%|█████▎    | 207/390 [02:43<00:55,  3.32it/s] 53%|█████▎    | 208/390 [02:44<00:54,  3.33it/s] 54%|█████▎    | 209/390 [02:44<00:54,  3.34it/s] 54%|█████▍    | 210/390 [02:45<01:04,  2.79it/s] 54%|█████▍    | 211/390 [02:45<01:00,  2.94it/s] 54%|█████▍    | 212/390 [02:45<00:58,  3.05it/s] 55%|█████▍    | 213/390 [02:45<00:56,  3.13it/s] 55%|█████▍    | 214/390 [02:46<00:55,  3.19it/s] 55%|█████▌    | 215/390 [02:46<00:54,  3.24it/s] 55%|█████▌    | 216/390 [02:46<00:53,  3.27it/s] 56%|█████▌    | 217/390 [02:47<00:52,  3.29it/s] 56%|█████▌    | 218/390 [02:47<00:52,  3.31it/s] 56%|█████▌    | 219/390 [02:47<00:51,  3.32it/s] 56%|█████▋    | 220/390 [02:48<01:08,  2.47it/s] 57%|█████▋    | 221/390 [02:48<01:03,  2.68it/s] 57%|█████▋    | 222/390 [02:48<00:58,  2.85it/s] 57%|█████▋    | 223/390 [02:49<00:55,  2.98it/s] 57%|█████▋    | 224/390 [02:49<00:53,  3.08it/s] 58%|█████▊    | 225/390 [02:49<00:52,  3.16it/s] 58%|█████▊    | 226/390 [02:50<00:51,  3.21it/s] 58%|█████▊    | 227/390 [02:50<00:50,  3.25it/s] 58%|█████▊    | 228/390 [02:50<00:49,  3.28it/s] 59%|█████▊    | 229/390 [02:51<00:52,  3.07it/s] 59%|█████▉    | 230/390 [02:51<00:50,  3.15it/s] 59%|█████▉    | 231/390 [02:51<00:49,  3.21it/s] 59%|█████▉    | 232/390 [02:52<00:48,  3.25it/s] 60%|█████▉    | 233/390 [02:52<00:47,  3.28it/s] 60%|██████    | 234/390 [02:52<00:47,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 17:12:11,244 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:12:11,244 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:12:11,244 >>   Batch size = 8
{'eval_loss': 0.9931758046150208, 'eval_runtime': 10.0575, 'eval_samples_per_second': 347.798, 'eval_steps_per_second': 43.549, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.71it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.97it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.70it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.46it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.66it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.21it/s][A
  8%|▊         | 37/438 [00:00<00:08, 44.95it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.62it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.71it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.84it/s][A
 13%|█▎        | 57/438 [00:01<00:10, 37.18it/s][A
 14%|█▍        | 62/438 [00:01<00:09, 39.32it/s][A
 15%|█▌        | 67/438 [00:01<00:09, 40.96it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 42.13it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 42.89it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 43.56it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.04it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.83it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.09it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.28it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.55it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.73it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.91it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.96it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.87it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.52it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.32it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.40it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.51it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.74it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.77it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.85it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.90it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.72it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.52it/s][A
 44%|████▍     | 192/438 [00:04<00:07, 34.36it/s][A
 45%|████▍     | 197/438 [00:04<00:06, 37.13it/s][A
 46%|████▌     | 202/438 [00:04<00:06, 39.14it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 40.79it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 41.94it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 42.84it/s][A
 51%|█████     | 222/438 [00:05<00:04, 43.44it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 43.72it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 43.57it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 43.56it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.76it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.22it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.47it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.66it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.81it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.86it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.70it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.49it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.30it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.72it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.87it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.85it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.77it/s][A
 74%|███████▎  | 322/438 [00:07<00:04, 28.42it/s][A
 75%|███████▍  | 327/438 [00:07<00:03, 32.02it/s][A
 76%|███████▌  | 332/438 [00:07<00:03, 35.09it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 37.59it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 39.55it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 41.04it/s][A
 80%|████████  | 352/438 [00:08<00:02, 42.26it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.02it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.07it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 43.17it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.37it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.87it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.17it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.41it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.61it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.83it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.78it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.43it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.39it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.29it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.45it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.53it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.70it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.70it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.70it/s][A 60%|██████    | 234/390 [03:02<00:47,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:12:22,698 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 17:12:23,040 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:12:32,347 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:12:32,615 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:12:32,944 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:37<34:58, 13.54s/it] 61%|██████    | 236/390 [03:37<24:35,  9.58s/it] 61%|██████    | 237/390 [03:38<17:36,  6.91s/it] 61%|██████    | 238/390 [03:39<13:16,  5.24s/it] 61%|██████▏   | 239/390 [03:39<09:27,  3.76s/it] 62%|██████▏   | 240/390 [03:40<06:47,  2.72s/it] 62%|██████▏   | 241/390 [03:40<04:56,  1.99s/it] 62%|██████▏   | 242/390 [03:40<03:56,  1.60s/it] 62%|██████▏   | 243/390 [03:41<02:57,  1.21s/it] 63%|██████▎   | 244/390 [03:41<02:16,  1.07it/s] 63%|██████▎   | 245/390 [03:41<01:48,  1.34it/s] 63%|██████▎   | 246/390 [03:42<01:27,  1.64it/s] 63%|██████▎   | 247/390 [03:42<01:13,  1.93it/s] 64%|██████▎   | 248/390 [03:42<01:04,  2.22it/s] 64%|██████▍   | 249/390 [03:43<01:03,  2.23it/s] 64%|██████▍   | 250/390 [03:43<00:56,  2.48it/s] 64%|██████▍   | 251/390 [03:43<00:51,  2.70it/s] 65%|██████▍   | 252/390 [03:44<00:48,  2.87it/s] 65%|██████▍   | 253/390 [03:44<00:45,  3.00it/s] 65%|██████▌   | 254/390 [03:44<00:43,  3.09it/s] 65%|██████▌   | 255/390 [03:45<00:42,  3.17it/s] 66%|██████▌   | 256/390 [03:45<00:41,  3.22it/s] 66%|██████▌   | 257/390 [03:45<00:40,  3.26it/s] 66%|██████▌   | 258/390 [03:45<00:40,  3.29it/s] 66%|██████▋   | 259/390 [03:46<01:04,  2.02it/s] 67%|██████▋   | 260/390 [03:47<00:56,  2.29it/s] 67%|██████▋   | 261/390 [03:47<00:50,  2.54it/s] 67%|██████▋   | 262/390 [03:47<00:46,  2.74it/s] 67%|██████▋   | 263/390 [03:48<00:43,  2.90it/s] 68%|██████▊   | 264/390 [03:48<00:41,  3.02it/s] 68%|██████▊   | 265/390 [03:48<00:40,  3.11it/s] 68%|██████▊   | 266/390 [03:48<00:38,  3.18it/s] 68%|██████▊   | 267/390 [03:49<00:42,  2.92it/s] 69%|██████▊   | 268/390 [03:49<00:40,  3.04it/s] 69%|██████▉   | 269/390 [03:49<00:38,  3.13it/s] 69%|██████▉   | 270/390 [03:50<00:37,  3.19it/s] 69%|██████▉   | 271/390 [03:50<00:36,  3.24it/s] 70%|██████▉   | 272/390 [03:50<00:36,  3.27it/s] 70%|███████   | 273/390 [03:51<00:35,  3.30it/s] 70%|███████   | 274/390 [03:51<00:34,  3.32it/s] 71%|███████   | 275/390 [03:51<00:34,  3.33it/s] 71%|███████   | 276/390 [03:52<00:34,  3.34it/s] 71%|███████   | 277/390 [03:52<00:40,  2.79it/s] 71%|███████▏  | 278/390 [03:52<00:38,  2.94it/s] 72%|███████▏  | 279/390 [03:53<00:36,  3.05it/s] 72%|███████▏  | 280/390 [03:53<00:35,  3.13it/s] 72%|███████▏  | 281/390 [03:53<00:34,  3.20it/s] 72%|███████▏  | 282/390 [03:54<00:33,  3.24it/s] 73%|███████▎  | 283/390 [03:54<00:32,  3.28it/s] 73%|███████▎  | 284/390 [03:54<00:32,  3.30it/s] 73%|███████▎  | 285/390 [03:54<00:31,  3.32it/s] 73%|███████▎  | 286/390 [03:55<00:31,  3.33it/s] 74%|███████▎  | 287/390 [03:55<00:32,  3.20it/s] 74%|███████▍  | 288/390 [03:55<00:31,  3.25it/s] 74%|███████▍  | 289/390 [03:56<00:30,  3.28it/s] 74%|███████▍  | 290/390 [03:56<00:30,  3.30it/s] 75%|███████▍  | 291/390 [03:56<00:29,  3.31it/s] 75%|███████▍  | 292/390 [03:57<00:29,  3.32it/s] 75%|███████▌  | 293/390 [03:57<00:29,  3.33it/s] 75%|███████▌  | 294/390 [03:57<00:28,  3.34it/s] 76%|███████▌  | 295/390 [03:57<00:28,  3.34it/s] 76%|███████▌  | 296/390 [03:58<00:28,  3.34it/s] 76%|███████▌  | 297/390 [03:58<00:28,  3.22it/s] 76%|███████▋  | 298/390 [03:58<00:28,  3.26it/s] 77%|███████▋  | 299/390 [03:59<00:27,  3.29it/s] 77%|███████▋  | 300/390 [03:59<00:27,  3.31it/s] 77%|███████▋  | 301/390 [03:59<00:26,  3.32it/s] 77%|███████▋  | 302/390 [04:00<00:26,  3.33it/s] 78%|███████▊  | 303/390 [04:00<00:26,  3.32it/s] 78%|███████▊  | 304/390 [04:00<00:25,  3.33it/s] 78%|███████▊  | 305/390 [04:00<00:25,  3.33it/s] 78%|███████▊  | 306/390 [04:01<00:25,  3.34it/s] 79%|███████▊  | 307/390 [04:02<00:37,  2.23it/s] 79%|███████▉  | 308/390 [04:02<00:33,  2.47it/s] 79%|███████▉  | 309/390 [04:02<00:30,  2.69it/s] 79%|███████▉  | 310/390 [04:02<00:28,  2.86it/s] 80%|███████▉  | 311/390 [04:03<00:26,  2.99it/s] 80%|████████  | 312/390 [04:03<00:25,  3.09it/s][INFO|trainer.py:2140] 2023-08-28 17:13:22,151 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:13:22,151 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:13:22,151 >>   Batch size = 8
{'eval_loss': 1.006530523300171, 'eval_runtime': 10.1909, 'eval_samples_per_second': 343.248, 'eval_steps_per_second': 42.98, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.71it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.34it/s][A
  4%|▍         | 17/438 [00:00<00:08, 46.79it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.90it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.34it/s][A
  7%|▋         | 32/438 [00:00<00:09, 45.06it/s][A
  8%|▊         | 37/438 [00:00<00:09, 41.54it/s][A
 10%|▉         | 42/438 [00:00<00:09, 42.50it/s][A
 11%|█         | 47/438 [00:01<00:09, 43.28it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.85it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.20it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.42it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.45it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.06it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.20it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.40it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.65it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.71it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.76it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.91it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.76it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.49it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.34it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.33it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.52it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.64it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.81it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.87it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.90it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.90it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.58it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.36it/s][A
 39%|███▉      | 172/438 [00:04<00:09, 27.18it/s][A
 40%|████      | 177/438 [00:04<00:08, 30.90it/s][A
 42%|████▏     | 182/438 [00:04<00:07, 34.17it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 36.85it/s][A
 44%|████▍     | 192/438 [00:04<00:06, 38.96it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 40.62it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 41.85it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 42.68it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 42.88it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 43.14it/s][A
 51%|█████     | 222/438 [00:05<00:04, 43.51it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 43.95it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.30it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.50it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.71it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.85it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.80it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.48it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.21it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.19it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.37it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.52it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.67it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.78it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.91it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 35.51it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 38.02it/s][A
 70%|███████   | 307/438 [00:07<00:03, 39.97it/s][A
 71%|███████   | 312/438 [00:07<00:03, 41.36it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 42.37it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.16it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.79it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.93it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.76it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 43.67it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 43.78it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.15it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.39it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.63it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.72it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.83it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.73it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.38it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.20it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.26it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.36it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.49it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.71it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.80it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.89it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.72it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 38.32it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 40.12it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 41.47it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 41.47it/s][A 80%|████████  | 312/390 [04:13<00:25,  3.09it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:13:32,556 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 17:13:32,820 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:13:41,008 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:13:41,421 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:13:41,626 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:46<16:57, 13.22s/it] 81%|████████  | 314/390 [04:47<11:49,  9.34s/it] 81%|████████  | 315/390 [04:47<08:19,  6.66s/it] 81%|████████  | 316/390 [04:47<05:51,  4.75s/it] 81%|████████▏ | 317/390 [04:48<04:09,  3.42s/it] 82%|████████▏ | 318/390 [04:48<02:58,  2.48s/it] 82%|████████▏ | 319/390 [04:48<02:09,  1.83s/it] 82%|████████▏ | 320/390 [04:49<01:35,  1.37s/it] 82%|████████▏ | 321/390 [04:49<01:12,  1.05s/it] 83%|████████▎ | 322/390 [04:49<00:55,  1.22it/s] 83%|████████▎ | 323/390 [04:49<00:44,  1.50it/s] 83%|████████▎ | 324/390 [04:50<00:36,  1.80it/s] 83%|████████▎ | 325/390 [04:50<00:32,  2.02it/s] 84%|████████▎ | 326/390 [04:50<00:27,  2.29it/s] 84%|████████▍ | 327/390 [04:51<00:24,  2.53it/s] 84%|████████▍ | 328/390 [04:51<00:22,  2.73it/s] 84%|████████▍ | 329/390 [04:51<00:21,  2.90it/s] 85%|████████▍ | 330/390 [04:52<00:19,  3.02it/s] 85%|████████▍ | 331/390 [04:52<00:18,  3.11it/s] 85%|████████▌ | 332/390 [04:52<00:18,  3.18it/s] 85%|████████▌ | 333/390 [04:52<00:17,  3.23it/s] 86%|████████▌ | 334/390 [04:53<00:17,  3.27it/s] 86%|████████▌ | 335/390 [04:53<00:18,  3.03it/s] 86%|████████▌ | 336/390 [04:53<00:17,  3.12it/s] 86%|████████▋ | 337/390 [04:54<00:16,  3.19it/s] 87%|████████▋ | 338/390 [04:54<00:16,  3.24it/s] 87%|████████▋ | 339/390 [04:54<00:15,  3.27it/s] 87%|████████▋ | 340/390 [04:55<00:15,  3.30it/s] 87%|████████▋ | 341/390 [04:55<00:14,  3.32it/s] 88%|████████▊ | 342/390 [04:55<00:14,  3.33it/s] 88%|████████▊ | 343/390 [04:56<00:14,  3.34it/s] 88%|████████▊ | 344/390 [04:56<00:13,  3.35it/s] 88%|████████▊ | 345/390 [04:56<00:14,  3.20it/s] 89%|████████▊ | 346/390 [04:56<00:13,  3.24it/s] 89%|████████▉ | 347/390 [04:57<00:13,  3.28it/s] 89%|████████▉ | 348/390 [04:57<00:12,  3.30it/s] 89%|████████▉ | 349/390 [04:57<00:12,  3.31it/s] 90%|████████▉ | 350/390 [04:58<00:12,  3.32it/s] 90%|█████████ | 351/390 [04:58<00:11,  3.33it/s] 90%|█████████ | 352/390 [04:58<00:11,  3.34it/s] 91%|█████████ | 353/390 [04:59<00:11,  3.34it/s] 91%|█████████ | 354/390 [04:59<00:10,  3.34it/s] 91%|█████████ | 355/390 [04:59<00:11,  2.97it/s] 91%|█████████▏| 356/390 [05:00<00:11,  3.07it/s] 92%|█████████▏| 357/390 [05:00<00:10,  3.15it/s] 92%|█████████▏| 358/390 [05:00<00:09,  3.21it/s] 92%|█████████▏| 359/390 [05:00<00:09,  3.25it/s] 92%|█████████▏| 360/390 [05:01<00:09,  3.28it/s] 93%|█████████▎| 361/390 [05:01<00:08,  3.30it/s] 93%|█████████▎| 362/390 [05:01<00:08,  3.31it/s] 93%|█████████▎| 363/390 [05:02<00:08,  3.33it/s] 93%|█████████▎| 364/390 [05:02<00:07,  3.34it/s] 94%|█████████▎| 365/390 [05:03<00:10,  2.49it/s] 94%|█████████▍| 366/390 [05:03<00:08,  2.70it/s] 94%|█████████▍| 367/390 [05:03<00:08,  2.87it/s] 94%|█████████▍| 368/390 [05:04<00:07,  3.00it/s] 95%|█████████▍| 369/390 [05:04<00:06,  3.10it/s] 95%|█████████▍| 370/390 [05:04<00:06,  3.17it/s] 95%|█████████▌| 371/390 [05:04<00:05,  3.22it/s] 95%|█████████▌| 372/390 [05:05<00:05,  3.26it/s] 96%|█████████▌| 373/390 [05:05<00:05,  3.28it/s] 96%|█████████▌| 374/390 [05:05<00:05,  3.04it/s] 96%|█████████▌| 375/390 [05:06<00:04,  3.13it/s] 96%|█████████▋| 376/390 [05:06<00:04,  3.19it/s] 97%|█████████▋| 377/390 [05:06<00:04,  3.24it/s] 97%|█████████▋| 378/390 [05:07<00:03,  3.27it/s] 97%|█████████▋| 379/390 [05:07<00:03,  3.29it/s] 97%|█████████▋| 380/390 [05:07<00:03,  3.31it/s] 98%|█████████▊| 381/390 [05:07<00:02,  3.32it/s] 98%|█████████▊| 382/390 [05:08<00:02,  3.33it/s] 98%|█████████▊| 383/390 [05:08<00:02,  3.33it/s] 98%|█████████▊| 384/390 [05:09<00:02,  2.41it/s] 99%|█████████▊| 385/390 [05:09<00:01,  2.63it/s] 99%|█████████▉| 386/390 [05:09<00:01,  2.81it/s] 99%|█████████▉| 387/390 [05:10<00:01,  2.95it/s] 99%|█████████▉| 388/390 [05:10<00:00,  3.06it/s]100%|█████████▉| 389/390 [05:10<00:00,  3.14it/s]100%|██████████| 390/390 [05:11<00:00,  3.20it/s][INFO|trainer.py:2140] 2023-08-28 17:14:29,625 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:14:29,625 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:14:29,625 >>   Batch size = 8
{'eval_loss': 1.0151362419128418, 'eval_runtime': 10.2219, 'eval_samples_per_second': 342.208, 'eval_steps_per_second': 42.849, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.20it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.39it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.36it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.72it/s][A
  7%|▋         | 32/438 [00:00<00:14, 27.84it/s][A
  8%|▊         | 37/438 [00:01<00:12, 31.79it/s][A
 10%|▉         | 42/438 [00:01<00:11, 34.87it/s][A
 11%|█         | 47/438 [00:01<00:10, 37.63it/s][A
 12%|█▏        | 52/438 [00:01<00:09, 39.67it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 41.25it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 42.34it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 42.99it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.17it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 43.50it/s][A
 19%|█▊        | 82/438 [00:02<00:08, 43.94it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 44.23it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.45it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.68it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.77it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.80it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/438 [00:03<00:06, 44.50it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.58it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.72it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.82it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.90it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.91it/s][A
 36%|███▌      | 157/438 [00:03<00:07, 35.81it/s][A
 37%|███▋      | 161/438 [00:03<00:09, 30.75it/s][A
 38%|███▊      | 166/438 [00:04<00:07, 34.24it/s][A
 39%|███▉      | 171/438 [00:04<00:07, 36.98it/s][A
 40%|████      | 176/438 [00:04<00:06, 39.24it/s][A
 41%|████▏     | 181/438 [00:04<00:06, 40.86it/s][A
 42%|████▏     | 186/438 [00:04<00:05, 42.11it/s][A
 44%|████▎     | 191/438 [00:04<00:05, 42.94it/s][A
 45%|████▍     | 196/438 [00:04<00:05, 43.59it/s][A
 46%|████▌     | 201/438 [00:04<00:05, 43.54it/s][A
 47%|████▋     | 206/438 [00:04<00:05, 43.57it/s][A
 48%|████▊     | 211/438 [00:05<00:05, 43.86it/s][A
 49%|████▉     | 216/438 [00:05<00:05, 44.12it/s][A
 50%|█████     | 221/438 [00:05<00:04, 44.43it/s][A
 52%|█████▏    | 226/438 [00:05<00:04, 44.68it/s][A
 53%|█████▎    | 231/438 [00:05<00:04, 44.88it/s][A
 54%|█████▍    | 236/438 [00:05<00:04, 44.93it/s][A
 55%|█████▌    | 241/438 [00:05<00:04, 44.81it/s][A
 56%|█████▌    | 246/438 [00:05<00:04, 44.58it/s][A
 57%|█████▋    | 251/438 [00:05<00:04, 44.44it/s][A
 58%|█████▊    | 256/438 [00:06<00:04, 44.41it/s][A
 60%|█████▉    | 261/438 [00:06<00:03, 44.39it/s][A
 61%|██████    | 266/438 [00:06<00:03, 44.56it/s][A
 62%|██████▏   | 271/438 [00:06<00:03, 44.75it/s][A
 63%|██████▎   | 276/438 [00:06<00:03, 44.90it/s][A
 64%|██████▍   | 281/438 [00:06<00:03, 44.96it/s][A
 65%|██████▌   | 286/438 [00:06<00:03, 44.83it/s][A
 66%|██████▋   | 291/438 [00:07<00:03, 44.58it/s][A
 68%|██████▊   | 296/438 [00:07<00:06, 23.66it/s][A
 69%|██████▊   | 301/438 [00:07<00:04, 27.66it/s][A
 70%|██████▉   | 306/438 [00:07<00:04, 31.29it/s][A
 71%|███████   | 311/438 [00:07<00:03, 34.47it/s][A
 72%|███████▏  | 316/438 [00:07<00:03, 37.14it/s][A
 73%|███████▎  | 321/438 [00:07<00:02, 39.24it/s][A
 74%|███████▍  | 326/438 [00:07<00:02, 40.84it/s][A
 76%|███████▌  | 331/438 [00:08<00:02, 41.87it/s][A
 77%|███████▋  | 336/438 [00:08<00:02, 42.33it/s][A
 78%|███████▊  | 341/438 [00:08<00:02, 42.74it/s][A
 79%|███████▉  | 346/438 [00:08<00:02, 43.25it/s][A
 80%|████████  | 351/438 [00:08<00:01, 43.70it/s][A
 81%|████████▏ | 356/438 [00:08<00:01, 44.10it/s][A
 82%|████████▏ | 361/438 [00:08<00:01, 44.40it/s][A
 84%|████████▎ | 366/438 [00:08<00:01, 44.74it/s][A
 85%|████████▍ | 371/438 [00:08<00:01, 44.86it/s][A
 86%|████████▌ | 376/438 [00:09<00:01, 44.67it/s][A
 87%|████████▋ | 381/438 [00:09<00:01, 44.39it/s][A
 88%|████████▊ | 386/438 [00:09<00:01, 44.28it/s][A
 89%|████████▉ | 391/438 [00:09<00:01, 44.36it/s][A
 90%|█████████ | 396/438 [00:09<00:00, 44.49it/s][A
 92%|█████████▏| 401/438 [00:09<00:00, 44.59it/s][A
 93%|█████████▎| 406/438 [00:10<00:01, 27.35it/s][A
 94%|█████████▎| 410/438 [00:10<00:01, 23.22it/s][A
 95%|█████████▍| 415/438 [00:10<00:00, 27.44it/s][A
 96%|█████████▌| 420/438 [00:10<00:00, 31.26it/s][A
 97%|█████████▋| 425/438 [00:10<00:00, 34.57it/s][A
 98%|█████████▊| 430/438 [00:10<00:00, 37.23it/s][A
 99%|█████████▉| 435/438 [00:10<00:00, 39.30it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 39.30it/s][A100%|██████████| 390/390 [05:21<00:00,  3.20it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:14:40,852 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 17:14:42,151 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:14:58,294 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:14:59,763 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:15:00,227 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:15:28,336 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:15:28,373 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78 (score: 0.9780838489532471).
                                                 100%|██████████| 390/390 [06:57<00:00,  3.20it/s]100%|██████████| 390/390 [06:57<00:00,  1.07s/it]
[INFO|trainer.py:1894] 2023-08-28 17:16:17,703 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 17:16:18,488 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:16:27,136 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:16:28,267 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:16:28,466 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:16:30,342 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,726 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,727 >>   train_loss               =     0.5005
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,727 >>   train_runtime            = 0:06:57.74
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,727 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,727 >>   train_samples_per_second =     59.834
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:30,727 >>   train_steps_per_second   =      0.934
{'eval_loss': 1.0183409452438354, 'eval_runtime': 10.8873, 'eval_samples_per_second': 321.292, 'eval_steps_per_second': 40.23, 'epoch': 4.99}
{'train_runtime': 417.7406, 'train_samples_per_second': 59.834, 'train_steps_per_second': 0.934, 'train_loss': 0.5004845643654848, 'epoch': 4.99}
08/28/2023 17:16:32 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:16:32,677 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:16:32,677 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 17:16:32,677 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.87it/s]  3%|▎         | 12/438 [00:00<00:08, 49.63it/s]  4%|▍         | 18/438 [00:00<00:08, 47.81it/s]  5%|▌         | 23/438 [00:00<00:08, 47.08it/s]  6%|▋         | 28/438 [00:00<00:08, 46.57it/s]  8%|▊         | 33/438 [00:00<00:08, 46.34it/s]  9%|▊         | 38/438 [00:00<00:10, 38.35it/s] 10%|▉         | 43/438 [00:00<00:09, 40.48it/s] 11%|█         | 48/438 [00:01<00:09, 41.90it/s] 12%|█▏        | 53/438 [00:01<00:08, 42.93it/s] 13%|█▎        | 58/438 [00:01<00:08, 43.81it/s] 14%|█▍        | 63/438 [00:01<00:08, 44.32it/s] 16%|█▌        | 68/438 [00:01<00:08, 44.73it/s] 17%|█▋        | 73/438 [00:01<00:08, 44.98it/s] 18%|█▊        | 78/438 [00:01<00:08, 44.63it/s] 19%|█▉        | 83/438 [00:01<00:07, 44.43it/s] 20%|██        | 88/438 [00:01<00:07, 44.61it/s] 21%|██        | 93/438 [00:02<00:07, 44.84it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.11it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.30it/s] 25%|██▍       | 108/438 [00:02<00:07, 45.42it/s] 26%|██▌       | 113/438 [00:02<00:07, 45.49it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.24it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.11it/s] 29%|██▉       | 128/438 [00:02<00:06, 44.78it/s] 30%|███       | 133/438 [00:02<00:06, 44.77it/s] 32%|███▏      | 138/438 [00:03<00:06, 44.91it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.02it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.13it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.31it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.40it/s] 37%|███▋      | 163/438 [00:03<00:06, 45.41it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.28it/s] 39%|███▉      | 173/438 [00:03<00:07, 36.19it/s] 41%|████      | 178/438 [00:04<00:06, 38.63it/s] 42%|████▏     | 183/438 [00:04<00:06, 40.50it/s] 43%|████▎     | 188/438 [00:04<00:05, 41.91it/s] 44%|████▍     | 193/438 [00:04<00:05, 42.98it/s] 45%|████▌     | 198/438 [00:04<00:05, 43.70it/s] 46%|████▋     | 203/438 [00:04<00:05, 44.25it/s] 47%|████▋     | 208/438 [00:04<00:05, 44.60it/s] 49%|████▊     | 213/438 [00:04<00:05, 44.32it/s] 50%|████▉     | 218/438 [00:04<00:04, 44.17it/s] 51%|█████     | 223/438 [00:05<00:04, 44.28it/s] 52%|█████▏    | 228/438 [00:05<00:04, 44.53it/s] 53%|█████▎    | 233/438 [00:05<00:04, 44.83it/s] 54%|█████▍    | 238/438 [00:05<00:04, 45.09it/s] 55%|█████▌    | 243/438 [00:05<00:04, 45.28it/s] 57%|█████▋    | 248/438 [00:05<00:04, 45.44it/s] 58%|█████▊    | 253/438 [00:05<00:04, 45.24it/s] 59%|█████▉    | 258/438 [00:05<00:04, 44.90it/s] 60%|██████    | 263/438 [00:05<00:03, 44.65it/s] 61%|██████    | 268/438 [00:06<00:03, 44.58it/s] 62%|██████▏   | 273/438 [00:06<00:03, 44.71it/s] 63%|██████▎   | 278/438 [00:06<00:03, 44.91it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.00it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.09it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.25it/s] 68%|██████▊   | 298/438 [00:06<00:03, 45.23it/s] 69%|██████▉   | 303/438 [00:06<00:03, 44.92it/s] 70%|███████   | 308/438 [00:06<00:03, 41.47it/s] 71%|███████▏  | 313/438 [00:07<00:03, 37.05it/s] 73%|███████▎  | 318/438 [00:07<00:03, 39.21it/s] 74%|███████▎  | 323/438 [00:07<00:02, 40.90it/s] 75%|███████▍  | 328/438 [00:07<00:02, 42.14it/s] 76%|███████▌  | 333/438 [00:07<00:02, 43.06it/s] 77%|███████▋  | 338/438 [00:07<00:02, 43.69it/s] 78%|███████▊  | 343/438 [00:07<00:02, 44.22it/s] 79%|███████▉  | 348/438 [00:07<00:02, 44.59it/s] 81%|████████  | 353/438 [00:08<00:01, 44.38it/s] 82%|████████▏ | 358/438 [00:08<00:01, 44.31it/s] 83%|████████▎ | 363/438 [00:08<00:01, 44.35it/s] 84%|████████▍ | 368/438 [00:08<00:01, 44.65it/s] 85%|████████▌ | 373/438 [00:08<00:01, 44.81it/s] 86%|████████▋ | 378/438 [00:08<00:01, 44.94it/s] 87%|████████▋ | 383/438 [00:08<00:01, 45.06it/s] 89%|████████▊ | 388/438 [00:08<00:01, 45.17it/s] 90%|████████▉ | 393/438 [00:08<00:00, 45.13it/s] 91%|█████████ | 398/438 [00:09<00:00, 44.85it/s] 92%|█████████▏| 403/438 [00:09<00:00, 44.71it/s] 93%|█████████▎| 408/438 [00:09<00:00, 44.67it/s] 94%|█████████▍| 413/438 [00:09<00:00, 44.81it/s] 95%|█████████▌| 418/438 [00:09<00:00, 44.89it/s] 97%|█████████▋| 423/438 [00:09<00:00, 44.97it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.03it/s] 99%|█████████▉| 433/438 [00:09<00:00, 45.15it/s]100%|██████████| 438/438 [00:09<00:00, 45.10it/s]100%|██████████| 438/438 [00:10<00:00, 43.80it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:16:42,702 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   eval_loss               =     0.9781
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   eval_runtime            = 0:00:10.02
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   eval_samples_per_second =    348.941
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   eval_steps_per_second   =     43.692
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:42,702 >>   perplexity              =     2.6594
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:13,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:13,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:13,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:13,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:13,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:17:14,995 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:17:14,996 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:15,666 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:16,956 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:17,037 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:21,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:21,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:21,644 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:21,644 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:21,644 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:17:22,602 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:17:22,603 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:23,586 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:23,967 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:23,967 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 6it [00:04,  1.36it/s]Extractor Predicting: 7it [00:05,  1.39it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:08,  1.35it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.32it/s]Extractor Predicting: 15it [00:11,  1.36it/s]Extractor Predicting: 16it [00:11,  1.35it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:13,  1.37it/s]Extractor Predicting: 19it [00:13,  1.38it/s]Extractor Predicting: 20it [00:14,  1.39it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:16,  1.39it/s]Extractor Predicting: 23it [00:16,  1.39it/s]Extractor Predicting: 24it [00:17,  1.40it/s]Extractor Predicting: 25it [00:18,  1.38it/s]Extractor Predicting: 26it [00:19,  1.38it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.26it/s]Extractor Predicting: 29it [00:21,  1.25it/s]Extractor Predicting: 30it [00:22,  1.22it/s]Extractor Predicting: 31it [00:23,  1.25it/s]Extractor Predicting: 32it [00:24,  1.18it/s]Extractor Predicting: 33it [00:24,  1.21it/s]Extractor Predicting: 34it [00:25,  1.21it/s]Extractor Predicting: 35it [00:26,  1.22it/s]Extractor Predicting: 36it [00:27,  1.20it/s]Extractor Predicting: 37it [00:28,  1.19it/s]Extractor Predicting: 38it [00:29,  1.20it/s]Extractor Predicting: 39it [00:29,  1.21it/s]Extractor Predicting: 40it [00:31,  1.05it/s]Extractor Predicting: 41it [00:31,  1.09it/s]Extractor Predicting: 42it [00:32,  1.12it/s]Extractor Predicting: 43it [00:33,  1.16it/s]Extractor Predicting: 44it [00:34,  1.09it/s]Extractor Predicting: 45it [00:35,  1.13it/s]Extractor Predicting: 46it [00:36,  1.15it/s]Extractor Predicting: 47it [00:37,  1.19it/s]Extractor Predicting: 48it [00:38,  1.06it/s]Extractor Predicting: 49it [00:39,  1.07it/s]Extractor Predicting: 50it [00:39,  1.10it/s]Extractor Predicting: 51it [00:40,  1.14it/s]Extractor Predicting: 52it [00:41,  1.08it/s]Extractor Predicting: 53it [00:42,  1.10it/s]Extractor Predicting: 54it [00:43,  1.16it/s]Extractor Predicting: 55it [00:44,  1.19it/s]Extractor Predicting: 56it [00:45,  1.19it/s]Extractor Predicting: 57it [00:45,  1.20it/s]Extractor Predicting: 58it [00:46,  1.20it/s]Extractor Predicting: 59it [00:47,  1.19it/s]Extractor Predicting: 60it [00:48,  1.27it/s]Extractor Predicting: 61it [00:49,  1.28it/s]Extractor Predicting: 62it [00:49,  1.28it/s]Extractor Predicting: 63it [00:50,  1.27it/s]Extractor Predicting: 64it [00:51,  1.27it/s]Extractor Predicting: 65it [00:52,  1.31it/s]Extractor Predicting: 66it [00:52,  1.28it/s]Extractor Predicting: 67it [00:53,  1.29it/s]Extractor Predicting: 68it [00:54,  1.26it/s]Extractor Predicting: 69it [00:55,  1.26it/s]Extractor Predicting: 70it [00:56,  1.30it/s]Extractor Predicting: 71it [00:56,  1.30it/s]Extractor Predicting: 72it [00:57,  1.31it/s]Extractor Predicting: 73it [00:58,  1.33it/s]Extractor Predicting: 74it [00:58,  1.36it/s]Extractor Predicting: 75it [00:59,  1.37it/s]Extractor Predicting: 76it [01:00,  1.36it/s]Extractor Predicting: 77it [01:01,  1.34it/s]Extractor Predicting: 78it [01:01,  1.35it/s]Extractor Predicting: 79it [01:02,  1.35it/s]Extractor Predicting: 80it [01:03,  1.35it/s]Extractor Predicting: 81it [01:04,  1.33it/s]Extractor Predicting: 82it [01:05,  1.29it/s]Extractor Predicting: 83it [01:05,  1.32it/s]Extractor Predicting: 84it [01:06,  1.31it/s]Extractor Predicting: 85it [01:07,  1.32it/s]Extractor Predicting: 86it [01:08,  1.26it/s]Extractor Predicting: 87it [01:08,  1.28it/s]Extractor Predicting: 88it [01:09,  1.33it/s]Extractor Predicting: 89it [01:10,  1.39it/s]Extractor Predicting: 90it [01:10,  1.37it/s]Extractor Predicting: 91it [01:11,  1.38it/s]Extractor Predicting: 92it [01:12,  1.41it/s]Extractor Predicting: 93it [01:13,  1.42it/s]Extractor Predicting: 94it [01:13,  1.44it/s]Extractor Predicting: 95it [01:14,  1.43it/s]Extractor Predicting: 96it [01:15,  1.36it/s]Extractor Predicting: 97it [01:15,  1.38it/s]Extractor Predicting: 98it [01:16,  1.42it/s]Extractor Predicting: 99it [01:17,  1.42it/s]Extractor Predicting: 100it [01:18,  1.39it/s]Extractor Predicting: 101it [01:18,  1.32it/s]Extractor Predicting: 102it [01:19,  1.35it/s]Extractor Predicting: 103it [01:20,  1.37it/s]Extractor Predicting: 104it [01:21,  1.38it/s]Extractor Predicting: 105it [01:21,  1.37it/s]Extractor Predicting: 106it [01:22,  1.32it/s]Extractor Predicting: 107it [01:23,  1.32it/s]Extractor Predicting: 108it [01:24,  1.36it/s]Extractor Predicting: 109it [01:24,  1.39it/s]Extractor Predicting: 110it [01:25,  1.42it/s]Extractor Predicting: 111it [01:26,  1.36it/s]Extractor Predicting: 112it [01:26,  1.40it/s]Extractor Predicting: 113it [01:27,  1.41it/s]Extractor Predicting: 114it [01:28,  1.39it/s]Extractor Predicting: 115it [01:29,  1.39it/s]Extractor Predicting: 116it [01:29,  1.32it/s]Extractor Predicting: 117it [01:30,  1.31it/s]Extractor Predicting: 118it [01:31,  1.31it/s]Extractor Predicting: 119it [01:32,  1.30it/s]Extractor Predicting: 120it [01:33,  1.20it/s]Extractor Predicting: 121it [01:33,  1.23it/s]Extractor Predicting: 122it [01:34,  1.25it/s]Extractor Predicting: 123it [01:35,  1.25it/s]Extractor Predicting: 124it [01:36,  1.15it/s]Extractor Predicting: 125it [01:37,  1.19it/s]Extractor Predicting: 126it [01:38,  1.21it/s]Extractor Predicting: 127it [01:38,  1.22it/s]Extractor Predicting: 128it [01:40,  1.05s/it]Extractor Predicting: 129it [01:41,  1.03it/s]Extractor Predicting: 130it [01:42,  1.11it/s]Extractor Predicting: 131it [01:42,  1.13it/s]Extractor Predicting: 132it [01:43,  1.17it/s]Extractor Predicting: 133it [01:44,  1.19it/s]Extractor Predicting: 134it [01:45,  1.23it/s]Extractor Predicting: 135it [01:46,  1.20it/s]Extractor Predicting: 136it [01:46,  1.20it/s]Extractor Predicting: 137it [01:47,  1.23it/s]Extractor Predicting: 138it [01:48,  1.17it/s]Extractor Predicting: 139it [01:50,  1.05s/it]Extractor Predicting: 140it [01:50,  1.04it/s]Extractor Predicting: 141it [01:51,  1.09it/s]Extractor Predicting: 142it [01:52,  1.14it/s]Extractor Predicting: 143it [01:53,  1.14it/s]Extractor Predicting: 144it [01:54,  1.16it/s]Extractor Predicting: 145it [01:54,  1.55it/s]Extractor Predicting: 145it [01:54,  1.27it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:45,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:45,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:45,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:45,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:45,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:19:47,918 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:19:47,919 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:19:48,696 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:19:50,230 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:19:50,230 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:58,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:59,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:59,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:59,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:59,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:20:01,285 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:20:01,286 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:20:02,838 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:20:03,102 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:20:03,102 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.481029810298103,
  "recall": 0.2029731275014294,
  "score": 0.28548451950140735,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.24it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.31it/s]Extractor Predicting: 13it [00:10,  1.25it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.33it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.29it/s]Extractor Predicting: 20it [00:15,  1.27it/s]Extractor Predicting: 21it [00:16,  1.30it/s]Extractor Predicting: 22it [00:17,  1.27it/s]Extractor Predicting: 23it [00:17,  1.29it/s]Extractor Predicting: 24it [00:18,  1.26it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.28it/s]Extractor Predicting: 27it [00:21,  1.28it/s]Extractor Predicting: 28it [00:21,  1.26it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.28it/s]Extractor Predicting: 31it [00:24,  1.29it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.33it/s]Extractor Predicting: 34it [00:26,  1.35it/s]Extractor Predicting: 35it [00:26,  1.39it/s]Extractor Predicting: 36it [00:27,  1.35it/s]Extractor Predicting: 37it [00:28,  1.32it/s]Extractor Predicting: 38it [00:29,  1.34it/s]Extractor Predicting: 39it [00:30,  1.34it/s]Extractor Predicting: 40it [00:30,  1.36it/s]Extractor Predicting: 41it [00:31,  1.35it/s]Extractor Predicting: 42it [00:32,  1.35it/s]Extractor Predicting: 43it [00:32,  1.36it/s]Extractor Predicting: 44it [00:33,  1.41it/s]Extractor Predicting: 45it [00:34,  1.40it/s]Extractor Predicting: 46it [00:34,  1.44it/s]Extractor Predicting: 47it [00:35,  1.37it/s]Extractor Predicting: 48it [00:36,  1.36it/s]Extractor Predicting: 49it [00:37,  1.35it/s]Extractor Predicting: 50it [00:38,  1.33it/s]Extractor Predicting: 51it [00:39,  1.21it/s]Extractor Predicting: 52it [00:39,  1.24it/s]Extractor Predicting: 53it [00:40,  1.30it/s]Extractor Predicting: 54it [00:41,  1.26it/s]Extractor Predicting: 55it [00:42,  1.26it/s]Extractor Predicting: 56it [00:42,  1.27it/s]Extractor Predicting: 57it [00:43,  1.29it/s]Extractor Predicting: 58it [00:44,  1.31it/s]Extractor Predicting: 59it [00:45,  1.32it/s]Extractor Predicting: 60it [00:45,  1.33it/s]Extractor Predicting: 61it [00:46,  1.24it/s]Extractor Predicting: 62it [00:47,  1.28it/s]Extractor Predicting: 63it [00:48,  1.27it/s]Extractor Predicting: 64it [00:49,  1.24it/s]Extractor Predicting: 65it [00:49,  1.28it/s]Extractor Predicting: 66it [00:50,  1.29it/s]Extractor Predicting: 67it [00:51,  1.34it/s]Extractor Predicting: 68it [00:52,  1.35it/s]Extractor Predicting: 69it [00:52,  1.33it/s]Extractor Predicting: 70it [00:53,  1.29it/s]Extractor Predicting: 71it [00:54,  1.30it/s]Extractor Predicting: 72it [00:55,  1.30it/s]Extractor Predicting: 73it [00:56,  1.30it/s]Extractor Predicting: 74it [00:56,  1.28it/s]Extractor Predicting: 75it [00:57,  1.29it/s]Extractor Predicting: 76it [00:58,  1.28it/s]Extractor Predicting: 77it [00:59,  1.29it/s]Extractor Predicting: 78it [00:59,  1.28it/s]Extractor Predicting: 79it [01:00,  1.31it/s]Extractor Predicting: 80it [01:01,  1.34it/s]Extractor Predicting: 81it [01:02,  1.28it/s]Extractor Predicting: 82it [01:03,  1.26it/s]Extractor Predicting: 83it [01:03,  1.29it/s]Extractor Predicting: 84it [01:04,  1.31it/s]Extractor Predicting: 85it [01:05,  1.32it/s]Extractor Predicting: 86it [01:06,  1.28it/s]Extractor Predicting: 87it [01:06,  1.30it/s]Extractor Predicting: 88it [01:07,  1.35it/s]Extractor Predicting: 89it [01:08,  1.37it/s]Extractor Predicting: 90it [01:08,  1.39it/s]Extractor Predicting: 91it [01:09,  1.21it/s]Extractor Predicting: 92it [01:10,  1.24it/s]Extractor Predicting: 93it [01:11,  1.29it/s]Extractor Predicting: 94it [01:12,  1.30it/s]Extractor Predicting: 95it [01:12,  1.29it/s]Extractor Predicting: 96it [01:13,  1.29it/s]Extractor Predicting: 97it [01:14,  1.31it/s]Extractor Predicting: 98it [01:15,  1.31it/s]Extractor Predicting: 99it [01:16,  1.32it/s]Extractor Predicting: 100it [01:16,  1.34it/s]Extractor Predicting: 101it [01:17,  1.25it/s]Extractor Predicting: 102it [01:18,  1.26it/s]Extractor Predicting: 103it [01:19,  1.29it/s]Extractor Predicting: 104it [01:19,  1.31it/s]Extractor Predicting: 105it [01:20,  1.25it/s]Extractor Predicting: 106it [01:21,  1.27it/s]Extractor Predicting: 107it [01:22,  1.29it/s]Extractor Predicting: 108it [01:23,  1.31it/s]Extractor Predicting: 109it [01:23,  1.30it/s]Extractor Predicting: 110it [01:24,  1.29it/s]Extractor Predicting: 111it [01:25,  1.32it/s]Extractor Predicting: 112it [01:26,  1.25it/s]Extractor Predicting: 113it [01:27,  1.12it/s]Extractor Predicting: 114it [01:28,  1.16it/s]Extractor Predicting: 115it [01:28,  1.22it/s]Extractor Predicting: 116it [01:29,  1.25it/s]Extractor Predicting: 117it [01:30,  1.15it/s]Extractor Predicting: 118it [01:31,  1.20it/s]Extractor Predicting: 119it [01:32,  1.23it/s]Extractor Predicting: 120it [01:32,  1.26it/s]Extractor Predicting: 121it [01:33,  1.23it/s]Extractor Predicting: 122it [01:34,  1.26it/s]Extractor Predicting: 123it [01:35,  1.30it/s]Extractor Predicting: 124it [01:35,  1.31it/s]Extractor Predicting: 125it [01:36,  1.28it/s]Extractor Predicting: 126it [01:37,  1.28it/s]Extractor Predicting: 127it [01:38,  1.29it/s]Extractor Predicting: 128it [01:39,  1.33it/s]Extractor Predicting: 129it [01:39,  1.29it/s]Extractor Predicting: 130it [01:40,  1.32it/s]Extractor Predicting: 131it [01:41,  1.34it/s]Extractor Predicting: 132it [01:42,  1.33it/s]Extractor Predicting: 133it [01:43,  1.23it/s]Extractor Predicting: 134it [01:43,  1.24it/s]Extractor Predicting: 135it [01:44,  1.28it/s]Extractor Predicting: 136it [01:45,  1.31it/s]Extractor Predicting: 137it [01:45,  1.32it/s]Extractor Predicting: 138it [01:46,  1.35it/s]Extractor Predicting: 139it [01:47,  1.32it/s]Extractor Predicting: 140it [01:48,  1.31it/s]Extractor Predicting: 140it [01:48,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:13,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:13,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:13,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:13,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:13,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:22:15,523 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:22:15,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:22:15,859 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:22:17,237 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:22:17,237 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:19,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:19,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:19,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:19,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:19,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:22:20,928 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:22:21,111 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:22:21,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:22:23,179 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:22:23,179 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4144144144144144,
  "recall": 0.17818831942789035,
  "score": 0.2492185872056678,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 3it [00:02,  1.32it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:22:29,149 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:22:29,150 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:22:29,229 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:22:29,230 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:22:29,295 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:23:11,144 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:23:11,190 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:23:11,823 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:23:11,825 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:23:12,190 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,006 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,006 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,006 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,006 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,007 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:23:13,007 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.0625,
  "score": 0.1111111111111111,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:23:14,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:14,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:15,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:16,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:17,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:18,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:19,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:20,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:20,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:21,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:21,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:23,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:23,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:24,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:25,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:26,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:26,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:27,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:28,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:29,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:24, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:30,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:30,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:31,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:32,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:33,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:33,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:34,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:35,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:36,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:37,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:38,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:39,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:39,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:40,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:41,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:41,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:43,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:44,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:45,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:46,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:46,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:15, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:47,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:48,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:49,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:49,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:50,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:50,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:51,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:52,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:52,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:53,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:53,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:54,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:55,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:56,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:56,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:57,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:58,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:58,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:00,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:00,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:01,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:47<01:50, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 17:24:01,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:02,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:03,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:04,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:04,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:05,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:06,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:07,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:08,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:09,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:09,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:10,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:11,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:12,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:12,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:13,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:13,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:14,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:15,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:16,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:16,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:03<01:33, 15.64s/it][WARNING|generation_utils.py:914] 2023-08-28 17:24:17,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:18,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:19,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:19,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:20,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:21,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:22,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:22,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:23,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:23,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:24,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:25,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:25,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:26,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:26,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:27,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:28,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:28,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:30,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:30,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:17<01:14, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-28 17:24:31,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:32,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:32,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:34,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:34,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:35,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:36,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:36,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:37,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:38,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:38,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:39,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:40,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:41,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:42,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:42,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:43,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:44,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:44,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:45,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:46,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:01, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-28 17:24:47,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:47,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:48,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:49,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:50,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:51,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:51,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:52,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:54,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:54,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:56,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:56,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:57,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:58,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:00,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:00,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:01,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:02,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:03,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:04,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:04,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:05,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:06,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:53<00:50, 16.79s/it][WARNING|generation_utils.py:914] 2023-08-28 17:25:07,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:07,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:08,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:08,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:09,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:10,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:10,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:11,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:12,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:12,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:13,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:14,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:14,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:15,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:15,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:16,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:17,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:17,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:18,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:19,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:31, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-28 17:25:20,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:21,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:21,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:22,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:23,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:24,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:24,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:25,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:25,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:26,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:27,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:27,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:28,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:28,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:29,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:30,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:30,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:31,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:32,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:33,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:33,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:20<00:15, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-28 17:25:34,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:35,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:36,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:36,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:37,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:38,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:39,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:39,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:40,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:41,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:42,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:42,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:43,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:43,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:44,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:45,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:46,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:47,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:47,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:25:49,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:35<00:00, 15.25s/it]Generating: 100%|██████████| 10/10 [02:35<00:00, 15.59s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:06,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:06,601 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:26:07,704 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:26:07,705 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:26:08,327 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:26:09,677 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:26:09,712 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:13,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:13,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:13,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:13,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:13,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:26:14,508 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:26:14,509 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:26:15,265 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:26:15,504 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:26:15,504 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : main subject .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2003 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : United States .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : sport .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year , the museum built a new " The Art of the Ancient City " museum in Oxford , England , also known as the " The Gallery of the Ancient City " . Head Entity : The The Art of the Ancient City , Tail Entity : Oxford , England .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married the fourth century B.C. Pope Pius IX of Rome . Head Entity : Pope Pius IX of Rome , Tail Entity : Catholic Church .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 6592
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6692, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.29it/s]Extractor Estimating: 3it [00:02,  1.34it/s]Extractor Estimating: 4it [00:02,  1.37it/s]Extractor Estimating: 5it [00:03,  1.23it/s]Extractor Estimating: 6it [00:04,  1.30it/s]Extractor Estimating: 7it [00:05,  1.33it/s]Extractor Estimating: 8it [00:06,  1.33it/s]Extractor Estimating: 9it [00:07,  1.22it/s]Extractor Estimating: 10it [00:07,  1.27it/s]Extractor Estimating: 11it [00:08,  1.35it/s]Extractor Estimating: 12it [00:09,  1.36it/s]Extractor Estimating: 13it [00:09,  1.36it/s]Extractor Estimating: 14it [00:10,  1.34it/s]Extractor Estimating: 15it [00:11,  1.36it/s]Extractor Estimating: 16it [00:12,  1.37it/s]Extractor Estimating: 17it [00:12,  1.39it/s]Extractor Estimating: 18it [00:13,  1.40it/s]Extractor Estimating: 19it [00:14,  1.35it/s]Extractor Estimating: 20it [00:15,  1.33it/s]Extractor Estimating: 21it [00:15,  1.36it/s]Extractor Estimating: 22it [00:16,  1.38it/s]Extractor Estimating: 23it [00:17,  1.36it/s]Extractor Estimating: 24it [00:18,  1.30it/s]Extractor Estimating: 25it [00:18,  1.32it/s]Extractor Estimating: 26it [00:19,  1.32it/s]Extractor Estimating: 27it [00:20,  1.31it/s]Extractor Estimating: 28it [00:21,  1.18it/s]Extractor Estimating: 29it [00:22,  1.20it/s]Extractor Estimating: 30it [00:22,  1.25it/s]Extractor Estimating: 31it [00:23,  1.27it/s]Extractor Estimating: 32it [00:24,  1.17it/s]Extractor Estimating: 33it [00:25,  1.21it/s]Extractor Estimating: 34it [00:26,  1.21it/s]Extractor Estimating: 35it [00:26,  1.24it/s]Extractor Estimating: 36it [00:27,  1.18it/s]Extractor Estimating: 37it [00:28,  1.21it/s]Extractor Estimating: 38it [00:29,  1.23it/s]Extractor Estimating: 39it [00:30,  1.26it/s]Extractor Estimating: 40it [00:31,  1.25it/s]Extractor Estimating: 41it [00:31,  1.29it/s]Extractor Estimating: 42it [00:32,  1.29it/s]Extractor Estimating: 43it [00:33,  1.28it/s]Extractor Estimating: 44it [00:34,  1.25it/s]Extractor Estimating: 45it [00:34,  1.26it/s]Extractor Estimating: 46it [00:35,  1.25it/s]Extractor Estimating: 47it [00:36,  1.27it/s]Extractor Estimating: 48it [00:37,  1.29it/s]Extractor Estimating: 49it [00:38,  1.26it/s]Extractor Estimating: 50it [00:38,  1.29it/s]Extractor Estimating: 51it [00:39,  1.34it/s]Extractor Estimating: 52it [00:40,  1.26it/s]Extractor Estimating: 53it [00:40,  1.36it/s]Extractor Estimating: 54it [00:41,  1.38it/s]Extractor Estimating: 55it [00:42,  1.45it/s]Extractor Estimating: 56it [00:42,  1.46it/s]Extractor Estimating: 57it [00:43,  1.49it/s]Extractor Estimating: 58it [00:44,  1.46it/s]Extractor Estimating: 59it [00:46,  1.02it/s]Extractor Estimating: 60it [00:46,  1.14it/s]Extractor Estimating: 61it [00:47,  1.23it/s]Extractor Estimating: 62it [00:47,  1.30it/s]Extractor Estimating: 63it [00:49,  1.12it/s]Extractor Estimating: 64it [00:49,  1.21it/s]Extractor Estimating: 65it [00:50,  1.24it/s]Extractor Estimating: 66it [00:51,  1.29it/s]Extractor Estimating: 67it [00:51,  1.33it/s]Extractor Estimating: 68it [00:52,  1.33it/s]Extractor Estimating: 69it [00:53,  1.41it/s]Extractor Estimating: 70it [00:54,  1.41it/s]Extractor Estimating: 71it [00:54,  1.43it/s]Extractor Estimating: 72it [00:55,  1.40it/s]Extractor Estimating: 73it [00:56,  1.43it/s]Extractor Estimating: 74it [00:56,  1.42it/s]Extractor Estimating: 75it [00:57,  1.43it/s]Extractor Estimating: 76it [00:58,  1.46it/s]Extractor Estimating: 77it [00:59,  1.38it/s]Extractor Estimating: 78it [00:59,  1.45it/s]Extractor Estimating: 79it [01:00,  1.51it/s]Extractor Estimating: 80it [01:00,  1.49it/s]Extractor Estimating: 81it [01:01,  1.52it/s]Extractor Estimating: 82it [01:02,  1.50it/s]Extractor Estimating: 83it [01:02,  1.48it/s]Extractor Estimating: 84it [01:03,  1.49it/s]Extractor Estimating: 85it [01:04,  1.44it/s]Extractor Estimating: 86it [01:05,  1.43it/s]Extractor Estimating: 87it [01:05,  1.36it/s]Extractor Estimating: 88it [01:06,  1.42it/s]Extractor Estimating: 89it [01:07,  1.48it/s]Extractor Estimating: 90it [01:07,  1.52it/s]Extractor Estimating: 91it [01:08,  1.49it/s]Extractor Estimating: 92it [01:09,  1.46it/s]Extractor Estimating: 93it [01:09,  1.52it/s]Extractor Estimating: 94it [01:10,  1.51it/s]Extractor Estimating: 95it [01:11,  1.54it/s]Extractor Estimating: 96it [01:11,  1.59it/s]Extractor Estimating: 97it [01:12,  1.53it/s]Extractor Estimating: 98it [01:12,  1.55it/s]Extractor Estimating: 99it [01:13,  1.51it/s]Extractor Estimating: 100it [01:14,  1.50it/s]Extractor Estimating: 101it [01:14,  1.62it/s]Extractor Estimating: 102it [01:15,  1.64it/s]Extractor Estimating: 103it [01:16,  1.61it/s]Extractor Estimating: 104it [01:16,  1.69it/s]Extractor Estimating: 105it [01:17,  1.72it/s]Extractor Estimating: 106it [01:17,  1.79it/s]Extractor Estimating: 107it [01:18,  1.78it/s]Extractor Estimating: 108it [01:19,  1.38it/s]Extractor Estimating: 109it [01:19,  1.53it/s]Extractor Estimating: 110it [01:20,  1.61it/s]Extractor Estimating: 111it [01:20,  1.68it/s]Extractor Estimating: 112it [01:21,  1.68it/s]Extractor Estimating: 113it [01:22,  1.71it/s]Extractor Estimating: 114it [01:22,  1.72it/s]Extractor Estimating: 115it [01:23,  1.78it/s]Extractor Estimating: 116it [01:23,  1.77it/s]Extractor Estimating: 117it [01:24,  1.73it/s]Extractor Estimating: 118it [01:24,  1.78it/s]Extractor Estimating: 119it [01:25,  1.65it/s]Extractor Estimating: 120it [01:26,  1.69it/s]Extractor Estimating: 121it [01:26,  1.75it/s]Extractor Estimating: 122it [01:27,  1.73it/s]Extractor Estimating: 123it [01:27,  1.77it/s]Extractor Estimating: 124it [01:28,  1.63it/s]Extractor Estimating: 125it [01:29,  1.61it/s]Extractor Estimating: 126it [01:29,  1.61it/s]Extractor Estimating: 127it [01:30,  1.60it/s]Extractor Estimating: 128it [01:31,  1.56it/s]Extractor Estimating: 129it [01:31,  1.58it/s]Extractor Estimating: 130it [01:32,  1.52it/s]Extractor Estimating: 131it [01:32,  1.59it/s]Extractor Estimating: 132it [01:33,  1.57it/s]Extractor Estimating: 133it [01:34,  1.58it/s]Extractor Estimating: 134it [01:34,  1.64it/s]Extractor Estimating: 135it [01:35,  1.57it/s]Extractor Estimating: 136it [01:36,  1.63it/s]Extractor Estimating: 137it [01:36,  1.57it/s]Extractor Estimating: 138it [01:37,  1.65it/s]Extractor Estimating: 139it [01:38,  1.56it/s]Extractor Estimating: 140it [01:38,  1.48it/s]Extractor Estimating: 141it [01:39,  1.54it/s]Extractor Estimating: 142it [01:39,  1.55it/s]Extractor Estimating: 143it [01:40,  1.59it/s]Extractor Estimating: 144it [01:41,  1.58it/s]Extractor Estimating: 145it [01:41,  1.56it/s]Extractor Estimating: 146it [01:42,  1.60it/s]Extractor Estimating: 147it [01:43,  1.64it/s]Extractor Estimating: 148it [01:43,  1.54it/s]Extractor Estimating: 149it [01:44,  1.61it/s]Extractor Estimating: 150it [01:44,  1.58it/s]Extractor Estimating: 151it [01:45,  1.53it/s]Extractor Estimating: 152it [01:46,  1.49it/s]Extractor Estimating: 153it [01:47,  1.37it/s]Extractor Estimating: 154it [01:48,  1.37it/s]Extractor Estimating: 155it [01:48,  1.40it/s]Extractor Estimating: 156it [01:49,  1.36it/s]Extractor Estimating: 157it [01:50,  1.30it/s]Extractor Estimating: 158it [01:51,  1.31it/s]Extractor Estimating: 159it [01:51,  1.34it/s]Extractor Estimating: 160it [01:52,  1.37it/s]Extractor Estimating: 161it [01:53,  1.32it/s]Extractor Estimating: 162it [01:53,  1.38it/s]Extractor Estimating: 163it [01:54,  1.35it/s]Extractor Estimating: 164it [01:55,  1.33it/s]Extractor Estimating: 165it [01:56,  1.32it/s]Extractor Estimating: 166it [01:56,  1.33it/s]Extractor Estimating: 167it [01:57,  1.22it/s]Extractor Estimating: 168it [01:58,  1.30it/s]Extractor Estimating: 169it [01:59,  1.32it/s]Extractor Estimating: 170it [02:00,  1.32it/s]Extractor Estimating: 171it [02:00,  1.30it/s]Extractor Estimating: 172it [02:01,  1.36it/s]Extractor Estimating: 173it [02:02,  1.32it/s]Extractor Estimating: 174it [02:02,  1.39it/s]Extractor Estimating: 175it [02:03,  1.41it/s]Extractor Estimating: 176it [02:04,  1.47it/s]Extractor Estimating: 177it [02:04,  1.58it/s]Extractor Estimating: 178it [02:05,  1.53it/s]Extractor Estimating: 179it [02:06,  1.59it/s]Extractor Estimating: 180it [02:06,  1.61it/s]Extractor Estimating: 181it [02:07,  1.63it/s]Extractor Estimating: 182it [02:07,  1.66it/s]Extractor Estimating: 183it [02:08,  1.62it/s]Extractor Estimating: 184it [02:09,  1.66it/s]Extractor Estimating: 185it [02:09,  1.68it/s]Extractor Estimating: 186it [02:10,  1.72it/s]Extractor Estimating: 187it [02:10,  1.66it/s]Extractor Estimating: 188it [02:11,  1.66it/s]Extractor Estimating: 189it [02:12,  1.71it/s]Extractor Estimating: 190it [02:12,  1.74it/s]Extractor Estimating: 191it [02:13,  1.76it/s]Extractor Estimating: 192it [02:13,  1.77it/s]Extractor Estimating: 193it [02:14,  1.69it/s]Extractor Estimating: 194it [02:14,  1.66it/s]Extractor Estimating: 195it [02:15,  1.69it/s]Extractor Estimating: 196it [02:16,  1.69it/s]Extractor Estimating: 197it [02:16,  1.75it/s]Extractor Estimating: 198it [02:17,  1.52it/s]Extractor Estimating: 199it [02:18,  1.58it/s]Extractor Estimating: 200it [02:18,  1.62it/s]Extractor Estimating: 201it [02:19,  1.65it/s]Extractor Estimating: 202it [02:19,  1.65it/s]Extractor Estimating: 203it [02:20,  1.60it/s]Extractor Estimating: 204it [02:21,  1.60it/s]Extractor Estimating: 205it [02:21,  1.61it/s]Extractor Estimating: 206it [02:22,  1.71it/s]Extractor Estimating: 207it [02:22,  1.69it/s]Extractor Estimating: 208it [02:23,  1.56it/s]Extractor Estimating: 209it [02:24,  1.61it/s]Extractor Estimating: 210it [02:24,  1.67it/s]Extractor Estimating: 211it [02:25,  1.71it/s]Extractor Estimating: 212it [02:25,  1.71it/s]Extractor Estimating: 213it [02:26,  1.67it/s]Extractor Estimating: 214it [02:27,  1.66it/s]Extractor Estimating: 215it [02:27,  1.70it/s]Extractor Estimating: 216it [02:28,  1.68it/s]Extractor Estimating: 217it [02:29,  1.51it/s]Extractor Estimating: 218it [02:29,  1.50it/s]Extractor Estimating: 219it [02:30,  1.53it/s]Extractor Estimating: 220it [02:31,  1.53it/s]Extractor Estimating: 221it [02:31,  1.55it/s]Extractor Estimating: 222it [02:32,  1.60it/s]Extractor Estimating: 223it [02:32,  1.52it/s]Extractor Estimating: 224it [02:33,  1.54it/s]Extractor Estimating: 225it [02:34,  1.49it/s]Extractor Estimating: 226it [02:35,  1.43it/s]Extractor Estimating: 227it [02:35,  1.50it/s]Extractor Estimating: 228it [02:36,  1.47it/s]Extractor Estimating: 229it [02:37,  1.49it/s]Extractor Estimating: 230it [02:37,  1.49it/s]Extractor Estimating: 231it [02:38,  1.50it/s]Extractor Estimating: 232it [02:39,  1.50it/s]Extractor Estimating: 233it [02:40,  1.32it/s]Extractor Estimating: 234it [02:40,  1.40it/s]Extractor Estimating: 235it [02:41,  1.45it/s]Extractor Estimating: 236it [02:41,  1.45it/s]Extractor Estimating: 237it [02:42,  1.46it/s]Extractor Estimating: 238it [02:43,  1.44it/s]Extractor Estimating: 239it [02:43,  1.48it/s]Extractor Estimating: 240it [02:44,  1.46it/s]Extractor Estimating: 241it [02:45,  1.45it/s]Extractor Estimating: 242it [02:46,  1.43it/s]Extractor Estimating: 243it [02:46,  1.37it/s]Extractor Estimating: 244it [02:47,  1.41it/s]Extractor Estimating: 245it [02:48,  1.17it/s]Extractor Estimating: 246it [02:49,  1.27it/s]Extractor Estimating: 247it [02:49,  1.38it/s]Extractor Estimating: 248it [02:50,  1.39it/s]Extractor Estimating: 248it [02:50,  1.45it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:41,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:41,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:41,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:41,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:41,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:29:43,393 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:29:43,394 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:29:44,783 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:29:46,160 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:29:46,160 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:50,489 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:50,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:50,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:50,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:29:50,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:29:52,024 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:29:52,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:29:52,885 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:29:53,186 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:29:53,186 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:06:25,887 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:06:26,266 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4954 mean pseudo reward: 0.9471209830308479
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl'}
train vocab size: 14377
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14477, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14477, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.089, loss:598.2107
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.163, loss:558.5896
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 93, avg_time 1.110, loss:507.1427
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 193, avg_time 1.107, loss:522.3012
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 1.112, loss:482.3673
>> valid entity prec:0.5759, rec:0.6351, f1:0.6041
>> valid relation prec:0.3859, rec:0.1947, f1:0.2588
>> valid relation with NER prec:0.3859, rec:0.1947, f1:0.2588
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.594, loss:468.8678
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 79, avg_time 1.099, loss:448.7924
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 179, avg_time 1.110, loss:455.1955
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 1.107, loss:418.2721
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 1.107, loss:464.8202
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5797, rec:0.5977, f1:0.5886
>> valid relation prec:0.3309, rec:0.1852, f1:0.2375
>> valid relation with NER prec:0.3309, rec:0.1852, f1:0.2375
g_step 1100, step 65, avg_time 2.563, loss:441.6897
g_step 1200, step 165, avg_time 1.126, loss:434.9843
g_step 1300, step 58, avg_time 1.092, loss:412.5837
g_step 1400, step 158, avg_time 1.119, loss:406.2353
g_step 1500, step 51, avg_time 1.083, loss:397.2710
>> valid entity prec:0.5730, rec:0.5747, f1:0.5738
>> valid relation prec:0.3620, rec:0.2047, f1:0.2615
>> valid relation with NER prec:0.3620, rec:0.2047, f1:0.2615
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 151, avg_time 2.596, loss:379.0018
g_step 1700, step 44, avg_time 1.102, loss:374.5695
g_step 1800, step 144, avg_time 1.115, loss:378.8736
g_step 1900, step 37, avg_time 1.099, loss:350.3172
g_step 2000, step 137, avg_time 1.102, loss:345.3803
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6287, rec:0.5342, f1:0.5776
>> valid relation prec:0.3594, rec:0.1855, f1:0.2447
>> valid relation with NER prec:0.3594, rec:0.1855, f1:0.2447
g_step 2100, step 30, avg_time 2.593, loss:351.1513
g_step 2200, step 130, avg_time 1.099, loss:339.1411
g_step 2300, step 23, avg_time 1.107, loss:331.7869
g_step 2400, step 123, avg_time 1.112, loss:317.8318
g_step 2500, step 16, avg_time 1.100, loss:309.0076
>> valid entity prec:0.5983, rec:0.5774, f1:0.5877
>> valid relation prec:0.3578, rec:0.2202, f1:0.2726
>> valid relation with NER prec:0.3578, rec:0.2202, f1:0.2726
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 116, avg_time 2.575, loss:295.4570
g_step 2700, step 9, avg_time 1.118, loss:326.9468
g_step 2800, step 109, avg_time 1.120, loss:296.1451
g_step 2900, step 2, avg_time 1.097, loss:288.1524
g_step 3000, step 102, avg_time 1.114, loss:275.9875
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5650, rec:0.5822, f1:0.5735
>> valid relation prec:0.3143, rec:0.2070, f1:0.2496
>> valid relation with NER prec:0.3143, rec:0.2070, f1:0.2496
g_step 3100, step 202, avg_time 2.583, loss:295.7818
g_step 3200, step 95, avg_time 1.123, loss:266.5054
g_step 3300, step 195, avg_time 1.086, loss:279.0172
g_step 3400, step 88, avg_time 1.105, loss:259.7226
g_step 3500, step 188, avg_time 1.095, loss:277.7726
>> valid entity prec:0.5623, rec:0.6282, f1:0.5934
>> valid relation prec:0.3279, rec:0.2579, f1:0.2887
>> valid relation with NER prec:0.3279, rec:0.2579, f1:0.2887
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 81, avg_time 2.637, loss:247.9309
g_step 3700, step 181, avg_time 1.132, loss:278.5970
g_step 3800, step 74, avg_time 1.098, loss:247.6049
g_step 3900, step 174, avg_time 1.111, loss:245.7687
g_step 4000, step 67, avg_time 1.108, loss:254.6979
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5777, rec:0.5921, f1:0.5848
>> valid relation prec:0.3211, rec:0.2204, f1:0.2614
>> valid relation with NER prec:0.3211, rec:0.2204, f1:0.2614
g_step 4100, step 167, avg_time 2.593, loss:253.5200
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:06:26 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:06:26 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-06-25_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:06:27 - WARNING - datasets.builder -   Using custom data configuration default-1c65ff9b5b6b67f3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1c65ff9b5b6b67f3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:06:32,382 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:06:32,383 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:06:32,383 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:06:32,384 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:06:32,488 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:06:32,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:06:33,996 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:06:37,262 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:06:37,377 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1c65ff9b5b6b67f3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.16ba/s] 40%|████      | 2/5 [00:00<00:00,  4.16ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.64ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.05ba/s]100%|██████████| 5/5 [00:01<00:00,  4.44ba/s]100%|██████████| 5/5 [00:01<00:00,  4.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.75ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.66ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.08ba/s]100%|██████████| 4/4 [00:00<00:00,  5.23ba/s]100%|██████████| 4/4 [00:00<00:00,  4.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:04,  1.11s/ba] 60%|██████    | 3/5 [00:01<00:00,  2.79ba/s]100%|██████████| 5/5 [00:01<00:00,  4.51ba/s]100%|██████████| 5/5 [00:01<00:00,  3.36ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.28ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.66ba/s]100%|██████████| 4/4 [00:00<00:00,  8.50ba/s]
[INFO|trainer.py:414] 2023-08-28 19:06:45,280 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:06:45,604 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:06:45,604 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 19:06:45,604 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:06:45,604 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:06:45,604 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:06:45,604 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:06:45,604 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.30it/s]  1%|          | 2/390 [00:00<01:54,  3.40it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:51,  3.45it/s]  1%|▏         | 5/390 [00:01<01:51,  3.46it/s]  2%|▏         | 6/390 [00:01<01:50,  3.46it/s]  2%|▏         | 7/390 [00:02<01:51,  3.43it/s]  2%|▏         | 8/390 [00:02<01:52,  3.40it/s]  2%|▏         | 9/390 [00:02<01:52,  3.39it/s]  3%|▎         | 10/390 [00:02<01:52,  3.38it/s]  3%|▎         | 11/390 [00:03<01:52,  3.37it/s]  3%|▎         | 12/390 [00:03<01:52,  3.37it/s]  3%|▎         | 13/390 [00:03<01:51,  3.37it/s]  4%|▎         | 14/390 [00:04<02:37,  2.38it/s]  4%|▍         | 15/390 [00:04<02:23,  2.61it/s]  4%|▍         | 16/390 [00:05<02:13,  2.80it/s]  4%|▍         | 17/390 [00:05<02:06,  2.95it/s]  5%|▍         | 18/390 [00:05<02:01,  3.06it/s]  5%|▍         | 19/390 [00:06<01:58,  3.14it/s]  5%|▌         | 20/390 [00:06<01:55,  3.20it/s]  5%|▌         | 21/390 [00:06<01:53,  3.25it/s]  6%|▌         | 22/390 [00:06<01:52,  3.28it/s]  6%|▌         | 23/390 [00:07<01:50,  3.31it/s]  6%|▌         | 24/390 [00:07<01:49,  3.33it/s]  6%|▋         | 25/390 [00:07<01:49,  3.34it/s]  7%|▋         | 26/390 [00:08<01:48,  3.35it/s]  7%|▋         | 27/390 [00:08<01:48,  3.35it/s]  7%|▋         | 28/390 [00:08<01:48,  3.35it/s]  7%|▋         | 29/390 [00:08<01:47,  3.35it/s]  8%|▊         | 30/390 [00:09<01:47,  3.35it/s]  8%|▊         | 31/390 [00:09<01:47,  3.35it/s]  8%|▊         | 32/390 [00:09<01:46,  3.36it/s]  8%|▊         | 33/390 [00:10<01:46,  3.36it/s]  9%|▊         | 34/390 [00:10<01:45,  3.36it/s]  9%|▉         | 35/390 [00:10<01:45,  3.36it/s]  9%|▉         | 36/390 [00:11<01:45,  3.36it/s]  9%|▉         | 37/390 [00:11<01:45,  3.36it/s] 10%|▉         | 38/390 [00:11<01:44,  3.36it/s] 10%|█         | 39/390 [00:11<01:44,  3.36it/s] 10%|█         | 40/390 [00:12<01:44,  3.36it/s] 11%|█         | 41/390 [00:12<01:43,  3.36it/s] 11%|█         | 42/390 [00:12<01:43,  3.36it/s] 11%|█         | 43/390 [00:13<01:43,  3.36it/s] 11%|█▏        | 44/390 [00:13<01:43,  3.36it/s] 12%|█▏        | 45/390 [00:13<01:42,  3.36it/s] 12%|█▏        | 46/390 [00:14<01:42,  3.36it/s] 12%|█▏        | 47/390 [00:14<01:42,  3.36it/s] 12%|█▏        | 48/390 [00:14<01:41,  3.36it/s] 13%|█▎        | 49/390 [00:14<01:41,  3.36it/s] 13%|█▎        | 50/390 [00:15<01:41,  3.36it/s] 13%|█▎        | 51/390 [00:16<02:31,  2.23it/s] 13%|█▎        | 52/390 [00:16<02:16,  2.48it/s] 14%|█▎        | 53/390 [00:16<02:05,  2.69it/s] 14%|█▍        | 54/390 [00:16<01:57,  2.86it/s] 14%|█▍        | 55/390 [00:17<01:52,  2.99it/s] 14%|█▍        | 56/390 [00:17<01:48,  3.09it/s] 15%|█▍        | 57/390 [00:17<01:45,  3.16it/s] 15%|█▍        | 58/390 [00:18<01:43,  3.21it/s] 15%|█▌        | 59/390 [00:18<01:41,  3.25it/s] 15%|█▌        | 60/390 [00:18<01:40,  3.28it/s] 16%|█▌        | 61/390 [00:19<01:39,  3.30it/s] 16%|█▌        | 62/390 [00:19<01:39,  3.31it/s] 16%|█▌        | 63/390 [00:19<01:38,  3.32it/s] 16%|█▋        | 64/390 [00:19<01:37,  3.33it/s] 17%|█▋        | 65/390 [00:20<01:37,  3.34it/s] 17%|█▋        | 66/390 [00:20<01:36,  3.34it/s] 17%|█▋        | 67/390 [00:20<01:36,  3.35it/s] 17%|█▋        | 68/390 [00:21<01:36,  3.35it/s] 18%|█▊        | 69/390 [00:21<01:35,  3.35it/s] 18%|█▊        | 70/390 [00:21<01:35,  3.35it/s] 18%|█▊        | 71/390 [00:22<01:35,  3.35it/s] 18%|█▊        | 72/390 [00:22<01:34,  3.35it/s] 19%|█▊        | 73/390 [00:22<01:34,  3.35it/s] 19%|█▉        | 74/390 [00:22<01:34,  3.35it/s] 19%|█▉        | 75/390 [00:23<01:34,  3.35it/s] 19%|█▉        | 76/390 [00:23<01:33,  3.35it/s] 20%|█▉        | 77/390 [00:23<01:33,  3.36it/s] 20%|██        | 78/390 [00:24<01:31,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 19:07:09,737 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:07:09,737 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:07:09,737 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.09it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.61it/s][A
  4%|▍         | 17/438 [00:00<00:09, 46.70it/s][A
  5%|▌         | 22/438 [00:00<00:09, 45.79it/s][A
  6%|▌         | 27/438 [00:00<00:09, 45.31it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.25it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.12it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.86it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.88it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.92it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.96it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.90it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.79it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.70it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.70it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.66it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.67it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.70it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.85it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.89it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.94it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.79it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.63it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.76it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.67it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.63it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.71it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.82it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.87it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.94it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.84it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.80it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.75it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.67it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.60it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.59it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.69it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.79it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.89it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.85it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.85it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.72it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.70it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.66it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.69it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.83it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.91it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.83it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.86it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.82it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.70it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.56it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.74it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.62it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.81it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.79it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.84it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.85it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.76it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.69it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.65it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.65it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.54it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.65it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.71it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.74it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.82it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.75it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.71it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.70it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.69it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.66it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.69it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.78it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.87it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.94it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.78it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.72it/s][A
 92%|█████████▏| 402/438 [00:08<00:00, 44.72it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.65it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.67it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.67it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.70it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.79it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.80it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.79it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 44.79it/s][A 20%|██        | 78/390 [00:33<01:31,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:07:19,952 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 19:07:20,443 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:07:27,316 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:07:27,799 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:07:27,967 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:01<58:27, 11.28s/it] 21%|██        | 80/390 [01:01<41:21,  8.00s/it] 21%|██        | 81/390 [01:01<29:18,  5.69s/it] 21%|██        | 82/390 [01:01<20:54,  4.07s/it] 21%|██▏       | 83/390 [01:02<15:02,  2.94s/it] 22%|██▏       | 84/390 [01:02<10:57,  2.15s/it] 22%|██▏       | 85/390 [01:02<08:05,  1.59s/it] 22%|██▏       | 86/390 [01:03<06:05,  1.20s/it] 22%|██▏       | 87/390 [01:03<04:42,  1.07it/s] 23%|██▎       | 88/390 [01:03<03:43,  1.35it/s] 23%|██▎       | 89/390 [01:04<03:03,  1.64it/s] 23%|██▎       | 90/390 [01:04<02:43,  1.84it/s] 23%|██▎       | 91/390 [01:04<02:20,  2.13it/s] 24%|██▎       | 92/390 [01:05<02:04,  2.39it/s] 24%|██▍       | 93/390 [01:05<01:53,  2.62it/s] 24%|██▍       | 94/390 [01:05<01:45,  2.80it/s] 24%|██▍       | 95/390 [01:05<01:40,  2.95it/s] 25%|██▍       | 96/390 [01:06<01:36,  3.06it/s] 25%|██▍       | 97/390 [01:06<01:33,  3.14it/s] 25%|██▌       | 98/390 [01:06<01:31,  3.21it/s] 25%|██▌       | 99/390 [01:07<01:29,  3.25it/s] 26%|██▌       | 100/390 [01:07<01:41,  2.86it/s] 26%|██▌       | 101/390 [01:07<01:36,  2.99it/s] 26%|██▌       | 102/390 [01:08<01:33,  3.09it/s] 26%|██▋       | 103/390 [01:08<01:30,  3.16it/s] 27%|██▋       | 104/390 [01:08<01:28,  3.21it/s] 27%|██▋       | 105/390 [01:09<01:27,  3.25it/s] 27%|██▋       | 106/390 [01:09<01:26,  3.28it/s] 27%|██▋       | 107/390 [01:09<01:25,  3.31it/s] 28%|██▊       | 108/390 [01:09<01:24,  3.32it/s] 28%|██▊       | 109/390 [01:10<01:24,  3.33it/s] 28%|██▊       | 110/390 [01:10<01:31,  3.06it/s] 28%|██▊       | 111/390 [01:10<01:28,  3.14it/s] 29%|██▊       | 112/390 [01:11<01:26,  3.20it/s] 29%|██▉       | 113/390 [01:11<01:25,  3.25it/s] 29%|██▉       | 114/390 [01:11<01:24,  3.28it/s] 29%|██▉       | 115/390 [01:12<01:23,  3.30it/s] 30%|██▉       | 116/390 [01:12<01:22,  3.31it/s] 30%|███       | 117/390 [01:12<01:22,  3.33it/s] 30%|███       | 118/390 [01:13<01:21,  3.33it/s] 31%|███       | 119/390 [01:13<01:21,  3.34it/s] 31%|███       | 120/390 [01:13<01:27,  3.10it/s] 31%|███       | 121/390 [01:13<01:24,  3.17it/s] 31%|███▏      | 122/390 [01:14<01:23,  3.22it/s] 32%|███▏      | 123/390 [01:14<01:21,  3.26it/s] 32%|███▏      | 124/390 [01:14<01:20,  3.29it/s] 32%|███▏      | 125/390 [01:15<01:20,  3.30it/s] 32%|███▏      | 126/390 [01:15<01:19,  3.31it/s] 33%|███▎      | 127/390 [01:15<01:33,  2.82it/s] 33%|███▎      | 128/390 [01:16<01:28,  2.96it/s] 33%|███▎      | 129/390 [01:16<01:25,  3.07it/s] 33%|███▎      | 130/390 [01:16<01:25,  3.02it/s] 34%|███▎      | 131/390 [01:17<01:28,  2.93it/s] 34%|███▍      | 132/390 [01:17<01:24,  3.05it/s] 34%|███▍      | 133/390 [01:17<01:22,  3.13it/s] 34%|███▍      | 134/390 [01:18<01:20,  3.19it/s] 35%|███▍      | 135/390 [01:18<01:18,  3.24it/s] 35%|███▍      | 136/390 [01:18<01:17,  3.28it/s] 35%|███▌      | 137/390 [01:19<01:16,  3.30it/s] 35%|███▌      | 138/390 [01:19<01:16,  3.32it/s] 36%|███▌      | 139/390 [01:19<01:15,  3.33it/s] 36%|███▌      | 140/390 [01:19<01:14,  3.33it/s] 36%|███▌      | 141/390 [01:20<01:24,  2.96it/s] 36%|███▋      | 142/390 [01:20<01:20,  3.07it/s] 37%|███▋      | 143/390 [01:20<01:18,  3.15it/s] 37%|███▋      | 144/390 [01:21<01:16,  3.20it/s] 37%|███▋      | 145/390 [01:21<01:15,  3.25it/s] 37%|███▋      | 146/390 [01:21<01:14,  3.27it/s] 38%|███▊      | 147/390 [01:22<01:13,  3.29it/s] 38%|███▊      | 148/390 [01:22<01:13,  3.31it/s] 38%|███▊      | 149/390 [01:22<01:12,  3.32it/s] 38%|███▊      | 150/390 [01:23<01:12,  3.32it/s] 39%|███▊      | 151/390 [01:23<01:41,  2.35it/s] 39%|███▉      | 152/390 [01:24<01:32,  2.58it/s] 39%|███▉      | 153/390 [01:24<01:25,  2.77it/s] 39%|███▉      | 154/390 [01:24<01:20,  2.92it/s] 40%|███▉      | 155/390 [01:24<01:17,  3.03it/s] 40%|████      | 156/390 [01:25<01:14,  3.12it/s][INFO|trainer.py:2140] 2023-08-28 19:08:10,945 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:08:10,945 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:08:10,945 >>   Batch size = 8
{'eval_loss': 1.0277519226074219, 'eval_runtime': 9.7823, 'eval_samples_per_second': 357.583, 'eval_steps_per_second': 44.775, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.66it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.40it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.34it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.67it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.30it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.00it/s][A
 10%|▉         | 42/438 [00:00<00:10, 36.42it/s][A
 11%|█         | 47/438 [00:01<00:10, 38.79it/s][A
 12%|█▏        | 52/438 [00:01<00:09, 40.55it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 41.89it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 42.92it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 43.65it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.05it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.33it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.09it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.09it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.29it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.56it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.80it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.93it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.88it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.77it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.51it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.35it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.54it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.68it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.69it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.81it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.86it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.92it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.72it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.52it/s][A
 40%|████      | 177/438 [00:04<00:07, 35.99it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 38.36it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 40.14it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 41.55it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 42.61it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.33it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.76it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.09it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 43.97it/s][A
 51%|█████     | 222/438 [00:05<00:04, 43.90it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.22it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.65it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.85it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.93it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.86it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.79it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.61it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.51it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.53it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.65it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.75it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.87it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.96it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.95it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.87it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.55it/s][A
 71%|███████   | 312/438 [00:07<00:04, 27.43it/s][A
 72%|███████▏  | 317/438 [00:07<00:03, 31.18it/s][A
 74%|███████▎  | 322/438 [00:07<00:03, 34.39it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 37.11it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 39.26it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 40.85it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 42.13it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 42.92it/s][A
 80%|████████  | 352/438 [00:08<00:01, 43.13it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.21it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.45it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 43.87it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.20it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.54it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.68it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.93it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 28.21it/s][A
 91%|█████████ | 397/438 [00:09<00:01, 31.95it/s][A
 92%|█████████▏| 402/438 [00:09<00:01, 35.04it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 37.59it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 39.62it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 41.05it/s][A
 96%|█████████▋| 422/438 [00:10<00:00, 42.23it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 37.57it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 39.40it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 41.01it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 41.01it/s][A 40%|████      | 156/390 [01:35<01:14,  3.12it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:08:22,933 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 19:08:23,539 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:08:34,950 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:08:35,593 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:08:35,971 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:06<48:29, 12.48s/it] 41%|████      | 158/390 [02:06<34:11,  8.84s/it] 41%|████      | 159/390 [02:06<24:10,  6.28s/it] 41%|████      | 160/390 [02:07<17:11,  4.48s/it] 41%|████▏     | 161/390 [02:07<12:19,  3.23s/it] 42%|████▏     | 162/390 [02:07<08:55,  2.35s/it] 42%|████▏     | 163/390 [02:07<06:33,  1.73s/it] 42%|████▏     | 164/390 [02:08<04:54,  1.30s/it] 42%|████▏     | 165/390 [02:08<03:45,  1.00s/it] 43%|████▎     | 166/390 [02:08<02:56,  1.27it/s] 43%|████▎     | 167/390 [02:09<02:23,  1.56it/s] 43%|████▎     | 168/390 [02:09<02:16,  1.62it/s] 43%|████▎     | 169/390 [02:10<01:55,  1.92it/s] 44%|████▎     | 170/390 [02:10<01:39,  2.20it/s] 44%|████▍     | 171/390 [02:10<01:29,  2.46it/s] 44%|████▍     | 172/390 [02:10<01:21,  2.67it/s] 44%|████▍     | 173/390 [02:11<01:16,  2.85it/s] 45%|████▍     | 174/390 [02:11<01:12,  2.98it/s] 45%|████▍     | 175/390 [02:11<01:09,  3.09it/s] 45%|████▌     | 176/390 [02:12<01:07,  3.16it/s] 45%|████▌     | 177/390 [02:12<01:06,  3.22it/s] 46%|████▌     | 178/390 [02:12<01:16,  2.78it/s] 46%|████▌     | 179/390 [02:13<01:11,  2.94it/s] 46%|████▌     | 180/390 [02:13<01:08,  3.05it/s] 46%|████▋     | 181/390 [02:13<01:06,  3.14it/s] 47%|████▋     | 182/390 [02:14<01:05,  3.20it/s] 47%|████▋     | 183/390 [02:14<01:03,  3.24it/s] 47%|████▋     | 184/390 [02:14<01:02,  3.28it/s] 47%|████▋     | 185/390 [02:14<01:02,  3.30it/s] 48%|████▊     | 186/390 [02:15<01:01,  3.32it/s] 48%|████▊     | 187/390 [02:15<01:00,  3.33it/s] 48%|████▊     | 188/390 [02:15<01:04,  3.15it/s] 48%|████▊     | 189/390 [02:16<01:02,  3.21it/s] 49%|████▊     | 190/390 [02:16<01:01,  3.25it/s] 49%|████▉     | 191/390 [02:16<01:00,  3.28it/s] 49%|████▉     | 192/390 [02:17<00:59,  3.30it/s] 49%|████▉     | 193/390 [02:17<00:59,  3.32it/s] 50%|████▉     | 194/390 [02:17<00:58,  3.33it/s] 50%|█████     | 195/390 [02:18<00:58,  3.34it/s] 50%|█████     | 196/390 [02:18<00:58,  3.34it/s] 51%|█████     | 197/390 [02:18<00:57,  3.35it/s] 51%|█████     | 198/390 [02:18<00:57,  3.36it/s] 51%|█████     | 199/390 [02:19<00:56,  3.36it/s] 51%|█████▏    | 200/390 [02:19<00:56,  3.36it/s] 52%|█████▏    | 201/390 [02:19<00:56,  3.35it/s] 52%|█████▏    | 202/390 [02:20<00:56,  3.35it/s] 52%|█████▏    | 203/390 [02:20<00:55,  3.35it/s] 52%|█████▏    | 204/390 [02:20<00:55,  3.35it/s] 53%|█████▎    | 205/390 [02:20<00:55,  3.36it/s] 53%|█████▎    | 206/390 [02:21<00:54,  3.36it/s] 53%|█████▎    | 207/390 [02:21<00:54,  3.36it/s] 53%|█████▎    | 208/390 [02:22<01:02,  2.91it/s] 54%|█████▎    | 209/390 [02:22<00:59,  3.03it/s] 54%|█████▍    | 210/390 [02:22<00:57,  3.12it/s] 54%|█████▍    | 211/390 [02:22<00:56,  3.19it/s] 54%|█████▍    | 212/390 [02:23<00:54,  3.24it/s] 55%|█████▍    | 213/390 [02:23<00:54,  3.27it/s] 55%|█████▍    | 214/390 [02:23<00:53,  3.30it/s] 55%|█████▌    | 215/390 [02:24<00:52,  3.32it/s] 55%|█████▌    | 216/390 [02:24<00:52,  3.33it/s] 56%|█████▌    | 217/390 [02:24<00:51,  3.34it/s] 56%|█████▌    | 218/390 [02:25<00:54,  3.18it/s] 56%|█████▌    | 219/390 [02:25<00:52,  3.23it/s] 56%|█████▋    | 220/390 [02:25<00:51,  3.27it/s] 57%|█████▋    | 221/390 [02:25<00:51,  3.30it/s] 57%|█████▋    | 222/390 [02:26<00:50,  3.32it/s] 57%|█████▋    | 223/390 [02:26<00:50,  3.33it/s] 57%|█████▋    | 224/390 [02:26<00:49,  3.33it/s] 58%|█████▊    | 225/390 [02:27<00:49,  3.34it/s] 58%|█████▊    | 226/390 [02:27<00:49,  3.35it/s] 58%|█████▊    | 227/390 [02:27<00:48,  3.35it/s] 58%|█████▊    | 228/390 [02:28<00:55,  2.93it/s] 59%|█████▊    | 229/390 [02:28<00:52,  3.05it/s] 59%|█████▉    | 230/390 [02:28<00:51,  3.13it/s] 59%|█████▉    | 231/390 [02:29<00:49,  3.20it/s] 59%|█████▉    | 232/390 [02:29<00:48,  3.24it/s] 60%|█████▉    | 233/390 [02:29<00:47,  3.27it/s] 60%|██████    | 234/390 [02:29<00:47,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 19:09:15,632 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:09:15,632 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:09:15,632 >>   Batch size = 8
{'eval_loss': 1.045719861984253, 'eval_runtime': 10.4375, 'eval_samples_per_second': 335.137, 'eval_steps_per_second': 41.964, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.83it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.70it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.52it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.51it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.77it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.30it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.00it/s][A
 10%|▉         | 42/438 [00:00<00:10, 39.14it/s][A
 11%|█         | 47/438 [00:01<00:09, 40.84it/s][A
 12%|█▏        | 52/438 [00:01<00:09, 42.12it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 43.03it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 43.67it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.09it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.49it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.62it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.40it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.16it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.19it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.41it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.58it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.76it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.91it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.04it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.02it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.79it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.48it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.47it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.56it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.67it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.87it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.95it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.06it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.99it/s][A
 39%|███▉      | 172/438 [00:04<00:05, 44.82it/s][A
 40%|████      | 177/438 [00:04<00:09, 28.17it/s][A
 42%|████▏     | 182/438 [00:04<00:08, 31.85it/s][A
 42%|████▏     | 186/438 [00:04<00:09, 26.52it/s][A
 44%|████▎     | 191/438 [00:04<00:07, 31.01it/s][A
 45%|████▍     | 196/438 [00:04<00:07, 34.35it/s][A
 46%|████▌     | 201/438 [00:04<00:06, 37.04it/s][A
 47%|████▋     | 206/438 [00:04<00:05, 39.20it/s][A
 48%|████▊     | 211/438 [00:05<00:05, 40.78it/s][A
 49%|████▉     | 216/438 [00:05<00:05, 42.08it/s][A
 50%|█████     | 221/438 [00:05<00:05, 42.93it/s][A
 52%|█████▏    | 226/438 [00:05<00:04, 43.49it/s][A
 53%|█████▎    | 231/438 [00:05<00:04, 43.49it/s][A
 54%|█████▍    | 236/438 [00:05<00:04, 43.50it/s][A
 55%|█████▌    | 241/438 [00:05<00:04, 44.22it/s][A
 56%|█████▌    | 246/438 [00:05<00:04, 44.47it/s][A
 57%|█████▋    | 251/438 [00:05<00:04, 44.67it/s][A
 58%|█████▊    | 256/438 [00:06<00:04, 44.77it/s][A
 60%|█████▉    | 261/438 [00:06<00:03, 44.87it/s][A
 61%|██████    | 266/438 [00:06<00:03, 44.86it/s][A
 62%|██████▏   | 271/438 [00:06<00:03, 44.71it/s][A
 63%|██████▎   | 276/438 [00:06<00:03, 44.53it/s][A
 64%|██████▍   | 281/438 [00:06<00:03, 44.40it/s][A
 65%|██████▌   | 286/438 [00:06<00:03, 44.65it/s][A
 66%|██████▋   | 291/438 [00:06<00:03, 44.74it/s][A
 68%|██████▊   | 296/438 [00:07<00:03, 44.91it/s][A
 69%|██████▊   | 301/438 [00:07<00:04, 33.21it/s][A
 70%|██████▉   | 306/438 [00:07<00:03, 36.19it/s][A
 71%|███████   | 311/438 [00:07<00:03, 38.49it/s][A
 72%|███████▏  | 316/438 [00:07<00:03, 40.34it/s][A
 73%|███████▎  | 321/438 [00:07<00:02, 41.73it/s][A
 74%|███████▍  | 326/438 [00:07<00:02, 42.78it/s][A
 76%|███████▌  | 331/438 [00:07<00:02, 43.48it/s][A
 77%|███████▋  | 336/438 [00:07<00:02, 43.79it/s][A
 78%|███████▊  | 341/438 [00:08<00:02, 43.76it/s][A
 79%|███████▉  | 346/438 [00:08<00:02, 43.77it/s][A
 80%|████████  | 351/438 [00:08<00:01, 43.84it/s][A
 81%|████████▏ | 356/438 [00:08<00:01, 44.18it/s][A
 82%|████████▏ | 361/438 [00:08<00:01, 44.49it/s][A
 84%|████████▎ | 366/438 [00:08<00:01, 44.72it/s][A
 85%|████████▍ | 371/438 [00:08<00:01, 44.85it/s][A
 86%|████████▌ | 376/438 [00:08<00:01, 44.90it/s][A
 87%|████████▋ | 381/438 [00:08<00:01, 44.74it/s][A
 88%|████████▊ | 386/438 [00:09<00:01, 44.60it/s][A
 89%|████████▉ | 391/438 [00:09<00:01, 44.39it/s][A
 90%|█████████ | 396/438 [00:09<00:00, 44.35it/s][A
 92%|█████████▏| 401/438 [00:09<00:00, 44.47it/s][A
 93%|█████████▎| 406/438 [00:09<00:00, 44.69it/s][A
 94%|█████████▍| 411/438 [00:09<00:00, 44.92it/s][A
 95%|█████████▍| 416/438 [00:09<00:00, 45.01it/s][A
 96%|█████████▌| 421/438 [00:09<00:00, 45.02it/s][A
 97%|█████████▋| 426/438 [00:10<00:00, 44.82it/s][A
 98%|█████████▊| 431/438 [00:10<00:00, 31.78it/s][A
100%|█████████▉| 436/438 [00:10<00:00, 34.92it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 34.92it/s][A 60%|██████    | 234/390 [02:40<00:47,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:09:27,963 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:09:28,875 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:09:37,253 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:09:37,454 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:09:37,607 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:13<34:39, 13.42s/it] 61%|██████    | 236/390 [03:14<24:23,  9.51s/it] 61%|██████    | 237/390 [03:14<17:11,  6.74s/it] 61%|██████    | 238/390 [03:14<12:11,  4.81s/it] 61%|██████▏   | 239/390 [03:15<08:41,  3.46s/it] 62%|██████▏   | 240/390 [03:15<06:16,  2.51s/it] 62%|██████▏   | 241/390 [03:15<04:35,  1.85s/it] 62%|██████▏   | 242/390 [03:16<03:24,  1.38s/it] 62%|██████▏   | 243/390 [03:16<02:35,  1.06s/it] 63%|██████▎   | 244/390 [03:16<02:01,  1.21it/s] 63%|██████▎   | 245/390 [03:17<01:37,  1.49it/s] 63%|██████▎   | 246/390 [03:17<01:38,  1.47it/s] 63%|██████▎   | 247/390 [03:18<01:21,  1.77it/s] 64%|██████▎   | 248/390 [03:18<01:09,  2.06it/s] 64%|██████▍   | 249/390 [03:18<01:00,  2.33it/s] 64%|██████▍   | 250/390 [03:18<00:54,  2.56it/s] 64%|██████▍   | 251/390 [03:19<00:50,  2.76it/s] 65%|██████▍   | 252/390 [03:19<00:47,  2.92it/s] 65%|██████▍   | 253/390 [03:19<00:45,  3.04it/s] 65%|██████▌   | 254/390 [03:20<00:45,  2.97it/s] 65%|██████▌   | 255/390 [03:20<00:43,  3.07it/s] 66%|██████▌   | 256/390 [03:20<00:42,  3.15it/s] 66%|██████▌   | 257/390 [03:21<00:41,  3.21it/s] 66%|██████▌   | 258/390 [03:21<00:40,  3.25it/s] 66%|██████▋   | 259/390 [03:21<00:39,  3.28it/s] 67%|██████▋   | 260/390 [03:21<00:39,  3.30it/s] 67%|██████▋   | 261/390 [03:22<00:38,  3.32it/s] 67%|██████▋   | 262/390 [03:22<00:38,  3.33it/s] 67%|██████▋   | 263/390 [03:22<00:38,  3.34it/s] 68%|██████▊   | 264/390 [03:23<00:41,  3.05it/s] 68%|██████▊   | 265/390 [03:23<00:39,  3.14it/s] 68%|██████▊   | 266/390 [03:23<00:38,  3.20it/s] 68%|██████▊   | 267/390 [03:24<00:37,  3.24it/s] 69%|██████▊   | 268/390 [03:24<00:37,  3.28it/s] 69%|██████▉   | 269/390 [03:24<00:36,  3.30it/s] 69%|██████▉   | 270/390 [03:25<00:36,  3.32it/s] 69%|██████▉   | 271/390 [03:25<00:35,  3.34it/s] 70%|██████▉   | 272/390 [03:25<00:35,  3.33it/s] 70%|███████   | 273/390 [03:25<00:35,  3.34it/s] 70%|███████   | 274/390 [03:26<00:36,  3.16it/s] 71%|███████   | 275/390 [03:26<00:35,  3.21it/s] 71%|███████   | 276/390 [03:26<00:35,  3.25it/s] 71%|███████   | 277/390 [03:27<00:34,  3.28it/s] 71%|███████▏  | 278/390 [03:27<00:33,  3.30it/s] 72%|███████▏  | 279/390 [03:27<00:33,  3.32it/s] 72%|███████▏  | 280/390 [03:28<00:32,  3.33it/s] 72%|███████▏  | 281/390 [03:28<00:32,  3.34it/s] 72%|███████▏  | 282/390 [03:28<00:32,  3.35it/s] 73%|███████▎  | 283/390 [03:28<00:31,  3.35it/s] 73%|███████▎  | 284/390 [03:29<00:34,  3.10it/s] 73%|███████▎  | 285/390 [03:29<00:33,  3.17it/s] 73%|███████▎  | 286/390 [03:29<00:32,  3.22it/s] 74%|███████▎  | 287/390 [03:30<00:31,  3.25it/s] 74%|███████▍  | 288/390 [03:30<00:31,  3.28it/s] 74%|███████▍  | 289/390 [03:30<00:30,  3.30it/s] 74%|███████▍  | 290/390 [03:31<00:30,  3.31it/s] 75%|███████▍  | 291/390 [03:31<00:29,  3.32it/s] 75%|███████▍  | 292/390 [03:31<00:29,  3.33it/s] 75%|███████▌  | 293/390 [03:32<00:29,  3.34it/s] 75%|███████▌  | 294/390 [03:32<00:37,  2.58it/s] 76%|███████▌  | 295/390 [03:32<00:34,  2.77it/s] 76%|███████▌  | 296/390 [03:33<00:32,  2.92it/s] 76%|███████▌  | 297/390 [03:33<00:30,  3.03it/s] 76%|███████▋  | 298/390 [03:33<00:29,  3.11it/s] 77%|███████▋  | 299/390 [03:34<00:28,  3.18it/s] 77%|███████▋  | 300/390 [03:35<00:46,  1.93it/s] 77%|███████▋  | 301/390 [03:35<00:45,  1.96it/s] 77%|███████▋  | 302/390 [03:35<00:39,  2.24it/s] 78%|███████▊  | 303/390 [03:36<00:35,  2.49it/s] 78%|███████▊  | 304/390 [03:36<00:31,  2.70it/s] 78%|███████▊  | 305/390 [03:36<00:29,  2.87it/s] 78%|███████▊  | 306/390 [03:37<00:28,  3.00it/s] 79%|███████▊  | 307/390 [03:37<00:26,  3.10it/s] 79%|███████▉  | 308/390 [03:37<00:25,  3.17it/s] 79%|███████▉  | 309/390 [03:38<00:25,  3.22it/s] 79%|███████▉  | 310/390 [03:38<00:24,  3.26it/s] 80%|███████▉  | 311/390 [03:38<00:28,  2.73it/s] 80%|████████  | 312/390 [03:39<00:26,  2.89it/s][INFO|trainer.py:2140] 2023-08-28 19:10:24,791 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:10:24,791 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:10:24,791 >>   Batch size = 8
{'eval_loss': 1.0675303936004639, 'eval_runtime': 10.4376, 'eval_samples_per_second': 335.136, 'eval_steps_per_second': 41.964, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.40it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.81it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.44it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.61it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.85it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.37it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.02it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.74it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.76it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.91it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.99it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.05it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.05it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.00it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.98it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.72it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.56it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.61it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.67it/s][A
 23%|██▎       | 102/438 [00:02<00:13, 24.48it/s][A
 24%|██▍       | 107/438 [00:02<00:11, 28.38it/s][A
 26%|██▌       | 112/438 [00:02<00:10, 31.93it/s][A
 27%|██▋       | 117/438 [00:02<00:09, 35.05it/s][A
 28%|██▊       | 122/438 [00:03<00:08, 37.60it/s][A
 29%|██▉       | 127/438 [00:03<00:07, 39.60it/s][A
 30%|███       | 132/438 [00:03<00:07, 41.15it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 42.17it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 42.54it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.11it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.62it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.96it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.38it/s][A
 38%|███▊      | 167/438 [00:04<00:06, 44.59it/s][A
 39%|███▉      | 172/438 [00:04<00:05, 44.73it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.85it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.62it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.37it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.47it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.66it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.79it/s][A
 48%|████▊     | 212/438 [00:05<00:05, 44.90it/s][A
 50%|████▉     | 217/438 [00:05<00:04, 44.96it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.92it/s][A
 52%|█████▏    | 227/438 [00:05<00:09, 23.03it/s][A
 53%|█████▎    | 232/438 [00:05<00:07, 27.04it/s][A
 54%|█████▍    | 237/438 [00:05<00:06, 30.74it/s][A
 55%|█████▌    | 242/438 [00:06<00:05, 34.03it/s][A
 56%|█████▋    | 247/438 [00:06<00:05, 36.82it/s][A
 58%|█████▊    | 252/438 [00:06<00:04, 38.92it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 40.62it/s][A
 60%|█████▉    | 262/438 [00:06<00:04, 41.79it/s][A
 61%|██████    | 267/438 [00:06<00:04, 42.33it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.73it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.14it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.63it/s][A
 66%|██████▌   | 287/438 [00:07<00:03, 44.10it/s][A
 67%|██████▋   | 292/438 [00:07<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:07<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 44.86it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.79it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.55it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.41it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.41it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.37it/s][A
 76%|███████▌  | 332/438 [00:08<00:02, 44.55it/s][A
 77%|███████▋  | 337/438 [00:08<00:02, 44.69it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 44.89it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 41.26it/s][A
 80%|████████  | 352/438 [00:08<00:02, 42.43it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.04it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.42it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 43.79it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.05it/s][A
 86%|████████▌ | 377/438 [00:09<00:01, 44.30it/s][A
 87%|████████▋ | 382/438 [00:09<00:01, 44.47it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.41it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.56it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.72it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.75it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.79it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.80it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.73it/s][A
 96%|█████████▋| 422/438 [00:10<00:00, 44.74it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 44.71it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.56it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.62it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.62it/s][A 80%|████████  | 312/390 [03:49<00:26,  2.89it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:10:35,607 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 19:10:36,110 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:10:44,437 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:10:44,686 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:10:44,819 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:21<16:46, 13.07s/it] 81%|████████  | 314/390 [04:22<11:42,  9.24s/it] 81%|████████  | 315/390 [04:22<08:11,  6.56s/it] 81%|████████  | 316/390 [04:22<05:46,  4.68s/it] 81%|████████▏ | 317/390 [04:23<04:05,  3.37s/it] 82%|████████▏ | 318/390 [04:23<02:56,  2.45s/it] 82%|████████▏ | 319/390 [04:23<02:07,  1.80s/it] 82%|████████▏ | 320/390 [04:23<01:34,  1.35s/it] 82%|████████▏ | 321/390 [04:24<01:11,  1.03s/it] 83%|████████▎ | 322/390 [04:24<00:57,  1.19it/s] 83%|████████▎ | 323/390 [04:24<00:45,  1.47it/s] 83%|████████▎ | 324/390 [04:25<00:37,  1.77it/s] 83%|████████▎ | 325/390 [04:25<00:31,  2.06it/s] 84%|████████▎ | 326/390 [04:25<00:27,  2.33it/s] 84%|████████▍ | 327/390 [04:26<00:24,  2.57it/s] 84%|████████▍ | 328/390 [04:26<00:22,  2.75it/s] 84%|████████▍ | 329/390 [04:26<00:20,  2.91it/s] 85%|████████▍ | 330/390 [04:27<00:19,  3.03it/s] 85%|████████▍ | 331/390 [04:27<00:18,  3.12it/s] 85%|████████▌ | 332/390 [04:28<00:26,  2.22it/s] 85%|████████▌ | 333/390 [04:28<00:23,  2.47it/s] 86%|████████▌ | 334/390 [04:28<00:20,  2.68it/s] 86%|████████▌ | 335/390 [04:29<00:19,  2.86it/s] 86%|████████▌ | 336/390 [04:29<00:18,  2.99it/s] 86%|████████▋ | 337/390 [04:29<00:17,  3.09it/s] 87%|████████▋ | 338/390 [04:29<00:16,  3.16it/s] 87%|████████▋ | 339/390 [04:30<00:15,  3.21it/s] 87%|████████▋ | 340/390 [04:30<00:15,  3.25it/s] 87%|████████▋ | 341/390 [04:30<00:17,  2.82it/s] 88%|████████▊ | 342/390 [04:31<00:16,  2.96it/s] 88%|████████▊ | 343/390 [04:31<00:15,  3.07it/s] 88%|████████▊ | 344/390 [04:31<00:14,  3.15it/s] 88%|████████▊ | 345/390 [04:32<00:14,  3.20it/s] 89%|████████▊ | 346/390 [04:32<00:13,  3.24it/s] 89%|████████▉ | 347/390 [04:32<00:13,  3.28it/s] 89%|████████▉ | 348/390 [04:33<00:12,  3.30it/s] 89%|████████▉ | 349/390 [04:33<00:12,  3.31it/s] 90%|████████▉ | 350/390 [04:33<00:12,  3.32it/s] 90%|█████████ | 351/390 [04:34<00:13,  2.91it/s] 90%|█████████ | 352/390 [04:34<00:12,  3.03it/s] 91%|█████████ | 353/390 [04:35<00:15,  2.44it/s] 91%|█████████ | 354/390 [04:35<00:13,  2.65it/s] 91%|█████████ | 355/390 [04:35<00:12,  2.83it/s] 91%|█████████▏| 356/390 [04:35<00:11,  2.97it/s] 92%|█████████▏| 357/390 [04:36<00:10,  3.08it/s] 92%|█████████▏| 358/390 [04:36<00:10,  3.16it/s] 92%|█████████▏| 359/390 [04:36<00:09,  3.21it/s] 92%|█████████▏| 360/390 [04:37<00:12,  2.40it/s] 93%|█████████▎| 361/390 [04:37<00:11,  2.63it/s] 93%|█████████▎| 362/390 [04:38<00:09,  2.81it/s] 93%|█████████▎| 363/390 [04:38<00:09,  2.95it/s] 93%|█████████▎| 364/390 [04:38<00:08,  3.06it/s] 94%|█████████▎| 365/390 [04:38<00:07,  3.14it/s] 94%|█████████▍| 366/390 [04:39<00:07,  3.20it/s] 94%|█████████▍| 367/390 [04:39<00:07,  3.24it/s] 94%|█████████▍| 368/390 [04:39<00:06,  3.27it/s] 95%|█████████▍| 369/390 [04:40<00:07,  2.89it/s] 95%|█████████▍| 370/390 [04:40<00:06,  3.01it/s] 95%|█████████▌| 371/390 [04:40<00:06,  3.10it/s] 95%|█████████▌| 372/390 [04:41<00:05,  3.17it/s] 96%|█████████▌| 373/390 [04:41<00:05,  3.23it/s] 96%|█████████▌| 374/390 [04:41<00:04,  3.26it/s] 96%|█████████▌| 375/390 [04:42<00:04,  3.29it/s] 96%|█████████▋| 376/390 [04:42<00:04,  3.31it/s] 97%|█████████▋| 377/390 [04:42<00:03,  3.32it/s] 97%|█████████▋| 378/390 [04:42<00:03,  3.33it/s] 97%|█████████▋| 379/390 [04:43<00:04,  2.45it/s] 97%|█████████▋| 380/390 [04:43<00:03,  2.66it/s] 98%|█████████▊| 381/390 [04:44<00:03,  2.83it/s] 98%|█████████▊| 382/390 [04:44<00:02,  2.97it/s] 98%|█████████▊| 383/390 [04:44<00:02,  3.08it/s] 98%|█████████▊| 384/390 [04:45<00:01,  3.15it/s] 99%|█████████▊| 385/390 [04:45<00:01,  3.21it/s] 99%|█████████▉| 386/390 [04:45<00:01,  3.24it/s] 99%|█████████▉| 387/390 [04:46<00:00,  3.27it/s] 99%|█████████▉| 388/390 [04:46<00:00,  3.08it/s]100%|█████████▉| 389/390 [04:46<00:00,  3.16it/s]100%|██████████| 390/390 [04:46<00:00,  3.21it/s][INFO|trainer.py:2140] 2023-08-28 19:11:32,605 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:11:32,605 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:11:32,605 >>   Batch size = 8
{'eval_loss': 1.076236367225647, 'eval_runtime': 10.4608, 'eval_samples_per_second': 334.391, 'eval_steps_per_second': 41.871, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.44it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.54it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.58it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.59it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.97it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.30it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.01it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.76it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.79it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.85it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.92it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.01it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.05it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.82it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.68it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.60it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.53it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.58it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 40.01it/s][A
 23%|██▎       | 102/438 [00:02<00:08, 41.44it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 42.55it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.33it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.81it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.14it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.49it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.56it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.26it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.38it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.77it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.88it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.95it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.92it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.81it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.53it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.50it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.59it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.74it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.83it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.93it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.85it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.74it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.54it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.51it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.53it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.60it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.79it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.95it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.93it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.71it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.53it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.51it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.60it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.66it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.89it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.89it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.90it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.67it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.52it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.42it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.50it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.39it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.62it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.79it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.93it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.92it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.70it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 42.86it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 43.33it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.61it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.93it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.22it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.46it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.64it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.80it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.49it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.53it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.55it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.49it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.59it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.66it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.76it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.85it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.85it/s][A100%|██████████| 390/390 [04:56<00:00,  3.21it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:11:43,025 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 19:11:43,372 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:11:50,971 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:11:51,213 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:11:51,303 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:12:06,369 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:12:06,394 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78 (score: 1.0277519226074219).
                                                 100%|██████████| 390/390 [05:51<00:00,  3.21it/s]100%|██████████| 390/390 [05:51<00:00,  1.11it/s]
[INFO|trainer.py:1894] 2023-08-28 19:12:37,068 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 19:12:37,852 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:12:45,650 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:12:47,222 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:12:47,566 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:12:48,284 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,284 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,284 >>   train_loss               =     0.3583
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,284 >>   train_runtime            = 0:05:50.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,284 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,284 >>   train_samples_per_second =     71.304
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:48,285 >>   train_steps_per_second   =      1.112
{'eval_loss': 1.080893635749817, 'eval_runtime': 9.8431, 'eval_samples_per_second': 355.377, 'eval_steps_per_second': 44.498, 'epoch': 4.99}
{'train_runtime': 350.6104, 'train_samples_per_second': 71.304, 'train_steps_per_second': 1.112, 'train_loss': 0.35830891927083336, 'epoch': 4.99}
08/28/2023 19:12:48 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:12:48,582 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:12:48,582 >>   Num examples = 3498
[INFO|trainer.py:2145] 2023-08-28 19:12:48,582 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.60it/s]  3%|▎         | 12/438 [00:00<00:17, 24.83it/s]  4%|▍         | 17/438 [00:00<00:13, 30.61it/s]  5%|▌         | 22/438 [00:00<00:11, 34.82it/s]  6%|▌         | 27/438 [00:00<00:10, 37.87it/s]  7%|▋         | 32/438 [00:00<00:10, 40.15it/s]  8%|▊         | 37/438 [00:00<00:09, 41.75it/s] 10%|▉         | 42/438 [00:01<00:09, 42.95it/s] 11%|█         | 47/438 [00:01<00:08, 43.58it/s] 12%|█▏        | 52/438 [00:01<00:08, 43.70it/s] 13%|█▎        | 57/438 [00:01<00:08, 43.86it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.09it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.43it/s] 16%|█▋        | 72/438 [00:01<00:08, 44.71it/s] 18%|█▊        | 77/438 [00:01<00:08, 44.97it/s] 19%|█▊        | 82/438 [00:01<00:07, 45.25it/s] 20%|█▉        | 87/438 [00:02<00:07, 45.35it/s] 21%|██        | 92/438 [00:02<00:07, 45.16it/s] 22%|██▏       | 97/438 [00:02<00:07, 44.87it/s] 23%|██▎       | 102/438 [00:02<00:07, 44.65it/s] 24%|██▍       | 107/438 [00:02<00:07, 44.63it/s] 26%|██▌       | 112/438 [00:02<00:07, 44.78it/s] 27%|██▋       | 117/438 [00:02<00:07, 44.78it/s] 28%|██▊       | 122/438 [00:02<00:07, 44.92it/s] 29%|██▉       | 127/438 [00:02<00:06, 45.02it/s] 30%|███       | 132/438 [00:03<00:06, 45.19it/s] 31%|███▏      | 137/438 [00:03<00:06, 45.20it/s] 32%|███▏      | 142/438 [00:03<00:06, 42.41it/s] 34%|███▎      | 147/438 [00:03<00:10, 28.60it/s] 35%|███▍      | 152/438 [00:03<00:08, 32.69it/s] 36%|███▌      | 157/438 [00:03<00:07, 35.66it/s] 37%|███▋      | 162/438 [00:03<00:07, 38.16it/s] 38%|███▊      | 167/438 [00:04<00:06, 40.19it/s] 39%|███▉      | 172/438 [00:04<00:06, 41.64it/s] 40%|████      | 177/438 [00:05<00:17, 14.85it/s] 42%|████▏     | 182/438 [00:05<00:13, 18.66it/s] 43%|████▎     | 187/438 [00:05<00:11, 22.65it/s] 44%|████▍     | 192/438 [00:05<00:09, 26.66it/s] 45%|████▍     | 197/438 [00:05<00:07, 30.44it/s] 46%|████▌     | 202/438 [00:05<00:06, 33.82it/s] 47%|████▋     | 207/438 [00:05<00:06, 36.65it/s] 48%|████▊     | 212/438 [00:05<00:05, 38.96it/s] 50%|████▉     | 217/438 [00:05<00:05, 40.35it/s] 51%|█████     | 222/438 [00:06<00:05, 41.29it/s] 52%|█████▏    | 227/438 [00:06<00:05, 42.10it/s] 53%|█████▎    | 232/438 [00:06<00:04, 42.90it/s] 54%|█████▍    | 237/438 [00:06<00:05, 38.05it/s] 55%|█████▌    | 242/438 [00:06<00:04, 39.97it/s] 56%|█████▋    | 247/438 [00:06<00:04, 41.47it/s] 58%|█████▊    | 252/438 [00:06<00:04, 42.46it/s] 59%|█████▊    | 257/438 [00:06<00:04, 43.43it/s] 60%|█████▉    | 262/438 [00:06<00:03, 44.02it/s] 61%|██████    | 267/438 [00:07<00:03, 44.42it/s] 62%|██████▏   | 272/438 [00:07<00:03, 44.65it/s] 63%|██████▎   | 277/438 [00:07<00:03, 44.36it/s] 64%|██████▍   | 282/438 [00:07<00:03, 44.27it/s] 66%|██████▌   | 287/438 [00:07<00:03, 44.34it/s] 67%|██████▋   | 292/438 [00:07<00:03, 44.62it/s] 68%|██████▊   | 297/438 [00:07<00:03, 44.84it/s] 69%|██████▉   | 302/438 [00:07<00:03, 44.97it/s] 70%|███████   | 307/438 [00:07<00:02, 45.10it/s] 71%|███████   | 312/438 [00:08<00:02, 45.16it/s] 72%|███████▏  | 317/438 [00:08<00:02, 45.10it/s] 74%|███████▎  | 322/438 [00:08<00:02, 44.75it/s] 75%|███████▍  | 327/438 [00:08<00:02, 44.58it/s] 76%|███████▌  | 332/438 [00:08<00:02, 44.64it/s] 77%|███████▋  | 337/438 [00:08<00:02, 44.70it/s] 78%|███████▊  | 342/438 [00:08<00:02, 44.91it/s] 79%|███████▉  | 347/438 [00:08<00:02, 44.94it/s] 80%|████████  | 352/438 [00:08<00:01, 45.05it/s] 82%|████████▏ | 357/438 [00:09<00:01, 45.10it/s] 83%|████████▎ | 362/438 [00:09<00:01, 44.95it/s] 84%|████████▍ | 367/438 [00:09<00:01, 44.76it/s] 85%|████████▍ | 372/438 [00:09<00:01, 38.18it/s] 86%|████████▌ | 377/438 [00:09<00:01, 40.16it/s] 87%|████████▋ | 382/438 [00:09<00:01, 41.54it/s] 88%|████████▊ | 387/438 [00:09<00:01, 42.61it/s] 89%|████████▉ | 392/438 [00:09<00:01, 43.37it/s] 91%|█████████ | 397/438 [00:10<00:00, 44.01it/s] 92%|█████████▏| 402/438 [00:10<00:00, 44.41it/s] 93%|█████████▎| 407/438 [00:10<00:00, 44.53it/s] 94%|█████████▍| 412/438 [00:10<00:00, 44.24it/s] 95%|█████████▌| 417/438 [00:10<00:00, 43.98it/s] 96%|█████████▋| 422/438 [00:10<00:00, 44.06it/s] 97%|█████████▋| 427/438 [00:10<00:00, 44.45it/s] 99%|█████████▊| 432/438 [00:10<00:00, 44.73it/s]100%|█████████▉| 437/438 [00:10<00:00, 44.90it/s]100%|██████████| 438/438 [00:10<00:00, 39.96it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:12:59,567 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   eval_loss               =     1.0278
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   eval_runtime            = 0:00:10.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   eval_samples            =       3498
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   eval_samples_per_second =     318.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   eval_steps_per_second   =     39.875
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:12:59,567 >>   perplexity              =     2.7948
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:30,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:30,651 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:30,651 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:30,651 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:30,651 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:13:32,823 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:13:32,824 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:13:33,707 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:13:34,918 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:13:34,918 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:37,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:37,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:37,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:37,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:13:37,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:13:38,529 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:13:38,530 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:13:38,905 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:13:39,867 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:13:39,867 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'original language of film or TV show', 'participating team', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.35it/s]Extractor Predicting: 5it [00:03,  1.25it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.30it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:08,  1.35it/s]Extractor Predicting: 12it [00:09,  1.34it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.22it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:12,  1.31it/s]Extractor Predicting: 18it [00:13,  1.25it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:16,  1.33it/s]Extractor Predicting: 22it [00:16,  1.27it/s]Extractor Predicting: 23it [00:17,  1.31it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:19,  1.34it/s]Extractor Predicting: 26it [00:19,  1.30it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.35it/s]Extractor Predicting: 29it [00:22,  1.32it/s]Extractor Predicting: 30it [00:22,  1.27it/s]Extractor Predicting: 31it [00:23,  1.29it/s]Extractor Predicting: 32it [00:24,  1.26it/s]Extractor Predicting: 33it [00:25,  1.28it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:26,  1.26it/s]Extractor Predicting: 36it [00:27,  1.28it/s]Extractor Predicting: 37it [00:28,  1.26it/s]Extractor Predicting: 38it [00:29,  1.16it/s]Extractor Predicting: 39it [00:30,  1.19it/s]Extractor Predicting: 40it [00:31,  1.20it/s]Extractor Predicting: 41it [00:31,  1.21it/s]Extractor Predicting: 42it [00:33,  1.04it/s]Extractor Predicting: 43it [00:34,  1.09it/s]Extractor Predicting: 44it [00:34,  1.14it/s]Extractor Predicting: 45it [00:35,  1.17it/s]Extractor Predicting: 46it [00:36,  1.07it/s]Extractor Predicting: 47it [00:37,  1.12it/s]Extractor Predicting: 48it [00:38,  1.16it/s]Extractor Predicting: 49it [00:39,  1.19it/s]Extractor Predicting: 50it [00:40,  1.08it/s]Extractor Predicting: 51it [00:41,  1.11it/s]Extractor Predicting: 52it [00:41,  1.15it/s]Extractor Predicting: 53it [00:42,  1.16it/s]Extractor Predicting: 54it [00:43,  1.14it/s]Extractor Predicting: 55it [00:44,  1.17it/s]Extractor Predicting: 56it [00:45,  1.21it/s]Extractor Predicting: 57it [00:45,  1.22it/s]Extractor Predicting: 58it [00:46,  1.18it/s]Extractor Predicting: 59it [00:47,  1.17it/s]Extractor Predicting: 60it [00:48,  1.26it/s]Extractor Predicting: 61it [00:49,  1.28it/s]Extractor Predicting: 62it [00:50,  1.22it/s]Extractor Predicting: 63it [00:50,  1.23it/s]Extractor Predicting: 64it [00:51,  1.27it/s]Extractor Predicting: 65it [00:52,  1.30it/s]Extractor Predicting: 66it [00:53,  1.22it/s]Extractor Predicting: 67it [00:54,  1.25it/s]Extractor Predicting: 68it [00:54,  1.25it/s]Extractor Predicting: 69it [00:55,  1.25it/s]Extractor Predicting: 70it [00:56,  1.13it/s]Extractor Predicting: 71it [00:57,  1.18it/s]Extractor Predicting: 72it [00:58,  1.24it/s]Extractor Predicting: 73it [00:58,  1.28it/s]Extractor Predicting: 74it [00:59,  1.31it/s]Extractor Predicting: 75it [01:00,  1.34it/s]Extractor Predicting: 76it [01:01,  1.35it/s]Extractor Predicting: 77it [01:01,  1.35it/s]Extractor Predicting: 78it [01:02,  1.35it/s]Extractor Predicting: 79it [01:03,  1.32it/s]Extractor Predicting: 80it [01:04,  1.33it/s]Extractor Predicting: 81it [01:05,  1.24it/s]Extractor Predicting: 82it [01:05,  1.26it/s]Extractor Predicting: 83it [01:06,  1.28it/s]Extractor Predicting: 84it [01:07,  1.29it/s]Extractor Predicting: 85it [01:08,  1.30it/s]Extractor Predicting: 86it [01:08,  1.29it/s]Extractor Predicting: 87it [01:09,  1.24it/s]Extractor Predicting: 88it [01:10,  1.29it/s]Extractor Predicting: 89it [01:11,  1.36it/s]Extractor Predicting: 90it [01:11,  1.36it/s]Extractor Predicting: 91it [01:12,  1.40it/s]Extractor Predicting: 92it [01:13,  1.35it/s]Extractor Predicting: 93it [01:13,  1.39it/s]Extractor Predicting: 94it [01:14,  1.43it/s]Extractor Predicting: 95it [01:15,  1.43it/s]Extractor Predicting: 96it [01:16,  1.40it/s]Extractor Predicting: 97it [01:16,  1.39it/s]Extractor Predicting: 98it [01:17,  1.42it/s]Extractor Predicting: 99it [01:18,  1.42it/s]Extractor Predicting: 100it [01:18,  1.41it/s]Extractor Predicting: 101it [01:19,  1.40it/s]Extractor Predicting: 102it [01:20,  1.37it/s]Extractor Predicting: 103it [01:21,  1.39it/s]Extractor Predicting: 104it [01:21,  1.41it/s]Extractor Predicting: 105it [01:22,  1.39it/s]Extractor Predicting: 106it [01:23,  1.40it/s]Extractor Predicting: 107it [01:23,  1.36it/s]Extractor Predicting: 108it [01:24,  1.39it/s]Extractor Predicting: 109it [01:25,  1.40it/s]Extractor Predicting: 110it [01:26,  1.43it/s]Extractor Predicting: 111it [01:26,  1.44it/s]Extractor Predicting: 112it [01:27,  1.28it/s]Extractor Predicting: 113it [01:28,  1.30it/s]Extractor Predicting: 114it [01:29,  1.30it/s]Extractor Predicting: 115it [01:29,  1.33it/s]Extractor Predicting: 116it [01:30,  1.26it/s]Extractor Predicting: 117it [01:31,  1.27it/s]Extractor Predicting: 118it [01:32,  1.29it/s]Extractor Predicting: 119it [01:33,  1.28it/s]Extractor Predicting: 120it [01:34,  1.04s/it]Extractor Predicting: 121it [01:35,  1.03it/s]Extractor Predicting: 122it [01:36,  1.11it/s]Extractor Predicting: 123it [01:37,  1.08it/s]Extractor Predicting: 124it [01:38,  1.13it/s]Extractor Predicting: 125it [01:38,  1.17it/s]Extractor Predicting: 126it [01:39,  1.18it/s]Extractor Predicting: 127it [01:40,  1.06it/s]Extractor Predicting: 128it [01:41,  1.13it/s]Extractor Predicting: 129it [01:42,  1.17it/s]Extractor Predicting: 130it [01:43,  1.21it/s]Extractor Predicting: 131it [01:44,  1.04s/it]Extractor Predicting: 132it [01:45,  1.03it/s]Extractor Predicting: 133it [01:46,  1.08it/s]Extractor Predicting: 134it [01:47,  1.02s/it]Extractor Predicting: 135it [01:48,  1.05it/s]Extractor Predicting: 136it [01:49,  1.09it/s]Extractor Predicting: 137it [01:49,  1.14it/s]Extractor Predicting: 138it [01:51,  1.03it/s]Extractor Predicting: 139it [01:51,  1.10it/s]Extractor Predicting: 140it [01:52,  1.16it/s]Extractor Predicting: 141it [01:53,  1.18it/s]Extractor Predicting: 142it [01:54,  1.06it/s]Extractor Predicting: 143it [01:55,  1.13it/s]Extractor Predicting: 144it [01:56,  1.16it/s]Extractor Predicting: 145it [01:56,  1.53it/s]Extractor Predicting: 145it [01:56,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:02,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:03,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:03,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:03,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:03,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:16:04,132 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:16:04,134 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:16:04,923 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:16:06,230 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:16:07,085 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:10,946 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:11,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:11,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:11,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:11,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:16:12,312 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:16:12,314 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:16:13,071 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:16:13,307 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:16:13,307 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.35278614457831325,
  "recall": 0.26786735277301316,
  "score": 0.30451738706532333,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12632
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12732, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.33it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.30it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.29it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.33it/s]Extractor Predicting: 13it [00:09,  1.32it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.35it/s]Extractor Predicting: 17it [00:12,  1.32it/s]Extractor Predicting: 18it [00:13,  1.28it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:16,  1.24it/s]Extractor Predicting: 23it [00:17,  1.27it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.32it/s]Extractor Predicting: 26it [00:20,  1.27it/s]Extractor Predicting: 27it [00:20,  1.27it/s]Extractor Predicting: 28it [00:21,  1.28it/s]Extractor Predicting: 29it [00:22,  1.28it/s]Extractor Predicting: 30it [00:23,  1.26it/s]Extractor Predicting: 31it [00:23,  1.27it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.34it/s]Extractor Predicting: 34it [00:26,  1.27it/s]Extractor Predicting: 35it [00:26,  1.32it/s]Extractor Predicting: 36it [00:27,  1.29it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:30,  1.02s/it]Extractor Predicting: 39it [00:30,  1.06it/s]Extractor Predicting: 40it [00:31,  1.14it/s]Extractor Predicting: 41it [00:32,  1.17it/s]Extractor Predicting: 42it [00:33,  1.23it/s]Extractor Predicting: 43it [00:33,  1.26it/s]Extractor Predicting: 44it [00:34,  1.33it/s]Extractor Predicting: 45it [00:35,  1.34it/s]Extractor Predicting: 46it [00:35,  1.35it/s]Extractor Predicting: 47it [00:36,  1.33it/s]Extractor Predicting: 48it [00:37,  1.33it/s]Extractor Predicting: 49it [00:38,  1.33it/s]Extractor Predicting: 50it [00:39,  1.25it/s]Extractor Predicting: 51it [00:39,  1.29it/s]Extractor Predicting: 52it [00:40,  1.31it/s]Extractor Predicting: 53it [00:41,  1.35it/s]Extractor Predicting: 54it [00:42,  1.32it/s]Extractor Predicting: 55it [00:42,  1.31it/s]Extractor Predicting: 56it [00:43,  1.30it/s]Extractor Predicting: 57it [00:44,  1.31it/s]Extractor Predicting: 58it [00:45,  1.23it/s]Extractor Predicting: 59it [00:46,  1.26it/s]Extractor Predicting: 60it [00:46,  1.29it/s]Extractor Predicting: 61it [00:47,  1.30it/s]Extractor Predicting: 62it [00:48,  1.28it/s]Extractor Predicting: 63it [00:49,  1.27it/s]Extractor Predicting: 64it [00:50,  1.26it/s]Extractor Predicting: 65it [00:50,  1.31it/s]Extractor Predicting: 66it [00:51,  1.25it/s]Extractor Predicting: 67it [00:52,  1.31it/s]Extractor Predicting: 68it [00:52,  1.34it/s]Extractor Predicting: 69it [00:53,  1.32it/s]Extractor Predicting: 70it [00:54,  1.26it/s]Extractor Predicting: 71it [00:55,  1.26it/s]Extractor Predicting: 72it [00:56,  1.28it/s]Extractor Predicting: 73it [00:56,  1.29it/s]Extractor Predicting: 74it [00:57,  1.26it/s]Extractor Predicting: 75it [00:58,  1.27it/s]Extractor Predicting: 76it [00:59,  1.27it/s]Extractor Predicting: 77it [01:00,  1.29it/s]Extractor Predicting: 78it [01:00,  1.24it/s]Extractor Predicting: 79it [01:01,  1.28it/s]Extractor Predicting: 80it [01:02,  1.33it/s]Extractor Predicting: 81it [01:03,  1.28it/s]Extractor Predicting: 82it [01:04,  1.23it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.30it/s]Extractor Predicting: 85it [01:06,  1.32it/s]Extractor Predicting: 86it [01:07,  1.23it/s]Extractor Predicting: 87it [01:07,  1.27it/s]Extractor Predicting: 88it [01:08,  1.33it/s]Extractor Predicting: 89it [01:09,  1.35it/s]Extractor Predicting: 90it [01:10,  1.23it/s]Extractor Predicting: 91it [01:11,  1.25it/s]Extractor Predicting: 92it [01:11,  1.27it/s]Extractor Predicting: 93it [01:12,  1.31it/s]Extractor Predicting: 94it [01:13,  1.27it/s]Extractor Predicting: 95it [01:14,  1.30it/s]Extractor Predicting: 96it [01:14,  1.30it/s]Extractor Predicting: 97it [01:15,  1.30it/s]Extractor Predicting: 98it [01:16,  1.29it/s]Extractor Predicting: 99it [01:17,  1.30it/s]Extractor Predicting: 100it [01:17,  1.32it/s]Extractor Predicting: 101it [01:18,  1.32it/s]Extractor Predicting: 102it [01:19,  1.29it/s]Extractor Predicting: 103it [01:20,  1.31it/s]Extractor Predicting: 104it [01:21,  1.32it/s]Extractor Predicting: 105it [01:21,  1.32it/s]Extractor Predicting: 106it [01:22,  1.20it/s]Extractor Predicting: 107it [01:23,  1.24it/s]Extractor Predicting: 108it [01:24,  1.26it/s]Extractor Predicting: 109it [01:25,  1.29it/s]Extractor Predicting: 110it [01:26,  1.08it/s]Extractor Predicting: 111it [01:27,  1.16it/s]Extractor Predicting: 112it [01:27,  1.20it/s]Extractor Predicting: 113it [01:28,  1.23it/s]Extractor Predicting: 114it [01:29,  1.21it/s]Extractor Predicting: 115it [01:30,  1.26it/s]Extractor Predicting: 116it [01:30,  1.27it/s]Extractor Predicting: 117it [01:31,  1.29it/s]Extractor Predicting: 118it [01:32,  1.31it/s]Extractor Predicting: 119it [01:33,  1.31it/s]Extractor Predicting: 120it [01:33,  1.31it/s]Extractor Predicting: 121it [01:34,  1.31it/s]Extractor Predicting: 122it [01:35,  1.34it/s]Extractor Predicting: 123it [01:36,  1.35it/s]Extractor Predicting: 124it [01:36,  1.33it/s]Extractor Predicting: 125it [01:37,  1.34it/s]Extractor Predicting: 126it [01:38,  1.27it/s]Extractor Predicting: 127it [01:39,  1.28it/s]Extractor Predicting: 128it [01:39,  1.32it/s]Extractor Predicting: 129it [01:40,  1.33it/s]Extractor Predicting: 130it [01:41,  1.29it/s]Extractor Predicting: 131it [01:42,  1.32it/s]Extractor Predicting: 132it [01:42,  1.32it/s]Extractor Predicting: 133it [01:43,  1.34it/s]Extractor Predicting: 134it [01:44,  1.28it/s]Extractor Predicting: 135it [01:45,  1.32it/s]Extractor Predicting: 136it [01:46,  1.33it/s]Extractor Predicting: 137it [01:46,  1.34it/s]Extractor Predicting: 138it [01:47,  1.28it/s]Extractor Predicting: 139it [01:48,  1.27it/s]Extractor Predicting: 140it [01:49,  1.32it/s]Extractor Predicting: 140it [01:49,  1.28it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:23,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:23,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:23,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:23,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:23,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:18:25,204 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:18:25,205 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:18:26,263 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:18:27,711 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:18:27,764 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:31,982 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:32,094 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:32,094 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:32,095 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:32,095 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:18:34,753 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:18:34,755 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:18:35,779 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:18:36,688 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:18:36,689 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2756686798964625,
  "recall": 0.1904052443384982,
  "score": 0.225237927388086,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 3it [00:02,  1.29it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5142857142857142,
  "recall": 0.125,
  "score": 0.2011173184357542,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/', 'labels': ['competition class', 'location', 'operating system', 'owned by', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
