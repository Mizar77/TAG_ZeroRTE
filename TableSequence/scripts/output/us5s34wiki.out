/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'labels': ['country', 'part of', 'platform', 'publisher', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 22024
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22124, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.40s/it]Extractor Predicting: 2it [00:15,  6.63s/it]Extractor Predicting: 3it [00:16,  3.90s/it]Extractor Predicting: 4it [00:16,  2.58s/it]Extractor Predicting: 5it [00:17,  1.86s/it]Extractor Predicting: 6it [00:17,  1.42s/it]Extractor Predicting: 7it [00:18,  1.15s/it]Extractor Predicting: 8it [00:19,  1.02it/s]Extractor Predicting: 9it [00:19,  1.14it/s]Extractor Predicting: 10it [00:21,  1.02it/s]Extractor Predicting: 11it [00:21,  1.14it/s]Extractor Predicting: 12it [00:22,  1.23it/s]Extractor Predicting: 13it [00:23,  1.29it/s]Extractor Predicting: 14it [00:23,  1.33it/s]Extractor Predicting: 15it [00:24,  1.35it/s]Extractor Predicting: 16it [00:25,  1.38it/s]Extractor Predicting: 17it [00:25,  1.42it/s]Extractor Predicting: 18it [00:26,  1.35it/s]Extractor Predicting: 19it [00:27,  1.40it/s]Extractor Predicting: 20it [00:27,  1.42it/s]Extractor Predicting: 21it [00:28,  1.42it/s]Extractor Predicting: 22it [00:29,  1.44it/s]Extractor Predicting: 23it [00:30,  1.43it/s]Extractor Predicting: 24it [00:30,  1.46it/s]Extractor Predicting: 25it [00:31,  1.51it/s]Extractor Predicting: 26it [00:33,  1.01it/s]Extractor Predicting: 27it [00:33,  1.12it/s]Extractor Predicting: 28it [00:34,  1.24it/s]Extractor Predicting: 29it [00:34,  1.35it/s]Extractor Predicting: 30it [00:35,  1.41it/s]Extractor Predicting: 31it [00:36,  1.48it/s]Extractor Predicting: 32it [00:36,  1.47it/s]Extractor Predicting: 33it [00:37,  1.53it/s]Extractor Predicting: 34it [00:38,  1.56it/s]Extractor Predicting: 35it [00:38,  1.59it/s]Extractor Predicting: 36it [00:39,  1.57it/s]Extractor Predicting: 37it [00:39,  1.59it/s]Extractor Predicting: 38it [00:40,  1.61it/s]Extractor Predicting: 39it [00:41,  1.60it/s]Extractor Predicting: 40it [00:41,  1.62it/s]Extractor Predicting: 41it [00:42,  1.57it/s]Extractor Predicting: 42it [00:43,  1.60it/s]Extractor Predicting: 43it [00:43,  1.63it/s]Extractor Predicting: 44it [00:44,  1.63it/s]Extractor Predicting: 45it [00:44,  1.63it/s]Extractor Predicting: 46it [00:45,  1.63it/s]Extractor Predicting: 47it [00:46,  1.64it/s]Extractor Predicting: 48it [00:46,  1.62it/s]Extractor Predicting: 49it [00:47,  1.68it/s]Extractor Predicting: 50it [00:47,  1.65it/s]Extractor Predicting: 51it [00:48,  1.66it/s]Extractor Predicting: 52it [00:49,  1.67it/s]Extractor Predicting: 53it [00:49,  1.66it/s]Extractor Predicting: 54it [00:50,  1.68it/s]Extractor Predicting: 55it [00:50,  1.68it/s]Extractor Predicting: 56it [00:51,  1.68it/s]Extractor Predicting: 57it [00:52,  1.65it/s]Extractor Predicting: 58it [00:52,  1.63it/s]Extractor Predicting: 59it [00:53,  1.60it/s]Extractor Predicting: 60it [00:53,  1.64it/s]Extractor Predicting: 61it [00:54,  1.64it/s]Extractor Predicting: 62it [00:55,  1.61it/s]Extractor Predicting: 63it [00:55,  1.63it/s]Extractor Predicting: 64it [00:56,  1.62it/s]Extractor Predicting: 65it [00:57,  1.59it/s]Extractor Predicting: 66it [00:57,  1.59it/s]Extractor Predicting: 67it [00:58,  1.58it/s]Extractor Predicting: 68it [00:59,  1.52it/s]Extractor Predicting: 69it [00:59,  1.58it/s]Extractor Predicting: 70it [01:00,  1.62it/s]Extractor Predicting: 71it [01:00,  1.63it/s]Extractor Predicting: 72it [01:01,  1.60it/s]Extractor Predicting: 73it [01:02,  1.57it/s]Extractor Predicting: 74it [01:02,  1.58it/s]Extractor Predicting: 75it [01:03,  1.56it/s]Extractor Predicting: 76it [01:04,  1.54it/s]Extractor Predicting: 77it [01:04,  1.53it/s]Extractor Predicting: 78it [01:05,  1.60it/s]Extractor Predicting: 79it [01:05,  1.58it/s]Extractor Predicting: 80it [01:06,  1.59it/s]Extractor Predicting: 81it [01:07,  1.57it/s]Extractor Predicting: 82it [01:07,  1.60it/s]Extractor Predicting: 83it [01:08,  1.61it/s]Extractor Predicting: 84it [01:08,  1.66it/s]Extractor Predicting: 85it [01:09,  1.64it/s]Extractor Predicting: 86it [01:10,  1.60it/s]Extractor Predicting: 87it [01:10,  1.59it/s]Extractor Predicting: 88it [01:12,  1.01it/s]Extractor Predicting: 89it [01:13,  1.15it/s]Extractor Predicting: 90it [01:13,  1.26it/s]Extractor Predicting: 91it [01:14,  1.36it/s]Extractor Predicting: 92it [01:15,  1.39it/s]Extractor Predicting: 93it [01:15,  1.41it/s]Extractor Predicting: 94it [01:16,  1.46it/s]Extractor Predicting: 95it [01:17,  1.54it/s]Extractor Predicting: 96it [01:17,  1.57it/s]Extractor Predicting: 97it [01:18,  1.64it/s]Extractor Predicting: 98it [01:18,  1.69it/s]Extractor Predicting: 99it [01:19,  1.66it/s]Extractor Predicting: 100it [01:19,  1.75it/s]Extractor Predicting: 101it [01:20,  1.70it/s]Extractor Predicting: 102it [01:21,  1.69it/s]Extractor Predicting: 103it [01:21,  1.70it/s]Extractor Predicting: 104it [01:22,  1.70it/s]Extractor Predicting: 105it [01:22,  1.73it/s]Extractor Predicting: 106it [01:23,  1.69it/s]Extractor Predicting: 107it [01:24,  1.66it/s]Extractor Predicting: 108it [01:24,  1.66it/s]Extractor Predicting: 109it [01:25,  1.66it/s]Extractor Predicting: 110it [01:25,  1.66it/s]Extractor Predicting: 111it [01:26,  1.69it/s]Extractor Predicting: 112it [01:27,  1.70it/s]Extractor Predicting: 113it [01:27,  1.71it/s]Extractor Predicting: 114it [01:28,  1.73it/s]Extractor Predicting: 115it [01:28,  1.72it/s]Extractor Predicting: 116it [01:29,  1.75it/s]Extractor Predicting: 117it [01:29,  1.73it/s]Extractor Predicting: 118it [01:30,  1.74it/s]Extractor Predicting: 119it [01:31,  1.74it/s]Extractor Predicting: 120it [01:31,  1.73it/s]Extractor Predicting: 121it [01:32,  1.71it/s]Extractor Predicting: 122it [01:32,  1.68it/s]Extractor Predicting: 123it [01:33,  1.68it/s]Extractor Predicting: 124it [01:34,  1.67it/s]Extractor Predicting: 125it [01:34,  1.68it/s]Extractor Predicting: 126it [01:35,  1.68it/s]Extractor Predicting: 127it [01:35,  1.74it/s]Extractor Predicting: 128it [01:36,  1.74it/s]Extractor Predicting: 129it [01:36,  1.75it/s]Extractor Predicting: 130it [01:37,  1.73it/s]Extractor Predicting: 131it [01:38,  1.71it/s]Extractor Predicting: 132it [01:38,  1.75it/s]Extractor Predicting: 133it [01:39,  1.70it/s]Extractor Predicting: 134it [01:39,  1.74it/s]Extractor Predicting: 135it [01:40,  1.74it/s]Extractor Predicting: 136it [01:41,  1.72it/s]Extractor Predicting: 137it [01:41,  1.72it/s]Extractor Predicting: 138it [01:42,  1.70it/s]Extractor Predicting: 139it [01:42,  1.69it/s]Extractor Predicting: 140it [01:43,  1.68it/s]Extractor Predicting: 141it [01:43,  1.70it/s]Extractor Predicting: 142it [01:44,  1.53it/s]Extractor Predicting: 143it [01:45,  1.61it/s]Extractor Predicting: 144it [01:45,  1.65it/s]Extractor Predicting: 145it [01:46,  1.63it/s]Extractor Predicting: 146it [01:47,  1.62it/s]Extractor Predicting: 147it [01:47,  1.65it/s]Extractor Predicting: 148it [01:48,  1.64it/s]Extractor Predicting: 149it [01:48,  1.63it/s]Extractor Predicting: 150it [01:49,  1.65it/s]Extractor Predicting: 151it [01:50,  1.60it/s]Extractor Predicting: 152it [01:50,  1.54it/s]Extractor Predicting: 153it [01:51,  1.51it/s]Extractor Predicting: 154it [01:52,  1.50it/s]Extractor Predicting: 155it [01:52,  1.51it/s]Extractor Predicting: 156it [01:53,  1.51it/s]Extractor Predicting: 157it [01:54,  1.55it/s]Extractor Predicting: 158it [01:54,  1.54it/s]Extractor Predicting: 159it [01:55,  1.58it/s]Extractor Predicting: 160it [01:56,  1.61it/s]Extractor Predicting: 161it [01:56,  1.63it/s]Extractor Predicting: 162it [01:57,  1.64it/s]Extractor Predicting: 163it [01:57,  1.62it/s]Extractor Predicting: 164it [01:58,  1.65it/s]Extractor Predicting: 165it [01:59,  1.62it/s]Extractor Predicting: 166it [01:59,  1.60it/s]Extractor Predicting: 167it [02:00,  1.63it/s]Extractor Predicting: 168it [02:00,  1.61it/s]Extractor Predicting: 169it [02:01,  1.63it/s]Extractor Predicting: 170it [02:02,  1.66it/s]Extractor Predicting: 171it [02:02,  1.69it/s]Extractor Predicting: 172it [02:03,  1.67it/s]Extractor Predicting: 173it [02:03,  1.69it/s]Extractor Predicting: 174it [02:04,  1.68it/s]Extractor Predicting: 175it [02:05,  1.64it/s]Extractor Predicting: 176it [02:05,  1.62it/s]Extractor Predicting: 177it [02:06,  1.59it/s]Extractor Predicting: 178it [02:07,  1.66it/s]Extractor Predicting: 179it [02:07,  1.78it/s]Extractor Predicting: 180it [02:07,  1.87it/s]Extractor Predicting: 181it [02:08,  1.86it/s]Extractor Predicting: 182it [02:09,  1.85it/s]Extractor Predicting: 183it [02:09,  1.72it/s]Extractor Predicting: 184it [02:10,  1.64it/s]Extractor Predicting: 185it [02:10,  1.65it/s]Extractor Predicting: 186it [02:11,  1.64it/s]Extractor Predicting: 187it [02:12,  1.62it/s]Extractor Predicting: 188it [02:12,  1.59it/s]Extractor Predicting: 189it [02:13,  1.59it/s]Extractor Predicting: 190it [02:14,  1.57it/s]Extractor Predicting: 191it [02:14,  1.55it/s]Extractor Predicting: 192it [02:15,  1.57it/s]Extractor Predicting: 193it [02:16,  1.65it/s]Extractor Predicting: 194it [02:16,  1.66it/s]Extractor Predicting: 195it [02:17,  1.68it/s]Extractor Predicting: 196it [02:17,  1.65it/s]Extractor Predicting: 197it [02:18,  1.64it/s]Extractor Predicting: 198it [02:19,  1.58it/s]Extractor Predicting: 199it [02:19,  1.59it/s]Extractor Predicting: 200it [02:20,  1.61it/s]Extractor Predicting: 201it [02:20,  1.63it/s]Extractor Predicting: 202it [02:21,  1.61it/s]Extractor Predicting: 203it [02:22,  1.62it/s]Extractor Predicting: 204it [02:22,  1.62it/s]Extractor Predicting: 205it [02:23,  1.65it/s]Extractor Predicting: 206it [02:23,  1.64it/s]Extractor Predicting: 207it [02:24,  1.63it/s]Extractor Predicting: 208it [02:25,  1.64it/s]Extractor Predicting: 209it [02:25,  1.63it/s]Extractor Predicting: 210it [02:26,  1.66it/s]Extractor Predicting: 211it [02:27,  1.62it/s]Extractor Predicting: 212it [02:27,  1.58it/s]Extractor Predicting: 213it [02:28,  1.57it/s]Extractor Predicting: 214it [02:29,  1.56it/s]Extractor Predicting: 215it [02:29,  1.59it/s]Extractor Predicting: 216it [02:30,  1.58it/s]Extractor Predicting: 217it [02:30,  1.64it/s]Extractor Predicting: 218it [02:31,  1.66it/s]Extractor Predicting: 219it [02:32,  1.65it/s]Extractor Predicting: 220it [02:32,  1.65it/s]Extractor Predicting: 221it [02:33,  1.65it/s]Extractor Predicting: 222it [02:33,  1.65it/s]Extractor Predicting: 223it [02:34,  1.62it/s]Extractor Predicting: 224it [02:35,  1.58it/s]Extractor Predicting: 225it [02:35,  1.59it/s]Extractor Predicting: 226it [02:36,  1.61it/s]Extractor Predicting: 227it [02:37,  1.61it/s]Extractor Predicting: 228it [02:37,  1.57it/s]Extractor Predicting: 229it [02:38,  1.57it/s]Extractor Predicting: 230it [02:38,  1.57it/s]Extractor Predicting: 231it [02:39,  1.55it/s]Extractor Predicting: 232it [02:40,  1.60it/s]Extractor Predicting: 233it [02:40,  1.57it/s]Extractor Predicting: 234it [02:41,  1.56it/s]Extractor Predicting: 235it [02:42,  1.57it/s]Extractor Predicting: 236it [02:42,  1.58it/s]Extractor Predicting: 237it [02:43,  1.58it/s]Extractor Predicting: 238it [02:44,  1.55it/s]Extractor Predicting: 239it [02:44,  1.58it/s]Extractor Predicting: 240it [02:45,  1.59it/s]Extractor Predicting: 241it [02:45,  1.61it/s]Extractor Predicting: 242it [02:46,  1.59it/s]Extractor Predicting: 243it [02:47,  1.60it/s]Extractor Predicting: 244it [02:47,  1.60it/s]Extractor Predicting: 245it [02:48,  1.56it/s]Extractor Predicting: 246it [02:49,  1.59it/s]Extractor Predicting: 247it [02:49,  1.62it/s]Extractor Predicting: 248it [02:50,  1.57it/s]Extractor Predicting: 249it [02:50,  1.57it/s]Extractor Predicting: 250it [02:51,  1.63it/s]Extractor Predicting: 251it [02:52,  1.65it/s]Extractor Predicting: 252it [02:52,  1.69it/s]Extractor Predicting: 253it [02:53,  1.68it/s]Extractor Predicting: 254it [02:53,  1.67it/s]Extractor Predicting: 255it [02:54,  1.64it/s]Extractor Predicting: 256it [02:55,  1.64it/s]Extractor Predicting: 257it [02:55,  1.64it/s]Extractor Predicting: 258it [02:56,  1.65it/s]Extractor Predicting: 259it [02:56,  1.65it/s]Extractor Predicting: 260it [02:57,  1.65it/s]Extractor Predicting: 261it [02:58,  1.67it/s]Extractor Predicting: 262it [02:58,  1.71it/s]Extractor Predicting: 263it [02:59,  1.70it/s]Extractor Predicting: 264it [02:59,  1.71it/s]Extractor Predicting: 265it [03:00,  1.70it/s]Extractor Predicting: 266it [03:01,  1.69it/s]Extractor Predicting: 267it [03:01,  1.67it/s]Extractor Predicting: 268it [03:02,  1.64it/s]Extractor Predicting: 269it [03:02,  1.67it/s]Extractor Predicting: 270it [03:03,  1.46it/s]Extractor Predicting: 271it [03:04,  1.53it/s]Extractor Predicting: 272it [03:04,  1.58it/s]Extractor Predicting: 273it [03:05,  1.60it/s]Extractor Predicting: 274it [03:06,  1.63it/s]Extractor Predicting: 275it [03:06,  1.62it/s]Extractor Predicting: 276it [03:07,  1.65it/s]Extractor Predicting: 277it [03:07,  1.66it/s]Extractor Predicting: 278it [03:08,  1.66it/s]Extractor Predicting: 279it [03:09,  1.64it/s]Extractor Predicting: 280it [03:09,  1.62it/s]Extractor Predicting: 281it [03:10,  1.62it/s]Extractor Predicting: 282it [03:11,  1.60it/s]Extractor Predicting: 283it [03:11,  1.60it/s]Extractor Predicting: 284it [03:12,  1.65it/s]Extractor Predicting: 285it [03:12,  1.63it/s]Extractor Predicting: 286it [03:13,  1.60it/s]Extractor Predicting: 287it [03:14,  1.58it/s]Extractor Predicting: 288it [03:14,  1.60it/s]Extractor Predicting: 289it [03:15,  1.56it/s]Extractor Predicting: 290it [03:16,  1.57it/s]Extractor Predicting: 291it [03:16,  1.59it/s]Extractor Predicting: 292it [03:17,  1.58it/s]Extractor Predicting: 293it [03:17,  1.58it/s]Extractor Predicting: 294it [03:18,  1.59it/s]Extractor Predicting: 295it [03:19,  1.61it/s]Extractor Predicting: 296it [03:19,  1.61it/s]Extractor Predicting: 297it [03:20,  1.60it/s]Extractor Predicting: 298it [03:21,  1.64it/s]Extractor Predicting: 299it [03:21,  1.60it/s]Extractor Predicting: 300it [03:22,  1.56it/s]Extractor Predicting: 301it [03:23,  1.55it/s]Extractor Predicting: 302it [03:23,  1.58it/s]Extractor Predicting: 303it [03:24,  1.60it/s]Extractor Predicting: 304it [03:24,  1.62it/s]Extractor Predicting: 305it [03:25,  1.64it/s]Extractor Predicting: 306it [03:26,  1.61it/s]Extractor Predicting: 307it [03:26,  1.64it/s]Extractor Predicting: 308it [03:27,  1.65it/s]Extractor Predicting: 309it [03:27,  1.61it/s]Extractor Predicting: 310it [03:28,  1.56it/s]Extractor Predicting: 311it [03:29,  1.51it/s]Extractor Predicting: 312it [03:29,  1.49it/s]Extractor Predicting: 313it [03:30,  1.47it/s]Extractor Predicting: 314it [03:31,  1.44it/s]Extractor Predicting: 315it [03:32,  1.44it/s]Extractor Predicting: 316it [03:32,  1.43it/s]Extractor Predicting: 317it [03:33,  1.44it/s]Extractor Predicting: 318it [03:34,  1.44it/s]Extractor Predicting: 319it [03:34,  1.43it/s]Extractor Predicting: 320it [03:35,  1.43it/s]Extractor Predicting: 321it [03:36,  1.42it/s]Extractor Predicting: 322it [03:37,  1.42it/s]Extractor Predicting: 323it [03:37,  1.43it/s]Extractor Predicting: 324it [03:38,  1.44it/s]Extractor Predicting: 325it [03:39,  1.43it/s]Extractor Predicting: 326it [03:39,  1.45it/s]Extractor Predicting: 327it [03:40,  1.50it/s]Extractor Predicting: 328it [03:41,  1.51it/s]Extractor Predicting: 329it [03:41,  1.54it/s]Extractor Predicting: 330it [03:42,  1.60it/s]Extractor Predicting: 331it [03:42,  1.61it/s]Extractor Predicting: 332it [03:43,  1.64it/s]Extractor Predicting: 333it [03:44,  1.63it/s]Extractor Predicting: 334it [03:44,  1.60it/s]Extractor Predicting: 335it [03:45,  1.61it/s]Extractor Predicting: 336it [03:45,  1.56it/s]Extractor Predicting: 337it [03:46,  1.61it/s]Extractor Predicting: 338it [03:47,  1.62it/s]Extractor Predicting: 339it [03:47,  1.61it/s]Extractor Predicting: 340it [03:48,  1.59it/s]Extractor Predicting: 341it [03:49,  1.55it/s]Extractor Predicting: 342it [03:49,  1.55it/s]Extractor Predicting: 343it [03:50,  1.55it/s]Extractor Predicting: 344it [03:51,  1.52it/s]Extractor Predicting: 345it [03:51,  1.51it/s]Extractor Predicting: 346it [03:52,  1.49it/s]Extractor Predicting: 347it [03:53,  1.49it/s]Extractor Predicting: 348it [03:53,  1.50it/s]Extractor Predicting: 349it [03:54,  1.52it/s]Extractor Predicting: 350it [03:55,  1.50it/s]Extractor Predicting: 351it [03:55,  1.52it/s]Extractor Predicting: 352it [03:56,  1.53it/s]Extractor Predicting: 353it [03:57,  1.51it/s]Extractor Predicting: 354it [03:57,  1.51it/s]Extractor Predicting: 355it [03:58,  1.53it/s]Extractor Predicting: 356it [03:59,  1.54it/s]Extractor Predicting: 357it [03:59,  1.55it/s]Extractor Predicting: 358it [04:00,  1.54it/s]Extractor Predicting: 359it [04:01,  1.52it/s]Extractor Predicting: 360it [04:01,  1.55it/s]Extractor Predicting: 361it [04:02,  1.59it/s]Extractor Predicting: 362it [04:02,  1.57it/s]Extractor Predicting: 363it [04:03,  1.57it/s]Extractor Predicting: 364it [04:04,  1.57it/s]Extractor Predicting: 365it [04:04,  1.61it/s]Extractor Predicting: 366it [04:05,  1.58it/s]Extractor Predicting: 367it [04:06,  1.58it/s]Extractor Predicting: 368it [04:06,  1.57it/s]Extractor Predicting: 369it [04:07,  1.57it/s]Extractor Predicting: 370it [04:07,  1.58it/s]Extractor Predicting: 371it [04:08,  1.39it/s]Extractor Predicting: 372it [04:09,  1.43it/s]Extractor Predicting: 373it [04:10,  1.45it/s]Extractor Predicting: 374it [04:10,  1.50it/s]Extractor Predicting: 375it [04:11,  1.53it/s]Extractor Predicting: 376it [04:11,  1.57it/s]Extractor Predicting: 377it [04:12,  1.58it/s]Extractor Predicting: 378it [04:13,  1.59it/s]Extractor Predicting: 379it [04:13,  1.55it/s]Extractor Predicting: 380it [04:14,  1.55it/s]Extractor Predicting: 381it [04:15,  1.56it/s]Extractor Predicting: 382it [04:15,  1.56it/s]Extractor Predicting: 383it [04:16,  1.57it/s]Extractor Predicting: 384it [04:17,  1.58it/s]Extractor Predicting: 385it [04:17,  1.60it/s]Extractor Predicting: 386it [04:18,  1.61it/s]Extractor Predicting: 387it [04:18,  1.59it/s]Extractor Predicting: 388it [04:19,  1.57it/s]Extractor Predicting: 389it [04:20,  1.62it/s]Extractor Predicting: 390it [04:20,  1.62it/s]Extractor Predicting: 391it [04:21,  1.63it/s]Extractor Predicting: 392it [04:21,  1.65it/s]Extractor Predicting: 393it [04:22,  1.64it/s]Extractor Predicting: 394it [04:23,  1.63it/s]Extractor Predicting: 395it [04:23,  1.65it/s]Extractor Predicting: 396it [04:24,  1.62it/s]Extractor Predicting: 397it [04:25,  1.62it/s]Extractor Predicting: 398it [04:25,  1.65it/s]Extractor Predicting: 399it [04:26,  1.65it/s]Extractor Predicting: 400it [04:26,  1.60it/s]Extractor Predicting: 401it [04:27,  1.57it/s]Extractor Predicting: 402it [04:28,  1.54it/s]Extractor Predicting: 403it [04:28,  1.55it/s]Extractor Predicting: 404it [04:29,  1.58it/s]Extractor Predicting: 405it [04:30,  1.59it/s]Extractor Predicting: 406it [04:30,  1.59it/s]Extractor Predicting: 407it [04:31,  1.63it/s]Extractor Predicting: 408it [04:31,  1.67it/s]Extractor Predicting: 409it [04:32,  1.70it/s]Extractor Predicting: 410it [04:33,  1.72it/s]Extractor Predicting: 411it [04:33,  1.71it/s]Extractor Predicting: 412it [04:34,  1.73it/s]Extractor Predicting: 413it [04:34,  1.77it/s]Extractor Predicting: 414it [04:35,  1.78it/s]Extractor Predicting: 415it [04:35,  1.69it/s]Extractor Predicting: 416it [04:36,  1.62it/s]Extractor Predicting: 417it [04:37,  1.56it/s]Extractor Predicting: 418it [04:37,  1.53it/s]Extractor Predicting: 419it [04:38,  1.50it/s]Extractor Predicting: 420it [04:39,  1.50it/s]Extractor Predicting: 421it [04:40,  1.48it/s]Extractor Predicting: 422it [04:40,  1.50it/s]Extractor Predicting: 422it [04:40,  1.50it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.27719406674907293,
  "recall": 0.0637662614629985,
  "score": 0.10368144252441773,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13679
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13779, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.62it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:25,  1.58it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:26,  1.64it/s]Extractor Predicting: 44it [00:27,  1.62it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:30,  1.55it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:35,  1.53it/s]Extractor Predicting: 57it [00:35,  1.53it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:40,  1.65it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:42,  1.63it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:45,  1.61it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.62it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:48,  1.65it/s]Extractor Predicting: 78it [00:48,  1.67it/s]Extractor Predicting: 79it [00:49,  1.65it/s]Extractor Predicting: 80it [00:50,  1.64it/s]Extractor Predicting: 81it [00:50,  1.66it/s]Extractor Predicting: 82it [00:51,  1.50it/s]Extractor Predicting: 83it [00:52,  1.51it/s]Extractor Predicting: 84it [00:52,  1.53it/s]Extractor Predicting: 85it [00:53,  1.54it/s]Extractor Predicting: 86it [00:54,  1.57it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:56,  1.57it/s]Extractor Predicting: 91it [00:57,  1.60it/s]Extractor Predicting: 92it [00:57,  1.64it/s]Extractor Predicting: 93it [00:58,  1.64it/s]Extractor Predicting: 94it [00:58,  1.66it/s]Extractor Predicting: 95it [00:59,  1.65it/s]Extractor Predicting: 96it [01:00,  1.63it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:01,  1.66it/s]Extractor Predicting: 99it [01:01,  1.67it/s]Extractor Predicting: 100it [01:02,  1.64it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:04,  1.60it/s]Extractor Predicting: 104it [01:05,  1.63it/s]Extractor Predicting: 105it [01:05,  1.61it/s]Extractor Predicting: 106it [01:06,  1.61it/s]Extractor Predicting: 107it [01:06,  1.63it/s]Extractor Predicting: 108it [01:07,  1.66it/s]Extractor Predicting: 109it [01:08,  1.65it/s]Extractor Predicting: 110it [01:08,  1.67it/s]Extractor Predicting: 111it [01:09,  1.67it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:10,  1.63it/s]Extractor Predicting: 114it [01:11,  1.65it/s]Extractor Predicting: 115it [01:11,  1.64it/s]Extractor Predicting: 116it [01:12,  1.64it/s]Extractor Predicting: 117it [01:12,  1.69it/s]Extractor Predicting: 118it [01:13,  1.65it/s]Extractor Predicting: 119it [01:14,  1.66it/s]Extractor Predicting: 120it [01:14,  1.69it/s]Extractor Predicting: 121it [01:15,  1.69it/s]Extractor Predicting: 122it [01:15,  1.66it/s]Extractor Predicting: 123it [01:16,  1.65it/s]Extractor Predicting: 124it [01:17,  1.64it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:18,  1.61it/s]Extractor Predicting: 127it [01:19,  1.64it/s]Extractor Predicting: 128it [01:19,  1.66it/s]Extractor Predicting: 129it [01:20,  1.67it/s]Extractor Predicting: 130it [01:20,  1.64it/s]Extractor Predicting: 131it [01:21,  1.61it/s]Extractor Predicting: 132it [01:22,  1.62it/s]Extractor Predicting: 133it [01:22,  1.63it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:23,  1.64it/s]Extractor Predicting: 136it [01:24,  1.69it/s]Extractor Predicting: 137it [01:25,  1.69it/s]Extractor Predicting: 138it [01:25,  1.65it/s]Extractor Predicting: 139it [01:26,  1.64it/s]Extractor Predicting: 140it [01:26,  1.67it/s]Extractor Predicting: 141it [01:27,  1.69it/s]Extractor Predicting: 142it [01:28,  1.71it/s]Extractor Predicting: 143it [01:28,  1.74it/s]Extractor Predicting: 144it [01:29,  1.70it/s]Extractor Predicting: 145it [01:29,  1.71it/s]Extractor Predicting: 146it [01:30,  1.70it/s]Extractor Predicting: 147it [01:31,  1.70it/s]Extractor Predicting: 148it [01:31,  1.69it/s]Extractor Predicting: 149it [01:32,  1.69it/s]Extractor Predicting: 150it [01:32,  1.70it/s]Extractor Predicting: 151it [01:33,  1.69it/s]Extractor Predicting: 152it [01:33,  1.72it/s]Extractor Predicting: 153it [01:34,  1.69it/s]Extractor Predicting: 154it [01:35,  1.68it/s]Extractor Predicting: 155it [01:35,  1.68it/s]Extractor Predicting: 156it [01:36,  1.67it/s]Extractor Predicting: 157it [01:36,  1.66it/s]Extractor Predicting: 158it [01:37,  1.68it/s]Extractor Predicting: 159it [01:38,  1.71it/s]Extractor Predicting: 160it [01:38,  1.58it/s]Extractor Predicting: 161it [01:39,  1.56it/s]Extractor Predicting: 162it [01:40,  1.56it/s]Extractor Predicting: 163it [01:40,  1.58it/s]Extractor Predicting: 164it [01:41,  1.64it/s]Extractor Predicting: 165it [01:41,  1.64it/s]Extractor Predicting: 166it [01:42,  1.58it/s]Extractor Predicting: 167it [01:43,  1.56it/s]Extractor Predicting: 168it [01:43,  1.57it/s]Extractor Predicting: 169it [01:44,  1.58it/s]Extractor Predicting: 170it [01:45,  1.57it/s]Extractor Predicting: 170it [01:45,  1.62it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.47149122807017546,
  "recall": 0.05276073619631902,
  "score": 0.09490178768483779,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 3869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 3969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:09,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.58it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.56it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.55,
  "recall": 0.011695906432748537,
  "score": 0.022904737116085372,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:20, 15.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:08, 16.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:42, 14.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:27, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:15<01:15, 15.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:31<01:02, 15.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:46<00:46, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:01<00:30, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:16<00:14, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:33<00:00, 15.73s/it]Generating: 100%|██████████| 10/10 [02:33<00:00, 15.36s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : country . Context : The show featured The Queen , Lady Gaga , Ashley Judd and Justin Timberlake . Head Entity : Lady Gaga , Tail Entity : The Netherlands .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8383152173913043, 'errors': {'', "('The Girl Who Loved You', 'country', '', 'In 1970 , she collaborated with Peter Schofield on an adaptation of John Updike s novel The Girl Who Loved You , based on the novel , directed by George Clooney .')"}}
['Relation : part of . Context : Later in the year ( 1143 ) , he married Alixandra Maria of Brescia , daughter of Leonora Ariander ( later the King ) , daughter of Alexander Berenice . Head Entity : 1243 , Tail Entity : Alexander Berenice .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : part of .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8522727272727273, 'errors': {'', "('', 'platform', 'web browser', 'is a web browser developed by Microsoft .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : sport .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', "('Zajac', 'sport', '', 'In May 2010 Zajac was awarded the National Academy Award for his performance in the 1992 Asian Games .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 536, 'raw': 704}
{'target': 600, 'success': 561, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : continent .', 'success_rate': 0.76, 'errors': {'', "('France', 'continent', '', 'During the Second World War , most of the population died from typhoid fever , but some , like the French , received extensive medical treatment in France ( including surgery ) .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Bishop of York', 'replaces', '', 'In 1569 , with the formation of the House of Commons , he became the second Bishop of York , becoming the first Bishop to die in the service of the Lord Mayor .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 10582
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10682, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.62it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.69it/s]Extractor Estimating: 7it [00:04,  1.67it/s]Extractor Estimating: 8it [00:04,  1.66it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.65it/s]Extractor Estimating: 11it [00:06,  1.59it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:08,  1.52it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:09,  1.60it/s]Extractor Estimating: 17it [00:10,  1.57it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:11,  1.56it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.66it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.63it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:16,  1.55it/s]Extractor Estimating: 28it [00:17,  1.53it/s]Extractor Estimating: 29it [00:18,  1.54it/s]Extractor Estimating: 30it [00:18,  1.61it/s]Extractor Estimating: 31it [00:19,  1.59it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:20,  1.54it/s]Extractor Estimating: 34it [00:21,  1.54it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:22,  1.57it/s]Extractor Estimating: 37it [00:23,  1.62it/s]Extractor Estimating: 38it [00:23,  1.56it/s]Extractor Estimating: 39it [00:24,  1.59it/s]Extractor Estimating: 40it [00:25,  1.61it/s]Extractor Estimating: 41it [00:25,  1.62it/s]Extractor Estimating: 42it [00:26,  1.63it/s]Extractor Estimating: 43it [00:26,  1.61it/s]Extractor Estimating: 44it [00:27,  1.58it/s]Extractor Estimating: 45it [00:28,  1.55it/s]Extractor Estimating: 46it [00:28,  1.54it/s]Extractor Estimating: 47it [00:29,  1.53it/s]Extractor Estimating: 48it [00:30,  1.55it/s]Extractor Estimating: 49it [00:30,  1.54it/s]Extractor Estimating: 50it [00:31,  1.63it/s]Extractor Estimating: 51it [00:32,  1.65it/s]Extractor Estimating: 52it [00:32,  1.72it/s]Extractor Estimating: 53it [00:33,  1.75it/s]Extractor Estimating: 54it [00:33,  1.80it/s]Extractor Estimating: 55it [00:34,  1.79it/s]Extractor Estimating: 56it [00:34,  1.80it/s]Extractor Estimating: 57it [00:35,  1.82it/s]Extractor Estimating: 58it [00:35,  1.78it/s]Extractor Estimating: 59it [00:36,  1.84it/s]Extractor Estimating: 60it [00:36,  1.87it/s]Extractor Estimating: 61it [00:37,  1.86it/s]Extractor Estimating: 62it [00:37,  1.90it/s]Extractor Estimating: 63it [00:38,  1.88it/s]Extractor Estimating: 64it [00:39,  1.83it/s]Extractor Estimating: 65it [00:39,  1.77it/s]Extractor Estimating: 66it [00:40,  1.79it/s]Extractor Estimating: 67it [00:40,  1.89it/s]Extractor Estimating: 68it [00:41,  1.90it/s]Extractor Estimating: 69it [00:41,  1.89it/s]Extractor Estimating: 70it [00:42,  1.93it/s]Extractor Estimating: 71it [00:42,  1.89it/s]Extractor Estimating: 72it [00:43,  1.92it/s]Extractor Estimating: 73it [00:43,  1.86it/s]Extractor Estimating: 74it [00:44,  1.75it/s]Extractor Estimating: 75it [00:45,  1.75it/s]Extractor Estimating: 76it [00:45,  1.68it/s]Extractor Estimating: 77it [00:46,  1.63it/s]Extractor Estimating: 78it [00:46,  1.61it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:48,  1.56it/s]Extractor Estimating: 81it [00:48,  1.59it/s]Extractor Estimating: 82it [00:49,  1.57it/s]Extractor Estimating: 83it [00:50,  1.62it/s]Extractor Estimating: 84it [00:50,  1.60it/s]Extractor Estimating: 85it [00:51,  1.58it/s]Extractor Estimating: 86it [00:52,  1.44it/s]Extractor Estimating: 87it [00:52,  1.52it/s]Extractor Estimating: 88it [00:53,  1.52it/s]Extractor Estimating: 89it [00:54,  1.56it/s]Extractor Estimating: 90it [00:54,  1.55it/s]Extractor Estimating: 91it [00:55,  1.51it/s]Extractor Estimating: 92it [00:56,  1.53it/s]Extractor Estimating: 93it [00:56,  1.54it/s]Extractor Estimating: 94it [00:57,  1.46it/s]Extractor Estimating: 95it [00:58,  1.17it/s]Extractor Estimating: 96it [00:59,  1.28it/s]Extractor Estimating: 97it [00:59,  1.35it/s]Extractor Estimating: 98it [01:00,  1.42it/s]Extractor Estimating: 99it [01:01,  1.49it/s]Extractor Estimating: 100it [01:01,  1.49it/s]Extractor Estimating: 101it [01:02,  1.56it/s]Extractor Estimating: 102it [01:03,  1.62it/s]Extractor Estimating: 103it [01:03,  1.66it/s]Extractor Estimating: 104it [01:04,  1.66it/s]Extractor Estimating: 105it [01:04,  1.65it/s]Extractor Estimating: 106it [01:05,  1.63it/s]Extractor Estimating: 107it [01:06,  1.63it/s]Extractor Estimating: 108it [01:06,  1.62it/s]Extractor Estimating: 109it [01:07,  1.60it/s]Extractor Estimating: 110it [01:07,  1.61it/s]Extractor Estimating: 111it [01:08,  1.65it/s]Extractor Estimating: 112it [01:09,  1.62it/s]Extractor Estimating: 113it [01:09,  1.64it/s]Extractor Estimating: 114it [01:10,  1.60it/s]Extractor Estimating: 115it [01:10,  1.61it/s]Extractor Estimating: 116it [01:11,  1.66it/s]Extractor Estimating: 117it [01:12,  1.62it/s]Extractor Estimating: 118it [01:12,  1.62it/s]Extractor Estimating: 119it [01:13,  1.63it/s]Extractor Estimating: 120it [01:14,  1.60it/s]Extractor Estimating: 121it [01:14,  1.61it/s]Extractor Estimating: 122it [01:15,  1.58it/s]Extractor Estimating: 123it [01:15,  1.64it/s]Extractor Estimating: 124it [01:16,  1.69it/s]Extractor Estimating: 125it [01:17,  1.62it/s]Extractor Estimating: 126it [01:17,  1.69it/s]Extractor Estimating: 127it [01:18,  1.71it/s]Extractor Estimating: 128it [01:18,  1.68it/s]Extractor Estimating: 129it [01:19,  1.66it/s]Extractor Estimating: 130it [01:20,  1.66it/s]Extractor Estimating: 131it [01:20,  1.68it/s]Extractor Estimating: 132it [01:21,  1.70it/s]Extractor Estimating: 133it [01:21,  1.68it/s]Extractor Estimating: 134it [01:22,  1.70it/s]Extractor Estimating: 135it [01:23,  1.68it/s]Extractor Estimating: 136it [01:23,  1.69it/s]Extractor Estimating: 137it [01:24,  1.69it/s]Extractor Estimating: 138it [01:24,  1.69it/s]Extractor Estimating: 139it [01:25,  1.71it/s]Extractor Estimating: 140it [01:25,  1.72it/s]Extractor Estimating: 141it [01:26,  1.69it/s]Extractor Estimating: 142it [01:27,  1.73it/s]Extractor Estimating: 143it [01:27,  1.67it/s]Extractor Estimating: 144it [01:28,  1.69it/s]Extractor Estimating: 145it [01:28,  1.65it/s]Extractor Estimating: 146it [01:29,  1.62it/s]Extractor Estimating: 147it [01:30,  1.61it/s]Extractor Estimating: 148it [01:30,  1.66it/s]Extractor Estimating: 149it [01:31,  1.67it/s]Extractor Estimating: 150it [01:32,  1.27it/s]Extractor Estimating: 151it [01:33,  1.38it/s]Extractor Estimating: 152it [01:33,  1.44it/s]Extractor Estimating: 153it [01:34,  1.52it/s]Extractor Estimating: 154it [01:35,  1.44it/s]Extractor Estimating: 155it [01:35,  1.51it/s]Extractor Estimating: 156it [01:36,  1.56it/s]Extractor Estimating: 157it [01:37,  1.50it/s]Extractor Estimating: 158it [01:37,  1.52it/s]Extractor Estimating: 159it [01:38,  1.57it/s]Extractor Estimating: 160it [01:38,  1.60it/s]Extractor Estimating: 161it [01:39,  1.59it/s]Extractor Estimating: 162it [01:40,  1.59it/s]Extractor Estimating: 163it [01:40,  1.64it/s]Extractor Estimating: 164it [01:41,  1.59it/s]Extractor Estimating: 165it [01:41,  1.63it/s]Extractor Estimating: 166it [01:42,  1.66it/s]Extractor Estimating: 167it [01:43,  1.65it/s]Extractor Estimating: 168it [01:43,  1.72it/s]Extractor Estimating: 169it [01:44,  1.68it/s]Extractor Estimating: 170it [01:44,  1.69it/s]Extractor Estimating: 171it [01:45,  1.65it/s]Extractor Estimating: 172it [01:46,  1.58it/s]Extractor Estimating: 173it [01:46,  1.55it/s]Extractor Estimating: 174it [01:47,  1.52it/s]Extractor Estimating: 175it [01:48,  1.55it/s]Extractor Estimating: 176it [01:48,  1.59it/s]Extractor Estimating: 177it [01:49,  1.56it/s]Extractor Estimating: 178it [01:50,  1.56it/s]Extractor Estimating: 179it [01:50,  1.57it/s]Extractor Estimating: 180it [01:51,  1.61it/s]Extractor Estimating: 181it [01:51,  1.57it/s]Extractor Estimating: 182it [01:52,  1.56it/s]Extractor Estimating: 183it [01:53,  1.57it/s]Extractor Estimating: 184it [01:53,  1.56it/s]Extractor Estimating: 185it [01:54,  1.54it/s]Extractor Estimating: 186it [01:55,  1.53it/s]Extractor Estimating: 187it [01:55,  1.54it/s]Extractor Estimating: 188it [01:56,  1.51it/s]Extractor Estimating: 189it [01:57,  1.52it/s]Extractor Estimating: 190it [01:57,  1.55it/s]Extractor Estimating: 191it [01:58,  1.51it/s]Extractor Estimating: 192it [01:59,  1.55it/s]Extractor Estimating: 193it [01:59,  1.60it/s]Extractor Estimating: 194it [02:00,  1.60it/s]Extractor Estimating: 195it [02:00,  1.61it/s]Extractor Estimating: 196it [02:01,  1.56it/s]Extractor Estimating: 197it [02:02,  1.62it/s]Extractor Estimating: 198it [02:02,  1.60it/s]Extractor Estimating: 199it [02:03,  1.59it/s]Extractor Estimating: 200it [02:04,  1.55it/s]Extractor Estimating: 201it [02:04,  1.60it/s]Extractor Estimating: 202it [02:05,  1.56it/s]Extractor Estimating: 203it [02:06,  1.54it/s]Extractor Estimating: 204it [02:06,  1.55it/s]Extractor Estimating: 205it [02:07,  1.57it/s]Extractor Estimating: 206it [02:07,  1.57it/s]Extractor Estimating: 207it [02:08,  1.61it/s]Extractor Estimating: 208it [02:09,  1.60it/s]Extractor Estimating: 209it [02:09,  1.62it/s]Extractor Estimating: 210it [02:10,  1.56it/s]Extractor Estimating: 211it [02:11,  1.56it/s]Extractor Estimating: 212it [02:11,  1.59it/s]Extractor Estimating: 213it [02:12,  1.62it/s]Extractor Estimating: 214it [02:13,  1.52it/s]Extractor Estimating: 215it [02:13,  1.50it/s]Extractor Estimating: 216it [02:14,  1.50it/s]Extractor Estimating: 217it [02:15,  1.50it/s]Extractor Estimating: 218it [02:15,  1.51it/s]Extractor Estimating: 219it [02:16,  1.49it/s]Extractor Estimating: 220it [02:17,  1.50it/s]Extractor Estimating: 221it [02:17,  1.51it/s]Extractor Estimating: 222it [02:18,  1.55it/s]Extractor Estimating: 223it [02:18,  1.58it/s]Extractor Estimating: 224it [02:19,  1.54it/s]Extractor Estimating: 225it [02:20,  1.53it/s]Extractor Estimating: 226it [02:21,  1.50it/s]Extractor Estimating: 227it [02:21,  1.56it/s]Extractor Estimating: 228it [02:22,  1.53it/s]Extractor Estimating: 229it [02:23,  1.41it/s]Extractor Estimating: 230it [02:23,  1.46it/s]Extractor Estimating: 231it [02:24,  1.53it/s]Extractor Estimating: 232it [02:24,  1.56it/s]Extractor Estimating: 233it [02:25,  1.51it/s]Extractor Estimating: 234it [02:26,  1.48it/s]Extractor Estimating: 235it [02:27,  1.51it/s]Extractor Estimating: 236it [02:27,  1.53it/s]Extractor Estimating: 237it [02:28,  1.55it/s]Extractor Estimating: 238it [02:28,  1.53it/s]Extractor Estimating: 239it [02:29,  1.53it/s]Extractor Estimating: 240it [02:30,  1.54it/s]Extractor Estimating: 241it [02:30,  1.54it/s]Extractor Estimating: 242it [02:31,  1.53it/s]Extractor Estimating: 243it [02:32,  1.53it/s]Extractor Estimating: 244it [02:32,  1.50it/s]Extractor Estimating: 245it [02:33,  1.50it/s]Extractor Estimating: 246it [02:34,  1.52it/s]Extractor Estimating: 247it [02:34,  1.53it/s]Extractor Estimating: 248it [02:35,  1.53it/s]Extractor Estimating: 249it [02:36,  1.54it/s]Extractor Estimating: 250it [02:36,  1.65it/s]Extractor Estimating: 250it [02:36,  1.60it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4972 mean pseudo reward: 0.950467067260237
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl'}
train vocab size: 29335
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29435, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29435, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:747.7361
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.065, loss:709.0296
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.065, loss:669.4367
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.068, loss:663.3493
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.057, loss:638.3342
>> valid entity prec:0.4614, rec:0.3103, f1:0.3711
>> valid relation prec:0.2326, rec:0.0478, f1:0.0793
>> valid relation with NER prec:0.2326, rec:0.0478, f1:0.0793
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 4.972, loss:660.6303
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.056, loss:604.8380
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.078, loss:646.2426
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.055, loss:641.4536
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.056, loss:660.1465
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5075, rec:0.3322, f1:0.4015
>> valid relation prec:0.2889, rec:0.0560, f1:0.0938
>> valid relation with NER prec:0.2889, rec:0.0560, f1:0.0938
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 4.963, loss:640.1419
g_step 1200, step 160, avg_time 1.069, loss:629.1021
g_step 1300, step 52, avg_time 1.067, loss:619.4956
g_step 1400, step 152, avg_time 1.077, loss:589.7398
g_step 1500, step 44, avg_time 1.051, loss:600.6093
>> valid entity prec:0.4778, rec:0.2958, f1:0.3654
>> valid relation prec:0.2901, rec:0.0612, f1:0.1011
>> valid relation with NER prec:0.2901, rec:0.0612, f1:0.1011
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 144, avg_time 4.962, loss:582.4687
g_step 1700, step 36, avg_time 1.055, loss:582.6576
g_step 1800, step 136, avg_time 1.062, loss:561.5030
g_step 1900, step 28, avg_time 1.063, loss:559.5436
g_step 2000, step 128, avg_time 1.069, loss:537.6676
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4501, rec:0.3531, f1:0.3958
>> valid relation prec:0.2016, rec:0.0555, f1:0.0871
>> valid relation with NER prec:0.2016, rec:0.0555, f1:0.0871
g_step 2100, step 20, avg_time 4.975, loss:519.0138
g_step 2200, step 120, avg_time 1.066, loss:516.8351
g_step 2300, step 12, avg_time 1.065, loss:508.1616
g_step 2400, step 112, avg_time 1.064, loss:484.9473
g_step 2500, step 4, avg_time 1.062, loss:492.3349
>> valid entity prec:0.4972, rec:0.3597, f1:0.4174
>> valid relation prec:0.1864, rec:0.0352, f1:0.0592
>> valid relation with NER prec:0.1864, rec:0.0352, f1:0.0592
new max entity f1 on valid!
g_step 2600, step 104, avg_time 4.976, loss:452.4512
g_step 2700, step 204, avg_time 1.063, loss:496.1752
g_step 2800, step 96, avg_time 1.067, loss:429.5567
g_step 2900, step 196, avg_time 1.060, loss:485.3092
g_step 3000, step 88, avg_time 1.057, loss:427.7230
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4601, rec:0.3757, f1:0.4136
>> valid relation prec:0.2204, rec:0.0503, f1:0.0819
>> valid relation with NER prec:0.2204, rec:0.0503, f1:0.0819
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 4.977, loss:463.1647
g_step 3200, step 80, avg_time 1.064, loss:424.2395
g_step 3300, step 180, avg_time 1.065, loss:426.6845
g_step 3400, step 72, avg_time 1.053, loss:400.4336
g_step 3500, step 172, avg_time 1.074, loss:413.0042
>> valid entity prec:0.4771, rec:0.3019, f1:0.3698
>> valid relation prec:0.1753, rec:0.0386, f1:0.0633
>> valid relation with NER prec:0.1753, rec:0.0386, f1:0.0633
g_step 3600, step 64, avg_time 4.955, loss:385.5184
g_step 3700, step 164, avg_time 1.060, loss:403.4222
g_step 3800, step 56, avg_time 1.068, loss:375.1726
g_step 3900, step 156, avg_time 1.066, loss:390.8929
g_step 4000, step 48, avg_time 1.056, loss:372.0733
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4841, rec:0.3996, f1:0.4378
>> valid relation prec:0.2098, rec:0.0546, f1:0.0867
>> valid relation with NER prec:0.2098, rec:0.0546, f1:0.0867
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 148, avg_time 4.983, loss:359.3055
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:07:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:07:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-07-04_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:07:05 - WARNING - datasets.builder -   Using custom data configuration default-c1f1ef60e7ddb187
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c1f1ef60e7ddb187/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:07:06,129 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:07:06,131 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:07:06,131 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:07:06,132 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:07:06,141 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:07:06,145 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:07:06,312 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:07:09,433 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:07:09,437 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c1f1ef60e7ddb187/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:07:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14da333250e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.86ba/s] 40%|████      | 2/5 [00:00<00:00,  3.74ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.11ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.43ba/s]100%|██████████| 5/5 [00:01<00:00,  3.78ba/s]100%|██████████| 5/5 [00:01<00:00,  3.69ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:03,  4.07ba/s] 13%|█▎        | 2/15 [00:00<00:02,  4.37ba/s] 20%|██        | 3/15 [00:00<00:02,  4.48ba/s] 27%|██▋       | 4/15 [00:00<00:02,  4.55ba/s] 33%|███▎      | 5/15 [00:01<00:02,  4.56ba/s] 40%|████      | 6/15 [00:01<00:01,  4.57ba/s] 47%|████▋     | 7/15 [00:01<00:01,  4.58ba/s] 53%|█████▎    | 8/15 [00:01<00:01,  4.58ba/s] 60%|██████    | 9/15 [00:01<00:01,  4.61ba/s] 67%|██████▋   | 10/15 [00:02<00:01,  4.62ba/s] 73%|███████▎  | 11/15 [00:02<00:00,  4.60ba/s] 80%|████████  | 12/15 [00:02<00:00,  4.61ba/s] 87%|████████▋ | 13/15 [00:02<00:00,  4.64ba/s] 93%|█████████▎| 14/15 [00:03<00:00,  4.61ba/s]100%|██████████| 15/15 [00:03<00:00,  4.87ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.73ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.08ba/s]100%|██████████| 5/5 [00:00<00:00,  8.42ba/s]100%|██████████| 5/5 [00:00<00:00,  7.61ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:01,  8.50ba/s] 20%|██        | 3/15 [00:00<00:01,  9.79ba/s] 33%|███▎      | 5/15 [00:00<00:00, 10.07ba/s] 47%|████▋     | 7/15 [00:00<00:00, 10.17ba/s] 60%|██████    | 9/15 [00:00<00:00, 10.12ba/s] 73%|███████▎  | 11/15 [00:01<00:00, 10.17ba/s] 87%|████████▋ | 13/15 [00:01<00:00, 10.22ba/s]100%|██████████| 15/15 [00:01<00:00, 12.01ba/s]100%|██████████| 15/15 [00:01<00:00, 10.78ba/s]
[INFO|trainer.py:414] 2023-08-28 13:07:16,460 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:07:16,476 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:07:16,476 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 13:07:16,476 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:07:16,476 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:07:16,476 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:07:16,476 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:07:16,476 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.30it/s]  1%|          | 2/390 [00:00<01:53,  3.41it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:51,  3.45it/s]  1%|▏         | 5/390 [00:01<01:51,  3.46it/s]  2%|▏         | 6/390 [00:01<01:50,  3.47it/s]  2%|▏         | 7/390 [00:02<01:50,  3.47it/s]  2%|▏         | 8/390 [00:02<01:50,  3.47it/s]  2%|▏         | 9/390 [00:02<01:49,  3.47it/s]  3%|▎         | 10/390 [00:02<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:49,  3.47it/s]  3%|▎         | 12/390 [00:03<01:48,  3.48it/s]  3%|▎         | 13/390 [00:03<01:48,  3.48it/s]  4%|▎         | 14/390 [00:04<01:48,  3.48it/s]  4%|▍         | 15/390 [00:04<01:47,  3.48it/s]  4%|▍         | 16/390 [00:04<01:47,  3.48it/s]  4%|▍         | 17/390 [00:04<01:47,  3.47it/s]  5%|▍         | 18/390 [00:05<01:47,  3.47it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:05<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:46,  3.47it/s]  6%|▌         | 22/390 [00:06<01:45,  3.47it/s]  6%|▌         | 23/390 [00:06<01:45,  3.47it/s]  6%|▌         | 24/390 [00:06<01:45,  3.47it/s]  6%|▋         | 25/390 [00:07<01:45,  3.47it/s]  7%|▋         | 26/390 [00:07<01:44,  3.47it/s]  7%|▋         | 27/390 [00:07<01:44,  3.47it/s]  7%|▋         | 28/390 [00:08<01:44,  3.47it/s]  7%|▋         | 29/390 [00:08<01:43,  3.47it/s]  8%|▊         | 30/390 [00:08<01:43,  3.47it/s]  8%|▊         | 31/390 [00:08<01:43,  3.47it/s]  8%|▊         | 32/390 [00:09<01:43,  3.47it/s]  8%|▊         | 33/390 [00:09<01:42,  3.47it/s]  9%|▊         | 34/390 [00:09<01:42,  3.47it/s]  9%|▉         | 35/390 [00:10<01:42,  3.47it/s]  9%|▉         | 36/390 [00:10<01:42,  3.47it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:10<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:41,  3.45it/s] 11%|█         | 41/390 [00:11<01:40,  3.46it/s] 11%|█         | 42/390 [00:12<01:40,  3.46it/s] 11%|█         | 43/390 [00:12<01:40,  3.47it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.47it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.47it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 52/390 [00:14<01:37,  3.46it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.46it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.46it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.46it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.46it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.47it/s] 15%|█▌        | 59/390 [00:17<01:35,  3.47it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.47it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.47it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.46it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.46it/s] 18%|█▊        | 69/390 [00:19<01:32,  3.46it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.47it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.46it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.46it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.46it/s] 20%|██        | 78/390 [00:22<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:07:39,021 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:07:39,022 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:07:39,022 >>   Batch size = 8

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.29it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.37it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.68it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.00it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.72it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.39it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.15it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.65it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.62it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.73it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.75it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.68it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.80it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.80it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.82it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.85it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.61it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.55it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.71it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.64it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.59it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.72it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.73it/s][A
  7%|▋         | 123/1759 [00:02<00:34, 46.75it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.67it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.52it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.61it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.68it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.60it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.69it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.63it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.63it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.64it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.74it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.60it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.61it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.65it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.68it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.66it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.68it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.62it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.66it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.53it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.63it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.58it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.60it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.64it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.62it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.71it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.67it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.56it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.61it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.60it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.67it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.70it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.54it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.60it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.67it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.65it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.65it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.49it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.42it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.55it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.58it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.55it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.57it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.37it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.57it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.58it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.40it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.56it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.56it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.52it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.55it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.51it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.47it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.45it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.27it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.28it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.31it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.43it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.51it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.42it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.49it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.45it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.54it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.48it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.45it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.60it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.57it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.54it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.57it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.43it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.54it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.38it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.52it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.58it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.62it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.55it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.54it/s][A
 29%|██▉       | 513/1759 [00:10<00:26, 46.40it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.48it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.47it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.46it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.44it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.55it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.51it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.56it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.46it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.39it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.44it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.43it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.39it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.51it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.52it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.58it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.59it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.51it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.33it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.43it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.37it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.39it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.49it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.45it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.48it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.41it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.26it/s][A
 37%|███▋      | 648/1759 [00:13<00:24, 46.24it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.23it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.33it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.34it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.34it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.41it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.45it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.59it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.48it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.46it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.35it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.32it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.38it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.40it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.38it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.41it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.48it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.50it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.48it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.46it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.34it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.34it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.41it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.38it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.39it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.47it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.42it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.46it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.49it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.43it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.39it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.37it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.32it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.37it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.40it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.52it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.49it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.47it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.47it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.36it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.39it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.40it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.43it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.41it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.49it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.44it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.45it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.39it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.34it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.35it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.37it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.45it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.34it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.36it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.32it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.38it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.37it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.36it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.29it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.37it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.36it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.45it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.42it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.46it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.47it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.31it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.39it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.40it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.25it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.39it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.33it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.39it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.51it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.46it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.44it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.48it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.41it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.34it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.26it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.35it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.39it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.40it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.50it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.50it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.41it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.42it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.34it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.32it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.40it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.38it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.49it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.42it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.38it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.39it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.41it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.44it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.37it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.29it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.28it/s][A
 66%|██████▌   | 1163/1759 [00:24<00:12, 46.41it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.40it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.42it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.40it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.41it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.36it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.35it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.38it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 43.81it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:12, 44.68it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:12, 45.18it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 45.57it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 45.94it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.10it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.24it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.24it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.02it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.00it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.11it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.29it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.37it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.49it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.48it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.50it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.25it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.09it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.22it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.28it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.39it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.50it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.51it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.48it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.42it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.35it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.33it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:09, 46.16it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.21it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.37it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.38it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.51it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.51it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.52it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.41it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.04it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:08, 46.27it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.24it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.35it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.45it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.37it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.46it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.52it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.52it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.41it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.32it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.17it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.21it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.49it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.51it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.56it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.49it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.37it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.35it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.19it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.25it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.31it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.37it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.29it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.44it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.56it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.47it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.43it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.28it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.27it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.28it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.29it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.33it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.48it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.46it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.49it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.50it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.27it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.33it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.34it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.23it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.30it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.35it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.33it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.44it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.44it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.42it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.41it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.32it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.36it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.26it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.26it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.28it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.40it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.52it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.44it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.45it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.37it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.33it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.33it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.38it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.46it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.17it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.51it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.51it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.52it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.40it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.34it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.25it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.31it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.37it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.37it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.34it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.35it/s][A                                                
                                                   [A 20%|██        | 78/390 [01:00<01:30,  3.46it/s]
100%|██████████| 1759/1759 [00:37<00:00, 46.35it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 13:08:16,955 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 13:08:16,986 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:08:19,525 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:08:19,540 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:08:19,552 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:09<1:13:49, 14.24s/it] 21%|██        | 80/390 [01:09<51:58, 10.06s/it]   21%|██        | 81/390 [01:09<36:42,  7.13s/it] 21%|██        | 82/390 [01:10<26:03,  5.08s/it] 21%|██▏       | 83/390 [01:10<18:37,  3.64s/it] 22%|██▏       | 84/390 [01:10<13:26,  2.63s/it] 22%|██▏       | 85/390 [01:11<09:48,  1.93s/it] 22%|██▏       | 86/390 [01:11<07:17,  1.44s/it] 22%|██▏       | 87/390 [01:11<05:31,  1.09s/it] 23%|██▎       | 88/390 [01:11<04:17,  1.17it/s] 23%|██▎       | 89/390 [01:12<03:25,  1.46it/s] 23%|██▎       | 90/390 [01:12<02:49,  1.77it/s] 23%|██▎       | 91/390 [01:12<02:24,  2.07it/s] 24%|██▎       | 92/390 [01:13<02:06,  2.36it/s] 24%|██▍       | 93/390 [01:13<01:54,  2.61it/s] 24%|██▍       | 94/390 [01:13<01:45,  2.82it/s] 24%|██▍       | 95/390 [01:13<01:38,  2.98it/s] 25%|██▍       | 96/390 [01:14<01:34,  3.11it/s] 25%|██▍       | 97/390 [01:14<01:31,  3.21it/s] 25%|██▌       | 98/390 [01:14<01:29,  3.28it/s] 25%|██▌       | 99/390 [01:15<01:27,  3.33it/s] 26%|██▌       | 100/390 [01:15<01:26,  3.37it/s] 26%|██▌       | 101/390 [01:15<01:25,  3.40it/s] 26%|██▌       | 102/390 [01:15<01:24,  3.39it/s] 26%|██▋       | 103/390 [01:16<01:24,  3.42it/s] 27%|██▋       | 104/390 [01:16<01:23,  3.43it/s] 27%|██▋       | 105/390 [01:16<01:22,  3.44it/s] 27%|██▋       | 106/390 [01:17<01:22,  3.44it/s] 27%|██▋       | 107/390 [01:17<01:21,  3.45it/s] 28%|██▊       | 108/390 [01:17<01:21,  3.46it/s] 28%|██▊       | 109/390 [01:17<01:21,  3.46it/s] 28%|██▊       | 110/390 [01:18<01:20,  3.46it/s] 28%|██▊       | 111/390 [01:18<01:20,  3.46it/s] 29%|██▊       | 112/390 [01:18<01:20,  3.46it/s] 29%|██▉       | 113/390 [01:19<01:20,  3.45it/s] 29%|██▉       | 114/390 [01:19<01:19,  3.46it/s] 29%|██▉       | 115/390 [01:19<01:19,  3.46it/s] 30%|██▉       | 116/390 [01:20<01:19,  3.46it/s] 30%|███       | 117/390 [01:20<01:18,  3.46it/s] 30%|███       | 118/390 [01:20<01:18,  3.46it/s] 31%|███       | 119/390 [01:20<01:18,  3.46it/s] 31%|███       | 120/390 [01:21<01:18,  3.46it/s] 31%|███       | 121/390 [01:21<01:17,  3.46it/s] 31%|███▏      | 122/390 [01:21<01:17,  3.46it/s] 32%|███▏      | 123/390 [01:22<01:17,  3.46it/s] 32%|███▏      | 124/390 [01:22<01:16,  3.46it/s] 32%|███▏      | 125/390 [01:22<01:16,  3.46it/s] 32%|███▏      | 126/390 [01:22<01:16,  3.46it/s] 33%|███▎      | 127/390 [01:23<01:16,  3.46it/s] 33%|███▎      | 128/390 [01:23<01:15,  3.46it/s] 33%|███▎      | 129/390 [01:23<01:15,  3.46it/s] 33%|███▎      | 130/390 [01:24<01:15,  3.46it/s] 34%|███▎      | 131/390 [01:24<01:15,  3.45it/s] 34%|███▍      | 132/390 [01:24<01:14,  3.45it/s] 34%|███▍      | 133/390 [01:24<01:14,  3.45it/s] 34%|███▍      | 134/390 [01:25<01:14,  3.45it/s] 35%|███▍      | 135/390 [01:25<01:13,  3.45it/s] 35%|███▍      | 136/390 [01:25<01:13,  3.45it/s] 35%|███▌      | 137/390 [01:26<01:13,  3.45it/s] 35%|███▌      | 138/390 [01:26<01:13,  3.45it/s] 36%|███▌      | 139/390 [01:26<01:12,  3.46it/s] 36%|███▌      | 140/390 [01:26<01:12,  3.45it/s] 36%|███▌      | 141/390 [01:27<01:12,  3.46it/s] 36%|███▋      | 142/390 [01:27<01:12,  3.44it/s] 37%|███▋      | 143/390 [01:27<01:11,  3.44it/s] 37%|███▋      | 144/390 [01:28<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:28<01:11,  3.45it/s] 37%|███▋      | 146/390 [01:28<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:28<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:29<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:29<01:09,  3.45it/s] 38%|███▊      | 150/390 [01:29<01:09,  3.45it/s] 39%|███▊      | 151/390 [01:30<01:09,  3.45it/s] 39%|███▉      | 152/390 [01:30<01:08,  3.46it/s] 39%|███▉      | 153/390 [01:30<01:10,  3.38it/s] 39%|███▉      | 154/390 [01:31<01:09,  3.40it/s] 40%|███▉      | 155/390 [01:31<01:08,  3.42it/s] 40%|████      | 156/390 [01:31<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:08:48,131 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:08:48,131 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:08:48,131 >>   Batch size = 8
{'eval_loss': 0.8475257158279419, 'eval_runtime': 37.8859, 'eval_samples_per_second': 371.299, 'eval_steps_per_second': 46.429, 'epoch': 0.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.03it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.43it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.57it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.88it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.46it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.08it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 46.91it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.48it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.56it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.54it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.62it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.45it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.51it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.54it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.54it/s][A
  5%|▍         | 83/1759 [00:01<00:36, 46.50it/s][A
  5%|▌         | 88/1759 [00:01<00:36, 46.10it/s][A
  5%|▌         | 93/1759 [00:01<00:36, 46.15it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.27it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.39it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.44it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.42it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.45it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.51it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.43it/s][A
  8%|▊         | 133/1759 [00:02<00:35, 46.41it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.36it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.47it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.43it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.40it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.43it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.42it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.44it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.49it/s][A
 10%|█         | 178/1759 [00:03<00:34, 46.41it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.39it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.48it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.49it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.27it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.42it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.33it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.43it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.42it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.40it/s][A
 13%|█▎        | 228/1759 [00:04<00:33, 46.39it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.33it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.45it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.49it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.40it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.44it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.34it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.41it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.39it/s][A
 16%|█▌        | 273/1759 [00:05<00:32, 46.40it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.36it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.40it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.48it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.44it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.36it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.43it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.43it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.46it/s][A
 18%|█▊        | 318/1759 [00:06<00:31, 46.34it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.33it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.36it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.41it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.47it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.41it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.29it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.39it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.41it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.44it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.46it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.41it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.28it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.34it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.39it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.40it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.36it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.38it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.41it/s][A
 23%|██▎       | 413/1759 [00:08<00:29, 46.41it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.46it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.40it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.25it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.36it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.23it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.31it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.29it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.41it/s][A
 26%|██▌       | 458/1759 [00:09<00:28, 46.35it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.37it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.37it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.39it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.41it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.34it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.39it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.45it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.32it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.34it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.41it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.31it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.35it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.39it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.35it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.46it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.46it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.38it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.34it/s][A
 31%|███▏      | 553/1759 [00:11<00:26, 46.34it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.35it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.38it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.35it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.40it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.35it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.44it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.44it/s][A
 34%|███▍      | 598/1759 [00:12<00:25, 46.32it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.33it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.39it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.39it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.39it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.36it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.31it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.41it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.43it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.34it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.36it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.35it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.38it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.39it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.33it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.41it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.27it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.37it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.36it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.39it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.40it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.37it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.34it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.38it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.37it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.36it/s][A
 42%|████▏     | 738/1759 [00:15<00:22, 46.37it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.35it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.40it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.36it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.39it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.33it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.33it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.31it/s][A
 44%|████▍     | 778/1759 [00:16<00:22, 44.51it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 45.17it/s][A
 45%|████▍     | 788/1759 [00:16<00:21, 45.53it/s][A
 45%|████▌     | 793/1759 [00:17<00:21, 45.80it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 45.90it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.11it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.21it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.21it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.13it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.11it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.29it/s][A
 47%|████▋     | 833/1759 [00:17<00:20, 46.29it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.38it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.45it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.35it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.33it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.24it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.16it/s][A
 50%|████▉     | 878/1759 [00:18<00:19, 46.31it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.31it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.48it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.45it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.36it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.39it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.32it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.28it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.33it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.35it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.26it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.28it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.47it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.42it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.42it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.30it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.28it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.35it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.37it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.38it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.40it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.38it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.35it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.38it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.42it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.41it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.32it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.23it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.31it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.39it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.41it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.44it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.42it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.27it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.36it/s][A
 60%|██████    | 1063/1759 [00:22<00:15, 46.36it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.34it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.36it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.33it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.31it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.31it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.44it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.38it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.39it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.48it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.27it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.32it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.34it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.32it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.42it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.40it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.37it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.33it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.31it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.42it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.34it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.36it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.26it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.23it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.36it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.40it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.44it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.47it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 46.32it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:11, 46.37it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.37it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.33it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.39it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.39it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.29it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.31it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.19it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.43it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.39it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.29it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.35it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.38it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.39it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.34it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.37it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.31it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.46it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.43it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.30it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.30it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.35it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.42it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.33it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.34it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.35it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.37it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.49it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.40it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.30it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.33it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.30it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.38it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.36it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.41it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:08, 46.31it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.34it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.43it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.38it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.36it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.33it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.27it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.36it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.30it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.32it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.36it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.40it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.32it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.33it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.39it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.36it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.37it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.46it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.31it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.36it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.36it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.38it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.47it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.35it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.35it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.24it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.29it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.38it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.39it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.36it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.35it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.38it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.35it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.32it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.25it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.31it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.34it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.38it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.39it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.28it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.38it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.33it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.42it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.44it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.43it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.26it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.31it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.29it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.41it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.34it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.35it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.40it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.27it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.36it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.35it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.36it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.37it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.28it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.35it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.33it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.43it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.23it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.30it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.32it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.34it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.37it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.34it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.36it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.33it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.23it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.34it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.26it/s][A                                                 
                                                   [A 40%|████      | 156/390 [02:09<01:08,  3.43it/s]
100%|██████████| 1759/1759 [00:37<00:00, 46.26it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 13:09:26,103 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 13:09:26,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:09:28,700 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:09:28,723 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:09:28,731 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:17<53:45, 13.84s/it] 41%|████      | 158/390 [02:17<37:48,  9.78s/it] 41%|████      | 159/390 [02:17<26:41,  6.93s/it] 41%|████      | 160/390 [02:17<18:55,  4.94s/it] 41%|████▏     | 161/390 [02:18<13:31,  3.54s/it] 42%|████▏     | 162/390 [02:18<09:45,  2.57s/it] 42%|████▏     | 163/390 [02:18<07:07,  1.88s/it] 42%|████▏     | 164/390 [02:19<05:17,  1.41s/it] 42%|████▏     | 165/390 [02:19<04:00,  1.07s/it] 43%|████▎     | 166/390 [02:19<03:07,  1.20it/s] 43%|████▎     | 167/390 [02:19<02:29,  1.49it/s] 43%|████▎     | 168/390 [02:20<02:03,  1.80it/s] 43%|████▎     | 169/390 [02:20<01:45,  2.10it/s] 44%|████▎     | 170/390 [02:20<01:32,  2.38it/s] 44%|████▍     | 171/390 [02:21<01:23,  2.63it/s] 44%|████▍     | 172/390 [02:21<01:16,  2.83it/s] 44%|████▍     | 173/390 [02:21<01:12,  2.99it/s] 45%|████▍     | 174/390 [02:21<01:09,  3.12it/s] 45%|████▍     | 175/390 [02:22<01:06,  3.21it/s] 45%|████▌     | 176/390 [02:22<01:05,  3.28it/s] 45%|████▌     | 177/390 [02:22<01:03,  3.33it/s] 46%|████▌     | 178/390 [02:23<01:02,  3.37it/s] 46%|████▌     | 179/390 [02:23<01:02,  3.40it/s] 46%|████▌     | 180/390 [02:23<01:01,  3.42it/s] 46%|████▋     | 181/390 [02:24<01:00,  3.43it/s] 47%|████▋     | 182/390 [02:24<01:00,  3.44it/s] 47%|████▋     | 183/390 [02:24<01:00,  3.44it/s] 47%|████▋     | 184/390 [02:24<00:59,  3.45it/s] 47%|████▋     | 185/390 [02:25<00:59,  3.45it/s] 48%|████▊     | 186/390 [02:25<00:59,  3.45it/s] 48%|████▊     | 187/390 [02:25<00:58,  3.45it/s] 48%|████▊     | 188/390 [02:26<00:58,  3.45it/s] 48%|████▊     | 189/390 [02:26<00:58,  3.45it/s] 49%|████▊     | 190/390 [02:26<00:57,  3.45it/s] 49%|████▉     | 191/390 [02:26<00:57,  3.45it/s] 49%|████▉     | 192/390 [02:27<00:57,  3.45it/s] 49%|████▉     | 193/390 [02:27<00:57,  3.45it/s] 50%|████▉     | 194/390 [02:27<00:56,  3.45it/s] 50%|█████     | 195/390 [02:28<00:56,  3.45it/s] 50%|█████     | 196/390 [02:28<00:56,  3.45it/s] 51%|█████     | 197/390 [02:28<00:55,  3.46it/s] 51%|█████     | 198/390 [02:28<00:55,  3.44it/s] 51%|█████     | 199/390 [02:29<00:55,  3.45it/s] 51%|█████▏    | 200/390 [02:29<00:55,  3.45it/s] 52%|█████▏    | 201/390 [02:29<00:54,  3.45it/s] 52%|█████▏    | 202/390 [02:30<00:54,  3.45it/s] 52%|█████▏    | 203/390 [02:30<00:54,  3.45it/s] 52%|█████▏    | 204/390 [02:30<00:53,  3.45it/s] 53%|█████▎    | 205/390 [02:30<00:53,  3.45it/s] 53%|█████▎    | 206/390 [02:31<00:53,  3.46it/s] 53%|█████▎    | 207/390 [02:31<00:52,  3.46it/s] 53%|█████▎    | 208/390 [02:31<00:52,  3.46it/s] 54%|█████▎    | 209/390 [02:32<00:52,  3.45it/s] 54%|█████▍    | 210/390 [02:32<00:52,  3.45it/s] 54%|█████▍    | 211/390 [02:32<00:51,  3.45it/s] 54%|█████▍    | 212/390 [02:32<00:51,  3.45it/s] 55%|█████▍    | 213/390 [02:33<00:51,  3.46it/s] 55%|█████▍    | 214/390 [02:33<00:50,  3.45it/s] 55%|█████▌    | 215/390 [02:33<00:50,  3.46it/s] 55%|█████▌    | 216/390 [02:34<00:50,  3.46it/s] 56%|█████▌    | 217/390 [02:34<00:50,  3.45it/s] 56%|█████▌    | 218/390 [02:34<00:49,  3.46it/s] 56%|█████▌    | 219/390 [02:35<00:49,  3.46it/s] 56%|█████▋    | 220/390 [02:35<00:49,  3.45it/s] 57%|█████▋    | 221/390 [02:35<00:48,  3.45it/s] 57%|█████▋    | 222/390 [02:35<00:48,  3.46it/s] 57%|█████▋    | 223/390 [02:36<00:48,  3.45it/s] 57%|█████▋    | 224/390 [02:36<00:48,  3.46it/s] 58%|█████▊    | 225/390 [02:36<00:47,  3.45it/s] 58%|█████▊    | 226/390 [02:37<00:47,  3.45it/s] 58%|█████▊    | 227/390 [02:37<00:47,  3.46it/s] 58%|█████▊    | 228/390 [02:37<00:46,  3.46it/s] 59%|█████▊    | 229/390 [02:37<00:46,  3.46it/s] 59%|█████▉    | 230/390 [02:38<00:46,  3.46it/s] 59%|█████▉    | 231/390 [02:38<00:46,  3.45it/s] 59%|█████▉    | 232/390 [02:38<00:45,  3.45it/s] 60%|█████▉    | 233/390 [02:39<00:45,  3.45it/s] 60%|██████    | 234/390 [02:39<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:09:55,881 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:09:55,882 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:09:55,882 >>   Batch size = 8
{'eval_loss': 0.8556057810783386, 'eval_runtime': 37.9443, 'eval_samples_per_second': 370.728, 'eval_steps_per_second': 46.357, 'epoch': 1.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.03it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.31it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.58it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.81it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.46it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.19it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 46.95it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.47it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.48it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.57it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.61it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.62it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.52it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.52it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.61it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.57it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.43it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.40it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.37it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.46it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.49it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.50it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.60it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.50it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.52it/s][A
  8%|▊         | 133/1759 [00:02<00:35, 46.36it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.32it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.39it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.36it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.43it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.55it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.62it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.62it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.43it/s][A
 10%|█         | 178/1759 [00:03<00:34, 46.39it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.41it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.42it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.38it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.40it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.32it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.52it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.56it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.53it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.47it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.46it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.42it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.38it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.36it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.41it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.38it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.50it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.48it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.53it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.48it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.34it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.36it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.33it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.24it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.33it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.36it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.44it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.47it/s][A
 18%|█▊        | 318/1759 [00:06<00:31, 46.45it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.52it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 333/1759 [00:07<00:31, 45.84it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.57it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.48it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.50it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.48it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.46it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.54it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.51it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.50it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.51it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.39it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.39it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.37it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.39it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.45it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.43it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.43it/s][A
 24%|██▍       | 418/1759 [00:08<00:29, 46.18it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.34it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.40it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.41it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.40it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.29it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.31it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.41it/s][A
 26%|██▌       | 458/1759 [00:09<00:28, 46.42it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.46it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.44it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.48it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.49it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.47it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.42it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.38it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.31it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.48it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.56it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.60it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.50it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.27it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.38it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.41it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.34it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.36it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.32it/s][A
 31%|███▏      | 553/1759 [00:11<00:26, 46.34it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.52it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.54it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.45it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.40it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.40it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.36it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.27it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.29it/s][A
 34%|███▍      | 598/1759 [00:12<00:25, 46.30it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.37it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.52it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.43it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.46it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.46it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.41it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.40it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.36it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.38it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.39it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.43it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.51it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.54it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.44it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.50it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.34it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.36it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.32it/s][A
 39%|███▉      | 693/1759 [00:14<00:23, 46.33it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.48it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.48it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.48it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.51it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.45it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.41it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.41it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.39it/s][A
 42%|████▏     | 738/1759 [00:15<00:22, 46.31it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.33it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.43it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.53it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.53it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.43it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.41it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.42it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.41it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.31it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.33it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.36it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.35it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.51it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.49it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.45it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.44it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.39it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.34it/s][A
 47%|████▋     | 833/1759 [00:17<00:20, 46.23it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.27it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.34it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.40it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.52it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.54it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.48it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.38it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.31it/s][A
 50%|████▉     | 878/1759 [00:18<00:19, 46.32it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.33it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.34it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.39it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.39it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.42it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.52it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.43it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.45it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.34it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.30it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.35it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.32it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.41it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.33it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.46it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.44it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.41it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.44it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.44it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.40it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.36it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.38it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.39it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.32it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.40it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.46it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.32it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.39it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.37it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.36it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.39it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.41it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.31it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.32it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.33it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.42it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.42it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.35it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.20it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.10it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.11it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.09it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.24it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.31it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.38it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.38it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.37it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.39it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.37it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.40it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.42it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.42it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:14, 42.54it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:13, 43.65it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:13, 44.54it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:13, 45.04it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 45.48it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 45.85it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.03it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.14it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 45.69it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 45.77it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 45.83it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:12, 45.92it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.08it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.15it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.28it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.43it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.34it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.22it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.15it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.26it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.25it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.37it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.45it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.55it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.61it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.54it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.35it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.31it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 43.05it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:10, 44.10it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:10, 44.78it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 45.37it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 45.76it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.01it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.24it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.27it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 45.89it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 45.82it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:09, 46.16it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.22it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.35it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.43it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.52it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.55it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.56it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.42it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.32it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:08, 46.37it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.41it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.41it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.44it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.42it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.53it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.45it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.39it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.41it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.30it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.39it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.32it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.39it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.49it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.43it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.45it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.44it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.39it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.42it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.38it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.36it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.45it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.44it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.46it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.45it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.45it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.42it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.35it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.36it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.37it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.42it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.46it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.29it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.35it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.36it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.30it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.27it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 45.94it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.18it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.19it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.32it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.37it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.29it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.34it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.27it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.19it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.23it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.21it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.30it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.27it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.40it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.41it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.37it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.44it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.43it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.40it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.40it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.34it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.42it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.34it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.08it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.45it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.46it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.48it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.44it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.38it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.41it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.39it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.44it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.35it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.36it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.38it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.39it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.46it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.45it/s][A                                                 
                                                   [A 60%|██████    | 234/390 [03:17<00:45,  3.45it/s]
100%|██████████| 1759/1759 [00:37<00:00, 46.45it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 13:10:33,872 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:10:33,891 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:10:36,445 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:10:36,461 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:10:36,472 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:25<35:58, 13.93s/it] 61%|██████    | 236/390 [03:25<25:14,  9.84s/it] 61%|██████    | 237/390 [03:25<17:46,  6.97s/it] 61%|██████    | 238/390 [03:25<12:35,  4.97s/it] 61%|██████▏   | 239/390 [03:26<08:58,  3.56s/it] 62%|██████▏   | 240/390 [03:26<06:27,  2.58s/it] 62%|██████▏   | 241/390 [03:26<04:42,  1.89s/it] 62%|██████▏   | 242/390 [03:27<03:28,  1.41s/it] 62%|██████▏   | 243/390 [03:27<02:37,  1.07s/it] 63%|██████▎   | 244/390 [03:27<02:02,  1.19it/s] 63%|██████▎   | 245/390 [03:27<01:37,  1.48it/s] 63%|██████▎   | 246/390 [03:28<01:20,  1.79it/s] 63%|██████▎   | 247/390 [03:28<01:08,  2.09it/s] 64%|██████▎   | 248/390 [03:28<00:59,  2.38it/s] 64%|██████▍   | 249/390 [03:29<00:53,  2.62it/s] 64%|██████▍   | 250/390 [03:29<00:49,  2.83it/s] 64%|██████▍   | 251/390 [03:29<00:46,  3.00it/s] 65%|██████▍   | 252/390 [03:30<00:44,  3.12it/s] 65%|██████▍   | 253/390 [03:30<00:42,  3.22it/s] 65%|██████▌   | 254/390 [03:30<00:41,  3.29it/s] 65%|██████▌   | 255/390 [03:30<00:40,  3.34it/s] 66%|██████▌   | 256/390 [03:31<00:39,  3.38it/s] 66%|██████▌   | 257/390 [03:31<00:39,  3.41it/s] 66%|██████▌   | 258/390 [03:31<00:38,  3.42it/s] 66%|██████▋   | 259/390 [03:32<00:38,  3.44it/s] 67%|██████▋   | 260/390 [03:32<00:37,  3.43it/s] 67%|██████▋   | 261/390 [03:32<00:37,  3.44it/s] 67%|██████▋   | 262/390 [03:32<00:37,  3.45it/s] 67%|██████▋   | 263/390 [03:33<00:36,  3.46it/s] 68%|██████▊   | 264/390 [03:33<00:36,  3.46it/s] 68%|██████▊   | 265/390 [03:33<00:36,  3.46it/s] 68%|██████▊   | 266/390 [03:34<00:35,  3.46it/s] 68%|██████▊   | 267/390 [03:34<00:35,  3.47it/s] 69%|██████▊   | 268/390 [03:34<00:35,  3.47it/s] 69%|██████▉   | 269/390 [03:34<00:34,  3.47it/s] 69%|██████▉   | 270/390 [03:35<00:34,  3.47it/s] 69%|██████▉   | 271/390 [03:35<00:34,  3.46it/s] 70%|██████▉   | 272/390 [03:35<00:34,  3.46it/s] 70%|███████   | 273/390 [03:36<00:33,  3.46it/s] 70%|███████   | 274/390 [03:36<00:33,  3.46it/s] 71%|███████   | 275/390 [03:36<00:33,  3.46it/s] 71%|███████   | 276/390 [03:36<00:32,  3.46it/s] 71%|███████   | 277/390 [03:37<00:32,  3.46it/s] 71%|███████▏  | 278/390 [03:37<00:32,  3.47it/s] 72%|███████▏  | 279/390 [03:37<00:32,  3.47it/s] 72%|███████▏  | 280/390 [03:38<00:31,  3.47it/s] 72%|███████▏  | 281/390 [03:38<00:31,  3.47it/s] 72%|███████▏  | 282/390 [03:38<00:31,  3.45it/s] 73%|███████▎  | 283/390 [03:38<00:30,  3.46it/s] 73%|███████▎  | 284/390 [03:39<00:30,  3.46it/s] 73%|███████▎  | 285/390 [03:39<00:30,  3.46it/s] 73%|███████▎  | 286/390 [03:39<00:30,  3.46it/s] 74%|███████▎  | 287/390 [03:40<00:29,  3.46it/s] 74%|███████▍  | 288/390 [03:40<00:29,  3.47it/s] 74%|███████▍  | 289/390 [03:40<00:29,  3.46it/s] 74%|███████▍  | 290/390 [03:40<00:28,  3.47it/s] 75%|███████▍  | 291/390 [03:41<00:28,  3.46it/s] 75%|███████▍  | 292/390 [03:41<00:28,  3.47it/s] 75%|███████▌  | 293/390 [03:41<00:28,  3.43it/s] 75%|███████▌  | 294/390 [03:42<00:27,  3.44it/s] 76%|███████▌  | 295/390 [03:42<00:27,  3.44it/s] 76%|███████▌  | 296/390 [03:42<00:27,  3.45it/s] 76%|███████▌  | 297/390 [03:43<00:26,  3.45it/s] 76%|███████▋  | 298/390 [03:43<00:26,  3.46it/s] 77%|███████▋  | 299/390 [03:43<00:26,  3.46it/s] 77%|███████▋  | 300/390 [03:43<00:26,  3.46it/s] 77%|███████▋  | 301/390 [03:44<00:25,  3.46it/s] 77%|███████▋  | 302/390 [03:44<00:25,  3.46it/s] 78%|███████▊  | 303/390 [03:44<00:25,  3.45it/s] 78%|███████▊  | 304/390 [03:45<00:25,  3.42it/s] 78%|███████▊  | 305/390 [03:45<00:24,  3.43it/s] 78%|███████▊  | 306/390 [03:45<00:24,  3.43it/s] 79%|███████▊  | 307/390 [03:45<00:24,  3.44it/s] 79%|███████▉  | 308/390 [03:46<00:23,  3.45it/s] 79%|███████▉  | 309/390 [03:46<00:23,  3.45it/s] 79%|███████▉  | 310/390 [03:46<00:23,  3.35it/s] 80%|███████▉  | 311/390 [03:47<00:23,  3.38it/s] 80%|████████  | 312/390 [03:47<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 13:11:03,920 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:11:03,920 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:11:03,920 >>   Batch size = 8
{'eval_loss': 0.8652781248092651, 'eval_runtime': 37.9681, 'eval_samples_per_second': 370.495, 'eval_steps_per_second': 46.328, 'epoch': 2.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.00it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.36it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.54it/s][A
  1%|▏         | 23/1759 [00:00<00:38, 45.44it/s][A
  2%|▏         | 28/1759 [00:00<00:37, 45.85it/s][A
  2%|▏         | 33/1759 [00:00<00:37, 46.10it/s][A
  2%|▏         | 38/1759 [00:00<00:37, 46.26it/s][A
  2%|▏         | 43/1759 [00:01<01:14, 23.12it/s][A
  3%|▎         | 48/1759 [00:01<01:02, 27.39it/s][A
  3%|▎         | 53/1759 [00:01<00:54, 31.38it/s][A
  3%|▎         | 58/1759 [00:01<00:48, 34.85it/s][A
  4%|▎         | 63/1759 [00:01<00:44, 37.74it/s][A
  4%|▍         | 68/1759 [00:01<00:42, 40.09it/s][A
  4%|▍         | 73/1759 [00:01<00:40, 41.91it/s][A
  4%|▍         | 78/1759 [00:02<00:38, 43.25it/s][A
  5%|▍         | 83/1759 [00:02<00:37, 44.17it/s][A
  5%|▌         | 88/1759 [00:02<00:37, 44.64it/s][A
  5%|▌         | 93/1759 [00:02<00:36, 45.16it/s][A
  6%|▌         | 98/1759 [00:02<00:36, 45.61it/s][A
  6%|▌         | 103/1759 [00:02<00:36, 45.85it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.04it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.16it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.32it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.33it/s][A
  7%|▋         | 128/1759 [00:03<00:35, 46.28it/s][A
  8%|▊         | 133/1759 [00:03<00:35, 46.27it/s][A
  8%|▊         | 138/1759 [00:03<00:35, 46.28it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.26it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.16it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.31it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.36it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.46it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.51it/s][A
 10%|▉         | 173/1759 [00:04<00:34, 46.54it/s][A
 10%|█         | 178/1759 [00:04<00:33, 46.54it/s][A
 10%|█         | 183/1759 [00:04<00:33, 46.57it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.50it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.40it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.52it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.49it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.57it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.53it/s][A
 12%|█▏        | 218/1759 [00:05<00:33, 46.36it/s][A
 13%|█▎        | 223/1759 [00:05<00:33, 46.52it/s][A
 13%|█▎        | 228/1759 [00:05<00:32, 46.52it/s][A
 13%|█▎        | 233/1759 [00:05<00:32, 46.51it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.51it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.46it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.49it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.50it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.48it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.51it/s][A
 15%|█▌        | 268/1759 [00:06<00:32, 46.49it/s][A
 16%|█▌        | 273/1759 [00:06<00:31, 46.47it/s][A
 16%|█▌        | 278/1759 [00:06<00:31, 46.47it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.48it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.57it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.61it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.42it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.49it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.45it/s][A
 18%|█▊        | 313/1759 [00:07<00:31, 46.46it/s][A
 18%|█▊        | 318/1759 [00:07<00:31, 46.47it/s][A
 18%|█▊        | 323/1759 [00:07<00:30, 46.40it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.54it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.60it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.47it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.42it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.47it/s][A
 20%|██        | 358/1759 [00:08<00:30, 46.45it/s][A
 21%|██        | 363/1759 [00:08<00:30, 46.45it/s][A
 21%|██        | 368/1759 [00:08<00:29, 46.47it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.41it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.49it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.60it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.45it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.40it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.37it/s][A
 23%|██▎       | 403/1759 [00:09<00:29, 46.39it/s][A
 23%|██▎       | 408/1759 [00:09<00:29, 46.41it/s][A
 23%|██▎       | 413/1759 [00:09<00:29, 46.41it/s][A
 24%|██▍       | 418/1759 [00:09<00:28, 46.44it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.54it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.46it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.45it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.45it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.41it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.44it/s][A
 26%|██▌       | 453/1759 [00:10<00:28, 46.43it/s][A
 26%|██▌       | 458/1759 [00:10<00:28, 46.34it/s][A
 26%|██▋       | 463/1759 [00:10<00:27, 46.39it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.41it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.50it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.57it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.55it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.51it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.38it/s][A
 28%|██▊       | 498/1759 [00:11<00:27, 46.44it/s][A
 29%|██▊       | 503/1759 [00:11<00:27, 46.26it/s][A
 29%|██▉       | 508/1759 [00:11<00:26, 46.43it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.44it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.39it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.47it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.54it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.50it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.54it/s][A
 31%|███       | 543/1759 [00:12<00:26, 46.43it/s][A
 31%|███       | 548/1759 [00:12<00:26, 46.43it/s][A
 31%|███▏      | 553/1759 [00:12<00:26, 46.36it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.38it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.43it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.42it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.44it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.52it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.51it/s][A
 34%|███▎      | 593/1759 [00:13<00:25, 46.41it/s][A
 34%|███▍      | 598/1759 [00:13<00:25, 46.42it/s][A
 34%|███▍      | 603/1759 [00:13<00:24, 46.36it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.39it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.42it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.42it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.39it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.52it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.48it/s][A
 36%|███▋      | 638/1759 [00:14<00:24, 46.42it/s][A
 37%|███▋      | 643/1759 [00:14<00:24, 46.41it/s][A
 37%|███▋      | 648/1759 [00:14<00:23, 46.42it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.47it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.36it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.41it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.43it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.43it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.52it/s][A
 39%|███▉      | 683/1759 [00:15<00:23, 46.51it/s][A
 39%|███▉      | 688/1759 [00:15<00:23, 46.39it/s][A
 39%|███▉      | 693/1759 [00:15<00:22, 46.45it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.40it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.46it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.46it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.42it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.43it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.43it/s][A
 41%|████▏     | 728/1759 [00:16<00:22, 46.44it/s][A
 42%|████▏     | 733/1759 [00:16<00:22, 46.45it/s][A
 42%|████▏     | 738/1759 [00:16<00:22, 46.38it/s][A
 42%|████▏     | 743/1759 [00:16<00:21, 46.48it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.43it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.44it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.49it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.39it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.39it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.43it/s][A
 44%|████▍     | 778/1759 [00:17<00:21, 46.34it/s][A
 45%|████▍     | 783/1759 [00:17<00:21, 46.44it/s][A
 45%|████▍     | 788/1759 [00:17<00:20, 46.43it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.42it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.46it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.44it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.47it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.44it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.39it/s][A
 47%|████▋     | 823/1759 [00:18<00:20, 46.39it/s][A
 47%|████▋     | 828/1759 [00:18<00:20, 46.32it/s][A
 47%|████▋     | 833/1759 [00:18<00:19, 46.45it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.43it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.50it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.31it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.51it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.48it/s][A
 49%|████▉     | 868/1759 [00:19<00:19, 46.42it/s][A
 50%|████▉     | 873/1759 [00:19<00:19, 46.45it/s][A
 50%|████▉     | 878/1759 [00:19<00:19, 46.31it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.37it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.43it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.48it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.44it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.45it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.48it/s][A
 52%|█████▏    | 918/1759 [00:20<00:18, 46.40it/s][A
 52%|█████▏    | 923/1759 [00:20<00:18, 46.35it/s][A
 53%|█████▎    | 928/1759 [00:20<00:17, 46.40it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.40it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.44it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.35it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.41it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.47it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.46it/s][A
 55%|█████▍    | 963/1759 [00:21<00:17, 46.46it/s][A
 55%|█████▌    | 968/1759 [00:21<00:17, 46.34it/s][A
 55%|█████▌    | 973/1759 [00:21<00:16, 46.35it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.37it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.39it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.46it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.44it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.39it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.38it/s][A
 57%|█████▋    | 1008/1759 [00:22<00:16, 46.43it/s][A
 58%|█████▊    | 1013/1759 [00:22<00:16, 46.42it/s][A
 58%|█████▊    | 1018/1759 [00:22<00:16, 46.30it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.35it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.43it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.41it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.42it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.40it/s][A
 60%|█████▉    | 1053/1759 [00:23<00:15, 46.46it/s][A
 60%|██████    | 1058/1759 [00:23<00:15, 46.44it/s][A
 60%|██████    | 1063/1759 [00:23<00:15, 46.38it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.37it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.31it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.40it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.41it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.28it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.39it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.40it/s][A
 63%|██████▎   | 1103/1759 [00:24<00:14, 46.33it/s][A
 63%|██████▎   | 1108/1759 [00:24<00:14, 46.34it/s][A
 63%|██████▎   | 1113/1759 [00:24<00:13, 46.28it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.42it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.42it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.38it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.43it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.39it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.45it/s][A
 65%|██████▌   | 1148/1759 [00:25<00:13, 46.42it/s][A
 66%|██████▌   | 1153/1759 [00:25<00:13, 46.37it/s][A
 66%|██████▌   | 1158/1759 [00:25<00:12, 46.36it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.32it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.45it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.43it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.39it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.47it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.41it/s][A
 68%|██████▊   | 1193/1759 [00:26<00:12, 46.49it/s][A
 68%|██████▊   | 1198/1759 [00:26<00:12, 46.47it/s][A
 68%|██████▊   | 1203/1759 [00:26<00:11, 46.40it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:11, 46.46it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.36it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.43it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.45it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.43it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.31it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.33it/s][A
 71%|███████   | 1243/1759 [00:27<00:11, 46.39it/s][A
 71%|███████   | 1248/1759 [00:27<00:11, 46.40it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.39it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.42it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.35it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.36it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.44it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.35it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.37it/s][A
 73%|███████▎  | 1288/1759 [00:28<00:10, 46.36it/s][A
 74%|███████▎  | 1293/1759 [00:28<00:10, 46.39it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:09, 46.37it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.40it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.36it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.38it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.39it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.34it/s][A
 76%|███████▌  | 1333/1759 [00:29<00:09, 46.34it/s][A
 76%|███████▌  | 1338/1759 [00:29<00:09, 46.39it/s][A
 76%|███████▋  | 1343/1759 [00:29<00:08, 46.41it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.45it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.43it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.31it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.40it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.40it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.45it/s][A
 78%|███████▊  | 1378/1759 [00:30<00:08, 46.35it/s][A
 79%|███████▊  | 1383/1759 [00:30<00:08, 46.36it/s][A
 79%|███████▉  | 1388/1759 [00:30<00:08, 46.36it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.37it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.45it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.44it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.42it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.39it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.35it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.40it/s][A
 81%|████████  | 1428/1759 [00:31<00:07, 46.35it/s][A
 81%|████████▏ | 1433/1759 [00:31<00:07, 46.36it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.35it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.36it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.43it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.42it/s][A
 84%|████████▎ | 1473/1759 [00:32<00:06, 46.29it/s][A
 84%|████████▍ | 1478/1759 [00:32<00:06, 46.35it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.36it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.41it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.41it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.43it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.46it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.45it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.37it/s][A
 86%|████████▋ | 1518/1759 [00:33<00:05, 46.41it/s][A
 87%|████████▋ | 1523/1759 [00:33<00:05, 46.33it/s][A
 87%|████████▋ | 1528/1759 [00:33<00:05, 40.08it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:05, 41.87it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:05, 43.13it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 44.17it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 44.87it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 45.31it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 45.64it/s][A
 89%|████████▉ | 1563/1759 [00:34<00:04, 45.85it/s][A
 89%|████████▉ | 1568/1759 [00:34<00:04, 45.35it/s][A
 89%|████████▉ | 1573/1759 [00:34<00:04, 45.52it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 45.89it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.11it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.29it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.31it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.49it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.45it/s][A
 91%|█████████▏| 1608/1759 [00:35<00:03, 46.42it/s][A
 92%|█████████▏| 1613/1759 [00:35<00:03, 46.15it/s][A
 92%|█████████▏| 1618/1759 [00:35<00:03, 46.17it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.18it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.24it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.41it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.50it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.57it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.61it/s][A
 94%|█████████▍| 1653/1759 [00:36<00:02, 46.33it/s][A
 94%|█████████▍| 1658/1759 [00:36<00:02, 46.28it/s][A
 95%|█████████▍| 1663/1759 [00:36<00:02, 46.13it/s][A
 95%|█████████▍| 1668/1759 [00:36<00:01, 46.13it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.21it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.19it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.29it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.28it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.34it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.24it/s][A
 97%|█████████▋| 1703/1759 [00:37<00:01, 46.20it/s][A
 97%|█████████▋| 1708/1759 [00:37<00:01, 46.10it/s][A
 97%|█████████▋| 1713/1759 [00:37<00:01, 45.95it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.16it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.24it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.32it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.45it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.48it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▉| 1748/1759 [00:38<00:00, 46.42it/s][A
100%|█████████▉| 1753/1759 [00:38<00:00, 46.35it/s][A
100%|█████████▉| 1758/1759 [00:38<00:00, 46.28it/s][A                                                 
                                                   [A 80%|████████  | 312/390 [04:25<00:22,  3.40it/s]
100%|██████████| 1759/1759 [00:38<00:00, 46.28it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 13:11:42,447 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 13:11:42,646 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:11:46,242 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:11:46,261 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:11:46,275 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:35<18:44, 14.61s/it] 81%|████████  | 314/390 [04:35<13:03, 10.31s/it] 81%|████████  | 315/390 [04:35<09:07,  7.31s/it] 81%|████████  | 316/390 [04:36<06:24,  5.20s/it] 81%|████████▏ | 317/390 [04:36<04:32,  3.73s/it] 82%|████████▏ | 318/390 [04:36<03:14,  2.70s/it] 82%|████████▏ | 319/390 [04:37<02:20,  1.97s/it] 82%|████████▏ | 320/390 [04:37<01:42,  1.47s/it] 82%|████████▏ | 321/390 [04:37<01:16,  1.11s/it] 83%|████████▎ | 322/390 [04:38<00:58,  1.15it/s] 83%|████████▎ | 323/390 [04:38<00:46,  1.44it/s] 83%|████████▎ | 324/390 [04:38<00:37,  1.75it/s] 83%|████████▎ | 325/390 [04:38<00:31,  2.05it/s] 84%|████████▎ | 326/390 [04:39<00:27,  2.34it/s] 84%|████████▍ | 327/390 [04:39<00:24,  2.59it/s] 84%|████████▍ | 328/390 [04:39<00:22,  2.80it/s] 84%|████████▍ | 329/390 [04:40<00:20,  2.98it/s] 85%|████████▍ | 330/390 [04:40<00:19,  3.11it/s] 85%|████████▍ | 331/390 [04:40<00:18,  3.21it/s] 85%|████████▌ | 332/390 [04:40<00:17,  3.28it/s] 85%|████████▌ | 333/390 [04:41<00:17,  3.34it/s] 86%|████████▌ | 334/390 [04:41<00:16,  3.38it/s] 86%|████████▌ | 335/390 [04:41<00:16,  3.40it/s] 86%|████████▌ | 336/390 [04:42<00:15,  3.41it/s] 86%|████████▋ | 337/390 [04:42<00:15,  3.43it/s] 87%|████████▋ | 338/390 [04:42<00:15,  3.44it/s] 87%|████████▋ | 339/390 [04:42<00:14,  3.45it/s] 87%|████████▋ | 340/390 [04:43<00:14,  3.45it/s] 87%|████████▋ | 341/390 [04:43<00:14,  3.45it/s] 88%|████████▊ | 342/390 [04:43<00:13,  3.46it/s] 88%|████████▊ | 343/390 [04:44<00:13,  3.46it/s] 88%|████████▊ | 344/390 [04:44<00:13,  3.47it/s] 88%|████████▊ | 345/390 [04:44<00:12,  3.47it/s] 89%|████████▊ | 346/390 [04:44<00:12,  3.47it/s] 89%|████████▉ | 347/390 [04:45<00:12,  3.45it/s] 89%|████████▉ | 348/390 [04:45<00:12,  3.46it/s] 89%|████████▉ | 349/390 [04:45<00:11,  3.46it/s] 90%|████████▉ | 350/390 [04:46<00:11,  3.46it/s] 90%|█████████ | 351/390 [04:46<00:11,  3.46it/s] 90%|█████████ | 352/390 [04:46<00:10,  3.46it/s] 91%|█████████ | 353/390 [04:46<00:10,  3.46it/s] 91%|█████████ | 354/390 [04:47<00:10,  3.46it/s] 91%|█████████ | 355/390 [04:47<00:10,  3.46it/s] 91%|█████████▏| 356/390 [04:47<00:09,  3.47it/s] 92%|█████████▏| 357/390 [04:48<00:09,  3.47it/s] 92%|█████████▏| 358/390 [04:48<00:09,  3.45it/s] 92%|█████████▏| 359/390 [04:48<00:08,  3.45it/s] 92%|█████████▏| 360/390 [04:48<00:08,  3.46it/s] 93%|█████████▎| 361/390 [04:49<00:08,  3.46it/s] 93%|█████████▎| 362/390 [04:49<00:08,  3.45it/s] 93%|█████████▎| 363/390 [04:49<00:07,  3.46it/s] 93%|█████████▎| 364/390 [04:50<00:07,  3.46it/s] 94%|█████████▎| 365/390 [04:50<00:07,  3.46it/s] 94%|█████████▍| 366/390 [04:50<00:06,  3.46it/s] 94%|█████████▍| 367/390 [04:51<00:06,  3.47it/s] 94%|█████████▍| 368/390 [04:51<00:06,  3.46it/s] 95%|█████████▍| 369/390 [04:51<00:06,  3.45it/s] 95%|█████████▍| 370/390 [04:51<00:05,  3.46it/s] 95%|█████████▌| 371/390 [04:52<00:05,  3.46it/s] 95%|█████████▌| 372/390 [04:52<00:05,  3.46it/s] 96%|█████████▌| 373/390 [04:52<00:04,  3.46it/s] 96%|█████████▌| 374/390 [04:53<00:04,  3.46it/s] 96%|█████████▌| 375/390 [04:53<00:04,  3.46it/s] 96%|█████████▋| 376/390 [04:53<00:04,  3.46it/s] 97%|█████████▋| 377/390 [04:53<00:03,  3.46it/s] 97%|█████████▋| 378/390 [04:54<00:03,  3.46it/s] 97%|█████████▋| 379/390 [04:54<00:03,  3.46it/s] 97%|█████████▋| 380/390 [04:54<00:02,  3.45it/s] 98%|█████████▊| 381/390 [04:55<00:02,  3.45it/s] 98%|█████████▊| 382/390 [04:55<00:02,  3.45it/s] 98%|█████████▊| 383/390 [04:55<00:02,  3.46it/s] 98%|█████████▊| 384/390 [04:55<00:01,  3.46it/s] 99%|█████████▊| 385/390 [04:56<00:01,  3.46it/s] 99%|█████████▉| 386/390 [04:56<00:01,  3.46it/s] 99%|█████████▉| 387/390 [04:56<00:00,  3.46it/s] 99%|█████████▉| 388/390 [04:57<00:00,  3.46it/s]100%|█████████▉| 389/390 [04:57<00:00,  3.46it/s]100%|██████████| 390/390 [04:57<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:12:14,145 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:12:14,145 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:12:14,145 >>   Batch size = 8
{'eval_loss': 0.8716601133346558, 'eval_runtime': 38.3116, 'eval_samples_per_second': 367.173, 'eval_steps_per_second': 45.913, 'epoch': 3.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.76it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.48it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.64it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.99it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.57it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.27it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.11it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.82it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.57it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.67it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.59it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.64it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.66it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.62it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.63it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.58it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.50it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.43it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.45it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.46it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.60it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.63it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.59it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.61it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.50it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.52it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.47it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.45it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.35it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.45it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.60it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.64it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.59it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.59it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.52it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.47it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.40it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.29it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.36it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.33it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.38it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.48it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.49it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.53it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.50it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.42it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.35it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.37it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.51it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.48it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.58it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.62it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.59it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.53it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.54it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.49it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.41it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.44it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.35it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.50it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.42it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.48it/s][A
 18%|█▊        | 318/1759 [00:06<00:31, 46.42it/s][A
 18%|█▊        | 323/1759 [00:06<00:31, 46.31it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.28it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.15it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.22it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.18it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.23it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.44it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.52it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.49it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.52it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.45it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.48it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.41it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.39it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.50it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.43it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.58it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.57it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.45it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.52it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.51it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.40it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.44it/s][A
 25%|██▍       | 438/1759 [00:09<00:30, 42.89it/s][A
 25%|██▌       | 443/1759 [00:09<00:29, 43.97it/s][A
 25%|██▌       | 448/1759 [00:09<00:29, 44.74it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 45.31it/s][A
 26%|██▌       | 458/1759 [00:09<00:28, 45.72it/s][A
 26%|██▋       | 463/1759 [00:09<00:28, 46.02it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.28it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.33it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 45.87it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 45.99it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.14it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.32it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.45it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.50it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.57it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.60it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.61it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.39it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.27it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.30it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.43it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.54it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.55it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.56it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.63it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.59it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.45it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.28it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.32it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.37it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.51it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.48it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.57it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.60it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.59it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.45it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.30it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.28it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.32it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.43it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.51it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.47it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.48it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.56it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.57it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.53it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.47it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.36it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.35it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.51it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.49it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.38it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.54it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.50it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.50it/s][A
 41%|████      | 718/1759 [00:15<00:22, 45.70it/s][A
 41%|████      | 723/1759 [00:15<00:22, 45.94it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.09it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.22it/s][A
 42%|████▏     | 738/1759 [00:15<00:22, 46.33it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.33it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.42it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.46it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.26it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.21it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.20it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.29it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.42it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.42it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.45it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.34it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.47it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.47it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.46it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.50it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.40it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.36it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.49it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.47it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.40it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.42it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.48it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.47it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.50it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.41it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.41it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.48it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.46it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.45it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.44it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.39it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.47it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.39it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.40it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.36it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.46it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.45it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.44it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.20it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.15it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.08it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 45.97it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.27it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.36it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.39it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.25it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.42it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.40it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.43it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.51it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.38it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.43it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.37it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.30it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.35it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.38it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.46it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.43it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.45it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.37it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.35it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.38it/s][A
 60%|██████    | 1063/1759 [00:22<00:15, 46.38it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.42it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.43it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.27it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.40it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.41it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.41it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.32it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.30it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.35it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.39it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.46it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.45it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.41it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.42it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.34it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.41it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.39it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.41it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.39it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.41it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.39it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.44it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 46.25it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:11, 46.33it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.34it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.28it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.44it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.46it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.39it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.47it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.42it/s][A
 71%|███████   | 1248/1759 [00:26<00:10, 46.46it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.37it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.33it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.33it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.42it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.40it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.38it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.47it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.34it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.38it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.24it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.28it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.33it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.34it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.33it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.46it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.39it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.35it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.39it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.39it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.31it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.22it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.21it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.28it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.39it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.41it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.39it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.45it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.45it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.41it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.38it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.19it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.30it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.17it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.16it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.32it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.38it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.42it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.44it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.30it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.20it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.20it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.29it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.33it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.39it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.39it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.41it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.46it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.43it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.37it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.27it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.28it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.26it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.31it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.40it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.39it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.40it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.34it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.39it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.33it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.33it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.35it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.20it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.29it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.37it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.42it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.40it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.44it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.34it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.31it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.36it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.39it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.39it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.35it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.22it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.40it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.42it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.40it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.33it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.41it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.30it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.35it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.37it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.31it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.41it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.30it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.29it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.35it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.40it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.39it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.23it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.34it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.36it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.34it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.37it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.33it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.27it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.37it/s][A                                                 
                                                   [A100%|██████████| 390/390 [05:35<00:00,  3.46it/s]
100%|██████████| 1759/1759 [00:37<00:00, 46.37it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 13:12:52,095 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 13:12:52,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:12:54,504 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:12:54,519 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:12:54,527 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:13:00,548 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:13:00,550 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78 (score: 0.8475257158279419).
                                                 100%|██████████| 390/390 [05:45<00:00,  3.46it/s]100%|██████████| 390/390 [05:45<00:00,  1.13it/s]
[INFO|trainer.py:1894] 2023-08-28 13:13:02,438 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 13:13:02,636 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:13:07,135 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:13:07,396 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:13:07,486 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:13:07,709 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   train_loss               =      0.669
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   train_runtime            = 0:05:45.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   train_samples_per_second =     72.272
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:07,709 >>   train_steps_per_second   =      1.127
{'eval_loss': 0.874146044254303, 'eval_runtime': 37.9272, 'eval_samples_per_second': 370.895, 'eval_steps_per_second': 46.378, 'epoch': 4.99}
{'train_runtime': 345.9143, 'train_samples_per_second': 72.272, 'train_steps_per_second': 1.127, 'train_loss': 0.6690040784004407, 'epoch': 4.99}
08/28/2023 13:13:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:13:07,743 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:13:07,744 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 13:13:07,744 >>   Batch size = 8
  0%|          | 0/1759 [00:00<?, ?it/s]  0%|          | 6/1759 [00:00<00:30, 57.77it/s]  1%|          | 12/1759 [00:00<00:34, 50.95it/s]  1%|          | 18/1759 [00:00<00:35, 49.00it/s]  1%|▏         | 23/1759 [00:00<00:35, 48.33it/s]  2%|▏         | 28/1759 [00:00<00:36, 47.95it/s]  2%|▏         | 33/1759 [00:00<00:36, 47.64it/s]  2%|▏         | 38/1759 [00:00<00:36, 47.40it/s]  2%|▏         | 43/1759 [00:00<00:36, 47.26it/s]  3%|▎         | 48/1759 [00:00<00:36, 46.95it/s]  3%|▎         | 53/1759 [00:01<00:36, 46.91it/s]  3%|▎         | 58/1759 [00:01<00:36, 46.93it/s]  4%|▎         | 63/1759 [00:01<00:36, 47.00it/s]  4%|▍         | 68/1759 [00:01<00:36, 46.94it/s]  4%|▍         | 73/1759 [00:01<00:35, 46.92it/s]  4%|▍         | 78/1759 [00:01<00:35, 47.02it/s]  5%|▍         | 83/1759 [00:01<00:35, 46.95it/s]  5%|▌         | 88/1759 [00:01<00:35, 46.84it/s]  5%|▌         | 93/1759 [00:01<00:35, 46.72it/s]  6%|▌         | 98/1759 [00:02<00:35, 46.65it/s]  6%|▌         | 103/1759 [00:02<00:35, 46.70it/s]  6%|▌         | 108/1759 [00:02<00:35, 46.75it/s]  6%|▋         | 113/1759 [00:02<00:35, 46.66it/s]  7%|▋         | 118/1759 [00:02<00:38, 42.60it/s]  7%|▋         | 123/1759 [00:02<00:37, 43.88it/s]  7%|▋         | 128/1759 [00:02<00:36, 44.87it/s]  8%|▊         | 133/1759 [00:02<00:35, 45.38it/s]  8%|▊         | 138/1759 [00:02<00:35, 45.96it/s]  8%|▊         | 143/1759 [00:03<00:34, 46.34it/s]  8%|▊         | 148/1759 [00:03<00:34, 46.59it/s]  9%|▊         | 153/1759 [00:03<00:34, 46.81it/s]  9%|▉         | 158/1759 [00:03<00:34, 46.37it/s]  9%|▉         | 163/1759 [00:03<00:34, 46.28it/s] 10%|▉         | 168/1759 [00:03<00:34, 46.52it/s] 10%|▉         | 173/1759 [00:03<00:33, 46.67it/s] 10%|█         | 178/1759 [00:03<00:33, 46.82it/s] 10%|█         | 183/1759 [00:03<00:33, 46.85it/s] 11%|█         | 188/1759 [00:04<00:33, 47.00it/s] 11%|█         | 193/1759 [00:04<00:33, 47.04it/s] 11%|█▏        | 198/1759 [00:04<00:33, 47.02it/s] 12%|█▏        | 203/1759 [00:04<00:33, 46.71it/s] 12%|█▏        | 208/1759 [00:04<00:33, 46.66it/s] 12%|█▏        | 213/1759 [00:04<00:33, 46.66it/s] 12%|█▏        | 218/1759 [00:04<00:32, 46.71it/s] 13%|█▎        | 223/1759 [00:04<00:32, 46.78it/s] 13%|█▎        | 228/1759 [00:04<00:32, 46.85it/s] 13%|█▎        | 233/1759 [00:04<00:32, 46.93it/s] 14%|█▎        | 238/1759 [00:05<00:32, 47.00it/s] 14%|█▍        | 243/1759 [00:05<00:32, 46.99it/s] 14%|█▍        | 248/1759 [00:05<00:32, 46.87it/s] 14%|█▍        | 253/1759 [00:05<00:32, 46.71it/s] 15%|█▍        | 258/1759 [00:05<00:32, 46.63it/s] 15%|█▍        | 263/1759 [00:05<00:32, 46.65it/s] 15%|█▌        | 268/1759 [00:05<00:31, 46.84it/s] 16%|█▌        | 273/1759 [00:05<00:31, 46.97it/s] 16%|█▌        | 278/1759 [00:05<00:31, 46.92it/s] 16%|█▌        | 283/1759 [00:06<00:31, 46.92it/s] 16%|█▋        | 288/1759 [00:06<00:31, 46.91it/s] 17%|█▋        | 293/1759 [00:06<00:31, 46.82it/s] 17%|█▋        | 298/1759 [00:06<00:31, 46.76it/s] 17%|█▋        | 303/1759 [00:06<00:31, 46.72it/s] 18%|█▊        | 308/1759 [00:06<00:31, 46.65it/s] 18%|█▊        | 313/1759 [00:06<00:30, 46.77it/s] 18%|█▊        | 318/1759 [00:06<00:30, 46.81it/s] 18%|█▊        | 323/1759 [00:06<00:30, 46.87it/s] 19%|█▊        | 328/1759 [00:07<00:30, 46.98it/s] 19%|█▉        | 333/1759 [00:07<00:30, 46.92it/s] 19%|█▉        | 338/1759 [00:07<00:30, 46.80it/s] 19%|█▉        | 343/1759 [00:07<00:30, 46.79it/s] 20%|█▉        | 348/1759 [00:07<00:30, 46.64it/s] 20%|██        | 353/1759 [00:07<00:30, 46.65it/s] 20%|██        | 358/1759 [00:07<00:30, 46.69it/s] 21%|██        | 363/1759 [00:07<00:29, 46.71it/s] 21%|██        | 368/1759 [00:07<00:29, 46.86it/s] 21%|██        | 373/1759 [00:07<00:29, 46.99it/s] 21%|██▏       | 378/1759 [00:08<00:29, 47.01it/s] 22%|██▏       | 383/1759 [00:08<00:29, 46.78it/s] 22%|██▏       | 388/1759 [00:08<00:29, 46.78it/s] 22%|██▏       | 393/1759 [00:08<00:29, 46.67it/s] 23%|██▎       | 398/1759 [00:08<00:29, 46.70it/s] 23%|██▎       | 403/1759 [00:08<00:29, 46.70it/s] 23%|██▎       | 408/1759 [00:08<00:28, 46.69it/s] 23%|██▎       | 413/1759 [00:08<00:28, 46.65it/s] 24%|██▍       | 418/1759 [00:08<00:28, 46.87it/s] 24%|██▍       | 423/1759 [00:09<00:28, 46.88it/s] 24%|██▍       | 428/1759 [00:09<00:28, 46.85it/s] 25%|██▍       | 433/1759 [00:09<00:28, 46.63it/s] 25%|██▍       | 438/1759 [00:09<00:28, 46.54it/s] 25%|██▌       | 443/1759 [00:09<00:28, 46.52it/s] 25%|██▌       | 448/1759 [00:09<00:28, 46.57it/s] 26%|██▌       | 453/1759 [00:09<00:27, 46.65it/s] 26%|██▌       | 458/1759 [00:09<00:27, 46.72it/s] 26%|██▋       | 463/1759 [00:09<00:27, 46.76it/s] 27%|██▋       | 468/1759 [00:10<00:27, 46.82it/s] 27%|██▋       | 473/1759 [00:10<00:27, 46.79it/s] 27%|██▋       | 478/1759 [00:10<00:27, 46.85it/s] 27%|██▋       | 483/1759 [00:10<00:27, 46.76it/s] 28%|██▊       | 488/1759 [00:10<00:27, 46.65it/s] 28%|██▊       | 493/1759 [00:10<00:27, 46.57it/s] 28%|██▊       | 498/1759 [00:10<00:27, 46.68it/s] 29%|██▊       | 503/1759 [00:10<00:26, 46.69it/s] 29%|██▉       | 508/1759 [00:10<00:26, 46.71it/s] 29%|██▉       | 513/1759 [00:10<00:26, 46.75it/s] 29%|██▉       | 518/1759 [00:11<00:26, 46.82it/s] 30%|██▉       | 523/1759 [00:11<00:26, 46.79it/s] 30%|███       | 528/1759 [00:11<00:26, 46.80it/s] 30%|███       | 533/1759 [00:11<00:26, 46.70it/s] 31%|███       | 538/1759 [00:11<00:26, 46.57it/s] 31%|███       | 543/1759 [00:11<00:26, 46.65it/s] 31%|███       | 548/1759 [00:11<00:25, 46.75it/s] 31%|███▏      | 553/1759 [00:11<00:25, 46.77it/s] 32%|███▏      | 558/1759 [00:11<00:25, 46.76it/s] 32%|███▏      | 563/1759 [00:12<00:25, 46.73it/s] 32%|███▏      | 568/1759 [00:12<00:25, 46.68it/s] 33%|███▎      | 573/1759 [00:12<00:25, 46.74it/s] 33%|███▎      | 578/1759 [00:12<00:25, 46.65it/s] 33%|███▎      | 583/1759 [00:12<00:25, 46.71it/s] 33%|███▎      | 588/1759 [00:12<00:25, 46.67it/s] 34%|███▎      | 593/1759 [00:12<00:24, 46.65it/s] 34%|███▍      | 598/1759 [00:12<00:24, 46.76it/s] 34%|███▍      | 603/1759 [00:12<00:24, 46.83it/s] 35%|███▍      | 608/1759 [00:13<00:24, 46.80it/s] 35%|███▍      | 613/1759 [00:13<00:24, 46.77it/s] 35%|███▌      | 618/1759 [00:13<00:24, 46.81it/s] 35%|███▌      | 623/1759 [00:13<00:24, 46.66it/s] 36%|███▌      | 628/1759 [00:13<00:24, 46.71it/s] 36%|███▌      | 633/1759 [00:13<00:24, 46.69it/s] 36%|███▋      | 638/1759 [00:13<00:24, 46.70it/s] 37%|███▋      | 643/1759 [00:13<00:23, 46.82it/s] 37%|███▋      | 648/1759 [00:13<00:23, 46.71it/s] 37%|███▋      | 653/1759 [00:13<00:23, 46.67it/s] 37%|███▋      | 658/1759 [00:14<00:23, 46.66it/s] 38%|███▊      | 663/1759 [00:14<00:23, 46.72it/s] 38%|███▊      | 668/1759 [00:14<00:23, 46.68it/s] 38%|███▊      | 673/1759 [00:14<00:23, 46.69it/s] 39%|███▊      | 678/1759 [00:14<00:23, 46.66it/s] 39%|███▉      | 683/1759 [00:14<00:23, 46.61it/s] 39%|███▉      | 688/1759 [00:14<00:22, 46.68it/s] 39%|███▉      | 693/1759 [00:14<00:22, 46.71it/s] 40%|███▉      | 698/1759 [00:14<00:22, 46.73it/s] 40%|███▉      | 703/1759 [00:15<00:22, 46.61it/s] 40%|████      | 708/1759 [00:15<00:22, 46.64it/s] 41%|████      | 713/1759 [00:15<00:22, 46.56it/s] 41%|████      | 718/1759 [00:15<00:22, 46.63it/s] 41%|████      | 723/1759 [00:15<00:22, 46.60it/s] 41%|████▏     | 728/1759 [00:15<00:22, 46.59it/s] 42%|████▏     | 733/1759 [00:15<00:21, 46.64it/s] 42%|████▏     | 738/1759 [00:15<00:21, 46.74it/s] 42%|████▏     | 743/1759 [00:15<00:21, 46.60it/s] 43%|████▎     | 748/1759 [00:16<00:21, 46.65it/s] 43%|████▎     | 753/1759 [00:16<00:21, 46.69it/s] 43%|████▎     | 758/1759 [00:16<00:21, 46.65it/s] 43%|████▎     | 763/1759 [00:16<00:21, 46.60it/s] 44%|████▎     | 768/1759 [00:16<00:21, 46.67it/s] 44%|████▍     | 773/1759 [00:16<00:21, 46.40it/s] 44%|████▍     | 778/1759 [00:16<00:21, 46.64it/s] 45%|████▍     | 783/1759 [00:16<00:20, 46.74it/s] 45%|████▍     | 788/1759 [00:16<00:20, 46.75it/s] 45%|████▌     | 793/1759 [00:16<00:20, 46.79it/s] 45%|████▌     | 798/1759 [00:17<00:20, 46.79it/s] 46%|████▌     | 803/1759 [00:17<00:20, 46.66it/s] 46%|████▌     | 808/1759 [00:17<00:20, 46.71it/s] 46%|████▌     | 813/1759 [00:17<00:20, 46.62it/s] 47%|████▋     | 818/1759 [00:17<00:20, 46.64it/s] 47%|████▋     | 823/1759 [00:17<00:20, 46.67it/s] 47%|████▋     | 828/1759 [00:17<00:19, 46.66it/s] 47%|████▋     | 833/1759 [00:17<00:19, 46.63it/s] 48%|████▊     | 838/1759 [00:17<00:19, 46.70it/s] 48%|████▊     | 843/1759 [00:18<00:19, 46.73it/s] 48%|████▊     | 848/1759 [00:18<00:19, 46.62it/s] 48%|████▊     | 853/1759 [00:18<00:19, 46.69it/s] 49%|████▉     | 858/1759 [00:18<00:19, 46.67it/s] 49%|████▉     | 863/1759 [00:18<00:19, 46.60it/s] 49%|████▉     | 868/1759 [00:18<00:19, 46.52it/s] 50%|████▉     | 873/1759 [00:18<00:19, 46.57it/s] 50%|████▉     | 878/1759 [00:18<00:18, 46.64it/s] 50%|█████     | 883/1759 [00:18<00:18, 46.66it/s] 50%|█████     | 888/1759 [00:19<00:18, 46.68it/s] 51%|█████     | 893/1759 [00:19<00:18, 46.59it/s] 51%|█████     | 898/1759 [00:19<00:18, 46.57it/s] 51%|█████▏    | 903/1759 [00:19<00:18, 46.66it/s] 52%|█████▏    | 908/1759 [00:19<00:18, 46.71it/s] 52%|█████▏    | 913/1759 [00:19<00:18, 46.66it/s] 52%|█████▏    | 918/1759 [00:19<00:18, 46.53it/s] 52%|█████▏    | 923/1759 [00:19<00:17, 46.54it/s] 53%|█████▎    | 928/1759 [00:19<00:17, 46.56it/s] 53%|█████▎    | 933/1759 [00:19<00:17, 46.62it/s] 53%|█████▎    | 938/1759 [00:20<00:17, 46.65it/s] 54%|█████▎    | 943/1759 [00:20<00:17, 46.66it/s] 54%|█████▍    | 948/1759 [00:20<00:17, 46.59it/s] 54%|█████▍    | 953/1759 [00:20<00:17, 46.59it/s] 54%|█████▍    | 958/1759 [00:20<00:17, 46.58it/s] 55%|█████▍    | 963/1759 [00:20<00:17, 46.48it/s] 55%|█████▌    | 968/1759 [00:20<00:17, 46.50it/s] 55%|█████▌    | 973/1759 [00:20<00:16, 46.54it/s] 56%|█████▌    | 978/1759 [00:20<00:16, 46.38it/s] 56%|█████▌    | 983/1759 [00:21<00:16, 46.46it/s] 56%|█████▌    | 988/1759 [00:21<00:16, 46.41it/s] 56%|█████▋    | 993/1759 [00:21<00:16, 46.41it/s] 57%|█████▋    | 998/1759 [00:21<00:16, 46.38it/s] 57%|█████▋    | 1003/1759 [00:21<00:16, 46.37it/s] 57%|█████▋    | 1008/1759 [00:21<00:16, 46.20it/s] 58%|█████▊    | 1013/1759 [00:21<00:16, 46.37it/s] 58%|█████▊    | 1018/1759 [00:21<00:15, 46.38it/s] 58%|█████▊    | 1023/1759 [00:21<00:15, 46.48it/s] 58%|█████▊    | 1028/1759 [00:22<00:15, 46.52it/s] 59%|█████▊    | 1033/1759 [00:22<00:15, 46.55it/s] 59%|█████▉    | 1038/1759 [00:22<00:15, 46.57it/s] 59%|█████▉    | 1043/1759 [00:22<00:15, 46.46it/s] 60%|█████▉    | 1048/1759 [00:22<00:15, 46.56it/s] 60%|█████▉    | 1053/1759 [00:22<00:15, 46.57it/s] 60%|██████    | 1058/1759 [00:22<00:15, 46.43it/s] 60%|██████    | 1063/1759 [00:22<00:14, 46.52it/s] 61%|██████    | 1068/1759 [00:22<00:14, 46.46it/s] 61%|██████    | 1073/1759 [00:22<00:14, 46.44it/s] 61%|██████▏   | 1078/1759 [00:23<00:14, 46.60it/s] 62%|██████▏   | 1083/1759 [00:23<00:14, 46.58it/s] 62%|██████▏   | 1088/1759 [00:23<00:14, 46.51it/s] 62%|██████▏   | 1093/1759 [00:23<00:14, 46.57it/s] 62%|██████▏   | 1098/1759 [00:23<00:14, 46.56it/s] 63%|██████▎   | 1103/1759 [00:23<00:14, 46.44it/s] 63%|██████▎   | 1108/1759 [00:23<00:14, 46.44it/s] 63%|██████▎   | 1113/1759 [00:23<00:13, 46.49it/s] 64%|██████▎   | 1118/1759 [00:23<00:13, 46.58it/s] 64%|██████▍   | 1123/1759 [00:24<00:13, 46.54it/s] 64%|██████▍   | 1128/1759 [00:24<00:13, 46.42it/s] 64%|██████▍   | 1133/1759 [00:24<00:13, 46.45it/s] 65%|██████▍   | 1138/1759 [00:24<00:13, 46.42it/s] 65%|██████▍   | 1143/1759 [00:24<00:13, 46.55it/s] 65%|██████▌   | 1148/1759 [00:24<00:13, 46.53it/s] 66%|██████▌   | 1153/1759 [00:24<00:13, 46.50it/s] 66%|██████▌   | 1158/1759 [00:24<00:12, 46.38it/s] 66%|██████▌   | 1163/1759 [00:24<00:12, 46.50it/s] 66%|██████▋   | 1168/1759 [00:25<00:12, 46.61it/s] 67%|██████▋   | 1173/1759 [00:25<00:12, 46.50it/s] 67%|██████▋   | 1178/1759 [00:25<00:12, 46.47it/s] 67%|██████▋   | 1183/1759 [00:25<00:12, 46.57it/s] 68%|██████▊   | 1188/1759 [00:25<00:12, 46.57it/s] 68%|██████▊   | 1193/1759 [00:25<00:12, 46.42it/s] 68%|██████▊   | 1198/1759 [00:25<00:12, 46.44it/s] 68%|██████▊   | 1203/1759 [00:25<00:11, 46.34it/s] 69%|██████▊   | 1208/1759 [00:25<00:11, 46.49it/s] 69%|██████▉   | 1213/1759 [00:25<00:11, 46.53it/s] 69%|██████▉   | 1218/1759 [00:26<00:11, 46.51it/s] 70%|██████▉   | 1223/1759 [00:26<00:11, 46.34it/s] 70%|██████▉   | 1228/1759 [00:26<00:11, 46.38it/s] 70%|███████   | 1233/1759 [00:26<00:11, 46.44it/s] 70%|███████   | 1238/1759 [00:26<00:11, 46.41it/s] 71%|███████   | 1243/1759 [00:26<00:11, 46.19it/s] 71%|███████   | 1248/1759 [00:26<00:11, 46.16it/s] 71%|███████   | 1253/1759 [00:26<00:10, 46.10it/s] 72%|███████▏  | 1258/1759 [00:26<00:10, 46.33it/s] 72%|███████▏  | 1263/1759 [00:27<00:10, 46.36it/s] 72%|███████▏  | 1268/1759 [00:27<00:10, 46.38it/s] 72%|███████▏  | 1273/1759 [00:27<00:10, 46.34it/s] 73%|███████▎  | 1278/1759 [00:27<00:10, 46.46it/s] 73%|███████▎  | 1283/1759 [00:27<00:10, 46.46it/s] 73%|███████▎  | 1288/1759 [00:27<00:10, 46.43it/s] 74%|███████▎  | 1293/1759 [00:27<00:10, 46.50it/s] 74%|███████▍  | 1298/1759 [00:27<00:09, 46.50it/s] 74%|███████▍  | 1303/1759 [00:27<00:09, 46.42it/s] 74%|███████▍  | 1308/1759 [00:28<00:09, 46.42it/s] 75%|███████▍  | 1313/1759 [00:28<00:09, 46.55it/s] 75%|███████▍  | 1318/1759 [00:28<00:09, 46.55it/s] 75%|███████▌  | 1323/1759 [00:28<00:09, 46.52it/s] 75%|███████▌  | 1328/1759 [00:28<00:09, 46.54it/s] 76%|███████▌  | 1333/1759 [00:28<00:09, 46.44it/s] 76%|███████▌  | 1338/1759 [00:28<00:09, 46.51it/s] 76%|███████▋  | 1343/1759 [00:28<00:08, 46.40it/s] 77%|███████▋  | 1348/1759 [00:28<00:08, 46.49it/s] 77%|███████▋  | 1353/1759 [00:29<00:08, 46.49it/s] 77%|███████▋  | 1358/1759 [00:29<00:08, 46.46it/s] 77%|███████▋  | 1363/1759 [00:29<00:08, 46.43it/s] 78%|███████▊  | 1368/1759 [00:29<00:08, 46.47it/s] 78%|███████▊  | 1373/1759 [00:29<00:08, 46.46it/s] 78%|███████▊  | 1378/1759 [00:29<00:08, 46.45it/s] 79%|███████▊  | 1383/1759 [00:29<00:08, 46.46it/s] 79%|███████▉  | 1388/1759 [00:29<00:07, 46.46it/s] 79%|███████▉  | 1393/1759 [00:29<00:07, 46.37it/s] 79%|███████▉  | 1398/1759 [00:29<00:07, 46.42it/s] 80%|███████▉  | 1403/1759 [00:30<00:07, 46.48it/s] 80%|████████  | 1408/1759 [00:30<00:07, 46.48it/s] 80%|████████  | 1413/1759 [00:30<00:07, 46.50it/s] 81%|████████  | 1418/1759 [00:30<00:07, 46.41it/s] 81%|████████  | 1423/1759 [00:30<00:07, 46.32it/s] 81%|████████  | 1428/1759 [00:30<00:07, 46.52it/s] 81%|████████▏ | 1433/1759 [00:30<00:07, 46.46it/s] 82%|████████▏ | 1438/1759 [00:30<00:06, 46.52it/s] 82%|████████▏ | 1443/1759 [00:30<00:06, 46.50it/s] 82%|████████▏ | 1448/1759 [00:31<00:06, 46.45it/s] 83%|████████▎ | 1453/1759 [00:31<00:06, 46.35it/s] 83%|████████▎ | 1458/1759 [00:31<00:06, 46.43it/s] 83%|████████▎ | 1463/1759 [00:31<00:06, 46.48it/s] 83%|████████▎ | 1468/1759 [00:31<00:06, 46.39it/s] 84%|████████▎ | 1473/1759 [00:31<00:06, 46.35it/s] 84%|████████▍ | 1478/1759 [00:31<00:06, 46.47it/s] 84%|████████▍ | 1483/1759 [00:31<00:05, 46.51it/s] 85%|████████▍ | 1488/1759 [00:31<00:05, 46.43it/s] 85%|████████▍ | 1493/1759 [00:32<00:05, 46.46it/s] 85%|████████▌ | 1498/1759 [00:32<00:05, 46.46it/s] 85%|████████▌ | 1503/1759 [00:32<00:05, 46.43it/s] 86%|████████▌ | 1508/1759 [00:32<00:05, 46.36it/s] 86%|████████▌ | 1513/1759 [00:32<00:05, 46.34it/s] 86%|████████▋ | 1518/1759 [00:32<00:05, 46.34it/s] 87%|████████▋ | 1523/1759 [00:32<00:05, 46.40it/s] 87%|████████▋ | 1528/1759 [00:32<00:04, 46.35it/s] 87%|████████▋ | 1533/1759 [00:32<00:04, 46.51it/s] 87%|████████▋ | 1538/1759 [00:32<00:04, 46.46it/s] 88%|████████▊ | 1543/1759 [00:33<00:04, 46.35it/s] 88%|████████▊ | 1548/1759 [00:33<00:04, 46.41it/s] 88%|████████▊ | 1553/1759 [00:33<00:04, 46.42it/s] 89%|████████▊ | 1558/1759 [00:33<00:04, 46.44it/s] 89%|████████▉ | 1563/1759 [00:33<00:04, 46.40it/s] 89%|████████▉ | 1568/1759 [00:33<00:04, 46.41it/s] 89%|████████▉ | 1573/1759 [00:33<00:04, 46.43it/s] 90%|████████▉ | 1578/1759 [00:33<00:03, 46.38it/s] 90%|████████▉ | 1583/1759 [00:33<00:03, 46.36it/s] 90%|█████████ | 1588/1759 [00:34<00:03, 46.40it/s] 91%|█████████ | 1593/1759 [00:34<00:03, 46.05it/s] 91%|█████████ | 1598/1759 [00:34<00:03, 46.26it/s] 91%|█████████ | 1603/1759 [00:34<00:03, 46.26it/s] 91%|█████████▏| 1608/1759 [00:34<00:03, 46.30it/s] 92%|█████████▏| 1613/1759 [00:34<00:03, 46.31it/s] 92%|█████████▏| 1618/1759 [00:34<00:03, 46.44it/s] 92%|█████████▏| 1623/1759 [00:34<00:02, 46.45it/s] 93%|█████████▎| 1628/1759 [00:34<00:02, 46.28it/s] 93%|█████████▎| 1633/1759 [00:35<00:02, 46.46it/s] 93%|█████████▎| 1638/1759 [00:35<00:02, 46.39it/s] 93%|█████████▎| 1643/1759 [00:35<00:02, 46.45it/s] 94%|█████████▎| 1648/1759 [00:35<00:02, 46.36it/s] 94%|█████████▍| 1653/1759 [00:35<00:02, 46.40it/s] 94%|█████████▍| 1658/1759 [00:35<00:02, 46.35it/s] 95%|█████████▍| 1663/1759 [00:35<00:02, 46.32it/s] 95%|█████████▍| 1668/1759 [00:35<00:01, 46.36it/s] 95%|█████████▌| 1673/1759 [00:35<00:01, 46.41it/s] 95%|█████████▌| 1678/1759 [00:36<00:01, 46.43it/s] 96%|█████████▌| 1683/1759 [00:36<00:01, 46.43it/s] 96%|█████████▌| 1688/1759 [00:36<00:01, 46.37it/s] 96%|█████████▌| 1693/1759 [00:36<00:01, 46.37it/s] 97%|█████████▋| 1698/1759 [00:36<00:01, 46.42it/s] 97%|█████████▋| 1703/1759 [00:36<00:01, 46.41it/s] 97%|█████████▋| 1708/1759 [00:36<00:01, 46.42it/s] 97%|█████████▋| 1713/1759 [00:36<00:00, 46.32it/s] 98%|█████████▊| 1718/1759 [00:36<00:00, 46.31it/s] 98%|█████████▊| 1723/1759 [00:36<00:00, 46.32it/s] 98%|█████████▊| 1728/1759 [00:37<00:00, 46.37it/s] 99%|█████████▊| 1733/1759 [00:37<00:00, 46.48it/s] 99%|█████████▉| 1738/1759 [00:37<00:00, 46.47it/s] 99%|█████████▉| 1743/1759 [00:37<00:00, 46.44it/s] 99%|█████████▉| 1748/1759 [00:37<00:00, 46.39it/s]100%|█████████▉| 1753/1759 [00:37<00:00, 46.41it/s]100%|█████████▉| 1758/1759 [00:37<00:00, 46.43it/s]100%|██████████| 1759/1759 [00:37<00:00, 46.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:13:45,525 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   eval_loss               =     0.8475
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   eval_runtime            = 0:00:37.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   eval_samples            =      14067
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   eval_samples_per_second =     372.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   eval_steps_per_second   =     46.558
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:13:45,525 >>   perplexity              =     2.3339
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:53,964 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:53,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:13:54,586 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:13:54,587 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:13:55,133 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:13:56,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:13:56,198 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:59,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:59,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:59,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:59,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:13:59,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:13:59,673 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:13:59,674 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:14:00,243 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:14:00,397 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:14:00,397 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'labels': ['country', 'part of', 'platform', 'publisher', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 22024
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22124, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.20it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:10,  1.41it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.41it/s]Extractor Predicting: 19it [00:12,  1.44it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:15,  1.40it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.48it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.51it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.55it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:22,  1.55it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:23,  1.56it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.58it/s]Extractor Predicting: 39it [00:26,  1.55it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.58it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.59it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.61it/s]Extractor Predicting: 50it [00:33,  1.59it/s]Extractor Predicting: 51it [00:33,  1.60it/s]Extractor Predicting: 52it [00:34,  1.60it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.62it/s]Extractor Predicting: 55it [00:36,  1.62it/s]Extractor Predicting: 56it [00:36,  1.61it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:38,  1.58it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.58it/s]Extractor Predicting: 61it [00:39,  1.59it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:41,  1.57it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.57it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.52it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.49it/s]Extractor Predicting: 77it [00:50,  1.49it/s]Extractor Predicting: 78it [00:51,  1.56it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:54,  1.57it/s]Extractor Predicting: 84it [00:54,  1.61it/s]Extractor Predicting: 85it [00:55,  1.60it/s]Extractor Predicting: 86it [00:56,  1.55it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.46it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.51it/s]Extractor Predicting: 93it [01:00,  1.47it/s]Extractor Predicting: 94it [01:01,  1.50it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:02,  1.56it/s]Extractor Predicting: 97it [01:03,  1.62it/s]Extractor Predicting: 98it [01:03,  1.66it/s]Extractor Predicting: 99it [01:04,  1.63it/s]Extractor Predicting: 100it [01:05,  1.71it/s]Extractor Predicting: 101it [01:05,  1.66it/s]Extractor Predicting: 102it [01:06,  1.64it/s]Extractor Predicting: 103it [01:06,  1.65it/s]Extractor Predicting: 104it [01:07,  1.65it/s]Extractor Predicting: 105it [01:08,  1.68it/s]Extractor Predicting: 106it [01:08,  1.65it/s]Extractor Predicting: 107it [01:09,  1.62it/s]Extractor Predicting: 108it [01:10,  1.43it/s]Extractor Predicting: 109it [01:10,  1.48it/s]Extractor Predicting: 110it [01:11,  1.51it/s]Extractor Predicting: 111it [01:12,  1.56it/s]Extractor Predicting: 112it [01:12,  1.60it/s]Extractor Predicting: 113it [01:13,  1.62it/s]Extractor Predicting: 114it [01:13,  1.65it/s]Extractor Predicting: 115it [01:14,  1.65it/s]Extractor Predicting: 116it [01:15,  1.69it/s]Extractor Predicting: 117it [01:15,  1.67it/s]Extractor Predicting: 118it [01:16,  1.67it/s]Extractor Predicting: 119it [01:16,  1.68it/s]Extractor Predicting: 120it [01:17,  1.70it/s]Extractor Predicting: 121it [01:18,  1.68it/s]Extractor Predicting: 122it [01:18,  1.64it/s]Extractor Predicting: 123it [01:19,  1.64it/s]Extractor Predicting: 124it [01:19,  1.62it/s]Extractor Predicting: 125it [01:20,  1.64it/s]Extractor Predicting: 126it [01:21,  1.64it/s]Extractor Predicting: 127it [01:21,  1.69it/s]Extractor Predicting: 128it [01:22,  1.69it/s]Extractor Predicting: 129it [01:22,  1.69it/s]Extractor Predicting: 130it [01:23,  1.67it/s]Extractor Predicting: 131it [01:24,  1.65it/s]Extractor Predicting: 132it [01:24,  1.68it/s]Extractor Predicting: 133it [01:25,  1.63it/s]Extractor Predicting: 134it [01:25,  1.67it/s]Extractor Predicting: 135it [01:26,  1.67it/s]Extractor Predicting: 136it [01:27,  1.66it/s]Extractor Predicting: 137it [01:27,  1.66it/s]Extractor Predicting: 138it [01:28,  1.63it/s]Extractor Predicting: 139it [01:28,  1.63it/s]Extractor Predicting: 140it [01:29,  1.62it/s]Extractor Predicting: 141it [01:30,  1.64it/s]Extractor Predicting: 142it [01:30,  1.67it/s]Extractor Predicting: 143it [01:31,  1.69it/s]Extractor Predicting: 144it [01:31,  1.69it/s]Extractor Predicting: 145it [01:32,  1.63it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:33,  1.62it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:35,  1.58it/s]Extractor Predicting: 150it [01:35,  1.59it/s]Extractor Predicting: 151it [01:36,  1.54it/s]Extractor Predicting: 152it [01:37,  1.49it/s]Extractor Predicting: 153it [01:37,  1.45it/s]Extractor Predicting: 154it [01:38,  1.46it/s]Extractor Predicting: 155it [01:39,  1.46it/s]Extractor Predicting: 156it [01:39,  1.46it/s]Extractor Predicting: 157it [01:40,  1.50it/s]Extractor Predicting: 158it [01:41,  1.48it/s]Extractor Predicting: 159it [01:41,  1.51it/s]Extractor Predicting: 160it [01:42,  1.54it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:43,  1.57it/s]Extractor Predicting: 163it [01:44,  1.55it/s]Extractor Predicting: 164it [01:45,  1.58it/s]Extractor Predicting: 165it [01:45,  1.55it/s]Extractor Predicting: 166it [01:46,  1.53it/s]Extractor Predicting: 167it [01:47,  1.56it/s]Extractor Predicting: 168it [01:47,  1.54it/s]Extractor Predicting: 169it [01:48,  1.56it/s]Extractor Predicting: 170it [01:48,  1.58it/s]Extractor Predicting: 171it [01:49,  1.61it/s]Extractor Predicting: 172it [01:50,  1.59it/s]Extractor Predicting: 173it [01:50,  1.62it/s]Extractor Predicting: 174it [01:51,  1.61it/s]Extractor Predicting: 175it [01:52,  1.58it/s]Extractor Predicting: 176it [01:52,  1.57it/s]Extractor Predicting: 177it [01:53,  1.54it/s]Extractor Predicting: 178it [01:53,  1.61it/s]Extractor Predicting: 179it [01:54,  1.72it/s]Extractor Predicting: 180it [01:54,  1.81it/s]Extractor Predicting: 181it [01:55,  1.79it/s]Extractor Predicting: 182it [01:56,  1.79it/s]Extractor Predicting: 183it [01:56,  1.67it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:58,  1.59it/s]Extractor Predicting: 186it [01:58,  1.58it/s]Extractor Predicting: 187it [01:59,  1.56it/s]Extractor Predicting: 188it [02:00,  1.53it/s]Extractor Predicting: 189it [02:00,  1.54it/s]Extractor Predicting: 190it [02:01,  1.51it/s]Extractor Predicting: 191it [02:02,  1.50it/s]Extractor Predicting: 192it [02:02,  1.52it/s]Extractor Predicting: 193it [02:03,  1.59it/s]Extractor Predicting: 194it [02:03,  1.60it/s]Extractor Predicting: 195it [02:04,  1.62it/s]Extractor Predicting: 196it [02:05,  1.58it/s]Extractor Predicting: 197it [02:05,  1.57it/s]Extractor Predicting: 198it [02:06,  1.53it/s]Extractor Predicting: 199it [02:07,  1.53it/s]Extractor Predicting: 200it [02:07,  1.55it/s]Extractor Predicting: 201it [02:08,  1.56it/s]Extractor Predicting: 202it [02:09,  1.54it/s]Extractor Predicting: 203it [02:09,  1.55it/s]Extractor Predicting: 204it [02:10,  1.56it/s]Extractor Predicting: 205it [02:10,  1.58it/s]Extractor Predicting: 206it [02:11,  1.58it/s]Extractor Predicting: 207it [02:12,  1.57it/s]Extractor Predicting: 208it [02:12,  1.59it/s]Extractor Predicting: 209it [02:13,  1.58it/s]Extractor Predicting: 210it [02:14,  1.59it/s]Extractor Predicting: 211it [02:14,  1.56it/s]Extractor Predicting: 212it [02:15,  1.53it/s]Extractor Predicting: 213it [02:16,  1.51it/s]Extractor Predicting: 214it [02:16,  1.51it/s]Extractor Predicting: 215it [02:17,  1.53it/s]Extractor Predicting: 216it [02:18,  1.53it/s]Extractor Predicting: 217it [02:18,  1.57it/s]Extractor Predicting: 218it [02:19,  1.59it/s]Extractor Predicting: 219it [02:19,  1.58it/s]Extractor Predicting: 220it [02:20,  1.59it/s]Extractor Predicting: 221it [02:21,  1.58it/s]Extractor Predicting: 222it [02:21,  1.59it/s]Extractor Predicting: 223it [02:22,  1.56it/s]Extractor Predicting: 224it [02:23,  1.52it/s]Extractor Predicting: 225it [02:23,  1.52it/s]Extractor Predicting: 226it [02:24,  1.54it/s]Extractor Predicting: 227it [02:25,  1.54it/s]Extractor Predicting: 228it [02:25,  1.52it/s]Extractor Predicting: 229it [02:26,  1.52it/s]Extractor Predicting: 230it [02:27,  1.51it/s]Extractor Predicting: 231it [02:27,  1.51it/s]Extractor Predicting: 232it [02:28,  1.55it/s]Extractor Predicting: 233it [02:29,  1.51it/s]Extractor Predicting: 234it [02:29,  1.50it/s]Extractor Predicting: 235it [02:30,  1.52it/s]Extractor Predicting: 236it [02:31,  1.53it/s]Extractor Predicting: 237it [02:31,  1.53it/s]Extractor Predicting: 238it [02:32,  1.50it/s]Extractor Predicting: 239it [02:33,  1.36it/s]Extractor Predicting: 240it [02:33,  1.41it/s]Extractor Predicting: 241it [02:34,  1.46it/s]Extractor Predicting: 242it [02:35,  1.47it/s]Extractor Predicting: 243it [02:35,  1.50it/s]Extractor Predicting: 244it [02:36,  1.51it/s]Extractor Predicting: 245it [02:37,  1.48it/s]Extractor Predicting: 246it [02:37,  1.52it/s]Extractor Predicting: 247it [02:38,  1.55it/s]Extractor Predicting: 248it [02:39,  1.51it/s]Extractor Predicting: 249it [02:39,  1.50it/s]Extractor Predicting: 250it [02:40,  1.56it/s]Extractor Predicting: 251it [02:41,  1.59it/s]Extractor Predicting: 252it [02:41,  1.62it/s]Extractor Predicting: 253it [02:42,  1.62it/s]Extractor Predicting: 254it [02:42,  1.61it/s]Extractor Predicting: 255it [02:43,  1.59it/s]Extractor Predicting: 256it [02:44,  1.58it/s]Extractor Predicting: 257it [02:44,  1.58it/s]Extractor Predicting: 258it [02:45,  1.59it/s]Extractor Predicting: 259it [02:46,  1.58it/s]Extractor Predicting: 260it [02:46,  1.59it/s]Extractor Predicting: 261it [02:47,  1.61it/s]Extractor Predicting: 262it [02:47,  1.65it/s]Extractor Predicting: 263it [02:48,  1.64it/s]Extractor Predicting: 264it [02:49,  1.66it/s]Extractor Predicting: 265it [02:49,  1.64it/s]Extractor Predicting: 266it [02:50,  1.62it/s]Extractor Predicting: 267it [02:50,  1.61it/s]Extractor Predicting: 268it [02:51,  1.58it/s]Extractor Predicting: 269it [02:52,  1.60it/s]Extractor Predicting: 270it [02:52,  1.59it/s]Extractor Predicting: 271it [02:53,  1.61it/s]Extractor Predicting: 272it [02:54,  1.62it/s]Extractor Predicting: 273it [02:54,  1.61it/s]Extractor Predicting: 274it [02:55,  1.61it/s]Extractor Predicting: 275it [02:55,  1.59it/s]Extractor Predicting: 276it [02:56,  1.61it/s]Extractor Predicting: 277it [02:57,  1.61it/s]Extractor Predicting: 278it [02:57,  1.61it/s]Extractor Predicting: 279it [02:58,  1.58it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [02:59,  1.56it/s]Extractor Predicting: 282it [03:00,  1.54it/s]Extractor Predicting: 283it [03:01,  1.54it/s]Extractor Predicting: 284it [03:01,  1.58it/s]Extractor Predicting: 285it [03:02,  1.56it/s]Extractor Predicting: 286it [03:03,  1.54it/s]Extractor Predicting: 287it [03:03,  1.52it/s]Extractor Predicting: 288it [03:04,  1.54it/s]Extractor Predicting: 289it [03:05,  1.49it/s]Extractor Predicting: 290it [03:05,  1.51it/s]Extractor Predicting: 291it [03:06,  1.53it/s]Extractor Predicting: 292it [03:07,  1.50it/s]Extractor Predicting: 293it [03:07,  1.51it/s]Extractor Predicting: 294it [03:08,  1.52it/s]Extractor Predicting: 295it [03:08,  1.53it/s]Extractor Predicting: 296it [03:09,  1.54it/s]Extractor Predicting: 297it [03:10,  1.53it/s]Extractor Predicting: 298it [03:10,  1.57it/s]Extractor Predicting: 299it [03:11,  1.53it/s]Extractor Predicting: 300it [03:12,  1.52it/s]Extractor Predicting: 301it [03:12,  1.50it/s]Extractor Predicting: 302it [03:13,  1.52it/s]Extractor Predicting: 303it [03:14,  1.53it/s]Extractor Predicting: 304it [03:14,  1.55it/s]Extractor Predicting: 305it [03:15,  1.58it/s]Extractor Predicting: 306it [03:16,  1.55it/s]Extractor Predicting: 307it [03:16,  1.56it/s]Extractor Predicting: 308it [03:17,  1.58it/s]Extractor Predicting: 309it [03:18,  1.55it/s]Extractor Predicting: 310it [03:18,  1.50it/s]Extractor Predicting: 311it [03:19,  1.45it/s]Extractor Predicting: 312it [03:20,  1.43it/s]Extractor Predicting: 313it [03:20,  1.41it/s]Extractor Predicting: 314it [03:21,  1.39it/s]Extractor Predicting: 315it [03:22,  1.38it/s]Extractor Predicting: 316it [03:23,  1.38it/s]Extractor Predicting: 317it [03:23,  1.38it/s]Extractor Predicting: 318it [03:24,  1.38it/s]Extractor Predicting: 319it [03:25,  1.37it/s]Extractor Predicting: 320it [03:26,  1.37it/s]Extractor Predicting: 321it [03:26,  1.36it/s]Extractor Predicting: 322it [03:27,  1.37it/s]Extractor Predicting: 323it [03:28,  1.37it/s]Extractor Predicting: 324it [03:28,  1.38it/s]Extractor Predicting: 325it [03:29,  1.37it/s]Extractor Predicting: 326it [03:30,  1.39it/s]Extractor Predicting: 327it [03:31,  1.44it/s]Extractor Predicting: 328it [03:31,  1.45it/s]Extractor Predicting: 329it [03:32,  1.48it/s]Extractor Predicting: 330it [03:32,  1.53it/s]Extractor Predicting: 331it [03:33,  1.55it/s]Extractor Predicting: 332it [03:34,  1.57it/s]Extractor Predicting: 333it [03:34,  1.56it/s]Extractor Predicting: 334it [03:35,  1.54it/s]Extractor Predicting: 335it [03:36,  1.55it/s]Extractor Predicting: 336it [03:36,  1.50it/s]Extractor Predicting: 337it [03:37,  1.55it/s]Extractor Predicting: 338it [03:38,  1.56it/s]Extractor Predicting: 339it [03:38,  1.56it/s]Extractor Predicting: 340it [03:39,  1.52it/s]Extractor Predicting: 341it [03:40,  1.48it/s]Extractor Predicting: 342it [03:40,  1.49it/s]Extractor Predicting: 343it [03:41,  1.48it/s]Extractor Predicting: 344it [03:42,  1.45it/s]Extractor Predicting: 345it [03:42,  1.44it/s]Extractor Predicting: 346it [03:43,  1.43it/s]Extractor Predicting: 347it [03:44,  1.43it/s]Extractor Predicting: 348it [03:45,  1.44it/s]Extractor Predicting: 349it [03:45,  1.45it/s]Extractor Predicting: 350it [03:46,  1.43it/s]Extractor Predicting: 351it [03:47,  1.46it/s]Extractor Predicting: 352it [03:47,  1.46it/s]Extractor Predicting: 353it [03:48,  1.45it/s]Extractor Predicting: 354it [03:49,  1.44it/s]Extractor Predicting: 355it [03:49,  1.46it/s]Extractor Predicting: 356it [03:50,  1.47it/s]Extractor Predicting: 357it [03:51,  1.48it/s]Extractor Predicting: 358it [03:51,  1.47it/s]Extractor Predicting: 359it [03:52,  1.46it/s]Extractor Predicting: 360it [03:53,  1.48it/s]Extractor Predicting: 361it [03:53,  1.52it/s]Extractor Predicting: 362it [03:54,  1.51it/s]Extractor Predicting: 363it [03:55,  1.51it/s]Extractor Predicting: 364it [03:55,  1.50it/s]Extractor Predicting: 365it [03:56,  1.55it/s]Extractor Predicting: 366it [03:57,  1.36it/s]Extractor Predicting: 367it [03:58,  1.40it/s]Extractor Predicting: 368it [03:58,  1.43it/s]Extractor Predicting: 369it [03:59,  1.43it/s]Extractor Predicting: 370it [04:00,  1.48it/s]Extractor Predicting: 371it [04:00,  1.47it/s]Extractor Predicting: 372it [04:01,  1.48it/s]Extractor Predicting: 373it [04:02,  1.46it/s]Extractor Predicting: 374it [04:02,  1.50it/s]Extractor Predicting: 375it [04:03,  1.51it/s]Extractor Predicting: 376it [04:03,  1.54it/s]Extractor Predicting: 377it [04:04,  1.54it/s]Extractor Predicting: 378it [04:05,  1.54it/s]Extractor Predicting: 379it [04:05,  1.50it/s]Extractor Predicting: 380it [04:06,  1.50it/s]Extractor Predicting: 381it [04:07,  1.50it/s]Extractor Predicting: 382it [04:07,  1.50it/s]Extractor Predicting: 383it [04:08,  1.51it/s]Extractor Predicting: 384it [04:09,  1.51it/s]Extractor Predicting: 385it [04:09,  1.53it/s]Extractor Predicting: 386it [04:10,  1.54it/s]Extractor Predicting: 387it [04:11,  1.52it/s]Extractor Predicting: 388it [04:11,  1.51it/s]Extractor Predicting: 389it [04:12,  1.55it/s]Extractor Predicting: 390it [04:13,  1.55it/s]Extractor Predicting: 391it [04:13,  1.56it/s]Extractor Predicting: 392it [04:14,  1.57it/s]Extractor Predicting: 393it [04:15,  1.56it/s]Extractor Predicting: 394it [04:15,  1.56it/s]Extractor Predicting: 395it [04:16,  1.57it/s]Extractor Predicting: 396it [04:17,  1.54it/s]Extractor Predicting: 397it [04:17,  1.55it/s]Extractor Predicting: 398it [04:18,  1.57it/s]Extractor Predicting: 399it [04:18,  1.57it/s]Extractor Predicting: 400it [04:19,  1.53it/s]Extractor Predicting: 401it [04:20,  1.50it/s]Extractor Predicting: 402it [04:20,  1.47it/s]Extractor Predicting: 403it [04:21,  1.48it/s]Extractor Predicting: 404it [04:22,  1.52it/s]Extractor Predicting: 405it [04:22,  1.52it/s]Extractor Predicting: 406it [04:23,  1.52it/s]Extractor Predicting: 407it [04:24,  1.55it/s]Extractor Predicting: 408it [04:24,  1.59it/s]Extractor Predicting: 409it [04:25,  1.62it/s]Extractor Predicting: 410it [04:25,  1.64it/s]Extractor Predicting: 411it [04:26,  1.64it/s]Extractor Predicting: 412it [04:27,  1.66it/s]Extractor Predicting: 413it [04:27,  1.69it/s]Extractor Predicting: 414it [04:28,  1.71it/s]Extractor Predicting: 415it [04:28,  1.63it/s]Extractor Predicting: 416it [04:29,  1.56it/s]Extractor Predicting: 417it [04:30,  1.49it/s]Extractor Predicting: 418it [04:31,  1.46it/s]Extractor Predicting: 419it [04:31,  1.44it/s]Extractor Predicting: 420it [04:32,  1.44it/s]Extractor Predicting: 421it [04:33,  1.42it/s]Extractor Predicting: 422it [04:33,  1.64it/s]Extractor Predicting: 422it [04:33,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:43,597 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:43,601 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:43,601 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:43,601 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:43,601 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:18:44,241 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:18:44,242 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:18:44,814 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:18:45,881 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:18:45,881 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:48,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:48,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:48,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:48,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:18:48,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:18:49,482 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:18:49,484 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:18:50,073 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:18:50,237 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:18:50,237 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3175109692859992,
  "recall": 0.05658633681666311,
  "score": 0.09605406057680704,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13679
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13779, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:24,  1.48it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.46it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:42,  1.57it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.55it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.54it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.57it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:53,  1.58it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:57,  1.55it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:00,  1.55it/s]Extractor Predicting: 94it [01:01,  1.57it/s]Extractor Predicting: 95it [01:02,  1.57it/s]Extractor Predicting: 96it [01:02,  1.55it/s]Extractor Predicting: 97it [01:03,  1.58it/s]Extractor Predicting: 98it [01:04,  1.57it/s]Extractor Predicting: 99it [01:04,  1.57it/s]Extractor Predicting: 100it [01:05,  1.56it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:06,  1.50it/s]Extractor Predicting: 103it [01:07,  1.52it/s]Extractor Predicting: 104it [01:08,  1.55it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:09,  1.40it/s]Extractor Predicting: 107it [01:10,  1.45it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:12,  1.56it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:14,  1.55it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:17,  1.58it/s]Extractor Predicting: 120it [01:18,  1.61it/s]Extractor Predicting: 121it [01:19,  1.61it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:20,  1.58it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:22,  1.56it/s]Extractor Predicting: 128it [01:23,  1.58it/s]Extractor Predicting: 129it [01:24,  1.58it/s]Extractor Predicting: 130it [01:24,  1.56it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:26,  1.54it/s]Extractor Predicting: 133it [01:26,  1.55it/s]Extractor Predicting: 134it [01:27,  1.55it/s]Extractor Predicting: 135it [01:28,  1.56it/s]Extractor Predicting: 136it [01:28,  1.60it/s]Extractor Predicting: 137it [01:29,  1.60it/s]Extractor Predicting: 138it [01:29,  1.58it/s]Extractor Predicting: 139it [01:30,  1.56it/s]Extractor Predicting: 140it [01:31,  1.59it/s]Extractor Predicting: 141it [01:31,  1.61it/s]Extractor Predicting: 142it [01:32,  1.63it/s]Extractor Predicting: 143it [01:32,  1.66it/s]Extractor Predicting: 144it [01:33,  1.62it/s]Extractor Predicting: 145it [01:34,  1.63it/s]Extractor Predicting: 146it [01:34,  1.62it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.61it/s]Extractor Predicting: 149it [01:36,  1.61it/s]Extractor Predicting: 150it [01:37,  1.62it/s]Extractor Predicting: 151it [01:37,  1.61it/s]Extractor Predicting: 152it [01:38,  1.64it/s]Extractor Predicting: 153it [01:39,  1.61it/s]Extractor Predicting: 154it [01:39,  1.61it/s]Extractor Predicting: 155it [01:40,  1.60it/s]Extractor Predicting: 156it [01:41,  1.60it/s]Extractor Predicting: 157it [01:41,  1.58it/s]Extractor Predicting: 158it [01:42,  1.60it/s]Extractor Predicting: 159it [01:42,  1.62it/s]Extractor Predicting: 160it [01:43,  1.65it/s]Extractor Predicting: 161it [01:44,  1.58it/s]Extractor Predicting: 162it [01:44,  1.55it/s]Extractor Predicting: 163it [01:45,  1.56it/s]Extractor Predicting: 164it [01:46,  1.60it/s]Extractor Predicting: 165it [01:46,  1.59it/s]Extractor Predicting: 166it [01:47,  1.51it/s]Extractor Predicting: 167it [01:48,  1.49it/s]Extractor Predicting: 168it [01:48,  1.50it/s]Extractor Predicting: 169it [01:49,  1.50it/s]Extractor Predicting: 170it [01:50,  1.48it/s]Extractor Predicting: 170it [01:50,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:20:48,183 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:20:48,184 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:48,764 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:49,803 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:49,803 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:52,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:52,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:52,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:52,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:52,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:20:53,341 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:20:53,342 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:53,907 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:54,053 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:54,054 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.44166666666666665,
  "recall": 0.07803680981595092,
  "score": 0.13263816475495308,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 3869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 3969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.51it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 33it [00:22,  1.48it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:21:17,647 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:21:17,649 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:21:17,719 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:21:17,720 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:21:17,770 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:21:22,017 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:21:22,021 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:21:22,030 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:21:22,031 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:21:22,036 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:21:22,042 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45977011494252873,
  "recall": 0.02126528442317916,
  "score": 0.04065040650406504,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:21:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:22,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:23,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:24,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:25,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:25,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:26,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:27,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:27,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:28,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:28,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:29,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:30,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:30,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:32,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:33,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:33,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:34,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:35,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:35,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:36,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:37,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:24, 16.10s/it][WARNING|generation_utils.py:914] 2023-08-28 13:21:38,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:39,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:39,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:40,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:41,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:42,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:42,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:43,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:44,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:44,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:45,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:46,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:46,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:47,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:48,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:49,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:50,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:50,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:51,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:52,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:53,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:54,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:54,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:13, 16.73s/it][WARNING|generation_utils.py:914] 2023-08-28 13:21:55,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:56,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:57,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:58,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:58,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:59,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:59,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:00,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:00,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:01,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:01,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:02,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:02,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:03,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:04,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:04,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:05,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:05,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:06,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:07,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:08,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:46<01:45, 15.05s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:08,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:09,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:10,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:10,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:11,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:11,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:12,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:13,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:14,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:14,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:15,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:16,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:16,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:17,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:18,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:19,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:19,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:20,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:20,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:21,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:22,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:00<01:29, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:23,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:23,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:24,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:25,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:25,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:26,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:27,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:27,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:29,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:30,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:30,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:31,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:32,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:32,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:33,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:34,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:35,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:35,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:36,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:37,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:38,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:16<01:15, 15.15s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:38,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:39,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:40,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:40,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:41,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:42,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:43,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:43,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:44,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:45,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:46,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:46,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:47,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:48,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:48,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:49,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:50,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:50,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:51,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:52,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:52,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:53,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:54,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:55,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:03, 15.91s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:56,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:57,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:57,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:58,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:59,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:00,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:00,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:01,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:02,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:03,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:03,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:04,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:05,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:06,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:07,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:08,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:08,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:09,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:10,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:11,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:47, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:11,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:12,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:13,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:13,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:14,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:15,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:16,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:17,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:17,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:18,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:18,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:19,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:20,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:21,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:21,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:22,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:23,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:23,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:24,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:25,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:03<00:30, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:26,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:26,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:27,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:28,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:29,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:29,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:30,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:30,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:31,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:32,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:33,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:33,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:34,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:35,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:36,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:36,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:37,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:38,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:38,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:39,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:17<00:14, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:40,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:40,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:41,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:42,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:42,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:43,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:44,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:45,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:45,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:46,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:47,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:48,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:49,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:50,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:50,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:51,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:53,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:53,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:54,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:55,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:56,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:34<00:00, 15.56s/it]Generating: 100%|██████████| 10/10 [02:34<00:00, 15.48s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:03,315 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:03,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:03,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:03,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:03,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:24:03,946 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:24:03,947 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:24:04,519 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:24:05,578 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:24:05,578 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:08,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:08,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:08,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:08,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:24:08,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:24:09,105 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:24:09,106 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:24:09,681 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:24:09,838 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:24:09,838 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : part of . Context : Later in the year ( 1143 ) , he married Alixandra , daughter of Juan de Morárez de Santa Isabel , Esquerito s second wife . Head Entity : November 1143 , Tail Entity : Alixandra .\n']
['Relation : part of . Context : Later in the year ( 1143 ) , he married Alixandra , daughter of Juan de Morárez de Santa Isabel , Esquerito s second wife . Head Entity : November 1143 , Tail Entity : Alixandra .\n', 'Relation : part of . Context : After he completed the course of the Mater ( the only part of the course ) , he decided to return to his hometown of Heidelberg and he took a job with the university student newspaper Nachtsbahn , in Hamburg . Head Entity : The course , Tail Entity : Mater .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : part of .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('chancellor', 'part of', '', 'In July 1983 , Führer , Chancellor of Germany in a private meeting arranged by the chancellor , Herman Van Rompuy , took him to Berlin for a short visit .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : sport .', 'success_rate': 0.8721590909090909, 'errors': {'', "('Václav Szczecin', 'sport', '', 'After his national debut against Czechoslovakia two years later , he was called up for the squad as a replacement for Václav Szczecin , who was out injured .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : continent .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('United States', 'continent', '', 'For years the United States has been the largest producer of opium in the world , having been a major supplier of raw opium for the United States .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 9589
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9689, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.60it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.61it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.72it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:09,  1.66it/s]Extractor Estimating: 16it [00:09,  1.63it/s]Extractor Estimating: 17it [00:10,  1.57it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:11,  1.57it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:12,  1.59it/s]Extractor Estimating: 22it [00:13,  1.56it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:14,  1.60it/s]Extractor Estimating: 25it [00:15,  1.58it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:16,  1.55it/s]Extractor Estimating: 28it [00:17,  1.54it/s]Extractor Estimating: 29it [00:18,  1.51it/s]Extractor Estimating: 30it [00:18,  1.48it/s]Extractor Estimating: 31it [00:19,  1.50it/s]Extractor Estimating: 32it [00:20,  1.54it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:21,  1.60it/s]Extractor Estimating: 35it [00:21,  1.60it/s]Extractor Estimating: 36it [00:22,  1.58it/s]Extractor Estimating: 37it [00:23,  1.56it/s]Extractor Estimating: 38it [00:23,  1.59it/s]Extractor Estimating: 39it [00:24,  1.57it/s]Extractor Estimating: 40it [00:25,  1.55it/s]Extractor Estimating: 41it [00:25,  1.61it/s]Extractor Estimating: 42it [00:26,  1.59it/s]Extractor Estimating: 43it [00:27,  1.48it/s]Extractor Estimating: 44it [00:27,  1.49it/s]Extractor Estimating: 45it [00:28,  1.52it/s]Extractor Estimating: 46it [00:29,  1.52it/s]Extractor Estimating: 47it [00:29,  1.51it/s]Extractor Estimating: 48it [00:30,  1.53it/s]Extractor Estimating: 49it [00:31,  1.50it/s]Extractor Estimating: 50it [00:31,  1.50it/s]Extractor Estimating: 51it [00:32,  1.60it/s]Extractor Estimating: 52it [00:32,  1.65it/s]Extractor Estimating: 53it [00:33,  1.74it/s]Extractor Estimating: 54it [00:33,  1.77it/s]Extractor Estimating: 55it [00:34,  1.83it/s]Extractor Estimating: 56it [00:34,  1.89it/s]Extractor Estimating: 57it [00:35,  1.91it/s]Extractor Estimating: 58it [00:35,  1.96it/s]Extractor Estimating: 59it [00:36,  1.95it/s]Extractor Estimating: 60it [00:36,  1.96it/s]Extractor Estimating: 61it [00:37,  1.89it/s]Extractor Estimating: 62it [00:37,  1.91it/s]Extractor Estimating: 63it [00:38,  1.93it/s]Extractor Estimating: 64it [00:38,  1.94it/s]Extractor Estimating: 65it [00:39,  1.94it/s]Extractor Estimating: 66it [00:40,  1.89it/s]Extractor Estimating: 67it [00:40,  1.93it/s]Extractor Estimating: 68it [00:41,  1.93it/s]Extractor Estimating: 69it [00:41,  1.98it/s]Extractor Estimating: 70it [00:42,  2.00it/s]Extractor Estimating: 71it [00:42,  2.02it/s]Extractor Estimating: 72it [00:43,  2.03it/s]Extractor Estimating: 73it [00:43,  1.93it/s]Extractor Estimating: 74it [00:44,  1.95it/s]Extractor Estimating: 75it [00:44,  1.99it/s]Extractor Estimating: 76it [00:45,  1.80it/s]Extractor Estimating: 77it [00:45,  1.73it/s]Extractor Estimating: 78it [00:46,  1.67it/s]Extractor Estimating: 79it [00:47,  1.66it/s]Extractor Estimating: 80it [00:47,  1.63it/s]Extractor Estimating: 81it [00:48,  1.61it/s]Extractor Estimating: 82it [00:49,  1.62it/s]Extractor Estimating: 83it [00:49,  1.62it/s]Extractor Estimating: 84it [00:50,  1.63it/s]Extractor Estimating: 85it [00:50,  1.61it/s]Extractor Estimating: 86it [00:51,  1.63it/s]Extractor Estimating: 87it [00:52,  1.58it/s]Extractor Estimating: 88it [00:52,  1.54it/s]Extractor Estimating: 89it [00:53,  1.51it/s]Extractor Estimating: 90it [00:54,  1.55it/s]Extractor Estimating: 91it [00:54,  1.59it/s]Extractor Estimating: 92it [00:55,  1.56it/s]Extractor Estimating: 93it [00:56,  1.56it/s]Extractor Estimating: 94it [00:56,  1.58it/s]Extractor Estimating: 95it [00:57,  1.64it/s]Extractor Estimating: 96it [00:57,  1.64it/s]Extractor Estimating: 97it [00:58,  1.63it/s]Extractor Estimating: 98it [00:59,  1.48it/s]Extractor Estimating: 99it [00:59,  1.48it/s]Extractor Estimating: 100it [01:00,  1.49it/s]Extractor Estimating: 101it [01:01,  1.57it/s]Extractor Estimating: 102it [01:01,  1.59it/s]Extractor Estimating: 103it [01:02,  1.63it/s]Extractor Estimating: 104it [01:02,  1.64it/s]Extractor Estimating: 105it [01:03,  1.64it/s]Extractor Estimating: 106it [01:04,  1.63it/s]Extractor Estimating: 107it [01:04,  1.67it/s]Extractor Estimating: 108it [01:05,  1.70it/s]Extractor Estimating: 109it [01:05,  1.66it/s]Extractor Estimating: 110it [01:06,  1.67it/s]Extractor Estimating: 111it [01:07,  1.63it/s]Extractor Estimating: 112it [01:07,  1.60it/s]Extractor Estimating: 113it [01:08,  1.63it/s]Extractor Estimating: 114it [01:09,  1.65it/s]Extractor Estimating: 115it [01:09,  1.69it/s]Extractor Estimating: 116it [01:10,  1.71it/s]Extractor Estimating: 117it [01:10,  1.74it/s]Extractor Estimating: 118it [01:11,  1.68it/s]Extractor Estimating: 119it [01:11,  1.73it/s]Extractor Estimating: 120it [01:12,  1.71it/s]Extractor Estimating: 121it [01:13,  1.75it/s]Extractor Estimating: 122it [01:13,  1.70it/s]Extractor Estimating: 123it [01:14,  1.66it/s]Extractor Estimating: 124it [01:14,  1.63it/s]Extractor Estimating: 125it [01:15,  1.61it/s]Extractor Estimating: 126it [01:16,  1.63it/s]Extractor Estimating: 127it [01:16,  1.63it/s]Extractor Estimating: 128it [01:17,  1.66it/s]Extractor Estimating: 129it [01:17,  1.66it/s]Extractor Estimating: 130it [01:18,  1.67it/s]Extractor Estimating: 131it [01:19,  1.65it/s]Extractor Estimating: 132it [01:19,  1.67it/s]Extractor Estimating: 133it [01:20,  1.70it/s]Extractor Estimating: 134it [01:20,  1.71it/s]Extractor Estimating: 135it [01:21,  1.71it/s]Extractor Estimating: 136it [01:22,  1.73it/s]Extractor Estimating: 137it [01:22,  1.77it/s]Extractor Estimating: 138it [01:23,  1.74it/s]Extractor Estimating: 139it [01:23,  1.66it/s]Extractor Estimating: 140it [01:24,  1.63it/s]Extractor Estimating: 141it [01:25,  1.67it/s]Extractor Estimating: 142it [01:25,  1.69it/s]Extractor Estimating: 143it [01:26,  1.69it/s]Extractor Estimating: 144it [01:26,  1.69it/s]Extractor Estimating: 145it [01:27,  1.73it/s]Extractor Estimating: 146it [01:27,  1.69it/s]Extractor Estimating: 147it [01:28,  1.62it/s]Extractor Estimating: 148it [01:29,  1.66it/s]Extractor Estimating: 149it [01:29,  1.63it/s]Extractor Estimating: 150it [01:30,  1.54it/s]Extractor Estimating: 151it [01:31,  1.59it/s]Extractor Estimating: 152it [01:31,  1.59it/s]Extractor Estimating: 153it [01:32,  1.60it/s]Extractor Estimating: 154it [01:33,  1.61it/s]Extractor Estimating: 155it [01:33,  1.62it/s]Extractor Estimating: 156it [01:34,  1.50it/s]Extractor Estimating: 157it [01:35,  1.51it/s]Extractor Estimating: 158it [01:35,  1.56it/s]Extractor Estimating: 159it [01:36,  1.58it/s]Extractor Estimating: 160it [01:36,  1.58it/s]Extractor Estimating: 161it [01:37,  1.57it/s]Extractor Estimating: 162it [01:38,  1.60it/s]Extractor Estimating: 163it [01:38,  1.56it/s]Extractor Estimating: 164it [01:39,  1.59it/s]Extractor Estimating: 165it [01:40,  1.62it/s]Extractor Estimating: 166it [01:40,  1.63it/s]Extractor Estimating: 167it [01:41,  1.62it/s]Extractor Estimating: 168it [01:41,  1.68it/s]Extractor Estimating: 169it [01:42,  1.64it/s]Extractor Estimating: 170it [01:43,  1.64it/s]Extractor Estimating: 171it [01:43,  1.61it/s]Extractor Estimating: 172it [01:44,  1.61it/s]Extractor Estimating: 173it [01:44,  1.61it/s]Extractor Estimating: 174it [01:45,  1.60it/s]Extractor Estimating: 175it [01:46,  1.57it/s]Extractor Estimating: 176it [01:46,  1.61it/s]Extractor Estimating: 177it [01:47,  1.60it/s]Extractor Estimating: 178it [01:48,  1.60it/s]Extractor Estimating: 179it [01:48,  1.60it/s]Extractor Estimating: 180it [01:49,  1.62it/s]Extractor Estimating: 181it [01:49,  1.62it/s]Extractor Estimating: 182it [01:50,  1.59it/s]Extractor Estimating: 183it [01:51,  1.57it/s]Extractor Estimating: 184it [01:51,  1.61it/s]Extractor Estimating: 185it [01:52,  1.61it/s]Extractor Estimating: 186it [01:53,  1.59it/s]Extractor Estimating: 187it [01:53,  1.61it/s]Extractor Estimating: 188it [01:54,  1.61it/s]Extractor Estimating: 189it [01:54,  1.61it/s]Extractor Estimating: 190it [01:55,  1.57it/s]Extractor Estimating: 191it [01:56,  1.48it/s]Extractor Estimating: 192it [01:57,  1.51it/s]Extractor Estimating: 193it [01:57,  1.52it/s]Extractor Estimating: 194it [01:58,  1.59it/s]Extractor Estimating: 195it [01:58,  1.53it/s]Extractor Estimating: 196it [01:59,  1.57it/s]Extractor Estimating: 197it [02:00,  1.58it/s]Extractor Estimating: 198it [02:00,  1.59it/s]Extractor Estimating: 199it [02:01,  1.59it/s]Extractor Estimating: 200it [02:02,  1.56it/s]Extractor Estimating: 201it [02:02,  1.59it/s]Extractor Estimating: 202it [02:03,  1.57it/s]Extractor Estimating: 203it [02:03,  1.59it/s]Extractor Estimating: 204it [02:04,  1.59it/s]Extractor Estimating: 205it [02:05,  1.59it/s]Extractor Estimating: 206it [02:05,  1.56it/s]Extractor Estimating: 207it [02:06,  1.61it/s]Extractor Estimating: 208it [02:07,  1.60it/s]Extractor Estimating: 209it [02:07,  1.62it/s]Extractor Estimating: 210it [02:08,  1.64it/s]Extractor Estimating: 211it [02:08,  1.61it/s]Extractor Estimating: 212it [02:09,  1.60it/s]Extractor Estimating: 213it [02:10,  1.53it/s]Extractor Estimating: 214it [02:10,  1.53it/s]Extractor Estimating: 215it [02:11,  1.54it/s]Extractor Estimating: 216it [02:12,  1.54it/s]Extractor Estimating: 217it [02:12,  1.58it/s]Extractor Estimating: 218it [02:13,  1.61it/s]Extractor Estimating: 219it [02:14,  1.60it/s]Extractor Estimating: 220it [02:14,  1.62it/s]Extractor Estimating: 221it [02:15,  1.61it/s]Extractor Estimating: 222it [02:15,  1.61it/s]Extractor Estimating: 223it [02:16,  1.57it/s]Extractor Estimating: 224it [02:17,  1.60it/s]Extractor Estimating: 225it [02:17,  1.60it/s]Extractor Estimating: 226it [02:18,  1.59it/s]Extractor Estimating: 227it [02:19,  1.59it/s]Extractor Estimating: 228it [02:19,  1.54it/s]Extractor Estimating: 229it [02:20,  1.54it/s]Extractor Estimating: 230it [02:21,  1.55it/s]Extractor Estimating: 231it [02:21,  1.51it/s]Extractor Estimating: 232it [02:22,  1.54it/s]Extractor Estimating: 233it [02:22,  1.59it/s]Extractor Estimating: 234it [02:23,  1.60it/s]Extractor Estimating: 235it [02:24,  1.59it/s]Extractor Estimating: 236it [02:24,  1.51it/s]Extractor Estimating: 237it [02:25,  1.52it/s]Extractor Estimating: 238it [02:26,  1.46it/s]Extractor Estimating: 239it [02:27,  1.46it/s]Extractor Estimating: 240it [02:27,  1.48it/s]Extractor Estimating: 241it [02:28,  1.54it/s]Extractor Estimating: 242it [02:28,  1.52it/s]Extractor Estimating: 243it [02:29,  1.53it/s]Extractor Estimating: 244it [02:30,  1.54it/s]Extractor Estimating: 245it [02:30,  1.54it/s]Extractor Estimating: 246it [02:31,  1.59it/s]Extractor Estimating: 247it [02:32,  1.56it/s]Extractor Estimating: 248it [02:32,  1.50it/s]Extractor Estimating: 249it [02:33,  1.50it/s]Extractor Estimating: 250it [02:34,  1.55it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:00,153 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:00,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:00,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:00,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:00,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:27:00,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:27:00,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:27:01,325 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:02,365 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:02,365 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:05,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:05,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:05,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:05,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:05,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:27:05,934 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:27:05,935 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:27:06,516 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:06,677 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:06,677 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:32:18,896 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:32:18,926 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4982 mean pseudo reward: 0.9405284649664009
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl'}
train vocab size: 27729
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27829, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27829, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.047, loss:681.9138
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.062, loss:574.8485
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.054, loss:529.2364
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.054, loss:535.0554
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.050, loss:486.7428
>> valid entity prec:0.4777, rec:0.3214, f1:0.3843
>> valid relation prec:0.2197, rec:0.0486, f1:0.0796
>> valid relation with NER prec:0.2197, rec:0.0486, f1:0.0796
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 4.963, loss:489.6825
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.059, loss:473.6083
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.046, loss:477.7337
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.054, loss:483.6161
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.051, loss:490.6107
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4702, rec:0.3408, f1:0.3952
>> valid relation prec:0.1149, rec:0.0274, f1:0.0443
>> valid relation with NER prec:0.1149, rec:0.0274, f1:0.0443
new max entity f1 on valid!
g_step 1100, step 60, avg_time 4.961, loss:459.2060
g_step 1200, step 160, avg_time 1.051, loss:490.8868
g_step 1300, step 52, avg_time 1.056, loss:447.0509
g_step 1400, step 152, avg_time 1.050, loss:441.4974
g_step 1500, step 44, avg_time 1.045, loss:422.8502
>> valid entity prec:0.4492, rec:0.3081, f1:0.3655
>> valid relation prec:0.1769, rec:0.0340, f1:0.0570
>> valid relation with NER prec:0.1769, rec:0.0340, f1:0.0570
g_step 1600, step 144, avg_time 4.966, loss:435.9715
g_step 1700, step 36, avg_time 1.056, loss:443.3929
g_step 1800, step 136, avg_time 1.050, loss:414.1814
g_step 1900, step 28, avg_time 1.053, loss:415.6131
g_step 2000, step 128, avg_time 1.047, loss:384.6628
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4351, rec:0.3069, f1:0.3599
>> valid relation prec:0.2335, rec:0.0505, f1:0.0830
>> valid relation with NER prec:0.2335, rec:0.0505, f1:0.0830
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 20, avg_time 4.962, loss:399.9654
g_step 2200, step 120, avg_time 1.062, loss:353.4391
g_step 2300, step 12, avg_time 1.043, loss:387.8912
g_step 2400, step 112, avg_time 1.050, loss:357.4627
g_step 2500, step 4, avg_time 1.061, loss:354.5867
>> valid entity prec:0.4620, rec:0.3184, f1:0.3770
>> valid relation prec:0.1756, rec:0.0377, f1:0.0620
>> valid relation with NER prec:0.1756, rec:0.0377, f1:0.0620
g_step 2600, step 104, avg_time 4.964, loss:354.8104
g_step 2700, step 204, avg_time 1.053, loss:363.5792
g_step 2800, step 96, avg_time 1.053, loss:312.5321
g_step 2900, step 196, avg_time 1.049, loss:350.9331
g_step 3000, step 88, avg_time 1.052, loss:318.9877
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4806, rec:0.3611, f1:0.4124
>> valid relation prec:0.1518, rec:0.0390, f1:0.0620
>> valid relation with NER prec:0.1518, rec:0.0390, f1:0.0620
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 4.963, loss:327.4429
g_step 3200, step 80, avg_time 1.047, loss:295.5065
g_step 3300, step 180, avg_time 1.060, loss:321.7702
g_step 3400, step 72, avg_time 1.047, loss:294.3496
g_step 3500, step 172, avg_time 1.054, loss:310.6367
>> valid entity prec:0.4618, rec:0.3668, f1:0.4088
>> valid relation prec:0.1374, rec:0.0352, f1:0.0560
>> valid relation with NER prec:0.1374, rec:0.0352, f1:0.0560
g_step 3600, step 64, avg_time 4.972, loss:284.7570
g_step 3700, step 164, avg_time 1.061, loss:290.9167
g_step 3800, step 56, avg_time 1.051, loss:294.4083
g_step 3900, step 156, avg_time 1.049, loss:284.6208
g_step 4000, step 48, avg_time 1.050, loss:279.8360
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4436, rec:0.3256, f1:0.3755
>> valid relation prec:0.1829, rec:0.0434, f1:0.0702
>> valid relation with NER prec:0.1829, rec:0.0434, f1:0.0702
g_step 4100, step 148, avg_time 4.978, loss:266.9798
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:32:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:32:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-32-18_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:32:19 - WARNING - datasets.builder -   Using custom data configuration default-864016c8e305c7c2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-864016c8e305c7c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:32:20,228 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:32:20,229 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:32:20,229 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:32:20,230 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:32:20,237 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:32:20,244 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:32:20,370 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:32:23,442 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:32:23,445 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-864016c8e305c7c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.23ba/s] 40%|████      | 2/5 [00:00<00:00,  4.06ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.40ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.54ba/s]100%|██████████| 5/5 [00:01<00:00,  4.62ba/s]100%|██████████| 5/5 [00:01<00:00,  4.40ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:03,  4.19ba/s] 13%|█▎        | 2/15 [00:00<00:02,  4.43ba/s] 20%|██        | 3/15 [00:00<00:02,  4.52ba/s] 27%|██▋       | 4/15 [00:00<00:02,  4.58ba/s] 33%|███▎      | 5/15 [00:01<00:02,  4.59ba/s] 40%|████      | 6/15 [00:01<00:01,  4.62ba/s] 47%|████▋     | 7/15 [00:01<00:01,  4.64ba/s] 53%|█████▎    | 8/15 [00:01<00:01,  4.64ba/s] 60%|██████    | 9/15 [00:02<00:01,  3.83ba/s] 67%|██████▋   | 10/15 [00:02<00:01,  4.02ba/s] 73%|███████▎  | 11/15 [00:02<00:00,  4.19ba/s] 80%|████████  | 12/15 [00:02<00:00,  4.33ba/s] 87%|████████▋ | 13/15 [00:02<00:00,  4.43ba/s] 93%|█████████▎| 14/15 [00:03<00:00,  4.49ba/s]100%|██████████| 15/15 [00:03<00:00,  4.68ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.21ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.79ba/s]100%|██████████| 5/5 [00:00<00:00, 10.04ba/s]100%|██████████| 5/5 [00:00<00:00,  9.86ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:01,  7.80ba/s] 20%|██        | 3/15 [00:00<00:01,  9.50ba/s] 33%|███▎      | 5/15 [00:00<00:01,  9.96ba/s] 47%|████▋     | 7/15 [00:00<00:00, 10.10ba/s] 60%|██████    | 9/15 [00:00<00:00, 10.18ba/s] 73%|███████▎  | 11/15 [00:01<00:00, 10.28ba/s] 87%|████████▋ | 13/15 [00:01<00:00, 10.30ba/s]100%|██████████| 15/15 [00:01<00:00, 11.91ba/s]100%|██████████| 15/15 [00:01<00:00, 10.72ba/s]
[INFO|trainer.py:414] 2023-08-28 15:32:30,246 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:32:30,264 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:32:30,264 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 15:32:30,264 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:32:30,264 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:32:30,264 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:32:30,264 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:32:30,264 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.29it/s]  1%|          | 2/390 [00:00<01:53,  3.41it/s]  1%|          | 3/390 [00:00<01:52,  3.44it/s]  1%|          | 4/390 [00:01<01:51,  3.46it/s]  1%|▏         | 5/390 [00:01<01:51,  3.47it/s]  2%|▏         | 6/390 [00:01<01:50,  3.47it/s]  2%|▏         | 7/390 [00:02<01:50,  3.47it/s]  2%|▏         | 8/390 [00:02<01:49,  3.47it/s]  2%|▏         | 9/390 [00:02<01:49,  3.48it/s]  3%|▎         | 10/390 [00:02<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:49,  3.47it/s]  3%|▎         | 12/390 [00:03<01:48,  3.47it/s]  3%|▎         | 13/390 [00:03<01:48,  3.48it/s]  4%|▎         | 14/390 [00:04<01:48,  3.48it/s]  4%|▍         | 15/390 [00:04<01:47,  3.48it/s]  4%|▍         | 16/390 [00:04<01:47,  3.48it/s]  4%|▍         | 17/390 [00:04<01:47,  3.48it/s]  5%|▍         | 18/390 [00:05<01:47,  3.48it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:05<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:46,  3.45it/s]  6%|▌         | 22/390 [00:06<01:46,  3.46it/s]  6%|▌         | 23/390 [00:06<01:45,  3.47it/s]  6%|▌         | 24/390 [00:06<01:45,  3.47it/s]  6%|▋         | 25/390 [00:07<01:45,  3.47it/s]  7%|▋         | 26/390 [00:07<01:44,  3.47it/s]  7%|▋         | 27/390 [00:07<01:44,  3.47it/s]  7%|▋         | 28/390 [00:08<01:44,  3.48it/s]  7%|▋         | 29/390 [00:08<01:43,  3.47it/s]  8%|▊         | 30/390 [00:08<01:43,  3.47it/s]  8%|▊         | 31/390 [00:08<01:43,  3.48it/s]  8%|▊         | 32/390 [00:09<01:43,  3.48it/s]  8%|▊         | 33/390 [00:09<01:42,  3.48it/s]  9%|▊         | 34/390 [00:09<01:42,  3.48it/s]  9%|▉         | 35/390 [00:10<01:42,  3.48it/s]  9%|▉         | 36/390 [00:10<01:41,  3.47it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:10<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:40,  3.47it/s] 11%|█         | 41/390 [00:11<01:40,  3.47it/s] 11%|█         | 42/390 [00:12<01:40,  3.46it/s] 11%|█         | 43/390 [00:12<01:40,  3.46it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.47it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.47it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 52/390 [00:14<01:37,  3.47it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.47it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.47it/s] 15%|█▌        | 59/390 [00:17<01:35,  3.46it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.46it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.46it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.45it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.46it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.46it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.46it/s] 18%|█▊        | 69/390 [00:19<01:32,  3.46it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.46it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.46it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.46it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 15:32:52,817 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:32:52,817 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:32:52,817 >>   Batch size = 8

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.19it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.42it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.91it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.06it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.74it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.38it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.04it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.73it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.65it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.71it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.86it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.86it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.84it/s][A
  4%|▍         | 73/1759 [00:01<00:35, 46.83it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.82it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.72it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.64it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.48it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.60it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.67it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.68it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.77it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.75it/s][A
  7%|▋         | 123/1759 [00:02<00:34, 46.76it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.74it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.67it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.58it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.64it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.61it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.68it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.79it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.73it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.76it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.73it/s][A
 10%|█         | 178/1759 [00:03<00:34, 46.48it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.49it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.53it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.54it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.59it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.67it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.72it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.62it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.68it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.63it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.69it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.68it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.60it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.54it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.53it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.70it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.73it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.67it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.58it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.63it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.67it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.61it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.59it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.65it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.65it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.63it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.70it/s][A
 18%|█▊        | 313/1759 [00:06<00:30, 46.65it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.68it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.62it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.55it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.61it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.57it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.46it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.61it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.57it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.61it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.66it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.61it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.57it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.58it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.63it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.65it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.63it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.67it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.58it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.64it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.61it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.55it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.62it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.40it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.38it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.53it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.60it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.61it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.65it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.59it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.62it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.68it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.68it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.65it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.64it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.64it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.64it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.66it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.51it/s][A
 29%|██▉       | 513/1759 [00:10<00:26, 46.58it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.61it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.59it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.61it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.60it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.64it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.51it/s][A
 31%|███       | 548/1759 [00:11<00:25, 46.66it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.71it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.58it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.59it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.62it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.62it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.60it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.53it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.49it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.61it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.69it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.54it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.57it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.66it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.54it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.62it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.55it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.49it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.61it/s][A
 37%|███▋      | 643/1759 [00:13<00:23, 46.64it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.65it/s][A
 37%|███▋      | 653/1759 [00:13<00:23, 46.65it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.64it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.51it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.58it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.60it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.57it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.59it/s][A
 39%|███▉      | 688/1759 [00:14<00:22, 46.61it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.60it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.56it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.59it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.55it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.52it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.60it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.65it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.55it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.55it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.53it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.56it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.65it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.61it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.50it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.56it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.61it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.62it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.48it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.56it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.61it/s][A
 45%|████▌     | 793/1759 [00:16<00:20, 46.57it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.55it/s][A
 46%|████▌     | 803/1759 [00:17<00:21, 44.92it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 45.39it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 45.80it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.08it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.20it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.43it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.50it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.50it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.40it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.37it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.34it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.52it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.58it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.55it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.54it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.61it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.60it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.48it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.42it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.44it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.37it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.56it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.61it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.57it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.64it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.68it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.56it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.49it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.39it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.38it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.42it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.52it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.51it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.51it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.67it/s][A
 56%|█████▌    | 978/1759 [00:20<00:16, 46.56it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.54it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.42it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.45it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.45it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.54it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.50it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.56it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.61it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.55it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.53it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.01it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.12it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.22it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.31it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.38it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.36it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.46it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.52it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.39it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.42it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.44it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.41it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.57it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.54it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:13, 46.59it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.61it/s][A
 64%|██████▎   | 1118/1759 [00:23<00:13, 46.48it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.45it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.40it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.36it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.54it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.62it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.55it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.64it/s][A
 66%|██████▌   | 1163/1759 [00:24<00:12, 46.56it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.53it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.51it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.42it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.38it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.52it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.55it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.58it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.66it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.62it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:13, 41.36it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:12, 42.78it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:12, 43.91it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 44.69it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 45.32it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 45.72it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 45.98it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.23it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.05it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.06it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.23it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.34it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.40it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.51it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.47it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.58it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.66it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.45it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.37it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.43it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.51it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.56it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.55it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.65it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.57it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.64it/s][A
 77%|███████▋  | 1348/1759 [00:28<00:08, 46.55it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.38it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.39it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.27it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.59it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.65it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.62it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.65it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.61it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.58it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.41it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.35it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.43it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.52it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.53it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.64it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.57it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:06, 46.58it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.53it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.44it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.35it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.49it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.55it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.60it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.52it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.47it/s][A
 85%|████████▍ | 1488/1759 [00:31<00:05, 46.43it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.42it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.39it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.49it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.61it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.65it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.47it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.54it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.41it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.55it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.49it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.54it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.61it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.61it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:03, 46.57it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.43it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.40it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.40it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.53it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.50it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.51it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.62it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.60it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.52it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.50it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.27it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.39it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.45it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.45it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 45.12it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 45.57it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 45.95it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.15it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.30it/s][A
 95%|█████████▌| 1673/1759 [00:35<00:01, 46.14it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.30it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.32it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.40it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.46it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.52it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.52it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.58it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.52it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.54it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.49it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.45it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.44it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.43it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.45it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.43it/s][A
                                                   [A                                                
100%|██████████| 1759/1759 [00:37<00:00, 46.43it/s][A 20%|██        | 78/390 [01:00<01:30,  3.44it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 15:33:30,777 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 15:33:30,801 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:33:32,986 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:33:32,998 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:33:33,006 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:07<1:11:36, 13.81s/it] 21%|██        | 80/390 [01:08<50:24,  9.76s/it]   21%|██        | 81/390 [01:08<35:37,  6.92s/it] 21%|██        | 82/390 [01:08<25:17,  4.93s/it] 21%|██▏       | 83/390 [01:09<18:05,  3.54s/it] 22%|██▏       | 84/390 [01:09<13:03,  2.56s/it] 22%|██▏       | 85/390 [01:09<09:33,  1.88s/it] 22%|██▏       | 86/390 [01:09<07:06,  1.40s/it] 22%|██▏       | 87/390 [01:10<05:23,  1.07s/it] 23%|██▎       | 88/390 [01:10<04:12,  1.20it/s] 23%|██▎       | 89/390 [01:10<03:21,  1.49it/s] 23%|██▎       | 90/390 [01:11<02:46,  1.80it/s] 23%|██▎       | 91/390 [01:11<02:22,  2.10it/s] 24%|██▎       | 92/390 [01:11<02:05,  2.38it/s] 24%|██▍       | 93/390 [01:11<01:53,  2.62it/s] 24%|██▍       | 94/390 [01:12<01:44,  2.83it/s] 24%|██▍       | 95/390 [01:12<01:38,  2.99it/s] 25%|██▍       | 96/390 [01:12<01:34,  3.12it/s] 25%|██▍       | 97/390 [01:13<01:31,  3.22it/s] 25%|██▌       | 98/390 [01:13<01:28,  3.28it/s] 25%|██▌       | 99/390 [01:13<01:27,  3.34it/s] 26%|██▌       | 100/390 [01:13<01:26,  3.37it/s] 26%|██▌       | 101/390 [01:14<01:25,  3.40it/s] 26%|██▌       | 102/390 [01:14<01:24,  3.40it/s] 26%|██▋       | 103/390 [01:14<01:23,  3.42it/s] 27%|██▋       | 104/390 [01:15<01:23,  3.43it/s] 27%|██▋       | 105/390 [01:15<01:22,  3.44it/s] 27%|██▋       | 106/390 [01:15<01:22,  3.45it/s] 27%|██▋       | 107/390 [01:15<01:22,  3.45it/s] 28%|██▊       | 108/390 [01:16<01:21,  3.45it/s] 28%|██▊       | 109/390 [01:16<01:21,  3.45it/s] 28%|██▊       | 110/390 [01:16<01:21,  3.45it/s] 28%|██▊       | 111/390 [01:17<01:20,  3.45it/s] 29%|██▊       | 112/390 [01:17<01:20,  3.46it/s] 29%|██▉       | 113/390 [01:17<01:20,  3.45it/s] 29%|██▉       | 114/390 [01:18<01:19,  3.45it/s] 29%|██▉       | 115/390 [01:18<01:19,  3.46it/s] 30%|██▉       | 116/390 [01:18<01:19,  3.46it/s] 30%|███       | 117/390 [01:18<01:18,  3.46it/s] 30%|███       | 118/390 [01:19<01:18,  3.46it/s] 31%|███       | 119/390 [01:19<01:18,  3.46it/s] 31%|███       | 120/390 [01:19<01:18,  3.46it/s] 31%|███       | 121/390 [01:20<01:17,  3.45it/s] 31%|███▏      | 122/390 [01:20<01:17,  3.46it/s] 32%|███▏      | 123/390 [01:20<01:17,  3.46it/s] 32%|███▏      | 124/390 [01:20<01:17,  3.44it/s] 32%|███▏      | 125/390 [01:21<01:16,  3.45it/s] 32%|███▏      | 126/390 [01:21<01:16,  3.45it/s] 33%|███▎      | 127/390 [01:21<01:16,  3.45it/s] 33%|███▎      | 128/390 [01:22<01:15,  3.45it/s] 33%|███▎      | 129/390 [01:22<01:15,  3.46it/s] 33%|███▎      | 130/390 [01:22<01:15,  3.46it/s] 34%|███▎      | 131/390 [01:22<01:14,  3.46it/s] 34%|███▍      | 132/390 [01:23<01:14,  3.46it/s] 34%|███▍      | 133/390 [01:23<01:14,  3.46it/s] 34%|███▍      | 134/390 [01:23<01:14,  3.46it/s] 35%|███▍      | 135/390 [01:24<01:14,  3.44it/s] 35%|███▍      | 136/390 [01:24<01:13,  3.45it/s] 35%|███▌      | 137/390 [01:24<01:13,  3.45it/s] 35%|███▌      | 138/390 [01:24<01:13,  3.45it/s] 36%|███▌      | 139/390 [01:25<01:12,  3.45it/s] 36%|███▌      | 140/390 [01:25<01:12,  3.45it/s] 36%|███▌      | 141/390 [01:25<01:12,  3.46it/s] 36%|███▋      | 142/390 [01:26<01:11,  3.46it/s] 37%|███▋      | 143/390 [01:26<01:11,  3.46it/s] 37%|███▋      | 144/390 [01:26<01:11,  3.46it/s] 37%|███▋      | 145/390 [01:26<01:10,  3.46it/s] 37%|███▋      | 146/390 [01:27<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:27<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:27<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:28<01:09,  3.45it/s] 38%|███▊      | 150/390 [01:28<01:09,  3.45it/s] 39%|███▊      | 151/390 [01:28<01:09,  3.46it/s] 39%|███▉      | 152/390 [01:29<01:08,  3.46it/s] 39%|███▉      | 153/390 [01:29<01:08,  3.46it/s] 39%|███▉      | 154/390 [01:29<01:08,  3.46it/s] 40%|███▉      | 155/390 [01:29<01:07,  3.46it/s] 40%|████      | 156/390 [01:30<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:34:00,470 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:34:00,470 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:34:00,470 >>   Batch size = 8
{'eval_loss': 0.8568162322044373, 'eval_runtime': 37.8467, 'eval_samples_per_second': 371.684, 'eval_steps_per_second': 46.477, 'epoch': 0.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.78it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.36it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.63it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.00it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.58it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.20it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.07it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.63it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.58it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.51it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.55it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.62it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.66it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.62it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.68it/s][A
  5%|▍         | 83/1759 [00:01<00:36, 46.56it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.46it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.40it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.53it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.49it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.55it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.64it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.64it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.62it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.61it/s][A
  8%|▊         | 133/1759 [00:02<00:35, 46.45it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.41it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.52it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.51it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.56it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.65it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.67it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.65it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.65it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.57it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.39it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.43it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.47it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.46it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.48it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.60it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.59it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.58it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.42it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.51it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.42it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.45it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.54it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.55it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.59it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.61it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.58it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.50it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.50it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.39it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.44it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.53it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.40it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.46it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.57it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.61it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.62it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.62it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.63it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.50it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.49it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.47it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.57it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.52it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.49it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.56it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.55it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.50it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.51it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.40it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.34it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.39it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.54it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.61it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.54it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.49it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.54it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.52it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.46it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.34it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.44it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.54it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.56it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.53it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.45it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.55it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.52it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.48it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.47it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.60it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.55it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.52it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.47it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.43it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.45it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.45it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.37it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.42it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.47it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.39it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.40it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.35it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.31it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.36it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.36it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.49it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.55it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.59it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.55it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.46it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.45it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.40it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.36it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.41it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.38it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.55it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.61it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.56it/s][A
 37%|███▋      | 643/1759 [00:13<00:23, 46.54it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.50it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.39it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.41it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.36it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.42it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.56it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.63it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.53it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.50it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.36it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.39it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.44it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.42it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.52it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.58it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.54it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.47it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.44it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.39it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.42it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.44it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.47it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.50it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.60it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.61it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.62it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.47it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.38it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.40it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.42it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.41it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.48it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.56it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.60it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.61it/s][A
 47%|████▋     | 828/1759 [00:17<00:19, 46.60it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.48it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.30it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.39it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.32it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.48it/s][A
 49%|████▉     | 858/1759 [00:18<00:20, 44.97it/s][A
 49%|████▉     | 864/1759 [00:18<00:19, 46.43it/s][A
 49%|████▉     | 869/1759 [00:18<00:19, 46.56it/s][A
 50%|████▉     | 874/1759 [00:18<00:18, 46.66it/s][A
 50%|████▉     | 879/1759 [00:18<00:18, 46.47it/s][A
 50%|█████     | 884/1759 [00:18<00:18, 46.29it/s][A
 51%|█████     | 889/1759 [00:19<00:18, 46.37it/s][A
 51%|█████     | 894/1759 [00:19<00:18, 46.35it/s][A
 51%|█████     | 899/1759 [00:19<00:18, 46.48it/s][A
 51%|█████▏    | 904/1759 [00:19<00:18, 46.42it/s][A
 52%|█████▏    | 909/1759 [00:19<00:18, 46.40it/s][A
 52%|█████▏    | 914/1759 [00:19<00:18, 46.52it/s][A
 52%|█████▏    | 919/1759 [00:19<00:18, 46.57it/s][A
 53%|█████▎    | 924/1759 [00:19<00:17, 46.52it/s][A
 53%|█████▎    | 929/1759 [00:19<00:17, 46.52it/s][A
 53%|█████▎    | 934/1759 [00:20<00:17, 46.41it/s][A
 53%|█████▎    | 939/1759 [00:20<00:17, 46.42it/s][A
 54%|█████▎    | 944/1759 [00:20<00:17, 46.42it/s][A
 54%|█████▍    | 949/1759 [00:20<00:17, 46.43it/s][A
 54%|█████▍    | 954/1759 [00:20<00:17, 46.49it/s][A
 55%|█████▍    | 959/1759 [00:20<00:17, 46.52it/s][A
 55%|█████▍    | 964/1759 [00:20<00:17, 46.56it/s][A
 55%|█████▌    | 969/1759 [00:20<00:16, 46.60it/s][A
 55%|█████▌    | 974/1759 [00:20<00:16, 46.56it/s][A
 56%|█████▌    | 979/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▌    | 984/1759 [00:21<00:16, 46.42it/s][A
 56%|█████▌    | 989/1759 [00:21<00:16, 46.36it/s][A
 57%|█████▋    | 994/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 999/1759 [00:21<00:16, 46.56it/s][A
 57%|█████▋    | 1004/1759 [00:21<00:16, 46.61it/s][A
 57%|█████▋    | 1009/1759 [00:21<00:16, 46.52it/s][A
 58%|█████▊    | 1014/1759 [00:21<00:16, 46.53it/s][A
 58%|█████▊    | 1019/1759 [00:21<00:15, 46.50it/s][A
 58%|█████▊    | 1024/1759 [00:22<00:15, 46.40it/s][A
 58%|█████▊    | 1029/1759 [00:22<00:15, 46.44it/s][A
 59%|█████▉    | 1034/1759 [00:22<00:15, 46.39it/s][A
 59%|█████▉    | 1039/1759 [00:22<00:15, 46.34it/s][A
 59%|█████▉    | 1044/1759 [00:22<00:15, 46.49it/s][A
 60%|█████▉    | 1049/1759 [00:22<00:15, 46.56it/s][A
 60%|█████▉    | 1054/1759 [00:22<00:15, 46.50it/s][A
 60%|██████    | 1059/1759 [00:22<00:15, 46.53it/s][A
 60%|██████    | 1064/1759 [00:22<00:14, 46.55it/s][A
 61%|██████    | 1069/1759 [00:22<00:14, 46.47it/s][A
 61%|██████    | 1074/1759 [00:23<00:14, 46.42it/s][A
 61%|██████▏   | 1079/1759 [00:23<00:14, 46.26it/s][A
 62%|██████▏   | 1084/1759 [00:23<00:14, 46.39it/s][A
 62%|██████▏   | 1089/1759 [00:23<00:14, 46.50it/s][A
 62%|██████▏   | 1094/1759 [00:23<00:14, 46.51it/s][A
 62%|██████▏   | 1099/1759 [00:23<00:14, 46.55it/s][A
 63%|██████▎   | 1104/1759 [00:23<00:14, 46.50it/s][A
 63%|██████▎   | 1109/1759 [00:23<00:13, 46.51it/s][A
 63%|██████▎   | 1114/1759 [00:23<00:13, 46.35it/s][A
 64%|██████▎   | 1119/1759 [00:24<00:13, 46.41it/s][A
 64%|██████▍   | 1124/1759 [00:24<00:13, 46.43it/s][A
 64%|██████▍   | 1129/1759 [00:24<00:13, 46.31it/s][A
 64%|██████▍   | 1134/1759 [00:24<00:13, 46.41it/s][A
 65%|██████▍   | 1139/1759 [00:24<00:13, 46.54it/s][A
 65%|██████▌   | 1144/1759 [00:24<00:13, 46.48it/s][A
 65%|██████▌   | 1149/1759 [00:24<00:13, 46.51it/s][A
 66%|██████▌   | 1154/1759 [00:24<00:13, 46.47it/s][A
 66%|██████▌   | 1159/1759 [00:24<00:12, 46.45it/s][A
 66%|██████▌   | 1164/1759 [00:25<00:12, 46.48it/s][A
 66%|██████▋   | 1169/1759 [00:25<00:12, 46.45it/s][A
 67%|██████▋   | 1174/1759 [00:25<00:12, 46.45it/s][A
 67%|██████▋   | 1179/1759 [00:25<00:12, 46.51it/s][A
 67%|██████▋   | 1184/1759 [00:25<00:12, 46.52it/s][A
 68%|██████▊   | 1189/1759 [00:25<00:12, 46.49it/s][A
 68%|██████▊   | 1194/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1199/1759 [00:25<00:12, 46.45it/s][A
 68%|██████▊   | 1204/1759 [00:25<00:11, 46.46it/s][A
 69%|██████▊   | 1209/1759 [00:25<00:11, 46.49it/s][A
 69%|██████▉   | 1214/1759 [00:26<00:11, 46.47it/s][A
 69%|██████▉   | 1219/1759 [00:26<00:11, 46.47it/s][A
 70%|██████▉   | 1224/1759 [00:26<00:11, 46.47it/s][A
 70%|██████▉   | 1229/1759 [00:26<00:11, 46.52it/s][A
 70%|███████   | 1234/1759 [00:26<00:11, 46.59it/s][A
 70%|███████   | 1239/1759 [00:26<00:11, 46.43it/s][A
 71%|███████   | 1244/1759 [00:26<00:11, 46.49it/s][A
 71%|███████   | 1249/1759 [00:26<00:10, 46.48it/s][A
 71%|███████▏  | 1254/1759 [00:26<00:10, 46.41it/s][A
 72%|███████▏  | 1259/1759 [00:27<00:10, 46.43it/s][A
 72%|███████▏  | 1264/1759 [00:27<00:10, 46.42it/s][A
 72%|███████▏  | 1269/1759 [00:27<00:10, 46.49it/s][A
 72%|███████▏  | 1274/1759 [00:27<00:10, 46.58it/s][A
 73%|███████▎  | 1279/1759 [00:27<00:10, 46.58it/s][A
 73%|███████▎  | 1284/1759 [00:27<00:10, 46.48it/s][A
 73%|███████▎  | 1289/1759 [00:27<00:10, 46.32it/s][A
 74%|███████▎  | 1294/1759 [00:27<00:10, 46.41it/s][A
 74%|███████▍  | 1299/1759 [00:27<00:09, 46.31it/s][A
 74%|███████▍  | 1304/1759 [00:28<00:09, 46.39it/s][A
 74%|███████▍  | 1309/1759 [00:28<00:09, 46.42it/s][A
 75%|███████▍  | 1314/1759 [00:28<00:09, 46.36it/s][A
 75%|███████▍  | 1319/1759 [00:28<00:09, 46.52it/s][A
 75%|███████▌  | 1324/1759 [00:28<00:09, 46.56it/s][A
 76%|███████▌  | 1329/1759 [00:28<00:09, 46.51it/s][A
 76%|███████▌  | 1334/1759 [00:28<00:09, 46.45it/s][A
 76%|███████▌  | 1339/1759 [00:28<00:09, 46.52it/s][A
 76%|███████▋  | 1344/1759 [00:28<00:08, 46.34it/s][A
 77%|███████▋  | 1349/1759 [00:29<00:08, 46.27it/s][A
 77%|███████▋  | 1354/1759 [00:29<00:08, 46.22it/s][A
 77%|███████▋  | 1359/1759 [00:29<00:08, 46.25it/s][A
 78%|███████▊  | 1364/1759 [00:29<00:08, 46.33it/s][A
 78%|███████▊  | 1369/1759 [00:29<00:08, 46.49it/s][A
 78%|███████▊  | 1374/1759 [00:29<00:08, 46.50it/s][A
 78%|███████▊  | 1379/1759 [00:29<00:08, 46.46it/s][A
 79%|███████▊  | 1384/1759 [00:29<00:08, 46.55it/s][A
 79%|███████▉  | 1389/1759 [00:29<00:07, 46.39it/s][A
 79%|███████▉  | 1394/1759 [00:29<00:07, 46.38it/s][A
 80%|███████▉  | 1399/1759 [00:30<00:07, 46.41it/s][A
 80%|███████▉  | 1404/1759 [00:30<00:07, 46.36it/s][A
 80%|████████  | 1409/1759 [00:30<00:07, 46.44it/s][A
 80%|████████  | 1414/1759 [00:30<00:07, 46.56it/s][A
 81%|████████  | 1419/1759 [00:30<00:07, 46.55it/s][A
 81%|████████  | 1424/1759 [00:30<00:07, 46.53it/s][A
 81%|████████  | 1429/1759 [00:30<00:07, 46.48it/s][A
 82%|████████▏ | 1434/1759 [00:30<00:07, 46.42it/s][A
 82%|████████▏ | 1439/1759 [00:30<00:06, 46.43it/s][A
 82%|████████▏ | 1444/1759 [00:31<00:06, 46.33it/s][A
 82%|████████▏ | 1449/1759 [00:31<00:06, 46.34it/s][A
 83%|████████▎ | 1454/1759 [00:31<00:06, 46.46it/s][A
 83%|████████▎ | 1459/1759 [00:31<00:06, 46.56it/s][A
 83%|████████▎ | 1464/1759 [00:31<00:06, 46.59it/s][A
 84%|████████▎ | 1469/1759 [00:31<00:06, 46.55it/s][A
 84%|████████▍ | 1474/1759 [00:31<00:06, 46.46it/s][A
 84%|████████▍ | 1479/1759 [00:31<00:06, 46.46it/s][A
 84%|████████▍ | 1484/1759 [00:31<00:05, 46.42it/s][A
 85%|████████▍ | 1489/1759 [00:32<00:05, 46.44it/s][A
 85%|████████▍ | 1494/1759 [00:32<00:05, 46.37it/s][A
 85%|████████▌ | 1499/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▌ | 1504/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▌ | 1509/1759 [00:32<00:05, 46.56it/s][A
 86%|████████▌ | 1514/1759 [00:32<00:05, 46.49it/s][A
 86%|████████▋ | 1519/1759 [00:32<00:05, 46.51it/s][A
 87%|████████▋ | 1524/1759 [00:32<00:05, 46.40it/s][A
 87%|████████▋ | 1529/1759 [00:32<00:04, 46.47it/s][A
 87%|████████▋ | 1534/1759 [00:32<00:04, 46.39it/s][A
 87%|████████▋ | 1539/1759 [00:33<00:04, 46.38it/s][A
 88%|████████▊ | 1544/1759 [00:33<00:04, 46.40it/s][A
 88%|████████▊ | 1549/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1554/1759 [00:33<00:04, 46.56it/s][A
 89%|████████▊ | 1559/1759 [00:33<00:04, 46.60it/s][A
 89%|████████▉ | 1564/1759 [00:33<00:04, 46.48it/s][A
 89%|████████▉ | 1569/1759 [00:33<00:04, 46.51it/s][A
 89%|████████▉ | 1574/1759 [00:33<00:03, 46.40it/s][A
 90%|████████▉ | 1579/1759 [00:33<00:03, 46.24it/s][A
 90%|█████████ | 1584/1759 [00:34<00:03, 46.42it/s][A
 90%|█████████ | 1589/1759 [00:34<00:03, 46.42it/s][A
 91%|█████████ | 1594/1759 [00:34<00:03, 46.54it/s][A
 91%|█████████ | 1599/1759 [00:34<00:03, 46.58it/s][A
 91%|█████████ | 1604/1759 [00:34<00:03, 46.60it/s][A
 91%|█████████▏| 1609/1759 [00:34<00:03, 46.44it/s][A
 92%|█████████▏| 1614/1759 [00:34<00:03, 46.42it/s][A
 92%|█████████▏| 1619/1759 [00:34<00:03, 46.51it/s][A
 92%|█████████▏| 1624/1759 [00:34<00:02, 46.51it/s][A
 93%|█████████▎| 1629/1759 [00:35<00:02, 46.46it/s][A
 93%|█████████▎| 1634/1759 [00:35<00:02, 46.33it/s][A
 93%|█████████▎| 1639/1759 [00:35<00:02, 46.37it/s][A
 93%|█████████▎| 1644/1759 [00:35<00:02, 46.47it/s][A
 94%|█████████▎| 1649/1759 [00:35<00:02, 46.57it/s][A
 94%|█████████▍| 1654/1759 [00:35<00:02, 46.49it/s][A
 94%|█████████▍| 1659/1759 [00:35<00:02, 46.42it/s][A
 95%|█████████▍| 1664/1759 [00:35<00:02, 46.41it/s][A
 95%|█████████▍| 1669/1759 [00:35<00:01, 46.42it/s][A
 95%|█████████▌| 1674/1759 [00:35<00:01, 46.45it/s][A
 95%|█████████▌| 1679/1759 [00:36<00:01, 46.47it/s][A
 96%|█████████▌| 1684/1759 [00:36<00:01, 46.35it/s][A
 96%|█████████▌| 1689/1759 [00:36<00:01, 46.39it/s][A
 96%|█████████▋| 1694/1759 [00:36<00:01, 46.53it/s][A
 97%|█████████▋| 1699/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1704/1759 [00:36<00:01, 46.45it/s][A
 97%|█████████▋| 1709/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1714/1759 [00:36<00:00, 46.38it/s][A
 98%|█████████▊| 1719/1759 [00:36<00:00, 46.40it/s][A
 98%|█████████▊| 1724/1759 [00:37<00:00, 46.49it/s][A
 98%|█████████▊| 1729/1759 [00:37<00:00, 46.47it/s][A
 99%|█████████▊| 1734/1759 [00:37<00:00, 46.49it/s][A
 99%|█████████▉| 1739/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▉| 1744/1759 [00:37<00:00, 46.41it/s][A
 99%|█████████▉| 1749/1759 [00:37<00:00, 46.48it/s][A
100%|█████████▉| 1754/1759 [00:37<00:00, 46.48it/s][A
100%|██████████| 1759/1759 [00:37<00:00, 46.42it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.42it/s][A 40%|████      | 156/390 [02:08<01:07,  3.46it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 15:34:38,337 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 15:34:38,361 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:34:40,597 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:34:40,616 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:34:40,628 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:15<53:53, 13.88s/it] 41%|████      | 158/390 [02:16<37:54,  9.80s/it] 41%|████      | 159/390 [02:16<26:45,  6.95s/it] 41%|████      | 160/390 [02:16<18:58,  4.95s/it] 41%|████▏     | 161/390 [02:16<13:33,  3.55s/it] 42%|████▏     | 162/390 [02:17<09:46,  2.57s/it] 42%|████▏     | 163/390 [02:17<07:08,  1.89s/it] 42%|████▏     | 164/390 [02:17<05:18,  1.41s/it] 42%|████▏     | 165/390 [02:18<04:01,  1.07s/it] 43%|████▎     | 166/390 [02:18<03:07,  1.19it/s] 43%|████▎     | 167/390 [02:18<02:29,  1.49it/s] 43%|████▎     | 168/390 [02:18<02:03,  1.79it/s] 43%|████▎     | 169/390 [02:19<01:45,  2.09it/s] 44%|████▎     | 170/390 [02:19<01:32,  2.37it/s] 44%|████▍     | 171/390 [02:19<01:23,  2.62it/s] 44%|████▍     | 172/390 [02:20<01:17,  2.83it/s] 44%|████▍     | 173/390 [02:20<01:12,  2.99it/s] 45%|████▍     | 174/390 [02:20<01:09,  3.12it/s] 45%|████▍     | 175/390 [02:20<01:06,  3.21it/s] 45%|████▌     | 176/390 [02:21<01:05,  3.28it/s] 45%|████▌     | 177/390 [02:21<01:03,  3.34it/s] 46%|████▌     | 178/390 [02:21<01:02,  3.37it/s] 46%|████▌     | 179/390 [02:22<01:02,  3.40it/s] 46%|████▌     | 180/390 [02:22<01:01,  3.41it/s] 46%|████▋     | 181/390 [02:22<01:01,  3.42it/s] 47%|████▋     | 182/390 [02:22<01:00,  3.43it/s] 47%|████▋     | 183/390 [02:23<01:00,  3.44it/s] 47%|████▋     | 184/390 [02:23<00:59,  3.45it/s] 47%|████▋     | 185/390 [02:23<00:59,  3.45it/s] 48%|████▊     | 186/390 [02:24<00:59,  3.45it/s] 48%|████▊     | 187/390 [02:24<00:58,  3.45it/s] 48%|████▊     | 188/390 [02:24<00:58,  3.45it/s] 48%|████▊     | 189/390 [02:24<00:58,  3.45it/s] 49%|████▊     | 190/390 [02:25<00:57,  3.46it/s] 49%|████▉     | 191/390 [02:25<00:57,  3.45it/s] 49%|████▉     | 192/390 [02:25<00:57,  3.45it/s] 49%|████▉     | 193/390 [02:26<00:57,  3.45it/s] 50%|████▉     | 194/390 [02:26<00:56,  3.45it/s] 50%|█████     | 195/390 [02:26<00:56,  3.46it/s] 50%|█████     | 196/390 [02:27<00:56,  3.46it/s] 51%|█████     | 197/390 [02:27<00:55,  3.46it/s] 51%|█████     | 198/390 [02:27<00:55,  3.46it/s] 51%|█████     | 199/390 [02:27<00:55,  3.46it/s] 51%|█████▏    | 200/390 [02:28<00:54,  3.46it/s] 52%|█████▏    | 201/390 [02:28<00:54,  3.46it/s] 52%|█████▏    | 202/390 [02:28<00:54,  3.45it/s] 52%|█████▏    | 203/390 [02:29<00:54,  3.45it/s] 52%|█████▏    | 204/390 [02:29<00:53,  3.45it/s] 53%|█████▎    | 205/390 [02:29<00:53,  3.45it/s] 53%|█████▎    | 206/390 [02:29<00:53,  3.46it/s] 53%|█████▎    | 207/390 [02:30<00:52,  3.46it/s] 53%|█████▎    | 208/390 [02:30<00:52,  3.45it/s] 54%|█████▎    | 209/390 [02:30<00:52,  3.46it/s] 54%|█████▍    | 210/390 [02:31<00:52,  3.46it/s] 54%|█████▍    | 211/390 [02:31<00:51,  3.45it/s] 54%|█████▍    | 212/390 [02:31<00:51,  3.46it/s] 55%|█████▍    | 213/390 [02:31<00:51,  3.42it/s] 55%|█████▍    | 214/390 [02:32<00:51,  3.43it/s] 55%|█████▌    | 215/390 [02:32<00:50,  3.44it/s] 55%|█████▌    | 216/390 [02:32<00:50,  3.44it/s] 56%|█████▌    | 217/390 [02:33<00:50,  3.45it/s] 56%|█████▌    | 218/390 [02:33<00:49,  3.45it/s] 56%|█████▌    | 219/390 [02:33<00:49,  3.45it/s] 56%|█████▋    | 220/390 [02:33<00:49,  3.45it/s] 57%|█████▋    | 221/390 [02:34<00:48,  3.45it/s] 57%|█████▋    | 222/390 [02:34<00:48,  3.45it/s] 57%|█████▋    | 223/390 [02:34<00:48,  3.45it/s] 57%|█████▋    | 224/390 [02:35<00:48,  3.44it/s] 58%|█████▊    | 225/390 [02:35<00:47,  3.45it/s] 58%|█████▊    | 226/390 [02:35<00:47,  3.45it/s] 58%|█████▊    | 227/390 [02:36<00:47,  3.45it/s] 58%|█████▊    | 228/390 [02:36<00:46,  3.45it/s] 59%|█████▊    | 229/390 [02:36<00:46,  3.45it/s] 59%|█████▉    | 230/390 [02:36<00:46,  3.45it/s] 59%|█████▉    | 231/390 [02:37<00:46,  3.46it/s] 59%|█████▉    | 232/390 [02:37<00:45,  3.46it/s] 60%|█████▉    | 233/390 [02:37<00:45,  3.46it/s] 60%|██████    | 234/390 [02:38<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:35:08,343 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:35:08,343 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:35:08,343 >>   Batch size = 8
{'eval_loss': 0.8728876113891602, 'eval_runtime': 37.8457, 'eval_samples_per_second': 371.693, 'eval_steps_per_second': 46.478, 'epoch': 1.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.61it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.15it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.53it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.95it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.58it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.23it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.11it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.68it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.66it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.71it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.63it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.53it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.59it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.65it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.71it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.72it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.50it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.49it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.52it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.54it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.56it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.54it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.55it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.62it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.66it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.63it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.55it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.48it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.53it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.49it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.57it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.57it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.62it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.63it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.56it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.63it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.60it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.42it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.48it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.47it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.49it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.61it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.64it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.66it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.50it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.44it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.38it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.43it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.47it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.53it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.59it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.61it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.58it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.59it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.59it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.46it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.41it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.46it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.52it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.58it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.63it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.60it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.61it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.61it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.46it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.40it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.51it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.60it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.59it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.57it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.54it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.52it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.41it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.45it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.47it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.46it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.56it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.58it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.64it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.61it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.54it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.48it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.45it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.45it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.49it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.46it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.55it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.57it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.49it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.53it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.40it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.54it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.60it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.49it/s][A
 28%|██▊       | 493/1759 [00:10<00:29, 43.35it/s][A
 28%|██▊       | 498/1759 [00:10<00:28, 45.03it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 45.55it/s][A
 29%|██▉       | 508/1759 [00:10<00:27, 45.92it/s][A
 29%|██▉       | 513/1759 [00:11<00:27, 46.13it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.25it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.42it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.51it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.55it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.18it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.24it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.38it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.48it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.57it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.59it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.60it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.62it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.62it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.41it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.47it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.35it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.53it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.56it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.57it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.63it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.65it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.54it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.45it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.36it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.32it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.36it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.50it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.55it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.59it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.68it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.54it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.55it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.43it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.55it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.47it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.49it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.58it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.53it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.55it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.36it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.34it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.42it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.51it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.55it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.58it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.53it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.61it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.64it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.61it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.56it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.37it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.49it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.55it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.48it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.58it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.59it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.51it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.50it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.49it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.42it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.40it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.52it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.50it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.54it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.53it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.46it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.35it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.34it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.35it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.43it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.47it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.41it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.52it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.39it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.43it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.44it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.44it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.39it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.44it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.54it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.53it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.55it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.59it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.50it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.42it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.35it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.44it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.47it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.31it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.49it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.58it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.54it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.53it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.43it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.42it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.39it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.38it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.49it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.54it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.53it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.50it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.45it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.48it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.44it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.32it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.40it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.37it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.53it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.58it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.57it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.50it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.44it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.44it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.31it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.36it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.37it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.52it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.49it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.46it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.37it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.37it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.34it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.17it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.28it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.43it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.52it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.55it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.55it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.56it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.47it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.41it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.40it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.36it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.48it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.54it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.56it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.56it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.53it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.46it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.41it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.45it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.30it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.44it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.50it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.54it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.59it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.55it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.48it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.39it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.44it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.44it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.34it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.46it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.51it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.60it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.61it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.48it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.30it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.47it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.45it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.51it/s][A
 77%|███████▋  | 1348/1759 [00:28<00:08, 46.59it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.45it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.57it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.58it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.52it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.50it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.39it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.37it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:08, 46.33it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.38it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.53it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.45it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.48it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.58it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.56it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.46it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.46it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.40it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.51it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.54it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.46it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.37it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.36it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.45it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.48it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.41it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.45it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.39it/s][A
 85%|████████▍ | 1488/1759 [00:31<00:05, 46.49it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.58it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.56it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.40it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.46it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.46it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.49it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.47it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.46it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.54it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.53it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.52it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.44it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.51it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.47it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.47it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.43it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.44it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.52it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.54it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.51it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.52it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.53it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.41it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.48it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.43it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.49it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.60it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.57it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.52it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.43it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.50it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.51it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.40it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.42it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.44it/s][A
 95%|█████████▌| 1673/1759 [00:35<00:01, 46.51it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.36it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.46it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.55it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.49it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.49it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.43it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.46it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.54it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.55it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.55it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.57it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.48it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.51it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.42it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.44it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.45it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.45it/s][A 60%|██████    | 234/390 [03:15<00:45,  3.46it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 15:35:46,219 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:35:46,240 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:35:48,939 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:35:48,959 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:35:48,970 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:24<36:32, 14.15s/it] 61%|██████    | 236/390 [03:24<25:38,  9.99s/it] 61%|██████    | 237/390 [03:25<18:03,  7.08s/it] 61%|██████    | 238/390 [03:25<12:46,  5.04s/it] 61%|██████▏   | 239/390 [03:25<09:06,  3.62s/it] 62%|██████▏   | 240/390 [03:25<06:32,  2.62s/it] 62%|██████▏   | 241/390 [03:26<04:45,  1.92s/it] 62%|██████▏   | 242/390 [03:26<03:31,  1.43s/it] 62%|██████▏   | 243/390 [03:26<02:39,  1.09s/it] 63%|██████▎   | 244/390 [03:27<02:03,  1.18it/s] 63%|██████▎   | 245/390 [03:27<01:38,  1.47it/s] 63%|██████▎   | 246/390 [03:27<01:20,  1.78it/s] 63%|██████▎   | 247/390 [03:27<01:08,  2.08it/s] 64%|██████▎   | 248/390 [03:28<01:00,  2.36it/s] 64%|██████▍   | 249/390 [03:28<00:54,  2.61it/s] 64%|██████▍   | 250/390 [03:28<00:49,  2.82it/s] 64%|██████▍   | 251/390 [03:29<00:46,  2.99it/s] 65%|██████▍   | 252/390 [03:29<00:44,  3.11it/s] 65%|██████▍   | 253/390 [03:29<00:42,  3.21it/s] 65%|██████▌   | 254/390 [03:30<00:41,  3.28it/s] 65%|██████▌   | 255/390 [03:30<00:40,  3.33it/s] 66%|██████▌   | 256/390 [03:30<00:39,  3.37it/s] 66%|██████▌   | 257/390 [03:30<00:39,  3.40it/s] 66%|██████▌   | 258/390 [03:31<00:38,  3.41it/s] 66%|██████▋   | 259/390 [03:31<00:38,  3.42it/s] 67%|██████▋   | 260/390 [03:31<00:37,  3.43it/s] 67%|██████▋   | 261/390 [03:32<00:37,  3.44it/s] 67%|██████▋   | 262/390 [03:32<00:37,  3.45it/s] 67%|██████▋   | 263/390 [03:32<00:36,  3.45it/s] 68%|██████▊   | 264/390 [03:32<00:36,  3.45it/s] 68%|██████▊   | 265/390 [03:33<00:36,  3.45it/s] 68%|██████▊   | 266/390 [03:33<00:35,  3.46it/s] 68%|██████▊   | 267/390 [03:33<00:35,  3.46it/s] 69%|██████▊   | 268/390 [03:34<00:35,  3.46it/s] 69%|██████▉   | 269/390 [03:34<00:35,  3.44it/s] 69%|██████▉   | 270/390 [03:34<00:34,  3.45it/s] 69%|██████▉   | 271/390 [03:34<00:34,  3.45it/s] 70%|██████▉   | 272/390 [03:35<00:34,  3.45it/s] 70%|███████   | 273/390 [03:35<00:33,  3.45it/s] 70%|███████   | 274/390 [03:35<00:33,  3.46it/s] 71%|███████   | 275/390 [03:36<00:33,  3.46it/s] 71%|███████   | 276/390 [03:36<00:32,  3.46it/s] 71%|███████   | 277/390 [03:36<00:32,  3.46it/s] 71%|███████▏  | 278/390 [03:36<00:32,  3.46it/s] 72%|███████▏  | 279/390 [03:37<00:32,  3.46it/s] 72%|███████▏  | 280/390 [03:37<00:31,  3.45it/s] 72%|███████▏  | 281/390 [03:37<00:31,  3.45it/s] 72%|███████▏  | 282/390 [03:38<00:31,  3.45it/s] 73%|███████▎  | 283/390 [03:38<00:30,  3.45it/s] 73%|███████▎  | 284/390 [03:38<00:30,  3.45it/s] 73%|███████▎  | 285/390 [03:38<00:30,  3.45it/s] 73%|███████▎  | 286/390 [03:39<00:30,  3.46it/s] 74%|███████▎  | 287/390 [03:39<00:29,  3.46it/s] 74%|███████▍  | 288/390 [03:39<00:29,  3.46it/s] 74%|███████▍  | 289/390 [03:40<00:29,  3.46it/s] 74%|███████▍  | 290/390 [03:40<00:28,  3.46it/s] 75%|███████▍  | 291/390 [03:40<00:28,  3.44it/s] 75%|███████▍  | 292/390 [03:41<00:28,  3.45it/s] 75%|███████▌  | 293/390 [03:41<00:28,  3.45it/s] 75%|███████▌  | 294/390 [03:41<00:27,  3.45it/s] 76%|███████▌  | 295/390 [03:41<00:27,  3.45it/s] 76%|███████▌  | 296/390 [03:42<00:27,  3.45it/s] 76%|███████▌  | 297/390 [03:42<00:26,  3.46it/s] 76%|███████▋  | 298/390 [03:42<00:26,  3.45it/s] 77%|███████▋  | 299/390 [03:43<00:26,  3.46it/s] 77%|███████▋  | 300/390 [03:43<00:26,  3.45it/s] 77%|███████▋  | 301/390 [03:43<00:25,  3.45it/s] 77%|███████▋  | 302/390 [03:43<00:25,  3.44it/s] 78%|███████▊  | 303/390 [03:44<00:25,  3.45it/s] 78%|███████▊  | 304/390 [03:44<00:24,  3.45it/s] 78%|███████▊  | 305/390 [03:44<00:24,  3.45it/s] 78%|███████▊  | 306/390 [03:45<00:24,  3.45it/s] 79%|███████▊  | 307/390 [03:45<00:24,  3.45it/s] 79%|███████▉  | 308/390 [03:45<00:23,  3.45it/s] 79%|███████▉  | 309/390 [03:45<00:23,  3.45it/s] 79%|███████▉  | 310/390 [03:46<00:23,  3.45it/s] 80%|███████▉  | 311/390 [03:46<00:22,  3.45it/s] 80%|████████  | 312/390 [03:46<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 15:36:17,108 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:36:17,111 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:36:17,111 >>   Batch size = 8
{'eval_loss': 0.8799923062324524, 'eval_runtime': 37.8461, 'eval_samples_per_second': 371.689, 'eval_steps_per_second': 46.478, 'epoch': 2.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.36it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.30it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.68it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.99it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.53it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.29it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.01it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.63it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.65it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.70it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.62it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.60it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.60it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.67it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.68it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.57it/s][A
  5%|▌         | 88/1759 [00:01<00:39, 42.20it/s][A
  5%|▌         | 93/1759 [00:02<00:38, 43.56it/s][A
  6%|▌         | 98/1759 [00:02<00:37, 44.47it/s][A
  6%|▌         | 103/1759 [00:02<00:36, 45.15it/s][A
  6%|▌         | 108/1759 [00:02<00:36, 45.54it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 45.81it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.14it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.37it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.44it/s][A
  8%|▊         | 133/1759 [00:02<00:35, 46.37it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.35it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.35it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.48it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.52it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.54it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.50it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.53it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.58it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.56it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.56it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.40it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.43it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.52it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.51it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.61it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.52it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.47it/s][A
 13%|█▎        | 233/1759 [00:05<00:32, 46.43it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.43it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.50it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.55it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.63it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.62it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.63it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.60it/s][A
 16%|█▌        | 273/1759 [00:05<00:32, 46.43it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.51it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.48it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.45it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.54it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.62it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.66it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.71it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.55it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.52it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.50it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.39it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.38it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.47it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.59it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.61it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.58it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.56it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.42it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.39it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.38it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.49it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.53it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.58it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.65it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.63it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.50it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.50it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.42it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.40it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.50it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.48it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.59it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.56it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.61it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.60it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.53it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.41it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.43it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.34it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.48it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.56it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.60it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.64it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.58it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.50it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.42it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.30it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.44it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.48it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.52it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.58it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.63it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.54it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.56it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.44it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.39it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.44it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.51it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.58it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.55it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.66it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.53it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.45it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.52it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.43it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.48it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.47it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.44it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.54it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.65it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.51it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.55it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.41it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.43it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.46it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.41it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.53it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.58it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.58it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.45it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.51it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.54it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.45it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.41it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.45it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.44it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.51it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.62it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.46it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.55it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.50it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.47it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.49it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.44it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.41it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.54it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.55it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.56it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.58it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.41it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.50it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.51it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.41it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.45it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.38it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.53it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.56it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.56it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.58it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.40it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.48it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.50it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.40it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.43it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.53it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.57it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.59it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.43it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.45it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.37it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.44it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.48it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.42it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.55it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.58it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.54it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.52it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.50it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.47it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.42it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.44it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.47it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.52it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.61it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.53it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.51it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.52it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.53it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.40it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.32it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.49it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.57it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.61it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.52it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.52it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.46it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.50it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.46it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.34it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.32it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.46it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.54it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.56it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.44it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.48it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.41it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.49it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.42it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.53it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.57it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.58it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.54it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.47it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.41it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.34it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.36it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.44it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.44it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.57it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.54it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.53it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.44it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.41it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.43it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.43it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.45it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.54it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.50it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.54it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.53it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.54it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.39it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.37it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.45it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.42it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.41it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.54it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.49it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.53it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.52it/s][A
 71%|███████   | 1248/1759 [00:26<00:10, 46.54it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.35it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.45it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.45it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.53it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.51it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.49it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.50it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.56it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.33it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.36it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.43it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.51it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.50it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.45it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.51it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.48it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.48it/s][A
 77%|███████▋  | 1348/1759 [00:28<00:08, 46.41it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.35it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.41it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.56it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.53it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.53it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.41it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.49it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.43it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.48it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.39it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.52it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.47it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.56it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.54it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.40it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.38it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.38it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.44it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.37it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.48it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.54it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.39it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.47it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.42it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.34it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.38it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.37it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.44it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.42it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.40it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.51it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.49it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.49it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.48it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.40it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.38it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.44it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.34it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.42it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.54it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.54it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.51it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.45it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.45it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.41it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.43it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.40it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.40it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.54it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.46it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.48it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.53it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.42it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.35it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.46it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.44it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.47it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.49it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.48it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.46it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.44it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.47it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.28it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.42it/s][A
 95%|█████████▌| 1673/1759 [00:35<00:01, 46.46it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.45it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.49it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.44it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.47it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.46it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.35it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.32it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.38it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.48it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.54it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.54it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.43it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.41it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.45it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.36it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.36it/s][A 80%|████████  | 312/390 [04:24<00:22,  3.44it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 15:36:54,992 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 15:36:55,010 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:36:57,382 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:36:57,394 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:36:57,401 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:32<17:47, 13.87s/it] 81%|████████  | 314/390 [04:32<12:24,  9.79s/it] 81%|████████  | 315/390 [04:32<08:40,  6.94s/it] 81%|████████  | 316/390 [04:33<06:05,  4.95s/it] 81%|████████▏ | 317/390 [04:33<04:19,  3.55s/it] 82%|████████▏ | 318/390 [04:33<03:05,  2.57s/it] 82%|████████▏ | 319/390 [04:34<02:13,  1.89s/it] 82%|████████▏ | 320/390 [04:34<01:38,  1.41s/it] 82%|████████▏ | 321/390 [04:34<01:13,  1.07s/it] 83%|████████▎ | 322/390 [04:34<00:56,  1.20it/s] 83%|████████▎ | 323/390 [04:35<00:45,  1.49it/s] 83%|████████▎ | 324/390 [04:35<00:36,  1.79it/s] 83%|████████▎ | 325/390 [04:35<00:31,  2.09it/s] 84%|████████▎ | 326/390 [04:36<00:26,  2.37it/s] 84%|████████▍ | 327/390 [04:36<00:24,  2.62it/s] 84%|████████▍ | 328/390 [04:36<00:21,  2.82it/s] 84%|████████▍ | 329/390 [04:36<00:20,  2.99it/s] 85%|████████▍ | 330/390 [04:37<00:19,  3.11it/s] 85%|████████▍ | 331/390 [04:37<00:18,  3.21it/s] 85%|████████▌ | 332/390 [04:37<00:17,  3.28it/s] 85%|████████▌ | 333/390 [04:38<00:17,  3.33it/s] 86%|████████▌ | 334/390 [04:38<00:16,  3.37it/s] 86%|████████▌ | 335/390 [04:38<00:16,  3.40it/s] 86%|████████▌ | 336/390 [04:38<00:15,  3.41it/s] 86%|████████▋ | 337/390 [04:39<00:15,  3.42it/s] 87%|████████▋ | 338/390 [04:39<00:15,  3.43it/s] 87%|████████▋ | 339/390 [04:39<00:14,  3.44it/s] 87%|████████▋ | 340/390 [04:40<00:14,  3.45it/s] 87%|████████▋ | 341/390 [04:40<00:14,  3.45it/s] 88%|████████▊ | 342/390 [04:40<00:13,  3.45it/s] 88%|████████▊ | 343/390 [04:41<00:13,  3.46it/s] 88%|████████▊ | 344/390 [04:41<00:13,  3.46it/s] 88%|████████▊ | 345/390 [04:41<00:13,  3.46it/s] 89%|████████▊ | 346/390 [04:41<00:12,  3.46it/s] 89%|████████▉ | 347/390 [04:42<00:12,  3.44it/s] 89%|████████▉ | 348/390 [04:42<00:12,  3.45it/s] 89%|████████▉ | 349/390 [04:42<00:11,  3.45it/s] 90%|████████▉ | 350/390 [04:43<00:11,  3.45it/s] 90%|█████████ | 351/390 [04:43<00:11,  3.46it/s] 90%|█████████ | 352/390 [04:43<00:11,  3.45it/s] 91%|█████████ | 353/390 [04:43<00:10,  3.46it/s] 91%|█████████ | 354/390 [04:44<00:10,  3.46it/s] 91%|█████████ | 355/390 [04:44<00:10,  3.46it/s] 91%|█████████▏| 356/390 [04:44<00:09,  3.46it/s] 92%|█████████▏| 357/390 [04:45<00:09,  3.46it/s] 92%|█████████▏| 358/390 [04:45<00:09,  3.45it/s] 92%|█████████▏| 359/390 [04:45<00:08,  3.45it/s] 92%|█████████▏| 360/390 [04:45<00:08,  3.45it/s] 93%|█████████▎| 361/390 [04:46<00:08,  3.45it/s] 93%|█████████▎| 362/390 [04:46<00:08,  3.45it/s] 93%|█████████▎| 363/390 [04:46<00:07,  3.45it/s] 93%|█████████▎| 364/390 [04:47<00:07,  3.45it/s] 94%|█████████▎| 365/390 [04:47<00:07,  3.45it/s] 94%|█████████▍| 366/390 [04:47<00:06,  3.45it/s] 94%|█████████▍| 367/390 [04:47<00:06,  3.45it/s] 94%|█████████▍| 368/390 [04:48<00:06,  3.46it/s] 95%|█████████▍| 369/390 [04:48<00:06,  3.42it/s] 95%|█████████▍| 370/390 [04:48<00:06,  3.30it/s] 95%|█████████▌| 371/390 [04:49<00:05,  3.34it/s] 95%|█████████▌| 372/390 [04:49<00:05,  3.37it/s] 96%|█████████▌| 373/390 [04:49<00:05,  3.40it/s] 96%|█████████▌| 374/390 [04:50<00:04,  3.41it/s] 96%|█████████▌| 375/390 [04:50<00:04,  3.43it/s] 96%|█████████▋| 376/390 [04:50<00:04,  3.43it/s] 97%|█████████▋| 377/390 [04:50<00:03,  3.44it/s] 97%|█████████▋| 378/390 [04:51<00:03,  3.44it/s] 97%|█████████▋| 379/390 [04:51<00:03,  3.45it/s] 97%|█████████▋| 380/390 [04:51<00:02,  3.44it/s] 98%|█████████▊| 381/390 [04:52<00:02,  3.45it/s] 98%|█████████▊| 382/390 [04:52<00:02,  3.45it/s] 98%|█████████▊| 383/390 [04:52<00:02,  3.45it/s] 98%|█████████▊| 384/390 [04:52<00:01,  3.45it/s] 99%|█████████▊| 385/390 [04:53<00:01,  3.45it/s] 99%|█████████▉| 386/390 [04:53<00:01,  3.45it/s] 99%|█████████▉| 387/390 [04:53<00:00,  3.45it/s] 99%|█████████▉| 388/390 [04:54<00:00,  3.45it/s]100%|█████████▉| 389/390 [04:54<00:00,  3.45it/s]100%|██████████| 390/390 [04:54<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 15:37:24,948 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:37:24,948 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:37:24,948 >>   Batch size = 8
{'eval_loss': 0.888980507850647, 'eval_runtime': 37.8679, 'eval_samples_per_second': 371.476, 'eval_steps_per_second': 46.451, 'epoch': 3.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.15it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.38it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.69it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.00it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.43it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.29it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 46.99it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.71it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.63it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.69it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.61it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.69it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.55it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.64it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.68it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.56it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.43it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.40it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.48it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.51it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.61it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.60it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.64it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.53it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.66it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.49it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.36it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.48it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.53it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.63it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.68it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.64it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.66it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.58it/s][A
 10%|█         | 178/1759 [00:03<00:34, 46.50it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.46it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.39it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.45it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.53it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.61it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.65it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.68it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.60it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.54it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.46it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.47it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.36it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.47it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.60it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.61it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.59it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.67it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.62it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.60it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.50it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.45it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.51it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.54it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.54it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.62it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.60it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.60it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.60it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.58it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.44it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.45it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.47it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.49it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.57it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.54it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.46it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.41it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.47it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.47it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.42it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.48it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.35it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.55it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.52it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.48it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.54it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.50it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.49it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.55it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.49it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.36it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.38it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.48it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.55it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.54it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.58it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.61it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.63it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.60it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.62it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.55it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.53it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.57it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.54it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.52it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.50it/s][A
 29%|██▉       | 513/1759 [00:10<00:26, 46.55it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.54it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.43it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.48it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.50it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.42it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.54it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.48it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.43it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.44it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.48it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.47it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.50it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.44it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.47it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.51it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.46it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.46it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.39it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.53it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.48it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.49it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.57it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.46it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.53it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.54it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.41it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.48it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.42it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.42it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.52it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.47it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.49it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.47it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.45it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.39it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.46it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.53it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.42it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.41it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.55it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.55it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.53it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.50it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.47it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.55it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.40it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.42it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.46it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.56it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.56it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.52it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.47it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.46it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.52it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.49it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.45it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.43it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.46it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.59it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.50it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.57it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.56it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.47it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.43it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.46it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.48it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.51it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.53it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.54it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.52it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.49it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.50it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.38it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.45it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.52it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.35it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.46it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.50it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.43it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.47it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.43it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.38it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.39it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.46it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.47it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.49it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.49it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.38it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.46it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.49it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.44it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.48it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.50it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.50it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.53it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.45it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.39it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.44it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.38it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.41it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.42it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.49it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.36it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.42it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.42it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.37it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.34it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.39it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.40it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.51it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.47it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.44it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.39it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.46it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.34it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.39it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.44it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.38it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.50it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.42it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.40it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.43it/s][A
 66%|██████▌   | 1163/1759 [00:24<00:12, 46.46it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.37it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.21it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.43it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.47it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.59it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.62it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.48it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.50it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.54it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.46it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.34it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.39it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.39it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.51it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.54it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.51it/s][A
 71%|███████   | 1248/1759 [00:26<00:10, 46.49it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.49it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.41it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.43it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.41it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.50it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.52it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.45it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.36it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.40it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.32it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.23it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.37it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.38it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.55it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.46it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.46it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.47it/s][A
 77%|███████▋  | 1348/1759 [00:28<00:08, 46.42it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.42it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.37it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.43it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.50it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.47it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.48it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.46it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.42it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.46it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.44it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.33it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.34it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.42it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.48it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.44it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.51it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.39it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.42it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.45it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.44it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.42it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.52it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.53it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.46it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.45it/s][A
 85%|████████▍ | 1488/1759 [00:31<00:05, 46.44it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.46it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.48it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.41it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.49it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.51it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.51it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.03it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.27it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.35it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.36it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.35it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.32it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.35it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.47it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.44it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.46it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.46it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.40it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.42it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.43it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.50it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.56it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.51it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.44it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.44it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.42it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.34it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.33it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.34it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.44it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.44it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.45it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.50it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.46it/s][A
 95%|█████████▌| 1673/1759 [00:35<00:01, 46.44it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.46it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.35it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.35it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.51it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.57it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.44it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.35it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.47it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.41it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.46it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.47it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.44it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.45it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.56it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.41it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.49it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.41it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.41it/s][A100%|██████████| 390/390 [05:32<00:00,  3.45it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 15:38:02,811 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 15:38:02,832 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:38:05,115 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:38:05,134 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:38:05,141 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:38:09,820 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:38:09,821 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78 (score: 0.8568162322044373).
                                                 100%|██████████| 390/390 [05:41<00:00,  3.45it/s]100%|██████████| 390/390 [05:41<00:00,  1.14it/s]
[INFO|trainer.py:1894] 2023-08-28 15:38:11,883 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 15:38:11,897 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:38:14,268 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:38:14,281 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:38:14,293 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:38:14,466 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,466 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,466 >>   train_loss               =     0.5852
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,466 >>   train_runtime            = 0:05:41.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,466 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,466 >>   train_samples_per_second =     73.182
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:14,467 >>   train_steps_per_second   =      1.142
{'eval_loss': 0.891291618347168, 'eval_runtime': 37.8426, 'eval_samples_per_second': 371.724, 'eval_steps_per_second': 46.482, 'epoch': 4.99}
{'train_runtime': 341.6137, 'train_samples_per_second': 73.182, 'train_steps_per_second': 1.142, 'train_loss': 0.5851569542518029, 'epoch': 4.99}
08/28/2023 15:38:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:38:14,509 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:38:14,509 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 15:38:14,509 >>   Batch size = 8
  0%|          | 0/1759 [00:00<?, ?it/s]  0%|          | 6/1759 [00:00<00:30, 57.84it/s]  1%|          | 12/1759 [00:00<00:34, 50.98it/s]  1%|          | 18/1759 [00:00<00:35, 49.19it/s]  1%|▏         | 23/1759 [00:00<00:35, 48.47it/s]  2%|▏         | 28/1759 [00:00<00:36, 47.94it/s]  2%|▏         | 33/1759 [00:00<00:36, 47.72it/s]  2%|▏         | 38/1759 [00:00<00:36, 47.54it/s]  2%|▏         | 43/1759 [00:00<00:36, 47.41it/s]  3%|▎         | 48/1759 [00:00<00:36, 47.05it/s]  3%|▎         | 53/1759 [00:01<00:36, 47.08it/s]  3%|▎         | 58/1759 [00:01<00:36, 47.14it/s]  4%|▎         | 63/1759 [00:01<00:36, 47.00it/s]  4%|▍         | 68/1759 [00:01<00:36, 46.91it/s]  4%|▍         | 73/1759 [00:01<00:35, 47.00it/s]  4%|▍         | 78/1759 [00:01<00:35, 46.99it/s]  5%|▍         | 83/1759 [00:01<00:35, 46.88it/s]  5%|▌         | 88/1759 [00:01<00:35, 46.91it/s]  5%|▌         | 93/1759 [00:01<00:35, 46.77it/s]  6%|▌         | 98/1759 [00:02<00:35, 46.75it/s]  6%|▌         | 103/1759 [00:02<00:35, 46.86it/s]  6%|▌         | 108/1759 [00:02<00:35, 46.74it/s]  6%|▋         | 113/1759 [00:02<00:35, 46.82it/s]  7%|▋         | 118/1759 [00:02<00:35, 46.84it/s]  7%|▋         | 123/1759 [00:02<00:34, 46.86it/s]  7%|▋         | 128/1759 [00:02<00:34, 46.99it/s]  8%|▊         | 133/1759 [00:02<00:34, 46.89it/s]  8%|▊         | 138/1759 [00:02<00:34, 46.79it/s]  8%|▊         | 143/1759 [00:03<00:34, 46.83it/s]  8%|▊         | 148/1759 [00:03<00:34, 46.77it/s]  9%|▊         | 153/1759 [00:03<00:34, 46.74it/s]  9%|▉         | 158/1759 [00:03<00:34, 46.79it/s]  9%|▉         | 163/1759 [00:03<00:34, 46.83it/s] 10%|▉         | 168/1759 [00:03<00:34, 46.79it/s] 10%|▉         | 173/1759 [00:03<00:33, 46.87it/s] 10%|█         | 178/1759 [00:03<00:33, 46.85it/s] 10%|█         | 183/1759 [00:03<00:33, 46.77it/s] 11%|█         | 188/1759 [00:03<00:33, 46.81it/s] 11%|█         | 193/1759 [00:04<00:33, 46.71it/s] 11%|█▏        | 198/1759 [00:04<00:33, 46.72it/s] 12%|█▏        | 203/1759 [00:04<00:33, 46.76it/s] 12%|█▏        | 208/1759 [00:04<00:33, 46.81it/s] 12%|█▏        | 213/1759 [00:04<00:33, 46.81it/s] 12%|█▏        | 218/1759 [00:04<00:34, 44.84it/s] 13%|█▎        | 223/1759 [00:04<00:33, 45.58it/s] 13%|█▎        | 228/1759 [00:04<00:33, 46.05it/s] 13%|█▎        | 233/1759 [00:04<00:33, 46.22it/s] 14%|█▎        | 238/1759 [00:05<00:32, 46.43it/s] 14%|█▍        | 243/1759 [00:05<00:32, 46.48it/s] 14%|█▍        | 248/1759 [00:05<00:32, 46.61it/s] 14%|█▍        | 253/1759 [00:05<00:32, 46.66it/s] 15%|█▍        | 258/1759 [00:05<00:32, 46.67it/s] 15%|█▍        | 263/1759 [00:05<00:32, 46.63it/s] 15%|█▌        | 268/1759 [00:05<00:31, 46.65it/s] 16%|█▌        | 273/1759 [00:05<00:31, 46.74it/s] 16%|█▌        | 278/1759 [00:05<00:31, 46.74it/s] 16%|█▌        | 283/1759 [00:06<00:31, 46.79it/s] 16%|█▋        | 288/1759 [00:06<00:31, 46.78it/s] 17%|█▋        | 293/1759 [00:06<00:31, 46.83it/s] 17%|█▋        | 298/1759 [00:06<00:31, 46.73it/s] 17%|█▋        | 303/1759 [00:06<00:31, 46.64it/s] 18%|█▊        | 308/1759 [00:06<00:31, 46.67it/s] 18%|█▊        | 313/1759 [00:06<00:30, 46.74it/s] 18%|█▊        | 318/1759 [00:06<00:30, 46.69it/s] 18%|█▊        | 323/1759 [00:06<00:30, 46.74it/s] 19%|█▊        | 328/1759 [00:06<00:30, 46.71it/s] 19%|█▉        | 333/1759 [00:07<00:30, 46.74it/s] 19%|█▉        | 338/1759 [00:07<00:30, 46.79it/s] 19%|█▉        | 343/1759 [00:07<00:30, 46.81it/s] 20%|█▉        | 348/1759 [00:07<00:30, 46.71it/s] 20%|██        | 353/1759 [00:07<00:30, 46.66it/s] 20%|██        | 358/1759 [00:07<00:30, 46.68it/s] 21%|██        | 363/1759 [00:07<00:29, 46.68it/s] 21%|██        | 368/1759 [00:07<00:29, 46.65it/s] 21%|██        | 373/1759 [00:07<00:29, 46.65it/s] 21%|██▏       | 378/1759 [00:08<00:29, 46.74it/s] 22%|██▏       | 383/1759 [00:08<00:29, 46.76it/s] 22%|██▏       | 388/1759 [00:08<00:29, 46.71it/s] 22%|██▏       | 393/1759 [00:08<00:29, 46.68it/s] 23%|██▎       | 398/1759 [00:08<00:29, 46.73it/s] 23%|██▎       | 403/1759 [00:08<00:29, 46.70it/s] 23%|██▎       | 408/1759 [00:08<00:28, 46.66it/s] 23%|██▎       | 413/1759 [00:08<00:28, 46.70it/s] 24%|██▍       | 418/1759 [00:08<00:29, 45.95it/s] 24%|██▍       | 423/1759 [00:09<00:28, 46.17it/s] 24%|██▍       | 428/1759 [00:09<00:28, 46.41it/s] 25%|██▍       | 433/1759 [00:09<00:28, 46.60it/s] 25%|██▍       | 438/1759 [00:09<00:28, 46.49it/s] 25%|██▌       | 443/1759 [00:09<00:28, 46.66it/s] 25%|██▌       | 448/1759 [00:09<00:28, 46.69it/s] 26%|██▌       | 453/1759 [00:09<00:28, 46.62it/s] 26%|██▌       | 458/1759 [00:09<00:27, 46.66it/s] 26%|██▋       | 463/1759 [00:09<00:27, 46.73it/s] 27%|██▋       | 468/1759 [00:09<00:27, 46.62it/s] 27%|██▋       | 473/1759 [00:10<00:27, 46.75it/s] 27%|██▋       | 478/1759 [00:10<00:27, 46.78it/s] 27%|██▋       | 483/1759 [00:10<00:27, 46.66it/s] 28%|██▊       | 488/1759 [00:10<00:27, 46.70it/s] 28%|██▊       | 493/1759 [00:10<00:27, 46.68it/s] 28%|██▊       | 498/1759 [00:10<00:27, 46.58it/s] 29%|██▊       | 503/1759 [00:10<00:26, 46.69it/s] 29%|██▉       | 508/1759 [00:10<00:26, 46.74it/s] 29%|██▉       | 513/1759 [00:10<00:26, 46.66it/s] 29%|██▉       | 518/1759 [00:11<00:26, 46.69it/s] 30%|██▉       | 523/1759 [00:11<00:26, 46.69it/s] 30%|███       | 528/1759 [00:11<00:26, 46.75it/s] 30%|███       | 533/1759 [00:11<00:26, 46.69it/s] 31%|███       | 538/1759 [00:11<00:26, 46.62it/s] 31%|███       | 543/1759 [00:11<00:26, 46.61it/s] 31%|███       | 548/1759 [00:11<00:25, 46.62it/s] 31%|███▏      | 553/1759 [00:11<00:25, 46.70it/s] 32%|███▏      | 558/1759 [00:11<00:25, 46.69it/s] 32%|███▏      | 563/1759 [00:12<00:25, 46.62it/s] 32%|███▏      | 568/1759 [00:12<00:25, 46.69it/s] 33%|███▎      | 573/1759 [00:12<00:25, 46.67it/s] 33%|███▎      | 578/1759 [00:12<00:25, 46.74it/s] 33%|███▎      | 583/1759 [00:12<00:25, 46.62it/s] 33%|███▎      | 588/1759 [00:12<00:25, 46.64it/s] 34%|███▎      | 593/1759 [00:12<00:24, 46.66it/s] 34%|███▍      | 598/1759 [00:12<00:24, 46.68it/s] 34%|███▍      | 603/1759 [00:12<00:24, 46.62it/s] 35%|███▍      | 608/1759 [00:12<00:24, 46.67it/s] 35%|███▍      | 613/1759 [00:13<00:24, 46.65it/s] 35%|███▌      | 618/1759 [00:13<00:24, 46.64it/s] 35%|███▌      | 623/1759 [00:13<00:24, 46.68it/s] 36%|███▌      | 628/1759 [00:13<00:24, 46.66it/s] 36%|███▌      | 633/1759 [00:13<00:24, 46.71it/s] 36%|███▋      | 638/1759 [00:13<00:24, 46.63it/s] 37%|███▋      | 643/1759 [00:13<00:23, 46.64it/s] 37%|███▋      | 648/1759 [00:13<00:23, 46.59it/s] 37%|███▋      | 653/1759 [00:13<00:23, 46.67it/s] 37%|███▋      | 658/1759 [00:14<00:23, 46.66it/s] 38%|███▊      | 663/1759 [00:14<00:23, 46.62it/s] 38%|███▊      | 668/1759 [00:14<00:23, 46.62it/s] 38%|███▊      | 673/1759 [00:14<00:23, 46.65it/s] 39%|███▊      | 678/1759 [00:14<00:23, 46.66it/s] 39%|███▉      | 683/1759 [00:14<00:23, 46.63it/s] 39%|███▉      | 688/1759 [00:14<00:22, 46.60it/s] 39%|███▉      | 693/1759 [00:14<00:22, 46.61it/s] 40%|███▉      | 698/1759 [00:14<00:22, 46.66it/s] 40%|███▉      | 703/1759 [00:15<00:22, 46.59it/s] 40%|████      | 708/1759 [00:15<00:22, 46.65it/s] 41%|████      | 713/1759 [00:15<00:22, 46.57it/s] 41%|████      | 718/1759 [00:15<00:22, 46.58it/s] 41%|████      | 723/1759 [00:15<00:22, 46.67it/s] 41%|████▏     | 728/1759 [00:15<00:22, 46.65it/s] 42%|████▏     | 733/1759 [00:15<00:21, 46.70it/s] 42%|████▏     | 738/1759 [00:15<00:21, 46.72it/s] 42%|████▏     | 743/1759 [00:15<00:21, 46.59it/s] 43%|████▎     | 748/1759 [00:15<00:21, 46.63it/s] 43%|████▎     | 753/1759 [00:16<00:21, 46.67it/s] 43%|████▎     | 758/1759 [00:16<00:21, 46.64it/s] 43%|████▎     | 763/1759 [00:16<00:21, 46.57it/s] 44%|████▎     | 768/1759 [00:16<00:21, 46.62it/s] 44%|████▍     | 773/1759 [00:16<00:21, 46.61it/s] 44%|████▍     | 778/1759 [00:16<00:21, 46.66it/s] 45%|████▍     | 783/1759 [00:16<00:20, 46.70it/s] 45%|████▍     | 788/1759 [00:16<00:20, 46.62it/s] 45%|████▌     | 793/1759 [00:16<00:20, 46.57it/s] 45%|████▌     | 798/1759 [00:17<00:20, 46.63it/s] 46%|████▌     | 803/1759 [00:17<00:20, 46.56it/s] 46%|████▌     | 808/1759 [00:17<00:20, 46.57it/s] 46%|████▌     | 813/1759 [00:17<00:20, 46.60it/s] 47%|████▋     | 818/1759 [00:17<00:20, 46.61it/s] 47%|████▋     | 823/1759 [00:17<00:20, 46.63it/s] 47%|████▋     | 828/1759 [00:17<00:19, 46.67it/s] 47%|████▋     | 833/1759 [00:17<00:19, 46.69it/s] 48%|████▊     | 838/1759 [00:17<00:19, 46.74it/s] 48%|████▊     | 843/1759 [00:18<00:19, 46.68it/s] 48%|████▊     | 848/1759 [00:18<00:19, 46.67it/s] 48%|████▊     | 853/1759 [00:18<00:19, 46.58it/s] 49%|████▉     | 858/1759 [00:18<00:19, 46.56it/s] 49%|████▉     | 863/1759 [00:18<00:19, 46.61it/s] 49%|████▉     | 868/1759 [00:18<00:19, 46.64it/s] 50%|████▉     | 873/1759 [00:18<00:19, 46.56it/s] 50%|████▉     | 878/1759 [00:18<00:18, 46.62it/s] 50%|█████     | 883/1759 [00:18<00:18, 46.58it/s] 50%|█████     | 888/1759 [00:19<00:18, 46.61it/s] 51%|█████     | 893/1759 [00:19<00:18, 46.67it/s] 51%|█████     | 898/1759 [00:19<00:18, 46.62it/s] 51%|█████▏    | 903/1759 [00:19<00:18, 46.59it/s] 52%|█████▏    | 908/1759 [00:19<00:18, 46.44it/s] 52%|█████▏    | 913/1759 [00:19<00:18, 46.53it/s] 52%|█████▏    | 918/1759 [00:19<00:18, 46.56it/s] 52%|█████▏    | 923/1759 [00:19<00:17, 46.58it/s] 53%|█████▎    | 928/1759 [00:19<00:17, 46.65it/s] 53%|█████▎    | 933/1759 [00:19<00:17, 46.62it/s] 53%|█████▎    | 938/1759 [00:20<00:17, 46.66it/s] 54%|█████▎    | 943/1759 [00:20<00:17, 46.62it/s] 54%|█████▍    | 948/1759 [00:20<00:17, 46.54it/s] 54%|█████▍    | 953/1759 [00:20<00:17, 46.48it/s] 54%|█████▍    | 958/1759 [00:20<00:17, 46.50it/s] 55%|█████▍    | 963/1759 [00:20<00:17, 46.56it/s] 55%|█████▌    | 968/1759 [00:20<00:16, 46.61it/s] 55%|█████▌    | 973/1759 [00:20<00:16, 46.63it/s] 56%|█████▌    | 978/1759 [00:20<00:16, 46.58it/s] 56%|█████▌    | 983/1759 [00:21<00:16, 46.63it/s] 56%|█████▌    | 988/1759 [00:21<00:16, 46.61it/s] 56%|█████▋    | 993/1759 [00:21<00:16, 46.54it/s] 57%|█████▋    | 998/1759 [00:21<00:16, 46.53it/s] 57%|█████▋    | 1003/1759 [00:21<00:16, 46.41it/s] 57%|█████▋    | 1008/1759 [00:21<00:16, 46.49it/s] 58%|█████▊    | 1013/1759 [00:21<00:16, 46.59it/s] 58%|█████▊    | 1018/1759 [00:21<00:15, 46.58it/s] 58%|█████▊    | 1023/1759 [00:21<00:15, 46.68it/s] 58%|█████▊    | 1028/1759 [00:22<00:15, 46.65it/s] 59%|█████▊    | 1033/1759 [00:22<00:15, 46.57it/s] 59%|█████▉    | 1038/1759 [00:22<00:15, 46.58it/s] 59%|█████▉    | 1043/1759 [00:22<00:15, 46.48it/s] 60%|█████▉    | 1048/1759 [00:22<00:15, 46.46it/s] 60%|█████▉    | 1053/1759 [00:22<00:15, 46.59it/s] 60%|██████    | 1058/1759 [00:22<00:15, 46.50it/s] 60%|██████    | 1063/1759 [00:22<00:14, 46.62it/s] 61%|██████    | 1068/1759 [00:22<00:14, 46.68it/s] 61%|██████    | 1073/1759 [00:22<00:14, 46.59it/s] 61%|██████▏   | 1078/1759 [00:23<00:14, 46.59it/s] 62%|██████▏   | 1083/1759 [00:23<00:14, 46.57it/s] 62%|██████▏   | 1088/1759 [00:23<00:14, 46.50it/s] 62%|██████▏   | 1093/1759 [00:23<00:14, 46.48it/s] 62%|██████▏   | 1098/1759 [00:23<00:14, 46.43it/s] 63%|██████▎   | 1103/1759 [00:23<00:14, 46.50it/s] 63%|██████▎   | 1108/1759 [00:23<00:13, 46.56it/s] 63%|██████▎   | 1113/1759 [00:23<00:13, 46.57it/s] 64%|██████▎   | 1118/1759 [00:23<00:13, 46.65it/s] 64%|██████▍   | 1123/1759 [00:24<00:13, 46.63it/s] 64%|██████▍   | 1128/1759 [00:24<00:13, 46.57it/s] 64%|██████▍   | 1133/1759 [00:24<00:13, 46.53it/s] 65%|██████▍   | 1138/1759 [00:24<00:13, 46.47it/s] 65%|██████▍   | 1143/1759 [00:24<00:13, 46.52it/s] 65%|██████▌   | 1148/1759 [00:24<00:13, 46.62it/s] 66%|██████▌   | 1153/1759 [00:24<00:12, 46.66it/s] 66%|██████▌   | 1158/1759 [00:24<00:12, 46.55it/s] 66%|██████▌   | 1163/1759 [00:24<00:12, 46.55it/s] 66%|██████▋   | 1168/1759 [00:25<00:12, 46.61it/s] 67%|██████▋   | 1173/1759 [00:25<00:12, 46.55it/s] 67%|██████▋   | 1178/1759 [00:25<00:12, 46.55it/s] 67%|██████▋   | 1183/1759 [00:25<00:12, 46.54it/s] 68%|██████▊   | 1188/1759 [00:25<00:12, 46.49it/s] 68%|██████▊   | 1193/1759 [00:25<00:12, 46.52it/s] 68%|██████▊   | 1198/1759 [00:25<00:12, 46.64it/s] 68%|██████▊   | 1203/1759 [00:25<00:11, 46.62it/s] 69%|██████▊   | 1208/1759 [00:25<00:11, 46.49it/s] 69%|██████▉   | 1213/1759 [00:25<00:11, 46.64it/s] 69%|██████▉   | 1218/1759 [00:26<00:11, 46.54it/s] 70%|██████▉   | 1223/1759 [00:26<00:11, 46.53it/s] 70%|██████▉   | 1228/1759 [00:26<00:11, 46.48it/s] 70%|███████   | 1233/1759 [00:26<00:11, 46.42it/s] 70%|███████   | 1238/1759 [00:26<00:11, 46.55it/s] 71%|███████   | 1243/1759 [00:26<00:11, 46.62it/s] 71%|███████   | 1248/1759 [00:26<00:10, 46.60it/s] 71%|███████   | 1253/1759 [00:26<00:10, 46.53it/s] 72%|███████▏  | 1258/1759 [00:26<00:10, 46.58it/s] 72%|███████▏  | 1263/1759 [00:27<00:10, 46.53it/s] 72%|███████▏  | 1268/1759 [00:27<00:10, 46.52it/s] 72%|███████▏  | 1273/1759 [00:27<00:10, 46.51it/s] 73%|███████▎  | 1278/1759 [00:27<00:10, 46.44it/s] 73%|███████▎  | 1283/1759 [00:27<00:10, 46.44it/s] 73%|███████▎  | 1288/1759 [00:27<00:10, 46.54it/s] 74%|███████▎  | 1293/1759 [00:27<00:10, 46.52it/s] 74%|███████▍  | 1298/1759 [00:27<00:09, 46.59it/s] 74%|███████▍  | 1303/1759 [00:27<00:09, 46.55it/s] 74%|███████▍  | 1308/1759 [00:28<00:09, 46.50it/s] 75%|███████▍  | 1313/1759 [00:28<00:09, 46.47it/s] 75%|███████▍  | 1318/1759 [00:28<00:09, 46.54it/s] 75%|███████▌  | 1323/1759 [00:28<00:09, 46.50it/s] 75%|███████▌  | 1328/1759 [00:28<00:09, 46.45it/s] 76%|███████▌  | 1333/1759 [00:28<00:09, 46.59it/s] 76%|███████▌  | 1338/1759 [00:28<00:09, 46.61it/s] 76%|███████▋  | 1343/1759 [00:28<00:08, 46.72it/s] 77%|███████▋  | 1348/1759 [00:28<00:08, 46.66it/s] 77%|███████▋  | 1353/1759 [00:28<00:08, 46.58it/s] 77%|███████▋  | 1358/1759 [00:29<00:08, 46.52it/s] 77%|███████▋  | 1363/1759 [00:29<00:08, 46.54it/s] 78%|███████▊  | 1368/1759 [00:29<00:08, 46.43it/s] 78%|███████▊  | 1373/1759 [00:29<00:08, 46.48it/s] 78%|███████▊  | 1378/1759 [00:29<00:08, 46.43it/s] 79%|███████▊  | 1383/1759 [00:29<00:08, 46.48it/s] 79%|███████▉  | 1388/1759 [00:29<00:07, 46.59it/s] 79%|███████▉  | 1393/1759 [00:29<00:07, 46.63it/s] 79%|███████▉  | 1398/1759 [00:29<00:07, 46.59it/s] 80%|███████▉  | 1403/1759 [00:30<00:07, 46.50it/s] 80%|████████  | 1408/1759 [00:30<00:07, 46.46it/s] 80%|████████  | 1413/1759 [00:30<00:07, 46.49it/s] 81%|████████  | 1418/1759 [00:30<00:07, 46.46it/s] 81%|████████  | 1423/1759 [00:30<00:07, 46.46it/s] 81%|████████  | 1428/1759 [00:30<00:07, 46.46it/s] 81%|████████▏ | 1433/1759 [00:30<00:07, 46.54it/s] 82%|████████▏ | 1438/1759 [00:30<00:06, 46.62it/s] 82%|████████▏ | 1443/1759 [00:30<00:06, 46.57it/s] 82%|████████▏ | 1448/1759 [00:31<00:06, 46.45it/s] 83%|████████▎ | 1453/1759 [00:31<00:06, 46.51it/s] 83%|████████▎ | 1458/1759 [00:31<00:06, 46.38it/s] 83%|████████▎ | 1463/1759 [00:31<00:06, 46.45it/s] 83%|████████▎ | 1468/1759 [00:31<00:06, 46.47it/s] 84%|████████▎ | 1473/1759 [00:31<00:06, 46.45it/s] 84%|████████▍ | 1478/1759 [00:31<00:06, 46.55it/s] 84%|████████▍ | 1483/1759 [00:31<00:05, 46.59it/s] 85%|████████▍ | 1488/1759 [00:31<00:05, 46.49it/s] 85%|████████▍ | 1493/1759 [00:31<00:05, 46.53it/s] 85%|████████▌ | 1498/1759 [00:32<00:05, 46.52it/s] 85%|████████▌ | 1503/1759 [00:32<00:05, 46.44it/s] 86%|████████▌ | 1508/1759 [00:32<00:05, 46.36it/s] 86%|████████▌ | 1513/1759 [00:32<00:05, 46.37it/s] 86%|████████▋ | 1518/1759 [00:32<00:05, 46.45it/s] 87%|████████▋ | 1523/1759 [00:32<00:05, 46.55it/s] 87%|████████▋ | 1528/1759 [00:32<00:04, 46.58it/s] 87%|████████▋ | 1533/1759 [00:32<00:04, 46.57it/s] 87%|████████▋ | 1538/1759 [00:32<00:04, 46.51it/s] 88%|████████▊ | 1543/1759 [00:33<00:04, 46.54it/s] 88%|████████▊ | 1548/1759 [00:33<00:04, 46.44it/s] 88%|████████▊ | 1553/1759 [00:33<00:04, 46.44it/s] 89%|████████▊ | 1558/1759 [00:33<00:04, 46.44it/s] 89%|████████▉ | 1563/1759 [00:33<00:04, 46.42it/s] 89%|████████▉ | 1568/1759 [00:33<00:04, 46.46it/s] 89%|████████▉ | 1573/1759 [00:33<00:03, 46.57it/s] 90%|████████▉ | 1578/1759 [00:33<00:03, 46.57it/s] 90%|████████▉ | 1583/1759 [00:33<00:03, 46.57it/s] 90%|█████████ | 1588/1759 [00:34<00:03, 46.47it/s] 91%|█████████ | 1593/1759 [00:34<00:03, 46.45it/s] 91%|█████████ | 1598/1759 [00:34<00:03, 46.50it/s] 91%|█████████ | 1603/1759 [00:34<00:03, 46.51it/s] 91%|█████████▏| 1608/1759 [00:34<00:03, 46.33it/s] 92%|█████████▏| 1613/1759 [00:34<00:03, 46.41it/s] 92%|█████████▏| 1618/1759 [00:34<00:03, 46.47it/s] 92%|█████████▏| 1623/1759 [00:34<00:02, 46.50it/s] 93%|█████████▎| 1628/1759 [00:34<00:02, 46.54it/s] 93%|█████████▎| 1633/1759 [00:35<00:02, 46.48it/s] 93%|█████████▎| 1638/1759 [00:35<00:02, 46.48it/s] 93%|█████████▎| 1643/1759 [00:35<00:02, 46.45it/s] 94%|█████████▎| 1648/1759 [00:35<00:02, 46.44it/s] 94%|█████████▍| 1653/1759 [00:35<00:02, 46.49it/s] 94%|█████████▍| 1658/1759 [00:35<00:02, 46.48it/s] 95%|█████████▍| 1663/1759 [00:35<00:02, 46.46it/s] 95%|█████████▍| 1668/1759 [00:35<00:01, 46.55it/s] 95%|█████████▌| 1673/1759 [00:35<00:01, 46.49it/s] 95%|█████████▌| 1678/1759 [00:35<00:01, 46.47it/s] 96%|█████████▌| 1683/1759 [00:36<00:01, 46.44it/s] 96%|█████████▌| 1688/1759 [00:36<00:01, 46.52it/s] 96%|█████████▌| 1693/1759 [00:36<00:01, 46.43it/s] 97%|█████████▋| 1698/1759 [00:36<00:01, 46.45it/s] 97%|█████████▋| 1703/1759 [00:36<00:01, 46.46it/s] 97%|█████████▋| 1708/1759 [00:36<00:01, 46.40it/s] 97%|█████████▋| 1713/1759 [00:36<00:00, 46.54it/s] 98%|█████████▊| 1718/1759 [00:36<00:00, 46.57it/s] 98%|█████████▊| 1723/1759 [00:36<00:00, 46.52it/s] 98%|█████████▊| 1728/1759 [00:37<00:00, 46.42it/s] 99%|█████████▊| 1733/1759 [00:37<00:00, 46.47it/s] 99%|█████████▉| 1738/1759 [00:37<00:00, 46.40it/s] 99%|█████████▉| 1743/1759 [00:37<00:00, 46.50it/s] 99%|█████████▉| 1748/1759 [00:37<00:00, 46.56it/s]100%|█████████▉| 1753/1759 [00:37<00:00, 46.49it/s]100%|█████████▉| 1758/1759 [00:37<00:00, 46.59it/s]100%|██████████| 1759/1759 [00:37<00:00, 46.63it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:38:52,253 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   eval_loss               =     0.8568
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   eval_runtime            = 0:00:37.74
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   eval_samples            =      14067
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   eval_samples_per_second =    372.696
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   eval_steps_per_second   =     46.604
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:38:52,253 >>   perplexity              =     2.3556
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:00,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:00,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:00,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:00,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:00,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:39:01,524 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:39:01,525 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:39:02,113 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:39:03,179 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:39:03,180 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:06,038 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:06,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:06,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:06,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:39:06,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:39:06,801 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:39:06,802 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:39:07,382 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:39:07,550 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:39:07,550 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'labels': ['country', 'part of', 'platform', 'publisher', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 22024
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22124, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:07,  1.44it/s]Extractor Predicting: 13it [00:08,  1.43it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:10,  1.40it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.41it/s]Extractor Predicting: 19it [00:12,  1.43it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.42it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:15,  1.41it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.48it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.51it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:20,  1.55it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:22,  1.55it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:23,  1.56it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:26,  1.55it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.58it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.59it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.61it/s]Extractor Predicting: 50it [00:33,  1.60it/s]Extractor Predicting: 51it [00:33,  1.60it/s]Extractor Predicting: 52it [00:34,  1.61it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.63it/s]Extractor Predicting: 55it [00:36,  1.62it/s]Extractor Predicting: 56it [00:36,  1.61it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:38,  1.58it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.58it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.55it/s]Extractor Predicting: 63it [00:41,  1.56it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.39it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:48,  1.47it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:50,  1.46it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.52it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:53,  1.54it/s]Extractor Predicting: 83it [00:54,  1.55it/s]Extractor Predicting: 84it [00:55,  1.60it/s]Extractor Predicting: 85it [00:55,  1.58it/s]Extractor Predicting: 86it [00:56,  1.54it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:57,  1.45it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.50it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:01,  1.50it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:03,  1.62it/s]Extractor Predicting: 98it [01:04,  1.66it/s]Extractor Predicting: 99it [01:04,  1.62it/s]Extractor Predicting: 100it [01:05,  1.70it/s]Extractor Predicting: 101it [01:06,  1.65it/s]Extractor Predicting: 102it [01:06,  1.64it/s]Extractor Predicting: 103it [01:07,  1.65it/s]Extractor Predicting: 104it [01:07,  1.65it/s]Extractor Predicting: 105it [01:08,  1.67it/s]Extractor Predicting: 106it [01:09,  1.65it/s]Extractor Predicting: 107it [01:09,  1.62it/s]Extractor Predicting: 108it [01:10,  1.61it/s]Extractor Predicting: 109it [01:11,  1.61it/s]Extractor Predicting: 110it [01:11,  1.61it/s]Extractor Predicting: 111it [01:12,  1.63it/s]Extractor Predicting: 112it [01:12,  1.65it/s]Extractor Predicting: 113it [01:13,  1.65it/s]Extractor Predicting: 114it [01:13,  1.68it/s]Extractor Predicting: 115it [01:14,  1.67it/s]Extractor Predicting: 116it [01:15,  1.70it/s]Extractor Predicting: 117it [01:15,  1.68it/s]Extractor Predicting: 118it [01:16,  1.68it/s]Extractor Predicting: 119it [01:16,  1.69it/s]Extractor Predicting: 120it [01:17,  1.71it/s]Extractor Predicting: 121it [01:18,  1.69it/s]Extractor Predicting: 122it [01:18,  1.64it/s]Extractor Predicting: 123it [01:19,  1.64it/s]Extractor Predicting: 124it [01:20,  1.62it/s]Extractor Predicting: 125it [01:20,  1.63it/s]Extractor Predicting: 126it [01:21,  1.64it/s]Extractor Predicting: 127it [01:21,  1.69it/s]Extractor Predicting: 128it [01:22,  1.69it/s]Extractor Predicting: 129it [01:22,  1.69it/s]Extractor Predicting: 130it [01:23,  1.68it/s]Extractor Predicting: 131it [01:24,  1.65it/s]Extractor Predicting: 132it [01:24,  1.69it/s]Extractor Predicting: 133it [01:25,  1.64it/s]Extractor Predicting: 134it [01:25,  1.67it/s]Extractor Predicting: 135it [01:26,  1.67it/s]Extractor Predicting: 136it [01:27,  1.65it/s]Extractor Predicting: 137it [01:27,  1.66it/s]Extractor Predicting: 138it [01:28,  1.63it/s]Extractor Predicting: 139it [01:29,  1.63it/s]Extractor Predicting: 140it [01:29,  1.61it/s]Extractor Predicting: 141it [01:30,  1.64it/s]Extractor Predicting: 142it [01:30,  1.66it/s]Extractor Predicting: 143it [01:31,  1.69it/s]Extractor Predicting: 144it [01:32,  1.69it/s]Extractor Predicting: 145it [01:32,  1.63it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:33,  1.62it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:35,  1.58it/s]Extractor Predicting: 150it [01:35,  1.59it/s]Extractor Predicting: 151it [01:36,  1.54it/s]Extractor Predicting: 152it [01:37,  1.49it/s]Extractor Predicting: 153it [01:37,  1.45it/s]Extractor Predicting: 154it [01:38,  1.46it/s]Extractor Predicting: 155it [01:39,  1.46it/s]Extractor Predicting: 156it [01:40,  1.45it/s]Extractor Predicting: 157it [01:40,  1.49it/s]Extractor Predicting: 158it [01:41,  1.48it/s]Extractor Predicting: 159it [01:41,  1.51it/s]Extractor Predicting: 160it [01:42,  1.54it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:43,  1.56it/s]Extractor Predicting: 163it [01:44,  1.54it/s]Extractor Predicting: 164it [01:45,  1.57it/s]Extractor Predicting: 165it [01:45,  1.55it/s]Extractor Predicting: 166it [01:46,  1.53it/s]Extractor Predicting: 167it [01:47,  1.56it/s]Extractor Predicting: 168it [01:47,  1.54it/s]Extractor Predicting: 169it [01:48,  1.56it/s]Extractor Predicting: 170it [01:49,  1.59it/s]Extractor Predicting: 171it [01:49,  1.61it/s]Extractor Predicting: 172it [01:50,  1.59it/s]Extractor Predicting: 173it [01:50,  1.62it/s]Extractor Predicting: 174it [01:51,  1.61it/s]Extractor Predicting: 175it [01:52,  1.57it/s]Extractor Predicting: 176it [01:52,  1.57it/s]Extractor Predicting: 177it [01:53,  1.53it/s]Extractor Predicting: 178it [01:54,  1.60it/s]Extractor Predicting: 179it [01:54,  1.71it/s]Extractor Predicting: 180it [01:55,  1.80it/s]Extractor Predicting: 181it [01:55,  1.78it/s]Extractor Predicting: 182it [01:56,  1.78it/s]Extractor Predicting: 183it [01:56,  1.66it/s]Extractor Predicting: 184it [01:57,  1.58it/s]Extractor Predicting: 185it [01:58,  1.59it/s]Extractor Predicting: 186it [01:58,  1.57it/s]Extractor Predicting: 187it [01:59,  1.56it/s]Extractor Predicting: 188it [02:00,  1.53it/s]Extractor Predicting: 189it [02:00,  1.54it/s]Extractor Predicting: 190it [02:01,  1.51it/s]Extractor Predicting: 191it [02:02,  1.50it/s]Extractor Predicting: 192it [02:02,  1.52it/s]Extractor Predicting: 193it [02:03,  1.59it/s]Extractor Predicting: 194it [02:03,  1.61it/s]Extractor Predicting: 195it [02:04,  1.62it/s]Extractor Predicting: 196it [02:05,  1.58it/s]Extractor Predicting: 197it [02:05,  1.57it/s]Extractor Predicting: 198it [02:06,  1.53it/s]Extractor Predicting: 199it [02:07,  1.53it/s]Extractor Predicting: 200it [02:07,  1.55it/s]Extractor Predicting: 201it [02:08,  1.56it/s]Extractor Predicting: 202it [02:09,  1.37it/s]Extractor Predicting: 203it [02:10,  1.42it/s]Extractor Predicting: 204it [02:10,  1.46it/s]Extractor Predicting: 205it [02:11,  1.51it/s]Extractor Predicting: 206it [02:11,  1.52it/s]Extractor Predicting: 207it [02:12,  1.52it/s]Extractor Predicting: 208it [02:13,  1.55it/s]Extractor Predicting: 209it [02:13,  1.55it/s]Extractor Predicting: 210it [02:14,  1.58it/s]Extractor Predicting: 211it [02:15,  1.55it/s]Extractor Predicting: 212it [02:15,  1.52it/s]Extractor Predicting: 213it [02:16,  1.50it/s]Extractor Predicting: 214it [02:17,  1.50it/s]Extractor Predicting: 215it [02:17,  1.53it/s]Extractor Predicting: 216it [02:18,  1.52it/s]Extractor Predicting: 217it [02:19,  1.57it/s]Extractor Predicting: 218it [02:19,  1.59it/s]Extractor Predicting: 219it [02:20,  1.58it/s]Extractor Predicting: 220it [02:20,  1.59it/s]Extractor Predicting: 221it [02:21,  1.58it/s]Extractor Predicting: 222it [02:22,  1.58it/s]Extractor Predicting: 223it [02:22,  1.56it/s]Extractor Predicting: 224it [02:23,  1.52it/s]Extractor Predicting: 225it [02:24,  1.52it/s]Extractor Predicting: 226it [02:24,  1.54it/s]Extractor Predicting: 227it [02:25,  1.55it/s]Extractor Predicting: 228it [02:26,  1.52it/s]Extractor Predicting: 229it [02:26,  1.52it/s]Extractor Predicting: 230it [02:27,  1.52it/s]Extractor Predicting: 231it [02:28,  1.50it/s]Extractor Predicting: 232it [02:28,  1.55it/s]Extractor Predicting: 233it [02:29,  1.51it/s]Extractor Predicting: 234it [02:30,  1.50it/s]Extractor Predicting: 235it [02:30,  1.52it/s]Extractor Predicting: 236it [02:31,  1.52it/s]Extractor Predicting: 237it [02:32,  1.52it/s]Extractor Predicting: 238it [02:32,  1.50it/s]Extractor Predicting: 239it [02:33,  1.52it/s]Extractor Predicting: 240it [02:34,  1.53it/s]Extractor Predicting: 241it [02:34,  1.54it/s]Extractor Predicting: 242it [02:35,  1.53it/s]Extractor Predicting: 243it [02:36,  1.54it/s]Extractor Predicting: 244it [02:36,  1.54it/s]Extractor Predicting: 245it [02:37,  1.50it/s]Extractor Predicting: 246it [02:38,  1.53it/s]Extractor Predicting: 247it [02:38,  1.56it/s]Extractor Predicting: 248it [02:39,  1.51it/s]Extractor Predicting: 249it [02:40,  1.51it/s]Extractor Predicting: 250it [02:40,  1.57it/s]Extractor Predicting: 251it [02:41,  1.59it/s]Extractor Predicting: 252it [02:41,  1.62it/s]Extractor Predicting: 253it [02:42,  1.61it/s]Extractor Predicting: 254it [02:43,  1.61it/s]Extractor Predicting: 255it [02:43,  1.58it/s]Extractor Predicting: 256it [02:44,  1.58it/s]Extractor Predicting: 257it [02:44,  1.58it/s]Extractor Predicting: 258it [02:45,  1.59it/s]Extractor Predicting: 259it [02:46,  1.58it/s]Extractor Predicting: 260it [02:46,  1.58it/s]Extractor Predicting: 261it [02:47,  1.61it/s]Extractor Predicting: 262it [02:48,  1.64it/s]Extractor Predicting: 263it [02:48,  1.64it/s]Extractor Predicting: 264it [02:49,  1.65it/s]Extractor Predicting: 265it [02:49,  1.64it/s]Extractor Predicting: 266it [02:50,  1.62it/s]Extractor Predicting: 267it [02:51,  1.61it/s]Extractor Predicting: 268it [02:51,  1.58it/s]Extractor Predicting: 269it [02:52,  1.60it/s]Extractor Predicting: 270it [02:53,  1.59it/s]Extractor Predicting: 271it [02:53,  1.60it/s]Extractor Predicting: 272it [02:54,  1.61it/s]Extractor Predicting: 273it [02:54,  1.60it/s]Extractor Predicting: 274it [02:55,  1.61it/s]Extractor Predicting: 275it [02:56,  1.59it/s]Extractor Predicting: 276it [02:56,  1.61it/s]Extractor Predicting: 277it [02:57,  1.61it/s]Extractor Predicting: 278it [02:58,  1.61it/s]Extractor Predicting: 279it [02:58,  1.58it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [02:59,  1.55it/s]Extractor Predicting: 282it [03:00,  1.54it/s]Extractor Predicting: 283it [03:01,  1.53it/s]Extractor Predicting: 284it [03:01,  1.58it/s]Extractor Predicting: 285it [03:02,  1.56it/s]Extractor Predicting: 286it [03:03,  1.54it/s]Extractor Predicting: 287it [03:03,  1.52it/s]Extractor Predicting: 288it [03:04,  1.54it/s]Extractor Predicting: 289it [03:05,  1.50it/s]Extractor Predicting: 290it [03:05,  1.51it/s]Extractor Predicting: 291it [03:06,  1.53it/s]Extractor Predicting: 292it [03:07,  1.51it/s]Extractor Predicting: 293it [03:07,  1.52it/s]Extractor Predicting: 294it [03:08,  1.53it/s]Extractor Predicting: 295it [03:09,  1.54it/s]Extractor Predicting: 296it [03:09,  1.55it/s]Extractor Predicting: 297it [03:10,  1.54it/s]Extractor Predicting: 298it [03:11,  1.57it/s]Extractor Predicting: 299it [03:11,  1.53it/s]Extractor Predicting: 300it [03:12,  1.52it/s]Extractor Predicting: 301it [03:13,  1.51it/s]Extractor Predicting: 302it [03:13,  1.53it/s]Extractor Predicting: 303it [03:14,  1.54it/s]Extractor Predicting: 304it [03:14,  1.56it/s]Extractor Predicting: 305it [03:15,  1.58it/s]Extractor Predicting: 306it [03:16,  1.55it/s]Extractor Predicting: 307it [03:16,  1.57it/s]Extractor Predicting: 308it [03:17,  1.58it/s]Extractor Predicting: 309it [03:18,  1.55it/s]Extractor Predicting: 310it [03:18,  1.50it/s]Extractor Predicting: 311it [03:19,  1.45it/s]Extractor Predicting: 312it [03:20,  1.43it/s]Extractor Predicting: 313it [03:21,  1.41it/s]Extractor Predicting: 314it [03:21,  1.38it/s]Extractor Predicting: 315it [03:22,  1.38it/s]Extractor Predicting: 316it [03:23,  1.37it/s]Extractor Predicting: 317it [03:24,  1.23it/s]Extractor Predicting: 318it [03:25,  1.27it/s]Extractor Predicting: 319it [03:25,  1.29it/s]Extractor Predicting: 320it [03:26,  1.31it/s]Extractor Predicting: 321it [03:27,  1.31it/s]Extractor Predicting: 322it [03:28,  1.32it/s]Extractor Predicting: 323it [03:28,  1.34it/s]Extractor Predicting: 324it [03:29,  1.35it/s]Extractor Predicting: 325it [03:30,  1.35it/s]Extractor Predicting: 326it [03:30,  1.37it/s]Extractor Predicting: 327it [03:31,  1.42it/s]Extractor Predicting: 328it [03:32,  1.44it/s]Extractor Predicting: 329it [03:32,  1.47it/s]Extractor Predicting: 330it [03:33,  1.53it/s]Extractor Predicting: 331it [03:34,  1.54it/s]Extractor Predicting: 332it [03:34,  1.57it/s]Extractor Predicting: 333it [03:35,  1.56it/s]Extractor Predicting: 334it [03:36,  1.54it/s]Extractor Predicting: 335it [03:36,  1.54it/s]Extractor Predicting: 336it [03:37,  1.50it/s]Extractor Predicting: 337it [03:37,  1.55it/s]Extractor Predicting: 338it [03:38,  1.56it/s]Extractor Predicting: 339it [03:39,  1.55it/s]Extractor Predicting: 340it [03:39,  1.53it/s]Extractor Predicting: 341it [03:40,  1.49it/s]Extractor Predicting: 342it [03:41,  1.49it/s]Extractor Predicting: 343it [03:42,  1.49it/s]Extractor Predicting: 344it [03:42,  1.46it/s]Extractor Predicting: 345it [03:43,  1.44it/s]Extractor Predicting: 346it [03:44,  1.43it/s]Extractor Predicting: 347it [03:44,  1.43it/s]Extractor Predicting: 348it [03:45,  1.44it/s]Extractor Predicting: 349it [03:46,  1.46it/s]Extractor Predicting: 350it [03:46,  1.44it/s]Extractor Predicting: 351it [03:47,  1.45it/s]Extractor Predicting: 352it [03:48,  1.46it/s]Extractor Predicting: 353it [03:48,  1.45it/s]Extractor Predicting: 354it [03:49,  1.44it/s]Extractor Predicting: 355it [03:50,  1.47it/s]Extractor Predicting: 356it [03:50,  1.48it/s]Extractor Predicting: 357it [03:51,  1.48it/s]Extractor Predicting: 358it [03:52,  1.47it/s]Extractor Predicting: 359it [03:53,  1.45it/s]Extractor Predicting: 360it [03:53,  1.47it/s]Extractor Predicting: 361it [03:54,  1.52it/s]Extractor Predicting: 362it [03:55,  1.51it/s]Extractor Predicting: 363it [03:55,  1.51it/s]Extractor Predicting: 364it [03:56,  1.49it/s]Extractor Predicting: 365it [03:56,  1.54it/s]Extractor Predicting: 366it [03:57,  1.52it/s]Extractor Predicting: 367it [03:58,  1.51it/s]Extractor Predicting: 368it [03:58,  1.50it/s]Extractor Predicting: 369it [03:59,  1.50it/s]Extractor Predicting: 370it [04:00,  1.53it/s]Extractor Predicting: 371it [04:00,  1.51it/s]Extractor Predicting: 372it [04:01,  1.50it/s]Extractor Predicting: 373it [04:02,  1.49it/s]Extractor Predicting: 374it [04:02,  1.51it/s]Extractor Predicting: 375it [04:03,  1.52it/s]Extractor Predicting: 376it [04:04,  1.55it/s]Extractor Predicting: 377it [04:04,  1.54it/s]Extractor Predicting: 378it [04:05,  1.54it/s]Extractor Predicting: 379it [04:06,  1.50it/s]Extractor Predicting: 380it [04:06,  1.50it/s]Extractor Predicting: 381it [04:07,  1.50it/s]Extractor Predicting: 382it [04:08,  1.50it/s]Extractor Predicting: 383it [04:08,  1.51it/s]Extractor Predicting: 384it [04:09,  1.52it/s]Extractor Predicting: 385it [04:10,  1.53it/s]Extractor Predicting: 386it [04:10,  1.54it/s]Extractor Predicting: 387it [04:11,  1.52it/s]Extractor Predicting: 388it [04:12,  1.51it/s]Extractor Predicting: 389it [04:12,  1.55it/s]Extractor Predicting: 390it [04:13,  1.55it/s]Extractor Predicting: 391it [04:14,  1.56it/s]Extractor Predicting: 392it [04:14,  1.58it/s]Extractor Predicting: 393it [04:15,  1.57it/s]Extractor Predicting: 394it [04:15,  1.57it/s]Extractor Predicting: 395it [04:16,  1.57it/s]Extractor Predicting: 396it [04:17,  1.55it/s]Extractor Predicting: 397it [04:17,  1.55it/s]Extractor Predicting: 398it [04:18,  1.58it/s]Extractor Predicting: 399it [04:19,  1.58it/s]Extractor Predicting: 400it [04:19,  1.54it/s]Extractor Predicting: 401it [04:20,  1.50it/s]Extractor Predicting: 402it [04:21,  1.48it/s]Extractor Predicting: 403it [04:21,  1.48it/s]Extractor Predicting: 404it [04:22,  1.52it/s]Extractor Predicting: 405it [04:23,  1.53it/s]Extractor Predicting: 406it [04:23,  1.53it/s]Extractor Predicting: 407it [04:24,  1.56it/s]Extractor Predicting: 408it [04:25,  1.60it/s]Extractor Predicting: 409it [04:25,  1.63it/s]Extractor Predicting: 410it [04:26,  1.65it/s]Extractor Predicting: 411it [04:26,  1.64it/s]Extractor Predicting: 412it [04:27,  1.66it/s]Extractor Predicting: 413it [04:27,  1.70it/s]Extractor Predicting: 414it [04:28,  1.71it/s]Extractor Predicting: 415it [04:29,  1.63it/s]Extractor Predicting: 416it [04:29,  1.56it/s]Extractor Predicting: 417it [04:30,  1.49it/s]Extractor Predicting: 418it [04:31,  1.46it/s]Extractor Predicting: 419it [04:32,  1.44it/s]Extractor Predicting: 420it [04:32,  1.44it/s]Extractor Predicting: 421it [04:33,  1.26it/s]Extractor Predicting: 422it [04:34,  1.48it/s]Extractor Predicting: 422it [04:34,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:51,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:51,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:51,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:51,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:51,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:43:52,056 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:43:52,057 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:43:52,614 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:43:53,679 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:43:53,680 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:56,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:56,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:56,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:56,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:43:56,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:43:57,154 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:43:57,155 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:43:57,826 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:43:57,993 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:43:57,993 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2853133769878391,
  "recall": 0.04336390132935238,
  "score": 0.07528540573896945,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13679
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13779, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.54it/s]Extractor Predicting: 65it [00:42,  1.58it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.56it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:45,  1.54it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.56it/s]Extractor Predicting: 78it [00:51,  1.57it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:53,  1.59it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.57it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:01,  1.58it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:02,  1.56it/s]Extractor Predicting: 97it [01:03,  1.59it/s]Extractor Predicting: 98it [01:04,  1.58it/s]Extractor Predicting: 99it [01:04,  1.59it/s]Extractor Predicting: 100it [01:05,  1.57it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:06,  1.51it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:07,  1.56it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.42it/s]Extractor Predicting: 108it [01:10,  1.48it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:11,  1.53it/s]Extractor Predicting: 111it [01:12,  1.54it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:14,  1.55it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:17,  1.58it/s]Extractor Predicting: 120it [01:18,  1.62it/s]Extractor Predicting: 121it [01:18,  1.61it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:20,  1.58it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:22,  1.57it/s]Extractor Predicting: 128it [01:23,  1.59it/s]Extractor Predicting: 129it [01:24,  1.59it/s]Extractor Predicting: 130it [01:24,  1.57it/s]Extractor Predicting: 131it [01:25,  1.54it/s]Extractor Predicting: 132it [01:26,  1.55it/s]Extractor Predicting: 133it [01:26,  1.56it/s]Extractor Predicting: 134it [01:27,  1.55it/s]Extractor Predicting: 135it [01:27,  1.57it/s]Extractor Predicting: 136it [01:28,  1.61it/s]Extractor Predicting: 137it [01:29,  1.61it/s]Extractor Predicting: 138it [01:29,  1.58it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:31,  1.61it/s]Extractor Predicting: 142it [01:32,  1.63it/s]Extractor Predicting: 143it [01:32,  1.67it/s]Extractor Predicting: 144it [01:33,  1.63it/s]Extractor Predicting: 145it [01:34,  1.64it/s]Extractor Predicting: 146it [01:34,  1.62it/s]Extractor Predicting: 147it [01:35,  1.62it/s]Extractor Predicting: 148it [01:35,  1.60it/s]Extractor Predicting: 149it [01:36,  1.62it/s]Extractor Predicting: 150it [01:37,  1.62it/s]Extractor Predicting: 151it [01:37,  1.62it/s]Extractor Predicting: 152it [01:38,  1.64it/s]Extractor Predicting: 153it [01:39,  1.62it/s]Extractor Predicting: 154it [01:39,  1.61it/s]Extractor Predicting: 155it [01:40,  1.61it/s]Extractor Predicting: 156it [01:40,  1.60it/s]Extractor Predicting: 157it [01:41,  1.59it/s]Extractor Predicting: 158it [01:42,  1.61it/s]Extractor Predicting: 159it [01:42,  1.63it/s]Extractor Predicting: 160it [01:43,  1.65it/s]Extractor Predicting: 161it [01:44,  1.58it/s]Extractor Predicting: 162it [01:44,  1.55it/s]Extractor Predicting: 163it [01:45,  1.56it/s]Extractor Predicting: 164it [01:45,  1.60it/s]Extractor Predicting: 165it [01:46,  1.60it/s]Extractor Predicting: 166it [01:47,  1.53it/s]Extractor Predicting: 167it [01:47,  1.50it/s]Extractor Predicting: 168it [01:48,  1.51it/s]Extractor Predicting: 169it [01:49,  1.51it/s]Extractor Predicting: 170it [01:50,  1.48it/s]Extractor Predicting: 170it [01:50,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:45:55,256 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:45:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:45:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:45:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:45:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:45:55,894 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:45:55,895 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:45:56,469 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:45:57,525 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:45:57,525 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:46:00,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:46:00,356 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:46:00,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:46:00,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:46:00,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:46:01,023 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:46:01,027 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:46:01,590 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:46:01,742 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:46:01,742 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3977832512315271,
  "recall": 0.0792638036809816,
  "score": 0.13218743605483937,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 3869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 3969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:06,  1.43it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:20,  1.51it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 33it [00:22,  1.48it/s]
[INFO|configuration_utils.py:515] 2023-08-28 15:46:24,814 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:46:24,815 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:46:24,821 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:46:24,822 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 15:46:24,825 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:46:28,393 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 15:46:28,401 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 15:46:28,415 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:46:28,415 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:46:28,421 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:46:28,435 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6303030303030303,
  "recall": 0.05528973950026582,
  "score": 0.10166177908113393,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 15:46:28,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:29,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:30,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:30,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:31,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:32,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:33,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:33,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:34,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:35,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:35,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:36,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:37,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:38,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:38,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:39,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:40,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:41,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:42,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:42,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:44,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:44,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:30, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-28 15:46:45,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:46,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:47,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:49,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:49,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:50,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:51,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:51,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:52,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:53,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:54,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:54,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:55,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:56,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:57,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:57,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:58,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:46:59,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:00,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:00,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:11, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 15:47:01,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:02,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:02,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:03,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:04,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:04,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:05,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:05,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:06,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:06,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:07,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:08,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:08,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:09,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:09,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:10,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:10,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:11,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:12,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:12,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:13,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:42, 14.62s/it][WARNING|generation_utils.py:914] 2023-08-28 15:47:14,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:14,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:15,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:16,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:16,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:17,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:18,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:19,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:19,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:20,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:21,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:22,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:22,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:23,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:24,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:24,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:25,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:26,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:27,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:28,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:00<01:29, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-28 15:47:29,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:29,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:30,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:31,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:32,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:32,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:33,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:34,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:34,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:35,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:36,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:36,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:37,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:38,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:39,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:39,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:40,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:41,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:41,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:42,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:43,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:15<01:14, 14.81s/it][WARNING|generation_utils.py:914] 2023-08-28 15:47:44,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:44,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:45,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:46,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:47,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:47,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:48,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:49,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:49,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:50,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:51,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:51,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:52,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:52,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:53,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:54,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:55,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:55,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:56,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:56,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:57,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:58,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:47:59,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:31<01:00, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-28 15:47:59,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:00,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:01,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:01,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:02,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:03,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:03,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:04,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:05,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:06,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:06,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:07,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:08,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:09,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:09,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:10,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:11,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:12,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:12,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:13,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:14,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:46<00:45, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-28 15:48:14,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:15,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:16,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:16,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:17,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:18,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:18,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:19,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:20,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:21,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:21,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:22,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:23,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:24,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:24,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:25,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:26,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:27,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:27,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:28,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:00<00:29, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 15:48:29,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:29,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:30,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:31,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:31,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:32,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:33,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:33,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:34,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:35,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:35,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:37,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:38,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:38,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:39,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:40,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:40,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:42,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:14<00:14, 14.49s/it][WARNING|generation_utils.py:914] 2023-08-28 15:48:42,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:43,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:44,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:45,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:45,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:46,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:47,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:48,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:49,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:49,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:50,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:51,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:52,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:52,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:53,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:54,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:55,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:55,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:56,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:57,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:58,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:48:58,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:30<00:00, 15.20s/it]Generating: 100%|██████████| 10/10 [02:30<00:00, 15.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:05,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:05,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:05,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:05,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:05,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:49:06,321 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:49:06,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:49:06,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:49:07,973 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:49:07,973 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:10,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:49:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:49:11,451 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:49:11,452 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:49:12,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:49:12,318 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:49:12,318 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : part of .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : sport .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.95625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 8634
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8734, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.48it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.56it/s]Extractor Estimating: 5it [00:03,  1.13it/s]Extractor Estimating: 6it [00:04,  1.26it/s]Extractor Estimating: 7it [00:05,  1.33it/s]Extractor Estimating: 8it [00:05,  1.44it/s]Extractor Estimating: 9it [00:06,  1.48it/s]Extractor Estimating: 10it [00:07,  1.50it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:08,  1.58it/s]Extractor Estimating: 13it [00:09,  1.49it/s]Extractor Estimating: 14it [00:09,  1.51it/s]Extractor Estimating: 15it [00:10,  1.54it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:12,  1.55it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.51it/s]Extractor Estimating: 21it [00:14,  1.51it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:15,  1.60it/s]Extractor Estimating: 24it [00:16,  1.55it/s]Extractor Estimating: 25it [00:16,  1.62it/s]Extractor Estimating: 26it [00:17,  1.62it/s]Extractor Estimating: 27it [00:17,  1.62it/s]Extractor Estimating: 28it [00:18,  1.61it/s]Extractor Estimating: 29it [00:19,  1.65it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:21,  1.59it/s]Extractor Estimating: 33it [00:21,  1.59it/s]Extractor Estimating: 34it [00:22,  1.63it/s]Extractor Estimating: 35it [00:22,  1.65it/s]Extractor Estimating: 36it [00:23,  1.66it/s]Extractor Estimating: 37it [00:24,  1.62it/s]Extractor Estimating: 38it [00:24,  1.58it/s]Extractor Estimating: 39it [00:25,  1.55it/s]Extractor Estimating: 40it [00:26,  1.57it/s]Extractor Estimating: 41it [00:26,  1.58it/s]Extractor Estimating: 42it [00:27,  1.57it/s]Extractor Estimating: 43it [00:27,  1.58it/s]Extractor Estimating: 44it [00:28,  1.57it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:30,  1.60it/s]Extractor Estimating: 48it [00:31,  1.58it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:32,  1.54it/s]Extractor Estimating: 51it [00:33,  1.60it/s]Extractor Estimating: 52it [00:33,  1.71it/s]Extractor Estimating: 53it [00:34,  1.75it/s]Extractor Estimating: 54it [00:34,  1.70it/s]Extractor Estimating: 55it [00:35,  1.76it/s]Extractor Estimating: 56it [00:35,  1.86it/s]Extractor Estimating: 57it [00:36,  2.00it/s]Extractor Estimating: 58it [00:36,  2.04it/s]Extractor Estimating: 59it [00:37,  2.01it/s]Extractor Estimating: 60it [00:37,  1.90it/s]Extractor Estimating: 61it [00:38,  1.97it/s]Extractor Estimating: 62it [00:38,  2.07it/s]Extractor Estimating: 63it [00:39,  2.01it/s]Extractor Estimating: 64it [00:39,  2.06it/s]Extractor Estimating: 65it [00:40,  2.07it/s]Extractor Estimating: 66it [00:40,  2.03it/s]Extractor Estimating: 67it [00:41,  2.02it/s]Extractor Estimating: 68it [00:41,  2.03it/s]Extractor Estimating: 69it [00:41,  2.07it/s]Extractor Estimating: 70it [00:42,  2.04it/s]Extractor Estimating: 71it [00:43,  1.86it/s]Extractor Estimating: 72it [00:43,  1.90it/s]Extractor Estimating: 73it [00:44,  1.91it/s]Extractor Estimating: 74it [00:44,  1.93it/s]Extractor Estimating: 75it [00:45,  1.91it/s]Extractor Estimating: 76it [00:45,  1.80it/s]Extractor Estimating: 77it [00:46,  1.70it/s]Extractor Estimating: 78it [00:47,  1.64it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:48,  1.61it/s]Extractor Estimating: 81it [00:49,  1.61it/s]Extractor Estimating: 82it [00:49,  1.55it/s]Extractor Estimating: 83it [00:50,  1.55it/s]Extractor Estimating: 84it [00:51,  1.49it/s]Extractor Estimating: 85it [00:51,  1.47it/s]Extractor Estimating: 86it [00:52,  1.43it/s]Extractor Estimating: 87it [00:53,  1.42it/s]Extractor Estimating: 88it [00:54,  1.32it/s]Extractor Estimating: 89it [00:54,  1.37it/s]Extractor Estimating: 90it [00:55,  1.43it/s]Extractor Estimating: 91it [00:56,  1.43it/s]Extractor Estimating: 92it [00:56,  1.46it/s]Extractor Estimating: 93it [00:57,  1.49it/s]Extractor Estimating: 94it [00:58,  1.48it/s]Extractor Estimating: 95it [00:58,  1.50it/s]Extractor Estimating: 96it [00:59,  1.48it/s]Extractor Estimating: 97it [01:00,  1.47it/s]Extractor Estimating: 98it [01:00,  1.48it/s]Extractor Estimating: 99it [01:01,  1.48it/s]Extractor Estimating: 100it [01:02,  1.48it/s]Extractor Estimating: 101it [01:02,  1.56it/s]Extractor Estimating: 102it [01:03,  1.58it/s]Extractor Estimating: 103it [01:03,  1.63it/s]Extractor Estimating: 104it [01:04,  1.66it/s]Extractor Estimating: 105it [01:05,  1.70it/s]Extractor Estimating: 106it [01:05,  1.71it/s]Extractor Estimating: 107it [01:06,  1.72it/s]Extractor Estimating: 108it [01:06,  1.72it/s]Extractor Estimating: 109it [01:07,  1.71it/s]Extractor Estimating: 110it [01:07,  1.76it/s]Extractor Estimating: 111it [01:08,  1.72it/s]Extractor Estimating: 112it [01:09,  1.74it/s]Extractor Estimating: 113it [01:09,  1.75it/s]Extractor Estimating: 114it [01:10,  1.73it/s]Extractor Estimating: 115it [01:10,  1.69it/s]Extractor Estimating: 116it [01:11,  1.74it/s]Extractor Estimating: 117it [01:11,  1.76it/s]Extractor Estimating: 118it [01:12,  1.73it/s]Extractor Estimating: 119it [01:13,  1.70it/s]Extractor Estimating: 120it [01:13,  1.69it/s]Extractor Estimating: 121it [01:14,  1.73it/s]Extractor Estimating: 122it [01:14,  1.74it/s]Extractor Estimating: 123it [01:15,  1.74it/s]Extractor Estimating: 124it [01:16,  1.64it/s]Extractor Estimating: 125it [01:16,  1.68it/s]Extractor Estimating: 126it [01:17,  1.67it/s]Extractor Estimating: 127it [01:18,  1.22it/s]Extractor Estimating: 128it [01:19,  1.35it/s]Extractor Estimating: 129it [01:19,  1.43it/s]Extractor Estimating: 130it [01:20,  1.47it/s]Extractor Estimating: 131it [01:20,  1.57it/s]Extractor Estimating: 132it [01:21,  1.65it/s]Extractor Estimating: 133it [01:22,  1.64it/s]Extractor Estimating: 134it [01:22,  1.67it/s]Extractor Estimating: 135it [01:23,  1.71it/s]Extractor Estimating: 136it [01:23,  1.74it/s]Extractor Estimating: 137it [01:24,  1.72it/s]Extractor Estimating: 138it [01:24,  1.75it/s]Extractor Estimating: 139it [01:25,  1.78it/s]Extractor Estimating: 140it [01:26,  1.80it/s]Extractor Estimating: 141it [01:26,  1.78it/s]Extractor Estimating: 142it [01:27,  1.79it/s]Extractor Estimating: 143it [01:27,  1.73it/s]Extractor Estimating: 144it [01:28,  1.76it/s]Extractor Estimating: 145it [01:28,  1.78it/s]Extractor Estimating: 146it [01:29,  1.78it/s]Extractor Estimating: 147it [01:30,  1.79it/s]Extractor Estimating: 148it [01:30,  1.81it/s]Extractor Estimating: 149it [01:31,  1.71it/s]Extractor Estimating: 150it [01:31,  1.75it/s]Extractor Estimating: 151it [01:32,  1.77it/s]Extractor Estimating: 152it [01:32,  1.72it/s]Extractor Estimating: 153it [01:33,  1.73it/s]Extractor Estimating: 154it [01:34,  1.72it/s]Extractor Estimating: 155it [01:34,  1.71it/s]Extractor Estimating: 156it [01:35,  1.65it/s]Extractor Estimating: 157it [01:35,  1.66it/s]Extractor Estimating: 158it [01:36,  1.63it/s]Extractor Estimating: 159it [01:37,  1.65it/s]Extractor Estimating: 160it [01:37,  1.52it/s]Extractor Estimating: 161it [01:38,  1.56it/s]Extractor Estimating: 162it [01:39,  1.54it/s]Extractor Estimating: 163it [01:39,  1.52it/s]Extractor Estimating: 164it [01:40,  1.59it/s]Extractor Estimating: 165it [01:41,  1.60it/s]Extractor Estimating: 166it [01:41,  1.61it/s]Extractor Estimating: 167it [01:42,  1.64it/s]Extractor Estimating: 168it [01:42,  1.62it/s]Extractor Estimating: 169it [01:43,  1.64it/s]Extractor Estimating: 170it [01:44,  1.62it/s]Extractor Estimating: 171it [01:44,  1.67it/s]Extractor Estimating: 172it [01:45,  1.73it/s]Extractor Estimating: 173it [01:45,  1.67it/s]Extractor Estimating: 174it [01:46,  1.72it/s]Extractor Estimating: 175it [01:47,  1.67it/s]Extractor Estimating: 176it [01:47,  1.63it/s]Extractor Estimating: 177it [01:48,  1.60it/s]Extractor Estimating: 178it [01:48,  1.61it/s]Extractor Estimating: 179it [01:49,  1.56it/s]Extractor Estimating: 180it [01:50,  1.54it/s]Extractor Estimating: 181it [01:50,  1.56it/s]Extractor Estimating: 182it [01:51,  1.57it/s]Extractor Estimating: 183it [01:52,  1.55it/s]Extractor Estimating: 184it [01:52,  1.53it/s]Extractor Estimating: 185it [01:53,  1.57it/s]Extractor Estimating: 186it [01:54,  1.55it/s]Extractor Estimating: 187it [01:54,  1.60it/s]Extractor Estimating: 188it [01:55,  1.55it/s]Extractor Estimating: 189it [01:56,  1.56it/s]Extractor Estimating: 190it [01:56,  1.55it/s]Extractor Estimating: 191it [01:57,  1.59it/s]Extractor Estimating: 192it [01:57,  1.54it/s]Extractor Estimating: 193it [01:58,  1.53it/s]Extractor Estimating: 194it [01:59,  1.56it/s]Extractor Estimating: 195it [01:59,  1.53it/s]Extractor Estimating: 196it [02:00,  1.58it/s]Extractor Estimating: 197it [02:01,  1.55it/s]Extractor Estimating: 198it [02:01,  1.60it/s]Extractor Estimating: 199it [02:02,  1.60it/s]Extractor Estimating: 200it [02:03,  1.61it/s]Extractor Estimating: 201it [02:03,  1.65it/s]Extractor Estimating: 202it [02:04,  1.64it/s]Extractor Estimating: 203it [02:04,  1.66it/s]Extractor Estimating: 204it [02:05,  1.62it/s]Extractor Estimating: 205it [02:06,  1.64it/s]Extractor Estimating: 206it [02:06,  1.62it/s]Extractor Estimating: 207it [02:07,  1.61it/s]Extractor Estimating: 208it [02:07,  1.62it/s]Extractor Estimating: 209it [02:08,  1.62it/s]Extractor Estimating: 210it [02:09,  1.59it/s]Extractor Estimating: 211it [02:09,  1.64it/s]Extractor Estimating: 212it [02:10,  1.67it/s]Extractor Estimating: 213it [02:10,  1.63it/s]Extractor Estimating: 214it [02:11,  1.60it/s]Extractor Estimating: 215it [02:12,  1.63it/s]Extractor Estimating: 216it [02:12,  1.59it/s]Extractor Estimating: 217it [02:13,  1.62it/s]Extractor Estimating: 218it [02:14,  1.60it/s]Extractor Estimating: 219it [02:14,  1.59it/s]Extractor Estimating: 220it [02:15,  1.60it/s]Extractor Estimating: 221it [02:16,  1.56it/s]Extractor Estimating: 222it [02:16,  1.55it/s]Extractor Estimating: 223it [02:17,  1.57it/s]Extractor Estimating: 224it [02:17,  1.60it/s]Extractor Estimating: 225it [02:18,  1.45it/s]Extractor Estimating: 226it [02:19,  1.50it/s]Extractor Estimating: 227it [02:20,  1.48it/s]Extractor Estimating: 228it [02:20,  1.49it/s]Extractor Estimating: 229it [02:21,  1.49it/s]Extractor Estimating: 230it [02:22,  1.51it/s]Extractor Estimating: 231it [02:22,  1.44it/s]Extractor Estimating: 232it [02:23,  1.47it/s]Extractor Estimating: 233it [02:24,  1.49it/s]Extractor Estimating: 234it [02:24,  1.51it/s]Extractor Estimating: 235it [02:25,  1.51it/s]Extractor Estimating: 236it [02:26,  1.53it/s]Extractor Estimating: 237it [02:26,  1.55it/s]Extractor Estimating: 238it [02:27,  1.51it/s]Extractor Estimating: 239it [02:28,  1.51it/s]Extractor Estimating: 240it [02:28,  1.51it/s]Extractor Estimating: 241it [02:29,  1.57it/s]Extractor Estimating: 242it [02:29,  1.53it/s]Extractor Estimating: 243it [02:30,  1.58it/s]Extractor Estimating: 244it [02:31,  1.54it/s]Extractor Estimating: 245it [02:31,  1.56it/s]Extractor Estimating: 246it [02:32,  1.55it/s]Extractor Estimating: 247it [02:33,  1.56it/s]Extractor Estimating: 248it [02:33,  1.57it/s]Extractor Estimating: 249it [02:34,  1.59it/s]Extractor Estimating: 250it [02:35,  1.58it/s]Extractor Estimating: 250it [02:35,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:02,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:02,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:02,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:02,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:02,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:52:02,612 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:52:02,613 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:52:02,879 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:52:03,923 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:52:03,923 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:05,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:05,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:05,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:05,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:52:05,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:52:05,989 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:52:05,990 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:52:06,252 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:52:06,403 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:52:06,403 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:58:12,735 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:58:12,771 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4991 mean pseudo reward: 0.9369876750672738
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl'}
train vocab size: 26142
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26242, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26242, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.066, loss:679.9859
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.090, loss:590.4094
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.060, loss:521.7506
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.068, loss:555.0508
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.057, loss:478.6332
>> valid entity prec:0.4728, rec:0.3908, f1:0.4279
>> valid relation prec:0.1394, rec:0.0407, f1:0.0630
>> valid relation with NER prec:0.1394, rec:0.0407, f1:0.0630
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 5.006, loss:497.4140
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.060, loss:464.1827
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.067, loss:480.7194
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.073, loss:449.8415
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.055, loss:468.4247
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4652, rec:0.3671, f1:0.4104
>> valid relation prec:0.1014, rec:0.0257, f1:0.0410
>> valid relation with NER prec:0.1014, rec:0.0257, f1:0.0410
g_step 1100, step 60, avg_time 4.969, loss:439.5700
g_step 1200, step 160, avg_time 1.077, loss:460.8670
g_step 1300, step 52, avg_time 1.065, loss:428.8327
g_step 1400, step 152, avg_time 1.058, loss:420.0746
g_step 1500, step 44, avg_time 1.067, loss:439.1122
>> valid entity prec:0.4629, rec:0.3211, f1:0.3792
>> valid relation prec:0.1304, rec:0.0312, f1:0.0503
>> valid relation with NER prec:0.1304, rec:0.0312, f1:0.0503
g_step 1600, step 144, avg_time 4.974, loss:405.5983
g_step 1700, step 36, avg_time 1.061, loss:401.7575
g_step 1800, step 136, avg_time 1.071, loss:386.7281
g_step 1900, step 28, avg_time 1.064, loss:393.8278
g_step 2000, step 128, avg_time 1.107, loss:358.9177
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4704, rec:0.3589, f1:0.4072
>> valid relation prec:0.1046, rec:0.0257, f1:0.0413
>> valid relation with NER prec:0.1046, rec:0.0257, f1:0.0413
g_step 2100, step 20, avg_time 5.016, loss:380.2862
g_step 2200, step 120, avg_time 1.051, loss:342.5560
g_step 2300, step 12, avg_time 1.067, loss:373.4265
g_step 2400, step 112, avg_time 1.073, loss:337.6480
g_step 2500, step 4, avg_time 1.054, loss:342.5112
>> valid entity prec:0.4536, rec:0.3217, f1:0.3764
>> valid relation prec:0.1391, rec:0.0355, f1:0.0566
>> valid relation with NER prec:0.1391, rec:0.0355, f1:0.0566
g_step 2600, step 104, avg_time 4.978, loss:320.3877
g_step 2700, step 204, avg_time 1.051, loss:349.7328
g_step 2800, step 96, avg_time 1.063, loss:309.3653
g_step 2900, step 196, avg_time 1.060, loss:335.2508
g_step 3000, step 88, avg_time 1.050, loss:290.6098
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4693, rec:0.3071, f1:0.3712
>> valid relation prec:0.1057, rec:0.0271, f1:0.0432
>> valid relation with NER prec:0.1057, rec:0.0271, f1:0.0432
g_step 3100, step 188, avg_time 4.956, loss:311.2736
g_step 3200, step 80, avg_time 1.047, loss:278.2302
g_step 3300, step 180, avg_time 1.076, loss:294.9871
g_step 3400, step 72, avg_time 1.057, loss:299.5175
g_step 3500, step 172, avg_time 1.072, loss:283.7339
>> valid entity prec:0.4845, rec:0.3152, f1:0.3819
>> valid relation prec:0.1007, rec:0.0219, f1:0.0360
>> valid relation with NER prec:0.1007, rec:0.0219, f1:0.0360
g_step 3600, step 64, avg_time 4.958, loss:267.9678
g_step 3700, step 164, avg_time 1.075, loss:272.7006
g_step 3800, step 56, avg_time 1.055, loss:258.5344
g_step 3900, step 156, avg_time 1.067, loss:270.1204
g_step 4000, step 48, avg_time 1.068, loss:254.4483
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4790, rec:0.3720, f1:0.4188
>> valid relation prec:0.1313, rec:0.0380, f1:0.0589
>> valid relation with NER prec:0.1313, rec:0.0380, f1:0.0589
g_step 4100, step 148, avg_time 4.967, loss:259.0107
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:58:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:58:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-58-12_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:58:13 - WARNING - datasets.builder -   Using custom data configuration default-7ec98165e08a6775
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7ec98165e08a6775/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.32 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:58:16,415 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:58:16,416 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:58:16,425 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:58:16,426 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:58:16,436 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,443 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,443 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,444 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,444 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,444 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:58:16,444 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:58:16,579 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:58:19,685 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:58:19,685 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7ec98165e08a6775/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.08ba/s] 40%|████      | 2/5 [00:00<00:00,  3.97ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.39ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.62ba/s]100%|██████████| 5/5 [00:01<00:00,  4.75ba/s]100%|██████████| 5/5 [00:01<00:00,  4.45ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:04,  3.15ba/s] 13%|█▎        | 2/15 [00:00<00:04,  2.93ba/s] 20%|██        | 3/15 [00:00<00:03,  3.46ba/s] 27%|██▋       | 4/15 [00:01<00:02,  3.86ba/s] 33%|███▎      | 5/15 [00:01<00:02,  4.12ba/s] 40%|████      | 6/15 [00:01<00:02,  4.29ba/s] 47%|████▋     | 7/15 [00:01<00:01,  4.42ba/s] 53%|█████▎    | 8/15 [00:01<00:01,  4.51ba/s] 60%|██████    | 9/15 [00:02<00:01,  4.56ba/s] 67%|██████▋   | 10/15 [00:02<00:01,  4.60ba/s] 73%|███████▎  | 11/15 [00:02<00:00,  4.63ba/s] 80%|████████  | 12/15 [00:02<00:00,  4.63ba/s] 87%|████████▋ | 13/15 [00:03<00:00,  4.66ba/s] 93%|█████████▎| 14/15 [00:03<00:00,  4.69ba/s]100%|██████████| 15/15 [00:03<00:00,  4.59ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.23ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.82ba/s]100%|██████████| 5/5 [00:00<00:00,  9.59ba/s]100%|██████████| 5/5 [00:00<00:00,  9.16ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:02,  5.63ba/s] 20%|██        | 3/15 [00:00<00:01,  8.53ba/s] 33%|███▎      | 5/15 [00:00<00:01,  9.33ba/s] 47%|████▋     | 7/15 [00:00<00:00,  9.72ba/s] 60%|██████    | 9/15 [00:00<00:00, 10.00ba/s] 73%|███████▎  | 11/15 [00:01<00:00, 10.17ba/s] 87%|████████▋ | 13/15 [00:01<00:00, 10.16ba/s]100%|██████████| 15/15 [00:01<00:00, 12.04ba/s]100%|██████████| 15/15 [00:01<00:00, 10.42ba/s]
[INFO|trainer.py:414] 2023-08-28 17:58:27,129 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:58:27,144 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:58:27,144 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 17:58:27,144 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:58:27,144 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:58:27,144 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:58:27,144 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:58:27,144 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<03:41,  1.76it/s]  1%|          | 2/390 [00:00<02:36,  2.47it/s]  1%|          | 3/390 [00:01<02:15,  2.85it/s]  1%|          | 4/390 [00:01<02:05,  3.07it/s]  1%|▏         | 5/390 [00:01<02:00,  3.21it/s]  2%|▏         | 6/390 [00:02<01:56,  3.29it/s]  2%|▏         | 7/390 [00:02<01:54,  3.35it/s]  2%|▏         | 8/390 [00:02<01:52,  3.39it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:03<01:50,  3.44it/s]  3%|▎         | 11/390 [00:03<01:49,  3.45it/s]  3%|▎         | 12/390 [00:03<01:49,  3.46it/s]  3%|▎         | 13/390 [00:04<01:48,  3.47it/s]  4%|▎         | 14/390 [00:04<01:48,  3.47it/s]  4%|▍         | 15/390 [00:04<01:48,  3.47it/s]  4%|▍         | 16/390 [00:04<01:47,  3.48it/s]  4%|▍         | 17/390 [00:05<01:47,  3.47it/s]  5%|▍         | 18/390 [00:05<01:46,  3.48it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:06<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:46,  3.48it/s]  6%|▌         | 22/390 [00:06<01:45,  3.48it/s]  6%|▌         | 23/390 [00:06<01:45,  3.48it/s]  6%|▌         | 24/390 [00:07<01:45,  3.48it/s]  6%|▋         | 25/390 [00:07<01:45,  3.47it/s]  7%|▋         | 26/390 [00:07<01:44,  3.47it/s]  7%|▋         | 27/390 [00:08<01:44,  3.48it/s]  7%|▋         | 28/390 [00:08<01:44,  3.47it/s]  7%|▋         | 29/390 [00:08<01:43,  3.47it/s]  8%|▊         | 30/390 [00:08<01:43,  3.47it/s]  8%|▊         | 31/390 [00:09<01:43,  3.47it/s]  8%|▊         | 32/390 [00:09<01:43,  3.47it/s]  8%|▊         | 33/390 [00:09<01:42,  3.47it/s]  9%|▊         | 34/390 [00:10<01:42,  3.47it/s]  9%|▉         | 35/390 [00:10<01:42,  3.47it/s]  9%|▉         | 36/390 [00:10<01:42,  3.47it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:11<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:40,  3.47it/s] 11%|█         | 41/390 [00:12<01:40,  3.47it/s] 11%|█         | 42/390 [00:12<01:40,  3.47it/s] 11%|█         | 43/390 [00:12<01:40,  3.47it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.47it/s] 12%|█▏        | 48/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 52/390 [00:15<01:37,  3.47it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.47it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.46it/s] 14%|█▍        | 55/390 [00:16<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.47it/s] 15%|█▌        | 59/390 [00:17<01:35,  3.47it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.47it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.46it/s] 16%|█▌        | 62/390 [00:18<01:34,  3.46it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.47it/s] 17%|█▋        | 65/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.47it/s] 18%|█▊        | 69/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 72/390 [00:21<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.47it/s] 19%|█▉        | 76/390 [00:22<01:30,  3.47it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.46it/s] 20%|██        | 78/390 [00:22<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 17:58:50,263 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:50,263 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 17:58:50,263 >>   Batch size = 8

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.88it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.52it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.73it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.13it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.82it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.57it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.42it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 47.10it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.93it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.79it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.85it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.90it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.75it/s][A
  4%|▍         | 73/1759 [00:01<00:35, 46.90it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.95it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.93it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.82it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.74it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.64it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.67it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.66it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.78it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.73it/s][A
  7%|▋         | 123/1759 [00:02<00:34, 46.84it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.88it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.91it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.81it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.77it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.68it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.62it/s][A
  9%|▉         | 158/1759 [00:03<00:53, 29.69it/s][A
  9%|▉         | 163/1759 [00:03<00:47, 33.37it/s][A
 10%|▉         | 168/1759 [00:03<00:43, 36.59it/s][A
 10%|▉         | 173/1759 [00:03<00:40, 39.19it/s][A
 10%|█         | 178/1759 [00:03<00:38, 41.27it/s][A
 10%|█         | 183/1759 [00:04<00:36, 42.88it/s][A
 11%|█         | 188/1759 [00:04<00:35, 44.03it/s][A
 11%|█         | 193/1759 [00:04<00:34, 44.88it/s][A
 11%|█▏        | 198/1759 [00:04<00:34, 45.24it/s][A
 12%|█▏        | 203/1759 [00:04<00:34, 45.64it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 45.81it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.06it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.40it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.52it/s][A
 13%|█▎        | 228/1759 [00:05<00:32, 46.69it/s][A
 13%|█▎        | 233/1759 [00:05<00:32, 46.81it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.78it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.70it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.59it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.63it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.70it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.63it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.61it/s][A
 16%|█▌        | 273/1759 [00:06<00:31, 46.80it/s][A
 16%|█▌        | 278/1759 [00:06<00:31, 46.89it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.91it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.71it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.65it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.67it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.73it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.75it/s][A
 18%|█▊        | 313/1759 [00:06<00:30, 46.82it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.71it/s][A
 18%|█▊        | 323/1759 [00:07<00:30, 46.68it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.83it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.85it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.69it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.61it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.68it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.70it/s][A
 20%|██        | 358/1759 [00:07<00:29, 46.80it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.79it/s][A
 21%|██        | 368/1759 [00:08<00:29, 46.65it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.75it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.79it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.67it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.72it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.71it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.64it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.73it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.65it/s][A
 23%|██▎       | 413/1759 [00:09<00:28, 46.73it/s][A
 24%|██▍       | 418/1759 [00:09<00:28, 46.62it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.62it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.62it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.50it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.61it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.59it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.65it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.68it/s][A
 26%|██▋       | 463/1759 [00:10<00:27, 46.57it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.68it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.56it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.57it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.57it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.52it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.63it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.62it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.60it/s][A
 29%|██▉       | 508/1759 [00:11<00:26, 46.63it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.67it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.60it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.47it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.52it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.56it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.61it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.58it/s][A
 31%|███       | 548/1759 [00:11<00:25, 46.58it/s][A
 31%|███▏      | 553/1759 [00:12<00:25, 46.66it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.60it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.62it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.56it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.52it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.53it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.58it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.62it/s][A
 34%|███▍      | 603/1759 [00:13<00:24, 46.63it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.51it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.42it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.56it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.45it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.57it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.46it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.47it/s][A
 37%|███▋      | 643/1759 [00:13<00:23, 46.54it/s][A
 37%|███▋      | 648/1759 [00:14<00:23, 46.59it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.60it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.56it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.56it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.62it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.59it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.42it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.53it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.52it/s][A
 39%|███▉      | 693/1759 [00:15<00:22, 46.55it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.54it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.37it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.48it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.51it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.52it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.49it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.41it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.45it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.56it/s][A
 42%|████▏     | 743/1759 [00:16<00:21, 46.57it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.56it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.61it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.61it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.67it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.65it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.60it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.44it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.52it/s][A
 45%|████▍     | 788/1759 [00:17<00:20, 46.55it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.54it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.52it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.61it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.50it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.54it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.56it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.52it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.47it/s][A
 47%|████▋     | 833/1759 [00:18<00:19, 46.50it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.51it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.56it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.54it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.54it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.58it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.47it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.54it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.57it/s][A
 50%|████▉     | 878/1759 [00:19<00:18, 46.56it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.53it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.50it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.52it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.50it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.51it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.55it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.54it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.44it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.56it/s][A
 53%|█████▎    | 928/1759 [00:20<00:17, 46.48it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.54it/s][A
 53%|█████▎    | 938/1759 [00:20<00:23, 34.33it/s][A
 54%|█████▎    | 943/1759 [00:20<00:21, 37.39it/s][A
 54%|█████▍    | 948/1759 [00:20<00:20, 39.76it/s][A
 54%|█████▍    | 953/1759 [00:20<00:19, 41.63it/s][A
 54%|█████▍    | 958/1759 [00:20<00:18, 43.00it/s][A
 55%|█████▍    | 963/1759 [00:20<00:18, 44.07it/s][A
 55%|█████▌    | 968/1759 [00:21<00:17, 44.83it/s][A
 55%|█████▌    | 973/1759 [00:21<00:17, 45.30it/s][A
 56%|█████▌    | 978/1759 [00:21<00:17, 45.65it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 45.69it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 45.91it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.10it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.32it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.36it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.36it/s][A
 58%|█████▊    | 1013/1759 [00:22<00:16, 46.52it/s][A
 58%|█████▊    | 1018/1759 [00:22<00:15, 46.60it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.48it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.32it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.28it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:23, 30.32it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:21, 33.91it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:19, 36.90it/s][A
 60%|█████▉    | 1053/1759 [00:23<00:17, 39.35it/s][A
 60%|██████    | 1058/1759 [00:23<00:16, 41.35it/s][A
 60%|██████    | 1063/1759 [00:23<00:16, 42.81it/s][A
 61%|██████    | 1068/1759 [00:23<00:15, 43.94it/s][A
 61%|██████    | 1073/1759 [00:23<00:15, 44.68it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:15, 45.04it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 45.46it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 45.65it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 45.95it/s][A
 62%|██████▏   | 1098/1759 [00:24<00:15, 43.49it/s][A
 63%|██████▎   | 1103/1759 [00:24<00:14, 44.31it/s][A
 63%|██████▎   | 1108/1759 [00:24<00:14, 45.01it/s][A
 63%|██████▎   | 1113/1759 [00:24<00:14, 45.54it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 45.92it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.01it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.28it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.34it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.22it/s][A
 65%|██████▍   | 1143/1759 [00:25<00:13, 46.21it/s][A
 65%|██████▌   | 1148/1759 [00:25<00:13, 46.29it/s][A
 66%|██████▌   | 1153/1759 [00:25<00:13, 46.34it/s][A
 66%|██████▌   | 1158/1759 [00:25<00:12, 46.40it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.50it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.60it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.50it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.63it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.51it/s][A
 68%|██████▊   | 1188/1759 [00:26<00:12, 46.48it/s][A
 68%|██████▊   | 1193/1759 [00:26<00:12, 46.46it/s][A
 68%|██████▊   | 1198/1759 [00:26<00:12, 46.40it/s][A
 68%|██████▊   | 1203/1759 [00:26<00:11, 46.50it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:11, 46.49it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.49it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.49it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.61it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.61it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.52it/s][A
 70%|███████   | 1238/1759 [00:27<00:11, 46.49it/s][A
 71%|███████   | 1243/1759 [00:27<00:11, 46.47it/s][A
 71%|███████   | 1248/1759 [00:27<00:11, 46.45it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.52it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.54it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.45it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.57it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.58it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.58it/s][A
 73%|███████▎  | 1283/1759 [00:28<00:10, 46.60it/s][A
 73%|███████▎  | 1288/1759 [00:28<00:10, 46.45it/s][A
 74%|███████▎  | 1293/1759 [00:28<00:10, 46.45it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:09, 46.46it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.44it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.56it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.57it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.60it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.63it/s][A
 75%|███████▌  | 1328/1759 [00:29<00:09, 46.52it/s][A
 76%|███████▌  | 1333/1759 [00:29<00:09, 44.54it/s][A
 76%|███████▌  | 1338/1759 [00:29<00:09, 45.10it/s][A
 76%|███████▋  | 1343/1759 [00:29<00:09, 45.54it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 45.81it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.13it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.34it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.32it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.45it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.18it/s][A
 78%|███████▊  | 1378/1759 [00:30<00:08, 46.31it/s][A
 79%|███████▊  | 1383/1759 [00:30<00:08, 46.36it/s][A
 79%|███████▉  | 1388/1759 [00:30<00:08, 46.28it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.49it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.56it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.61it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.53it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.53it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.35it/s][A
 81%|████████  | 1423/1759 [00:31<00:07, 46.39it/s][A
 81%|████████  | 1428/1759 [00:31<00:07, 46.44it/s][A
 81%|████████▏ | 1433/1759 [00:31<00:07, 46.48it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.48it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.46it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.58it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.58it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.61it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.59it/s][A
 83%|████████▎ | 1468/1759 [00:32<00:06, 46.45it/s][A
 84%|████████▎ | 1473/1759 [00:32<00:06, 46.36it/s][A
 84%|████████▍ | 1478/1759 [00:32<00:06, 46.43it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.51it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.40it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.51it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.60it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.65it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.62it/s][A
 86%|████████▌ | 1513/1759 [00:33<00:05, 46.59it/s][A
 86%|████████▋ | 1518/1759 [00:33<00:05, 46.45it/s][A
 87%|████████▋ | 1523/1759 [00:33<00:05, 46.39it/s][A
 87%|████████▋ | 1528/1759 [00:33<00:04, 46.37it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.44it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.46it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.55it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.58it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.59it/s][A
 89%|████████▉ | 1563/1759 [00:34<00:04, 46.55it/s][A
 89%|████████▉ | 1568/1759 [00:34<00:04, 46.50it/s][A
 89%|████████▉ | 1573/1759 [00:34<00:04, 43.03it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:04, 43.94it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 44.80it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 45.26it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 45.71it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.04it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.25it/s][A
 91%|█████████▏| 1608/1759 [00:35<00:03, 46.33it/s][A
 92%|█████████▏| 1613/1759 [00:35<00:03, 46.26it/s][A
 92%|█████████▏| 1618/1759 [00:35<00:03, 46.21it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.36it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.34it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.37it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.54it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.61it/s][A
 94%|█████████▍| 1653/1759 [00:36<00:02, 46.62it/s][A
 94%|█████████▍| 1658/1759 [00:36<00:02, 46.49it/s][A
 95%|█████████▍| 1663/1759 [00:36<00:02, 46.37it/s][A
 95%|█████████▍| 1668/1759 [00:36<00:01, 46.39it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.44it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.34it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.50it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.59it/s][A
 97%|█████████▋| 1698/1759 [00:37<00:01, 46.60it/s][A
 97%|█████████▋| 1703/1759 [00:37<00:01, 46.55it/s][A
 97%|█████████▋| 1708/1759 [00:37<00:01, 46.47it/s][A
 97%|█████████▋| 1713/1759 [00:37<00:00, 46.34it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.43it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.48it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.54it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.39it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.47it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.51it/s][A
 99%|█████████▉| 1748/1759 [00:38<00:00, 46.55it/s][A
100%|█████████▉| 1753/1759 [00:38<00:00, 46.50it/s][A
100%|█████████▉| 1758/1759 [00:38<00:00, 46.49it/s][A                                                
                                                   [A 20%|██        | 78/390 [01:01<01:30,  3.46it/s]
100%|██████████| 1759/1759 [00:38<00:00, 46.49it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 17:59:28,690 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 17:59:28,763 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:59:37,061 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:59:37,103 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:59:37,121 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:21<1:32:52, 17.92s/it] 21%|██        | 80/390 [01:22<1:05:21, 12.65s/it] 21%|██        | 81/390 [01:22<46:02,  8.94s/it]   21%|██        | 82/390 [01:22<32:34,  6.35s/it] 21%|██▏       | 83/390 [01:23<23:10,  4.53s/it] 22%|██▏       | 84/390 [01:23<16:36,  3.26s/it] 22%|██▏       | 85/390 [01:23<12:01,  2.37s/it] 22%|██▏       | 86/390 [01:23<08:49,  1.74s/it] 22%|██▏       | 87/390 [01:24<06:35,  1.31s/it] 23%|██▎       | 88/390 [01:24<05:02,  1.00s/it] 23%|██▎       | 89/390 [01:24<03:56,  1.27it/s] 23%|██▎       | 90/390 [01:25<03:10,  1.57it/s] 23%|██▎       | 91/390 [01:25<02:39,  1.88it/s] 24%|██▎       | 92/390 [01:25<02:16,  2.18it/s] 24%|██▍       | 93/390 [01:25<02:01,  2.45it/s] 24%|██▍       | 94/390 [01:26<01:50,  2.69it/s] 24%|██▍       | 95/390 [01:26<01:42,  2.88it/s] 25%|██▍       | 96/390 [01:26<01:36,  3.04it/s] 25%|██▍       | 97/390 [01:27<01:32,  3.16it/s] 25%|██▌       | 98/390 [01:27<01:30,  3.24it/s] 25%|██▌       | 99/390 [01:27<01:28,  3.31it/s] 26%|██▌       | 100/390 [01:27<01:26,  3.35it/s] 26%|██▌       | 101/390 [01:28<01:25,  3.39it/s] 26%|██▌       | 102/390 [01:28<01:24,  3.41it/s] 26%|██▋       | 103/390 [01:28<01:23,  3.43it/s] 27%|██▋       | 104/390 [01:29<01:23,  3.44it/s] 27%|██▋       | 105/390 [01:29<01:22,  3.45it/s] 27%|██▋       | 106/390 [01:29<01:22,  3.46it/s] 27%|██▋       | 107/390 [01:29<01:21,  3.46it/s] 28%|██▊       | 108/390 [01:30<01:21,  3.46it/s] 28%|██▊       | 109/390 [01:30<01:21,  3.47it/s] 28%|██▊       | 110/390 [01:30<01:20,  3.47it/s] 28%|██▊       | 111/390 [01:31<01:20,  3.47it/s] 29%|██▊       | 112/390 [01:31<01:23,  3.32it/s] 29%|██▉       | 113/390 [01:31<01:22,  3.36it/s] 29%|██▉       | 114/390 [01:32<01:21,  3.39it/s] 29%|██▉       | 115/390 [01:32<01:20,  3.41it/s] 30%|██▉       | 116/390 [01:32<01:19,  3.43it/s] 30%|███       | 117/390 [01:32<01:19,  3.44it/s] 30%|███       | 118/390 [01:33<01:18,  3.45it/s] 31%|███       | 119/390 [01:33<01:18,  3.45it/s] 31%|███       | 120/390 [01:33<01:18,  3.46it/s] 31%|███       | 121/390 [01:34<01:17,  3.46it/s] 31%|███▏      | 122/390 [01:34<01:17,  3.46it/s] 32%|███▏      | 123/390 [01:34<01:18,  3.40it/s] 32%|███▏      | 124/390 [01:34<01:17,  3.42it/s] 32%|███▏      | 125/390 [01:35<01:17,  3.43it/s] 32%|███▏      | 126/390 [01:35<01:16,  3.44it/s] 33%|███▎      | 127/390 [01:35<01:16,  3.45it/s] 33%|███▎      | 128/390 [01:36<01:15,  3.45it/s] 33%|███▎      | 129/390 [01:36<01:15,  3.45it/s] 33%|███▎      | 130/390 [01:36<01:15,  3.46it/s] 34%|███▎      | 131/390 [01:36<01:14,  3.46it/s] 34%|███▍      | 132/390 [01:37<01:14,  3.46it/s] 34%|███▍      | 133/390 [01:37<01:14,  3.46it/s] 34%|███▍      | 134/390 [01:37<01:20,  3.19it/s] 35%|███▍      | 135/390 [01:38<01:18,  3.26it/s] 35%|███▍      | 136/390 [01:38<01:16,  3.32it/s] 35%|███▌      | 137/390 [01:38<01:15,  3.36it/s] 35%|███▌      | 138/390 [01:39<01:14,  3.39it/s] 36%|███▌      | 139/390 [01:39<01:13,  3.41it/s] 36%|███▌      | 140/390 [01:39<01:12,  3.43it/s] 36%|███▌      | 141/390 [01:39<01:12,  3.44it/s] 36%|███▋      | 142/390 [01:40<01:11,  3.45it/s] 37%|███▋      | 143/390 [01:40<01:11,  3.45it/s] 37%|███▋      | 144/390 [01:40<01:11,  3.45it/s] 37%|███▋      | 145/390 [01:41<01:21,  3.01it/s] 37%|███▋      | 146/390 [01:41<01:17,  3.13it/s] 38%|███▊      | 147/390 [01:41<01:15,  3.22it/s] 38%|███▊      | 148/390 [01:42<01:13,  3.29it/s] 38%|███▊      | 149/390 [01:42<01:12,  3.34it/s] 38%|███▊      | 150/390 [01:42<01:11,  3.38it/s] 39%|███▊      | 151/390 [01:42<01:10,  3.40it/s] 39%|███▉      | 152/390 [01:43<01:09,  3.42it/s] 39%|███▉      | 153/390 [01:43<01:15,  3.13it/s] 39%|███▉      | 154/390 [01:43<01:13,  3.21it/s] 40%|███▉      | 155/390 [01:44<01:16,  3.07it/s] 40%|████      | 156/390 [01:44<01:13,  3.17it/s][INFO|trainer.py:2140] 2023-08-28 18:00:11,734 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:00:11,734 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 18:00:11,734 >>   Batch size = 8
{'eval_loss': 0.8789819478988647, 'eval_runtime': 38.3438, 'eval_samples_per_second': 366.865, 'eval_steps_per_second': 45.874, 'epoch': 0.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.31it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.42it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.70it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.12it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.73it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.46it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.34it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.89it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.78it/s][A
  3%|▎         | 53/1759 [00:01<00:48, 35.19it/s][A
  3%|▎         | 58/1759 [00:01<00:44, 38.43it/s][A
  4%|▎         | 63/1759 [00:01<00:41, 40.62it/s][A
  4%|▍         | 68/1759 [00:01<00:39, 42.40it/s][A
  4%|▍         | 73/1759 [00:01<00:38, 43.61it/s][A
  4%|▍         | 78/1759 [00:01<00:37, 44.52it/s][A
  5%|▍         | 83/1759 [00:01<00:37, 45.28it/s][A
  5%|▌         | 88/1759 [00:01<00:36, 45.82it/s][A
  5%|▌         | 93/1759 [00:02<00:36, 46.16it/s][A
  6%|▌         | 98/1759 [00:02<00:36, 45.79it/s][A
  6%|▌         | 103/1759 [00:02<00:36, 45.93it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.26it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.36it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.54it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.51it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.63it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.69it/s][A
  8%|▊         | 138/1759 [00:03<00:34, 46.78it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.79it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.74it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.73it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.65it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.75it/s][A
 10%|▉         | 168/1759 [00:03<00:33, 46.82it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.70it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.79it/s][A
 10%|█         | 183/1759 [00:04<00:33, 46.69it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.73it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.75it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.74it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.72it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.78it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.76it/s][A
 12%|█▏        | 218/1759 [00:04<00:32, 46.72it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.69it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.73it/s][A
 13%|█▎        | 233/1759 [00:05<00:32, 46.60it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.64it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.58it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.63it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.55it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.62it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.56it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.59it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.60it/s][A
 16%|█▌        | 278/1759 [00:06<00:31, 46.50it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.51it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.59it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.58it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.58it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.61it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.59it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.58it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.66it/s][A
 18%|█▊        | 323/1759 [00:07<00:30, 46.60it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.58it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.52it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.54it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.60it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.57it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.54it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.61it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.52it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.52it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.54it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.59it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.60it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.53it/s][A
 22%|██▏       | 393/1759 [00:08<00:30, 45.40it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 45.79it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.10it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.26it/s][A
 23%|██▎       | 413/1759 [00:08<00:29, 46.33it/s][A
 24%|██▍       | 418/1759 [00:09<00:28, 46.45it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.36it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.45it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.46it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.30it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.47it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.56it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.62it/s][A
 26%|██▋       | 463/1759 [00:10<00:27, 46.61it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.45it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.56it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.55it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.39it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.50it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.46it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.57it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.61it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.57it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.60it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.45it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.46it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.50it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.49it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.58it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.62it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.48it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.55it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.54it/s][A
 32%|███▏      | 563/1759 [00:12<00:28, 42.45it/s][A
 32%|███▏      | 568/1759 [00:12<00:27, 43.58it/s][A
 33%|███▎      | 573/1759 [00:12<00:26, 44.48it/s][A
 33%|███▎      | 578/1759 [00:12<00:26, 45.10it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 45.62it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 45.96it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.17it/s][A
 34%|███▍      | 598/1759 [00:12<00:25, 46.38it/s][A
 34%|███▍      | 603/1759 [00:13<00:25, 46.05it/s][A
 35%|███▍      | 608/1759 [00:13<00:25, 46.00it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.10it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.26it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.39it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.45it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.50it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.59it/s][A
 37%|███▋      | 643/1759 [00:13<00:23, 46.63it/s][A
 37%|███▋      | 648/1759 [00:14<00:23, 46.50it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.41it/s][A
 37%|███▋      | 658/1759 [00:14<00:28, 39.12it/s][A
 38%|███▊      | 663/1759 [00:14<00:26, 41.06it/s][A
 38%|███▊      | 668/1759 [00:14<00:25, 42.63it/s][A
 38%|███▊      | 673/1759 [00:14<00:24, 43.77it/s][A
 39%|███▊      | 678/1759 [00:14<00:24, 44.62it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 45.24it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 45.71it/s][A
 39%|███▉      | 693/1759 [00:15<00:23, 46.05it/s][A
 40%|███▉      | 698/1759 [00:15<00:23, 45.81it/s][A
 40%|███▉      | 703/1759 [00:15<00:23, 45.73it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.00it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.24it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.37it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.56it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.56it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.57it/s][A
 42%|████▏     | 738/1759 [00:16<00:21, 46.56it/s][A
 42%|████▏     | 743/1759 [00:16<00:21, 46.42it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.33it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.38it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.25it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.47it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.58it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.62it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.60it/s][A
 45%|████▍     | 783/1759 [00:17<00:20, 46.60it/s][A
 45%|████▍     | 788/1759 [00:17<00:20, 46.47it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.09it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.13it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.15it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.29it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.47it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.52it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.58it/s][A
 47%|████▋     | 828/1759 [00:17<00:19, 46.59it/s][A
 47%|████▋     | 833/1759 [00:18<00:19, 46.45it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.44it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.43it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.40it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.52it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.54it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.65it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.51it/s][A
 50%|████▉     | 878/1759 [00:19<00:18, 46.46it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.49it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 893/1759 [00:19<00:21, 40.13it/s][A
 51%|█████     | 898/1759 [00:19<00:20, 41.93it/s][A
 51%|█████▏    | 903/1759 [00:19<00:19, 43.27it/s][A
 52%|█████▏    | 908/1759 [00:19<00:19, 44.26it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 44.90it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 45.46it/s][A
 52%|█████▏    | 923/1759 [00:20<00:18, 45.78it/s][A
 53%|█████▎    | 928/1759 [00:20<00:18, 46.08it/s][A
 53%|█████▎    | 933/1759 [00:20<00:18, 45.70it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 45.79it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 45.99it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.23it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.38it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.50it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.49it/s][A
 55%|█████▌    | 968/1759 [00:21<00:17, 46.45it/s][A
 55%|█████▌    | 973/1759 [00:21<00:16, 46.54it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.31it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.44it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.37it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.49it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.58it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.57it/s][A
 58%|█████▊    | 1013/1759 [00:22<00:15, 46.63it/s][A
 58%|█████▊    | 1018/1759 [00:22<00:15, 46.45it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.38it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.37it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.35it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.41it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.48it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.56it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.59it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.53it/s][A
 60%|██████    | 1063/1759 [00:23<00:14, 46.54it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.44it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.41it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.36it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.40it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.49it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.58it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.60it/s][A
 63%|██████▎   | 1108/1759 [00:24<00:13, 46.58it/s][A
 63%|██████▎   | 1113/1759 [00:24<00:13, 46.44it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.35it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.33it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:17, 36.80it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:15, 39.26it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:15, 41.27it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:14, 42.78it/s][A
 65%|██████▌   | 1148/1759 [00:25<00:13, 43.85it/s][A
 66%|██████▌   | 1153/1759 [00:25<00:13, 44.69it/s][A
 66%|██████▌   | 1158/1759 [00:25<00:13, 45.10it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:13, 45.66it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 45.48it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 45.67it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 45.92it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.06it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.33it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.46it/s][A
 68%|██████▊   | 1198/1759 [00:26<00:12, 46.51it/s][A
 68%|██████▊   | 1203/1759 [00:26<00:11, 46.58it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:11, 46.48it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.22it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.14it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.19it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.28it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.36it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.49it/s][A
 71%|███████   | 1243/1759 [00:27<00:11, 46.54it/s][A
 71%|███████   | 1248/1759 [00:27<00:10, 46.55it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.65it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.39it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.33it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.30it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.33it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.43it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.42it/s][A
 73%|███████▎  | 1288/1759 [00:28<00:10, 46.50it/s][A
 74%|███████▎  | 1293/1759 [00:28<00:10, 46.54it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:09, 46.55it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.56it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.37it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.38it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.35it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.35it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.47it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.48it/s][A
 76%|███████▌  | 1338/1759 [00:29<00:09, 46.43it/s][A
 76%|███████▋  | 1343/1759 [00:29<00:08, 46.45it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.52it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.49it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.47it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.36it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.52it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.43it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.42it/s][A
 79%|███████▊  | 1383/1759 [00:30<00:08, 46.54it/s][A
 79%|███████▉  | 1388/1759 [00:30<00:07, 46.52it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.44it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.48it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.49it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.46it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.39it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.42it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.55it/s][A
 81%|████████  | 1428/1759 [00:31<00:07, 46.54it/s][A
 81%|████████▏ | 1433/1759 [00:31<00:07, 46.52it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.44it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.46it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.49it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.42it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.46it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.47it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.42it/s][A
 84%|████████▎ | 1473/1759 [00:32<00:06, 46.53it/s][A
 84%|████████▍ | 1478/1759 [00:32<00:06, 46.52it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.39it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.38it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.40it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.46it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.46it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.47it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.39it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.40it/s][A
 87%|████████▋ | 1523/1759 [00:33<00:05, 46.48it/s][A
 87%|████████▋ | 1528/1759 [00:33<00:04, 46.52it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.38it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.47it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.46it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.45it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.50it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.49it/s][A
 89%|████████▉ | 1568/1759 [00:34<00:04, 46.39it/s][A
 89%|████████▉ | 1573/1759 [00:34<00:04, 46.39it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.49it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.38it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.47it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.46it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.47it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.51it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.51it/s][A
 92%|█████████▏| 1613/1759 [00:35<00:03, 46.46it/s][A
 92%|█████████▏| 1618/1759 [00:35<00:03, 46.34it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.43it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.44it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.41it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.43it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.46it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.46it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.51it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.51it/s][A
 95%|█████████▍| 1663/1759 [00:36<00:02, 46.48it/s][A
 95%|█████████▍| 1668/1759 [00:36<00:01, 46.38it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.44it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.38it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.50it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.45it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.50it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.53it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.51it/s][A
 97%|█████████▋| 1708/1759 [00:37<00:01, 46.48it/s][A
 97%|█████████▋| 1713/1759 [00:37<00:00, 46.50it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.40it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.42it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.41it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.45it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.45it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.51it/s][A
100%|█████████▉| 1753/1759 [00:38<00:00, 46.47it/s][A
100%|█████████▉| 1758/1759 [00:38<00:00, 46.45it/s][A                                                 
                                                   [A 40%|████      | 156/390 [02:22<01:13,  3.17it/s]
100%|██████████| 1759/1759 [00:38<00:00, 46.45it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:00:49,941 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 18:00:49,967 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:56,395 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:56,426 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:56,448 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:39<1:05:00, 16.74s/it] 41%|████      | 158/390 [02:39<45:39, 11.81s/it]   41%|████      | 159/390 [02:40<32:09,  8.35s/it] 41%|████      | 160/390 [02:40<22:44,  5.93s/it] 41%|████▏     | 161/390 [02:40<16:10,  4.24s/it] 42%|████▏     | 162/390 [02:41<11:36,  3.05s/it] 42%|████▏     | 163/390 [02:41<08:24,  2.22s/it] 42%|████▏     | 164/390 [02:41<06:11,  1.64s/it] 42%|████▏     | 165/390 [02:41<04:38,  1.24s/it] 43%|████▎     | 166/390 [02:42<03:33,  1.05it/s] 43%|████▎     | 167/390 [02:42<02:47,  1.33it/s] 43%|████▎     | 168/390 [02:42<02:16,  1.63it/s] 43%|████▎     | 169/390 [02:43<01:54,  1.92it/s] 44%|████▎     | 170/390 [02:43<01:39,  2.22it/s] 44%|████▍     | 171/390 [02:43<01:29,  2.46it/s] 44%|████▍     | 172/390 [02:43<01:21,  2.69it/s] 44%|████▍     | 173/390 [02:44<01:15,  2.88it/s] 45%|████▍     | 174/390 [02:44<01:11,  3.04it/s] 45%|████▍     | 175/390 [02:44<01:08,  3.15it/s] 45%|████▌     | 176/390 [02:45<01:06,  3.24it/s] 45%|████▌     | 177/390 [02:45<01:04,  3.31it/s] 46%|████▌     | 178/390 [02:45<01:05,  3.25it/s] 46%|████▌     | 179/390 [02:46<01:34,  2.22it/s] 46%|████▌     | 180/390 [02:46<01:25,  2.44it/s] 46%|████▋     | 181/390 [02:47<01:17,  2.68it/s] 47%|████▋     | 182/390 [02:47<01:12,  2.87it/s] 47%|████▋     | 183/390 [02:47<01:08,  3.03it/s] 47%|████▋     | 184/390 [02:47<01:05,  3.15it/s] 47%|████▋     | 185/390 [02:48<01:03,  3.24it/s] 48%|████▊     | 186/390 [02:48<01:01,  3.30it/s] 48%|████▊     | 187/390 [02:48<01:00,  3.35it/s] 48%|████▊     | 188/390 [02:49<00:59,  3.38it/s] 48%|████▊     | 189/390 [02:49<00:58,  3.41it/s] 49%|████▊     | 190/390 [02:49<00:58,  3.42it/s] 49%|████▉     | 191/390 [02:50<00:58,  3.42it/s] 49%|████▉     | 192/390 [02:50<00:57,  3.43it/s] 49%|████▉     | 193/390 [02:50<00:57,  3.44it/s] 50%|████▉     | 194/390 [02:50<00:56,  3.45it/s] 50%|█████     | 195/390 [02:51<00:56,  3.45it/s] 50%|█████     | 196/390 [02:51<00:56,  3.45it/s] 51%|█████     | 197/390 [02:51<00:55,  3.46it/s] 51%|█████     | 198/390 [02:52<00:55,  3.46it/s] 51%|█████     | 199/390 [02:52<00:55,  3.46it/s] 51%|█████▏    | 200/390 [02:52<00:54,  3.46it/s] 52%|█████▏    | 201/390 [02:52<00:54,  3.46it/s] 52%|█████▏    | 202/390 [02:53<00:54,  3.46it/s] 52%|█████▏    | 203/390 [02:53<00:55,  3.38it/s] 52%|█████▏    | 204/390 [02:53<00:54,  3.40it/s] 53%|█████▎    | 205/390 [02:54<00:54,  3.42it/s] 53%|█████▎    | 206/390 [02:54<00:53,  3.43it/s] 53%|█████▎    | 207/390 [02:54<00:53,  3.44it/s] 53%|█████▎    | 208/390 [02:54<00:52,  3.45it/s] 54%|█████▎    | 209/390 [02:55<00:52,  3.45it/s] 54%|█████▍    | 210/390 [02:55<00:52,  3.45it/s] 54%|█████▍    | 211/390 [02:55<00:51,  3.45it/s] 54%|█████▍    | 212/390 [02:56<00:51,  3.46it/s] 55%|█████▍    | 213/390 [02:56<00:51,  3.46it/s] 55%|█████▍    | 214/390 [02:56<00:51,  3.42it/s] 55%|█████▌    | 215/390 [02:56<00:51,  3.43it/s] 55%|█████▌    | 216/390 [02:57<00:50,  3.44it/s] 56%|█████▌    | 217/390 [02:57<00:50,  3.44it/s] 56%|█████▌    | 218/390 [02:57<00:49,  3.45it/s] 56%|█████▌    | 219/390 [02:58<00:49,  3.45it/s] 56%|█████▋    | 220/390 [02:58<00:49,  3.45it/s] 57%|█████▋    | 221/390 [02:58<00:48,  3.46it/s] 57%|█████▋    | 222/390 [02:59<00:48,  3.44it/s] 57%|█████▋    | 223/390 [02:59<00:48,  3.45it/s] 57%|█████▋    | 224/390 [02:59<00:48,  3.45it/s] 58%|█████▊    | 225/390 [02:59<00:48,  3.43it/s] 58%|█████▊    | 226/390 [03:00<00:47,  3.44it/s] 58%|█████▊    | 227/390 [03:00<00:47,  3.45it/s] 58%|█████▊    | 228/390 [03:00<00:46,  3.45it/s] 59%|█████▊    | 229/390 [03:01<00:46,  3.45it/s] 59%|█████▉    | 230/390 [03:01<00:46,  3.45it/s] 59%|█████▉    | 231/390 [03:01<00:46,  3.46it/s] 59%|█████▉    | 232/390 [03:01<00:45,  3.46it/s] 60%|█████▉    | 233/390 [03:02<00:45,  3.46it/s] 60%|██████    | 234/390 [03:02<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 18:01:29,671 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:01:29,671 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 18:01:29,671 >>   Batch size = 8
{'eval_loss': 0.8955422043800354, 'eval_runtime': 38.1816, 'eval_samples_per_second': 368.424, 'eval_steps_per_second': 46.069, 'epoch': 1.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.09it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.35it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.77it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.13it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.79it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.47it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.15it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.75it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.76it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.66it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.70it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.79it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.77it/s][A
  4%|▍         | 73/1759 [00:01<00:35, 46.87it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.88it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.80it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.63it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.64it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.62it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.62it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.57it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.69it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.70it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.68it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.71it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.66it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.56it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.46it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.39it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.54it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.62it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.63it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.65it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 46.61it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.55it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.58it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.58it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.54it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.41it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.69it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.49it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.52it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.54it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.56it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.53it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.43it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.56it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.60it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.59it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.61it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.58it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.51it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.52it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.53it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.58it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.51it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.41it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.58it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.62it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.45it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.52it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.49it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.60it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.61it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.58it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.60it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.51it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.55it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.66it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.54it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.54it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.57it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.54it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.64it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.59it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.58it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.65it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.51it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.56it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.57it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.46it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.53it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.54it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.53it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.56it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.51it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.48it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.53it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.50it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.51it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.50it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.45it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.51it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.57it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.58it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.50it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.53it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.52it/s][A
 29%|██▉       | 513/1759 [00:10<00:26, 46.47it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.47it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.47it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.51it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.53it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.51it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.55it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.53it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.46it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.56it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.45it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.32it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.39it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.49it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.43it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.44it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.47it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.50it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.44it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.41it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.40it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.47it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.50it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.50it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.47it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.42it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.48it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.41it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.45it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.45it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.39it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.56it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.54it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.52it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.55it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.51it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.50it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.32it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.39it/s][A
 41%|████      | 713/1759 [00:15<00:22, 45.65it/s][A
 41%|████      | 718/1759 [00:15<00:22, 45.95it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.24it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.30it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.37it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.47it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.48it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.44it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.33it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.34it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.51it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.54it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.45it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.52it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.52it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.52it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.54it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.43it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.47it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.46it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.53it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.52it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.50it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.51it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.51it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.54it/s][A
 48%|████▊     | 848/1759 [00:18<00:20, 44.75it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 45.38it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 45.75it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 45.92it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.11it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.28it/s][A
 50%|████▉     | 878/1759 [00:18<00:19, 46.35it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.40it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.35it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.30it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.39it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.52it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.45it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.57it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.56it/s][A
 52%|█████▏    | 923/1759 [00:19<00:17, 46.52it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.60it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.53it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.46it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.46it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.27it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.38it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.44it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.46it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.47it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.45it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.49it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.52it/s][A
 56%|█████▌    | 988/1759 [00:21<00:17, 44.61it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 45.22it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 45.63it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 45.95it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.09it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.23it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:16, 46.30it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.38it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.25it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.31it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.42it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.51it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.58it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.56it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.59it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.55it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.45it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.40it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.47it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.59it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.57it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.44it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.55it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:13, 46.54it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.51it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.46it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.46it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.48it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.49it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.48it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.52it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.40it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.46it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.51it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.47it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.50it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.44it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.47it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.46it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.46it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.49it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.51it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.46it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.51it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.48it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.51it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.49it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.40it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.42it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.45it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.43it/s][A
 71%|███████   | 1248/1759 [00:26<00:10, 46.54it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.50it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.50it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.44it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.50it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.53it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.48it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.40it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.42it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.43it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.44it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.44it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.43it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.54it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.47it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.45it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.49it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.44it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.37it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.44it/s][A
 77%|███████▋  | 1348/1759 [00:28<00:08, 46.43it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.41it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.35it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.43it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.39it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.42it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.48it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.50it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.56it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.49it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.43it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.48it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.48it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.52it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.44it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.47it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.48it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.48it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.48it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.52it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.43it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.36it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.14it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.22it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.14it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.19it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.28it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.39it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.38it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.40it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.33it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.41it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.44it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.50it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.40it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.50it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.53it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.53it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.54it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.40it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.40it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.49it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.45it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.45it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.51it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.51it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.48it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.37it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 42.95it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 44.05it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 44.81it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 45.43it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 45.74it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.09it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.23it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.31it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 45.93it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 45.93it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.19it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.39it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.52it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.60it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.47it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.51it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.47it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.24it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.18it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.22it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.32it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.56it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.63it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.67it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.57it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.40it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.34it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 45.78it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 45.87it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 45.99it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.16it/s][A                                                 
                                                   [A 60%|██████    | 234/390 [03:40<00:45,  3.46it/s]
100%|██████████| 1759/1759 [00:37<00:00, 46.16it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:02:07,582 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:02:07,608 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:02:18,474 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:02:19,113 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:02:19,214 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [04:02<46:49, 18.12s/it] 61%|██████    | 236/390 [04:02<32:47, 12.77s/it] 61%|██████    | 237/390 [04:02<23:01,  9.03s/it] 61%|██████    | 238/390 [04:03<16:13,  6.41s/it] 61%|██████▏   | 239/390 [04:03<11:30,  4.57s/it] 62%|██████▏   | 240/390 [04:03<08:12,  3.29s/it] 62%|██████▏   | 241/390 [04:03<05:55,  2.39s/it] 62%|██████▏   | 242/390 [04:04<04:19,  1.76s/it] 62%|██████▏   | 243/390 [04:04<03:13,  1.32s/it] 63%|██████▎   | 244/390 [04:04<02:27,  1.01s/it] 63%|██████▎   | 245/390 [04:05<01:55,  1.26it/s] 63%|██████▎   | 246/390 [04:05<01:32,  1.56it/s] 63%|██████▎   | 247/390 [04:05<01:16,  1.87it/s] 64%|██████▎   | 248/390 [04:05<01:05,  2.17it/s] 64%|██████▍   | 249/390 [04:06<00:57,  2.44it/s] 64%|██████▍   | 250/390 [04:06<00:52,  2.68it/s] 64%|██████▍   | 251/390 [04:06<00:48,  2.88it/s] 65%|██████▍   | 252/390 [04:07<00:45,  3.03it/s] 65%|██████▍   | 253/390 [04:07<00:43,  3.15it/s] 65%|██████▌   | 254/390 [04:07<00:41,  3.24it/s] 65%|██████▌   | 255/390 [04:07<00:40,  3.31it/s] 66%|██████▌   | 256/390 [04:08<00:40,  3.27it/s] 66%|██████▌   | 257/390 [04:08<00:39,  3.33it/s] 66%|██████▌   | 258/390 [04:08<00:39,  3.37it/s] 66%|██████▋   | 259/390 [04:09<00:38,  3.40it/s] 67%|██████▋   | 260/390 [04:09<00:38,  3.42it/s] 67%|██████▋   | 261/390 [04:09<00:37,  3.43it/s] 67%|██████▋   | 262/390 [04:10<00:37,  3.44it/s] 67%|██████▋   | 263/390 [04:10<00:36,  3.45it/s] 68%|██████▊   | 264/390 [04:10<00:36,  3.46it/s] 68%|██████▊   | 265/390 [04:10<00:36,  3.46it/s] 68%|██████▊   | 266/390 [04:11<00:35,  3.46it/s] 68%|██████▊   | 267/390 [04:11<00:36,  3.39it/s] 69%|██████▊   | 268/390 [04:11<00:35,  3.41it/s] 69%|██████▉   | 269/390 [04:12<00:35,  3.43it/s] 69%|██████▉   | 270/390 [04:12<00:34,  3.44it/s] 69%|██████▉   | 271/390 [04:12<00:34,  3.45it/s] 70%|██████▉   | 272/390 [04:12<00:34,  3.45it/s] 70%|███████   | 273/390 [04:13<00:33,  3.46it/s] 70%|███████   | 274/390 [04:13<00:33,  3.46it/s] 71%|███████   | 275/390 [04:13<00:33,  3.46it/s] 71%|███████   | 276/390 [04:14<00:32,  3.46it/s] 71%|███████   | 277/390 [04:14<00:32,  3.47it/s] 71%|███████▏  | 278/390 [04:14<00:37,  3.01it/s] 72%|███████▏  | 279/390 [04:15<00:35,  3.13it/s] 72%|███████▏  | 280/390 [04:15<00:34,  3.23it/s] 72%|███████▏  | 281/390 [04:15<00:33,  3.29it/s] 72%|███████▏  | 282/390 [04:15<00:32,  3.34it/s] 73%|███████▎  | 283/390 [04:16<00:31,  3.37it/s] 73%|███████▎  | 284/390 [04:16<00:31,  3.40it/s] 73%|███████▎  | 285/390 [04:16<00:30,  3.41it/s] 73%|███████▎  | 286/390 [04:17<00:30,  3.42it/s] 74%|███████▎  | 287/390 [04:17<00:29,  3.44it/s] 74%|███████▍  | 288/390 [04:17<00:38,  2.67it/s] 74%|███████▍  | 289/390 [04:18<00:35,  2.87it/s] 74%|███████▍  | 290/390 [04:18<00:33,  3.02it/s] 75%|███████▍  | 291/390 [04:18<00:31,  3.14it/s] 75%|███████▍  | 292/390 [04:19<00:30,  3.23it/s] 75%|███████▌  | 293/390 [04:19<00:29,  3.30it/s] 75%|███████▌  | 294/390 [04:19<00:28,  3.35it/s] 76%|███████▌  | 295/390 [04:20<00:28,  3.38it/s] 76%|███████▌  | 296/390 [04:20<00:27,  3.40it/s] 76%|███████▌  | 297/390 [04:20<00:27,  3.42it/s] 76%|███████▋  | 298/390 [04:20<00:28,  3.21it/s] 77%|███████▋  | 299/390 [04:21<00:27,  3.28it/s] 77%|███████▋  | 300/390 [04:21<00:26,  3.34it/s] 77%|███████▋  | 301/390 [04:21<00:26,  3.37it/s] 77%|███████▋  | 302/390 [04:22<00:25,  3.40it/s] 78%|███████▊  | 303/390 [04:22<00:25,  3.42it/s] 78%|███████▊  | 304/390 [04:22<00:25,  3.43it/s] 78%|███████▊  | 305/390 [04:22<00:24,  3.44it/s] 78%|███████▊  | 306/390 [04:23<00:24,  3.45it/s] 79%|███████▊  | 307/390 [04:23<00:24,  3.45it/s] 79%|███████▉  | 308/390 [04:23<00:23,  3.46it/s] 79%|███████▉  | 309/390 [04:24<00:24,  3.30it/s] 79%|███████▉  | 310/390 [04:24<00:23,  3.34it/s] 80%|███████▉  | 311/390 [04:24<00:23,  3.38it/s] 80%|████████  | 312/390 [04:25<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 18:02:52,216 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:02:52,216 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 18:02:52,216 >>   Batch size = 8
{'eval_loss': 0.905792236328125, 'eval_runtime': 37.8894, 'eval_samples_per_second': 371.265, 'eval_steps_per_second': 46.425, 'epoch': 2.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.94it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.35it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.74it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.11it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.78it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.49it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.31it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.97it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.79it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.88it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.78it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.73it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.80it/s][A
  4%|▍         | 73/1759 [00:01<00:35, 46.91it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.84it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.81it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.75it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.64it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.66it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.74it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.71it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.79it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.86it/s][A
  7%|▋         | 123/1759 [00:02<00:34, 46.81it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.85it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.75it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.64it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.65it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.62it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.60it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.67it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.67it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.79it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.77it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.75it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.69it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.57it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.59it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.55it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.49it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.67it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.71it/s][A
 12%|█▏        | 218/1759 [00:04<00:32, 46.71it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.67it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.60it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.56it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.55it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.50it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.60it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.62it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.63it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.73it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.69it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.64it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.66it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.58it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.62it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.63it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.59it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.65it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.70it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.63it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.66it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.50it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.53it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.60it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.53it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.57it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.66it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.71it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.68it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.61it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.58it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.51it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.53it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.58it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.55it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.59it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.70it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.61it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.57it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.61it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.55it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.45it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.62it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.54it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.62it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.66it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.55it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.61it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.55it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.52it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.48it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.58it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.57it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.64it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.65it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.57it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.57it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.51it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.56it/s][A
 29%|██▉       | 513/1759 [00:10<00:26, 46.58it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.47it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.58it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.66it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.42it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.65it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.58it/s][A
 31%|███       | 548/1759 [00:11<00:25, 46.58it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.55it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.52it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.57it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.52it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.52it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.51it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.54it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.57it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.48it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.55it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.60it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.50it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.60it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.67it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.57it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.55it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.56it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.48it/s][A
 37%|███▋      | 643/1759 [00:13<00:23, 46.56it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.56it/s][A
 37%|███▋      | 653/1759 [00:13<00:23, 46.52it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.56it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.57it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.55it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.36it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.31it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.22it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.19it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.40it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.38it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.44it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.50it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.49it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.51it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.54it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.51it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.59it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.54it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.52it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.55it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.51it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.57it/s][A
 43%|████▎     | 763/1759 [00:16<00:31, 31.80it/s][A
 44%|████▎     | 768/1759 [00:16<00:28, 35.19it/s][A
 44%|████▍     | 773/1759 [00:16<00:25, 38.01it/s][A
 44%|████▍     | 778/1759 [00:16<00:24, 40.15it/s][A
 45%|████▍     | 783/1759 [00:16<00:23, 41.92it/s][A
 45%|████▍     | 788/1759 [00:17<00:22, 43.27it/s][A
 45%|████▌     | 793/1759 [00:17<00:21, 44.23it/s][A
 45%|████▌     | 798/1759 [00:17<00:21, 44.94it/s][A
 46%|████▌     | 803/1759 [00:17<00:21, 45.28it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 45.68it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 45.88it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.08it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.25it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.35it/s][A
 47%|████▋     | 833/1759 [00:18<00:19, 46.41it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.53it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.53it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.42it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.42it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.47it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.56it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.52it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.54it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.57it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.50it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.55it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.50it/s][A
 51%|█████     | 898/1759 [00:19<00:27, 31.22it/s][A
 51%|█████▏    | 903/1759 [00:19<00:24, 34.60it/s][A
 52%|█████▏    | 908/1759 [00:19<00:22, 37.50it/s][A
 52%|█████▏    | 913/1759 [00:19<00:21, 39.87it/s][A
 52%|█████▏    | 918/1759 [00:20<00:20, 41.71it/s][A
 52%|█████▏    | 923/1759 [00:20<00:19, 43.09it/s][A
 53%|█████▎    | 928/1759 [00:20<00:18, 44.15it/s][A
 53%|█████▎    | 933/1759 [00:20<00:18, 44.85it/s][A
 53%|█████▎    | 938/1759 [00:20<00:18, 45.23it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 45.60it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 45.90it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.14it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.30it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.34it/s][A
 55%|█████▌    | 968/1759 [00:21<00:17, 46.48it/s][A
 55%|█████▌    | 973/1759 [00:21<00:16, 46.51it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.53it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.51it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.52it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.50it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.40it/s][A
 58%|█████▊    | 1013/1759 [00:22<00:16, 46.34it/s][A
 58%|█████▊    | 1018/1759 [00:22<00:15, 46.38it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.25it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.21it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.13it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.14it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.15it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.17it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.28it/s][A
 60%|██████    | 1058/1759 [00:23<00:15, 46.37it/s][A
 60%|██████    | 1063/1759 [00:23<00:14, 46.47it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.38it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.41it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.47it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.55it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.43it/s][A
 63%|██████▎   | 1103/1759 [00:24<00:14, 46.57it/s][A
 63%|██████▎   | 1108/1759 [00:24<00:13, 46.57it/s][A
 63%|██████▎   | 1113/1759 [00:24<00:13, 46.56it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.47it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.47it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.53it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.56it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.54it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.50it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.51it/s][A
 66%|██████▌   | 1153/1759 [00:25<00:13, 46.48it/s][A
 66%|██████▌   | 1158/1759 [00:25<00:12, 46.53it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.51it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.42it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:18, 31.76it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:16, 35.14it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:15, 37.95it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:14, 40.23it/s][A
 68%|██████▊   | 1193/1759 [00:26<00:13, 42.00it/s][A
 68%|██████▊   | 1198/1759 [00:26<00:12, 43.25it/s][A
 68%|██████▊   | 1203/1759 [00:26<00:12, 44.28it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:19, 28.39it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:16, 32.13it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:15, 35.44it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:14, 38.24it/s][A
 70%|██████▉   | 1228/1759 [00:27<00:13, 40.43it/s][A
 70%|███████   | 1233/1759 [00:27<00:12, 42.10it/s][A
 70%|███████   | 1238/1759 [00:27<00:11, 43.43it/s][A
 71%|███████   | 1243/1759 [00:27<00:11, 44.33it/s][A
 71%|███████   | 1248/1759 [00:27<00:11, 44.84it/s][A
 71%|███████   | 1253/1759 [00:27<00:11, 45.31it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 45.59it/s][A
 72%|███████▏  | 1263/1759 [00:28<00:22, 22.31it/s][A
 72%|███████▏  | 1268/1759 [00:28<00:18, 26.67it/s][A
 72%|███████▏  | 1273/1759 [00:28<00:15, 30.60it/s][A
 73%|███████▎  | 1278/1759 [00:28<00:15, 31.52it/s][A
 73%|███████▎  | 1283/1759 [00:28<00:13, 34.87it/s][A
 73%|███████▎  | 1288/1759 [00:28<00:12, 37.67it/s][A
 74%|███████▎  | 1293/1759 [00:28<00:11, 40.07it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:11, 41.85it/s][A
 74%|███████▍  | 1303/1759 [00:29<00:10, 43.22it/s][A
 74%|███████▍  | 1308/1759 [00:29<00:10, 44.25it/s][A
 75%|███████▍  | 1313/1759 [00:29<00:09, 44.93it/s][A
 75%|███████▍  | 1318/1759 [00:29<00:09, 45.21it/s][A
 75%|███████▌  | 1323/1759 [00:29<00:09, 45.52it/s][A
 75%|███████▌  | 1328/1759 [00:29<00:09, 45.70it/s][A
 76%|███████▌  | 1333/1759 [00:29<00:09, 45.87it/s][A
 76%|███████▌  | 1338/1759 [00:29<00:09, 46.07it/s][A
 76%|███████▋  | 1343/1759 [00:29<00:08, 46.28it/s][A
 77%|███████▋  | 1348/1759 [00:30<00:08, 46.44it/s][A
 77%|███████▋  | 1353/1759 [00:30<00:08, 46.52it/s][A
 77%|███████▋  | 1358/1759 [00:30<00:08, 46.57it/s][A
 77%|███████▋  | 1363/1759 [00:30<00:08, 46.52it/s][A
 78%|███████▊  | 1368/1759 [00:30<00:08, 46.53it/s][A
 78%|███████▊  | 1373/1759 [00:30<00:08, 46.45it/s][A
 78%|███████▊  | 1378/1759 [00:30<00:08, 46.39it/s][A
 79%|███████▊  | 1383/1759 [00:30<00:08, 46.48it/s][A
 79%|███████▉  | 1388/1759 [00:30<00:08, 46.31it/s][A
 79%|███████▉  | 1393/1759 [00:31<00:07, 46.46it/s][A
 79%|███████▉  | 1398/1759 [00:31<00:07, 46.61it/s][A
 80%|███████▉  | 1403/1759 [00:31<00:07, 46.62it/s][A
 80%|████████  | 1408/1759 [00:31<00:07, 46.65it/s][A
 80%|████████  | 1413/1759 [00:31<00:07, 46.62it/s][A
 81%|████████  | 1418/1759 [00:31<00:07, 46.47it/s][A
 81%|████████  | 1423/1759 [00:31<00:07, 46.43it/s][A
 81%|████████  | 1428/1759 [00:31<00:07, 46.47it/s][A
 81%|████████▏ | 1433/1759 [00:31<00:07, 46.43it/s][A
 82%|████████▏ | 1438/1759 [00:32<00:06, 46.58it/s][A
 82%|████████▏ | 1443/1759 [00:32<00:06, 46.65it/s][A
 82%|████████▏ | 1448/1759 [00:32<00:06, 46.61it/s][A
 83%|████████▎ | 1453/1759 [00:32<00:06, 46.56it/s][A
 83%|████████▎ | 1458/1759 [00:32<00:06, 46.53it/s][A
 83%|████████▎ | 1463/1759 [00:32<00:06, 46.47it/s][A
 83%|████████▎ | 1468/1759 [00:32<00:06, 46.38it/s][A
 84%|████████▎ | 1473/1759 [00:32<00:06, 46.39it/s][A
 84%|████████▍ | 1478/1759 [00:32<00:06, 46.42it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.42it/s][A
 85%|████████▍ | 1488/1759 [00:33<00:05, 46.54it/s][A
 85%|████████▍ | 1493/1759 [00:33<00:05, 46.59it/s][A
 85%|████████▌ | 1498/1759 [00:33<00:05, 46.63it/s][A
 85%|████████▌ | 1503/1759 [00:33<00:05, 46.66it/s][A
 86%|████████▌ | 1508/1759 [00:33<00:05, 46.28it/s][A
 86%|████████▌ | 1513/1759 [00:33<00:05, 46.39it/s][A
 86%|████████▋ | 1518/1759 [00:33<00:05, 46.41it/s][A
 87%|████████▋ | 1523/1759 [00:33<00:05, 46.40it/s][A
 87%|████████▋ | 1528/1759 [00:33<00:04, 46.55it/s][A
 87%|████████▋ | 1533/1759 [00:34<00:04, 46.53it/s][A
 87%|████████▋ | 1538/1759 [00:34<00:04, 46.57it/s][A
 88%|████████▊ | 1543/1759 [00:34<00:04, 46.61it/s][A
 88%|████████▊ | 1548/1759 [00:34<00:04, 46.59it/s][A
 88%|████████▊ | 1553/1759 [00:34<00:04, 46.50it/s][A
 89%|████████▊ | 1558/1759 [00:34<00:04, 46.44it/s][A
 89%|████████▉ | 1563/1759 [00:34<00:04, 46.27it/s][A
 89%|████████▉ | 1568/1759 [00:34<00:04, 46.35it/s][A
 89%|████████▉ | 1573/1759 [00:34<00:03, 46.50it/s][A
 90%|████████▉ | 1578/1759 [00:35<00:03, 46.50it/s][A
 90%|████████▉ | 1583/1759 [00:35<00:03, 46.61it/s][A
 90%|█████████ | 1588/1759 [00:35<00:03, 46.57it/s][A
 91%|█████████ | 1593/1759 [00:35<00:03, 46.53it/s][A
 91%|█████████ | 1598/1759 [00:35<00:03, 46.55it/s][A
 91%|█████████ | 1603/1759 [00:35<00:03, 46.47it/s][A
 91%|█████████▏| 1608/1759 [00:35<00:03, 46.35it/s][A
 92%|█████████▏| 1613/1759 [00:35<00:03, 46.40it/s][A
 92%|█████████▏| 1618/1759 [00:35<00:03, 46.42it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.56it/s][A
 93%|█████████▎| 1628/1759 [00:36<00:02, 46.61it/s][A
 93%|█████████▎| 1633/1759 [00:36<00:02, 46.58it/s][A
 93%|█████████▎| 1638/1759 [00:36<00:02, 46.53it/s][A
 93%|█████████▎| 1643/1759 [00:36<00:02, 46.48it/s][A
 94%|█████████▎| 1648/1759 [00:36<00:02, 46.43it/s][A
 94%|█████████▍| 1653/1759 [00:36<00:02, 46.47it/s][A
 94%|█████████▍| 1658/1759 [00:36<00:02, 46.42it/s][A
 95%|█████████▍| 1663/1759 [00:36<00:02, 46.45it/s][A
 95%|█████████▍| 1668/1759 [00:36<00:01, 46.48it/s][A
 95%|█████████▌| 1673/1759 [00:37<00:01, 46.52it/s][A
 95%|█████████▌| 1678/1759 [00:37<00:01, 46.52it/s][A
 96%|█████████▌| 1683/1759 [00:37<00:01, 46.59it/s][A
 96%|█████████▌| 1688/1759 [00:37<00:01, 46.50it/s][A
 96%|█████████▌| 1693/1759 [00:37<00:01, 46.51it/s][A
 97%|█████████▋| 1698/1759 [00:37<00:01, 46.53it/s][A
 97%|█████████▋| 1703/1759 [00:37<00:01, 46.52it/s][A
 97%|█████████▋| 1708/1759 [00:37<00:01, 46.50it/s][A
 97%|█████████▋| 1713/1759 [00:37<00:00, 46.55it/s][A
 98%|█████████▊| 1718/1759 [00:38<00:00, 46.49it/s][A
 98%|█████████▊| 1723/1759 [00:38<00:00, 46.48it/s][A
 98%|█████████▊| 1728/1759 [00:38<00:00, 46.62it/s][A
 99%|█████████▊| 1733/1759 [00:38<00:00, 46.54it/s][A
 99%|█████████▉| 1738/1759 [00:38<00:00, 46.42it/s][A
 99%|█████████▉| 1743/1759 [00:38<00:00, 46.49it/s][A
 99%|█████████▉| 1748/1759 [00:38<00:00, 46.46it/s][A
100%|█████████▉| 1753/1759 [00:38<00:00, 46.50it/s][A
100%|█████████▉| 1758/1759 [00:38<00:00, 46.52it/s][A                                                 
                                                   [A 80%|████████  | 312/390 [05:03<00:22,  3.40it/s]
100%|██████████| 1759/1759 [00:38<00:00, 46.52it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:03:31,266 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 18:03:31,356 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:03:34,548 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:03:34,564 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:03:34,573 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [05:20<21:25, 16.70s/it] 81%|████████  | 314/390 [05:20<14:59, 11.83s/it] 81%|████████  | 315/390 [05:20<10:27,  8.37s/it] 81%|████████  | 316/390 [05:21<07:19,  5.95s/it] 81%|████████▏ | 317/390 [05:21<05:10,  4.25s/it] 82%|████████▏ | 318/390 [05:21<03:40,  3.06s/it] 82%|████████▏ | 319/390 [05:21<02:38,  2.23s/it] 82%|████████▏ | 320/390 [05:22<01:55,  1.65s/it] 82%|████████▏ | 321/390 [05:22<01:25,  1.24s/it] 83%|████████▎ | 322/390 [05:22<01:04,  1.05it/s] 83%|████████▎ | 323/390 [05:23<00:50,  1.33it/s] 83%|████████▎ | 324/390 [05:23<00:41,  1.59it/s] 83%|████████▎ | 325/390 [05:23<00:34,  1.89it/s] 84%|████████▎ | 326/390 [05:24<00:29,  2.19it/s] 84%|████████▍ | 327/390 [05:24<00:25,  2.46it/s] 84%|████████▍ | 328/390 [05:24<00:22,  2.70it/s] 84%|████████▍ | 329/390 [05:24<00:21,  2.89it/s] 85%|████████▍ | 330/390 [05:25<00:19,  3.04it/s] 85%|████████▍ | 331/390 [05:25<00:18,  3.16it/s] 85%|████████▌ | 332/390 [05:25<00:17,  3.24it/s] 85%|████████▌ | 333/390 [05:26<00:17,  3.31it/s] 86%|████████▌ | 334/390 [05:26<00:16,  3.36it/s] 86%|████████▌ | 335/390 [05:26<00:17,  3.19it/s] 86%|████████▌ | 336/390 [05:26<00:16,  3.27it/s] 86%|████████▋ | 337/390 [05:27<00:15,  3.33it/s] 87%|████████▋ | 338/390 [05:27<00:15,  3.37it/s] 87%|████████▋ | 339/390 [05:27<00:15,  3.40it/s] 87%|████████▋ | 340/390 [05:28<00:14,  3.42it/s] 87%|████████▋ | 341/390 [05:28<00:14,  3.43it/s] 88%|████████▊ | 342/390 [05:28<00:13,  3.44it/s] 88%|████████▊ | 343/390 [05:28<00:13,  3.45it/s] 88%|████████▊ | 344/390 [05:29<00:13,  3.45it/s] 88%|████████▊ | 345/390 [05:29<00:13,  3.45it/s] 89%|████████▊ | 346/390 [05:29<00:13,  3.37it/s] 89%|████████▉ | 347/390 [05:30<00:12,  3.40it/s] 89%|████████▉ | 348/390 [05:30<00:12,  3.42it/s] 89%|████████▉ | 349/390 [05:30<00:11,  3.43it/s] 90%|████████▉ | 350/390 [05:31<00:11,  3.45it/s] 90%|█████████ | 351/390 [05:31<00:11,  3.45it/s] 90%|█████████ | 352/390 [05:31<00:10,  3.45it/s] 91%|█████████ | 353/390 [05:31<00:10,  3.46it/s] 91%|█████████ | 354/390 [05:32<00:10,  3.46it/s] 91%|█████████ | 355/390 [05:32<00:10,  3.46it/s] 91%|█████████▏| 356/390 [05:32<00:09,  3.46it/s] 92%|█████████▏| 357/390 [05:33<00:09,  3.45it/s] 92%|█████████▏| 358/390 [05:33<00:09,  3.45it/s] 92%|█████████▏| 359/390 [05:33<00:08,  3.45it/s] 92%|█████████▏| 360/390 [05:33<00:08,  3.46it/s] 93%|█████████▎| 361/390 [05:34<00:08,  3.46it/s] 93%|█████████▎| 362/390 [05:34<00:08,  3.46it/s] 93%|█████████▎| 363/390 [05:34<00:07,  3.46it/s] 93%|█████████▎| 364/390 [05:35<00:07,  3.46it/s] 94%|█████████▎| 365/390 [05:35<00:07,  3.46it/s] 94%|█████████▍| 366/390 [05:35<00:06,  3.46it/s] 94%|█████████▍| 367/390 [05:35<00:06,  3.46it/s] 94%|█████████▍| 368/390 [05:36<00:06,  3.46it/s] 95%|█████████▍| 369/390 [05:36<00:06,  3.46it/s] 95%|█████████▍| 370/390 [05:36<00:05,  3.46it/s] 95%|█████████▌| 371/390 [05:37<00:05,  3.42it/s] 95%|█████████▌| 372/390 [05:37<00:05,  3.44it/s] 96%|█████████▌| 373/390 [05:37<00:04,  3.44it/s] 96%|█████████▌| 374/390 [05:37<00:04,  3.45it/s] 96%|█████████▌| 375/390 [05:38<00:04,  3.45it/s] 96%|█████████▋| 376/390 [05:38<00:04,  3.46it/s] 97%|█████████▋| 377/390 [05:38<00:03,  3.46it/s] 97%|█████████▋| 378/390 [05:39<00:03,  3.46it/s] 97%|█████████▋| 379/390 [05:39<00:03,  3.46it/s] 97%|█████████▋| 380/390 [05:39<00:02,  3.46it/s] 98%|█████████▊| 381/390 [05:39<00:02,  3.46it/s] 98%|█████████▊| 382/390 [05:40<00:02,  3.44it/s] 98%|█████████▊| 383/390 [05:40<00:02,  3.45it/s] 98%|█████████▊| 384/390 [05:40<00:01,  3.45it/s] 99%|█████████▊| 385/390 [05:41<00:01,  3.45it/s] 99%|█████████▉| 386/390 [05:41<00:01,  3.45it/s] 99%|█████████▉| 387/390 [05:41<00:00,  3.46it/s] 99%|█████████▉| 388/390 [05:42<00:00,  3.46it/s]100%|█████████▉| 389/390 [05:42<00:00,  3.46it/s]100%|██████████| 390/390 [05:42<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 18:04:09,738 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:04:09,738 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 18:04:09,738 >>   Batch size = 8
{'eval_loss': 0.9115927815437317, 'eval_runtime': 38.9227, 'eval_samples_per_second': 361.408, 'eval_steps_per_second': 45.192, 'epoch': 3.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.04it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.59it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.90it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.16it/s][A
  2%|▏         | 28/1759 [00:00<00:37, 45.76it/s][A
  2%|▏         | 33/1759 [00:00<00:37, 45.90it/s][A
  2%|▏         | 38/1759 [00:00<00:37, 46.10it/s][A
  2%|▏         | 43/1759 [00:00<00:37, 46.06it/s][A
  3%|▎         | 48/1759 [00:01<00:37, 46.08it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.16it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.31it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.43it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.44it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.43it/s][A
  4%|▍         | 78/1759 [00:01<00:36, 46.59it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.68it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.62it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.62it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.60it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.48it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.55it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.54it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.57it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.55it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.57it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.67it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.61it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.61it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.64it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.51it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.62it/s][A
  9%|▉         | 163/1759 [00:04<00:34, 46.57it/s][A
 10%|▉         | 168/1759 [00:04<01:28, 17.89it/s][A
 10%|▉         | 173/1759 [00:04<01:12, 21.95it/s][A
 10%|█         | 178/1759 [00:04<01:00, 26.11it/s][A
 10%|█         | 183/1759 [00:04<00:52, 30.14it/s][A
 11%|█         | 188/1759 [00:04<00:46, 33.74it/s][A
 11%|█         | 193/1759 [00:04<00:42, 36.74it/s][A
 11%|█▏        | 198/1759 [00:04<00:39, 39.31it/s][A
 12%|█▏        | 203/1759 [00:04<00:37, 41.28it/s][A
 12%|█▏        | 208/1759 [00:05<00:36, 42.45it/s][A
 12%|█▏        | 213/1759 [00:05<00:35, 43.50it/s][A
 12%|█▏        | 218/1759 [00:05<00:34, 44.46it/s][A
 13%|█▎        | 223/1759 [00:05<00:34, 45.12it/s][A
 13%|█▎        | 228/1759 [00:05<00:33, 45.52it/s][A
 13%|█▎        | 233/1759 [00:05<00:33, 45.95it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.21it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.39it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.48it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.32it/s][A
 15%|█▍        | 258/1759 [00:06<00:32, 46.32it/s][A
 15%|█▍        | 263/1759 [00:06<00:32, 46.35it/s][A
 15%|█▌        | 268/1759 [00:06<00:32, 46.41it/s][A
 16%|█▌        | 273/1759 [00:06<00:31, 46.58it/s][A
 16%|█▌        | 278/1759 [00:06<00:31, 46.64it/s][A
 16%|█▌        | 283/1759 [00:06<00:58, 25.02it/s][A
 16%|█▋        | 288/1759 [00:07<00:50, 29.08it/s][A
 17%|█▋        | 293/1759 [00:07<00:44, 32.77it/s][A
 17%|█▋        | 298/1759 [00:07<00:40, 36.07it/s][A
 17%|█▋        | 303/1759 [00:07<00:37, 38.74it/s][A
 18%|█▊        | 308/1759 [00:07<00:35, 40.85it/s][A
 18%|█▊        | 313/1759 [00:07<00:34, 42.49it/s][A
 18%|█▊        | 318/1759 [00:07<00:32, 43.71it/s][A
 18%|█▊        | 323/1759 [00:07<00:32, 44.14it/s][A
 19%|█▊        | 328/1759 [00:07<00:32, 44.72it/s][A
 19%|█▉        | 333/1759 [00:08<00:31, 45.30it/s][A
 19%|█▉        | 338/1759 [00:08<00:31, 45.71it/s][A
 19%|█▉        | 343/1759 [00:08<00:30, 46.06it/s][A
 20%|█▉        | 348/1759 [00:08<00:30, 46.25it/s][A
 20%|██        | 353/1759 [00:08<00:30, 46.32it/s][A
 20%|██        | 358/1759 [00:08<00:30, 46.53it/s][A
 21%|██        | 363/1759 [00:08<00:30, 46.50it/s][A
 21%|██        | 368/1759 [00:08<00:29, 46.43it/s][A
 21%|██        | 373/1759 [00:08<00:32, 43.31it/s][A
 21%|██▏       | 378/1759 [00:09<00:31, 44.36it/s][A
 22%|██▏       | 383/1759 [00:09<00:30, 45.02it/s][A
 22%|██▏       | 388/1759 [00:09<00:30, 45.58it/s][A
 22%|██▏       | 393/1759 [00:09<00:29, 45.86it/s][A
 23%|██▎       | 398/1759 [00:09<00:29, 46.14it/s][A
 23%|██▎       | 403/1759 [00:09<00:29, 46.39it/s][A
 23%|██▎       | 408/1759 [00:09<00:32, 41.62it/s][A
 23%|██▎       | 413/1759 [00:09<00:31, 43.04it/s][A
 24%|██▍       | 418/1759 [00:09<00:30, 44.07it/s][A
 24%|██▍       | 423/1759 [00:10<00:29, 44.69it/s][A
 24%|██▍       | 428/1759 [00:10<00:29, 45.22it/s][A
 25%|██▍       | 433/1759 [00:10<00:29, 45.56it/s][A
 25%|██▍       | 438/1759 [00:12<03:10,  6.92it/s][A
 25%|██▌       | 443/1759 [00:12<02:21,  9.32it/s][A
 25%|██▌       | 448/1759 [00:12<01:46, 12.28it/s][A
 26%|██▌       | 453/1759 [00:12<01:38, 13.24it/s][A
 26%|██▌       | 458/1759 [00:13<01:17, 16.87it/s][A
 26%|██▋       | 463/1759 [00:13<01:02, 20.90it/s][A
 27%|██▋       | 468/1759 [00:13<00:51, 25.08it/s][A
 27%|██▋       | 473/1759 [00:13<00:44, 29.14it/s][A
 27%|██▋       | 478/1759 [00:13<00:38, 32.91it/s][A
 27%|██▋       | 483/1759 [00:13<00:35, 36.15it/s][A
 28%|██▊       | 488/1759 [00:13<00:32, 38.82it/s][A
 28%|██▊       | 493/1759 [00:13<00:31, 40.49it/s][A
 28%|██▊       | 498/1759 [00:13<00:29, 42.11it/s][A
 29%|██▊       | 503/1759 [00:13<00:28, 43.33it/s][A
 29%|██▉       | 508/1759 [00:14<00:28, 44.37it/s][A
 29%|██▉       | 513/1759 [00:14<00:27, 45.07it/s][A
 29%|██▉       | 518/1759 [00:14<00:27, 45.60it/s][A
 30%|██▉       | 523/1759 [00:14<00:26, 45.86it/s][A
 30%|███       | 528/1759 [00:14<00:26, 46.16it/s][A
 30%|███       | 533/1759 [00:14<00:26, 46.32it/s][A
 31%|███       | 538/1759 [00:14<00:26, 46.30it/s][A
 31%|███       | 543/1759 [00:14<00:26, 46.34it/s][A
 31%|███       | 548/1759 [00:14<00:26, 46.31it/s][A
 31%|███▏      | 553/1759 [00:15<00:25, 46.54it/s][A
 32%|███▏      | 558/1759 [00:15<00:25, 46.58it/s][A
 32%|███▏      | 563/1759 [00:15<00:25, 46.64it/s][A
 32%|███▏      | 568/1759 [00:15<00:25, 46.63it/s][A
 33%|███▎      | 573/1759 [00:15<00:25, 46.72it/s][A
 33%|███▎      | 578/1759 [00:15<00:25, 46.67it/s][A
 33%|███▎      | 583/1759 [00:16<00:25, 46.55it/s][A
 33%|███▎      | 588/1759 [00:16<00:48, 24.00it/s][A
 34%|███▎      | 593/1759 [00:16<00:41, 28.10it/s][A
 34%|███▍      | 598/1759 [00:16<00:36, 31.95it/s][A
 34%|███▍      | 603/1759 [00:16<00:32, 35.30it/s][A
 35%|███▍      | 608/1759 [00:16<00:30, 38.08it/s][A
 35%|███▍      | 613/1759 [00:16<00:28, 40.32it/s][A
 35%|███▌      | 618/1759 [00:16<00:27, 42.08it/s][A
 35%|███▌      | 623/1759 [00:16<00:26, 43.39it/s][A
 36%|███▌      | 628/1759 [00:16<00:25, 44.08it/s][A
 36%|███▌      | 633/1759 [00:17<00:25, 44.71it/s][A
 36%|███▋      | 638/1759 [00:17<00:24, 45.26it/s][A
 37%|███▋      | 643/1759 [00:17<00:24, 45.70it/s][A
 37%|███▋      | 648/1759 [00:17<00:24, 45.98it/s][A
 37%|███▋      | 653/1759 [00:17<00:23, 46.25it/s][A
 37%|███▋      | 658/1759 [00:17<00:23, 46.31it/s][A
 38%|███▊      | 663/1759 [00:17<00:23, 46.45it/s][A
 38%|███▊      | 668/1759 [00:17<00:23, 46.54it/s][A
 38%|███▊      | 673/1759 [00:17<00:23, 46.42it/s][A
 39%|███▊      | 678/1759 [00:18<00:23, 46.36it/s][A
 39%|███▉      | 683/1759 [00:18<00:23, 46.40it/s][A
 39%|███▉      | 688/1759 [00:18<00:23, 46.50it/s][A
 39%|███▉      | 693/1759 [00:18<00:22, 46.59it/s][A
 40%|███▉      | 698/1759 [00:18<00:22, 46.63it/s][A
 40%|███▉      | 703/1759 [00:18<00:22, 46.69it/s][A
 40%|████      | 708/1759 [00:18<00:22, 46.66it/s][A
 41%|████      | 713/1759 [00:18<00:25, 41.05it/s][A
 41%|████      | 718/1759 [00:18<00:24, 42.61it/s][A
 41%|████      | 723/1759 [00:19<00:23, 43.77it/s][A
 41%|████▏     | 728/1759 [00:19<00:23, 44.68it/s][A
 42%|████▏     | 733/1759 [00:19<00:22, 45.26it/s][A
 42%|████▏     | 738/1759 [00:19<00:22, 45.65it/s][A
 42%|████▏     | 743/1759 [00:19<00:22, 46.03it/s][A
 43%|████▎     | 748/1759 [00:19<00:21, 46.25it/s][A
 43%|████▎     | 753/1759 [00:19<00:21, 45.78it/s][A
 43%|████▎     | 758/1759 [00:19<00:21, 45.85it/s][A
 43%|████▎     | 763/1759 [00:19<00:21, 46.12it/s][A
 44%|████▎     | 768/1759 [00:20<00:21, 46.37it/s][A
 44%|████▍     | 773/1759 [00:20<00:21, 46.49it/s][A
 44%|████▍     | 778/1759 [00:20<00:21, 46.56it/s][A
 45%|████▍     | 783/1759 [00:20<00:20, 46.65it/s][A
 45%|████▍     | 788/1759 [00:20<00:20, 46.70it/s][A
 45%|████▌     | 793/1759 [00:20<00:20, 46.64it/s][A
 45%|████▌     | 798/1759 [00:20<00:20, 46.44it/s][A
 46%|████▌     | 803/1759 [00:20<00:20, 46.31it/s][A
 46%|████▌     | 808/1759 [00:20<00:20, 46.36it/s][A
 46%|████▌     | 813/1759 [00:21<00:20, 46.47it/s][A
 47%|████▋     | 818/1759 [00:21<00:20, 46.56it/s][A
 47%|████▋     | 823/1759 [00:21<00:20, 46.63it/s][A
 47%|████▋     | 828/1759 [00:21<00:19, 46.67it/s][A
 47%|████▋     | 833/1759 [00:21<00:19, 46.71it/s][A
 48%|████▊     | 838/1759 [00:21<00:19, 46.60it/s][A
 48%|████▊     | 843/1759 [00:21<00:19, 46.49it/s][A
 48%|████▊     | 848/1759 [00:22<00:19, 46.38it/s][A
 48%|████▊     | 853/1759 [00:22<00:49, 18.29it/s][A
 49%|████▉     | 858/1759 [00:22<00:40, 22.37it/s][A
 49%|████▉     | 863/1759 [00:22<00:33, 26.52it/s][A
 49%|████▉     | 868/1759 [00:22<00:29, 30.48it/s][A
 50%|████▉     | 873/1759 [00:22<00:26, 34.02it/s][A
 50%|████▉     | 878/1759 [00:22<00:23, 37.06it/s][A
 50%|█████     | 883/1759 [00:23<00:22, 39.50it/s][A
 50%|█████     | 888/1759 [00:23<00:21, 41.42it/s][A
 51%|█████     | 893/1759 [00:23<00:20, 42.66it/s][A
 51%|█████     | 898/1759 [00:23<00:19, 43.73it/s][A
 51%|█████▏    | 903/1759 [00:23<00:19, 44.50it/s][A
 52%|█████▏    | 908/1759 [00:23<00:18, 45.19it/s][A
 52%|█████▏    | 913/1759 [00:23<00:18, 45.51it/s][A
 52%|█████▏    | 918/1759 [00:23<00:18, 45.78it/s][A
 52%|█████▏    | 923/1759 [00:23<00:18, 46.06it/s][A
 53%|█████▎    | 928/1759 [00:24<00:17, 46.28it/s][A
 53%|█████▎    | 933/1759 [00:24<00:17, 46.30it/s][A
 53%|█████▎    | 938/1759 [00:24<00:17, 46.31it/s][A
 54%|█████▎    | 943/1759 [00:24<00:17, 46.30it/s][A
 54%|█████▍    | 948/1759 [00:24<00:17, 46.35it/s][A
 54%|█████▍    | 953/1759 [00:24<00:17, 46.37it/s][A
 54%|█████▍    | 958/1759 [00:24<00:17, 46.45it/s][A
 55%|█████▍    | 963/1759 [00:24<00:17, 46.40it/s][A
 55%|█████▌    | 968/1759 [00:24<00:17, 46.45it/s][A
 55%|█████▌    | 973/1759 [00:24<00:16, 46.60it/s][A
 56%|█████▌    | 978/1759 [00:25<00:16, 46.55it/s][A
 56%|█████▌    | 983/1759 [00:25<00:16, 46.46it/s][A
 56%|█████▌    | 988/1759 [00:25<00:16, 46.51it/s][A
 56%|█████▋    | 993/1759 [00:25<00:16, 46.48it/s][A
 57%|█████▋    | 998/1759 [00:25<00:16, 46.50it/s][A
 57%|█████▋    | 1003/1759 [00:25<00:16, 46.50it/s][A
 57%|█████▋    | 1008/1759 [00:25<00:16, 46.45it/s][A
 58%|█████▊    | 1013/1759 [00:25<00:16, 46.53it/s][A
 58%|█████▊    | 1018/1759 [00:25<00:15, 46.52it/s][A
 58%|█████▊    | 1023/1759 [00:26<00:15, 46.55it/s][A
 58%|█████▊    | 1028/1759 [00:26<00:15, 46.56it/s][A
 59%|█████▊    | 1033/1759 [00:26<00:15, 46.55it/s][A
 59%|█████▉    | 1038/1759 [00:26<00:15, 46.47it/s][A
 59%|█████▉    | 1043/1759 [00:26<00:15, 46.44it/s][A
 60%|█████▉    | 1048/1759 [00:26<00:15, 46.31it/s][A
 60%|█████▉    | 1053/1759 [00:26<00:15, 46.37it/s][A
 60%|██████    | 1058/1759 [00:26<00:15, 46.28it/s][A
 60%|██████    | 1063/1759 [00:26<00:15, 46.30it/s][A
 61%|██████    | 1068/1759 [00:27<00:14, 46.29it/s][A
 61%|██████    | 1073/1759 [00:27<00:14, 46.25it/s][A
 61%|██████▏   | 1078/1759 [00:27<00:14, 46.35it/s][A
 62%|██████▏   | 1083/1759 [00:27<00:14, 46.40it/s][A
 62%|██████▏   | 1088/1759 [00:27<00:14, 46.39it/s][A
 62%|██████▏   | 1093/1759 [00:27<00:14, 46.44it/s][A
 62%|██████▏   | 1098/1759 [00:27<00:14, 46.46it/s][A
 63%|██████▎   | 1103/1759 [00:27<00:14, 46.44it/s][A
 63%|██████▎   | 1108/1759 [00:27<00:14, 46.40it/s][A
 63%|██████▎   | 1113/1759 [00:28<00:15, 42.21it/s][A
 64%|██████▎   | 1118/1759 [00:28<00:14, 43.51it/s][A
 64%|██████▍   | 1123/1759 [00:28<00:14, 44.44it/s][A
 64%|██████▍   | 1128/1759 [00:28<00:14, 45.05it/s][A
 64%|██████▍   | 1133/1759 [00:28<00:13, 45.59it/s][A
 65%|██████▍   | 1138/1759 [00:28<00:13, 45.93it/s][A
 65%|██████▍   | 1143/1759 [00:28<00:13, 46.17it/s][A
 65%|██████▌   | 1148/1759 [00:28<00:13, 46.38it/s][A
 66%|██████▌   | 1153/1759 [00:28<00:13, 45.90it/s][A
 66%|██████▌   | 1158/1759 [00:29<00:13, 46.03it/s][A
 66%|██████▌   | 1163/1759 [00:29<00:12, 46.13it/s][A
 66%|██████▋   | 1168/1759 [00:29<00:12, 46.29it/s][A
 67%|██████▋   | 1173/1759 [00:29<00:12, 46.36it/s][A
 67%|██████▋   | 1178/1759 [00:29<00:12, 46.45it/s][A
 67%|██████▋   | 1183/1759 [00:29<00:12, 46.55it/s][A
 68%|██████▊   | 1188/1759 [00:29<00:12, 46.64it/s][A
 68%|██████▊   | 1193/1759 [00:29<00:12, 46.53it/s][A
 68%|██████▊   | 1198/1759 [00:29<00:12, 46.36it/s][A
 68%|██████▊   | 1203/1759 [00:29<00:12, 46.25it/s][A
 69%|██████▊   | 1208/1759 [00:30<00:11, 46.28it/s][A
 69%|██████▉   | 1213/1759 [00:30<00:11, 46.39it/s][A
 69%|██████▉   | 1218/1759 [00:30<00:11, 46.42it/s][A
 70%|██████▉   | 1223/1759 [00:30<00:11, 46.27it/s][A
 70%|██████▉   | 1228/1759 [00:30<00:11, 46.46it/s][A
 70%|███████   | 1233/1759 [00:30<00:11, 46.53it/s][A
 70%|███████   | 1238/1759 [00:30<00:11, 46.62it/s][A
 71%|███████   | 1243/1759 [00:30<00:11, 46.54it/s][A
 71%|███████   | 1248/1759 [00:31<00:11, 46.43it/s][A
 71%|███████   | 1253/1759 [00:31<00:12, 39.39it/s][A
 72%|███████▏  | 1258/1759 [00:31<00:12, 41.37it/s][A
 72%|███████▏  | 1263/1759 [00:31<00:11, 42.86it/s][A
 72%|███████▏  | 1268/1759 [00:31<00:11, 43.86it/s][A
 72%|███████▏  | 1273/1759 [00:31<00:10, 44.67it/s][A
 73%|███████▎  | 1278/1759 [00:31<00:10, 45.29it/s][A
 73%|███████▎  | 1283/1759 [00:31<00:10, 45.63it/s][A
 73%|███████▎  | 1288/1759 [00:31<00:10, 46.00it/s][A
 74%|███████▎  | 1293/1759 [00:31<00:10, 45.90it/s][A
 74%|███████▍  | 1298/1759 [00:32<00:10, 46.05it/s][A
 74%|███████▍  | 1303/1759 [00:32<00:09, 46.20it/s][A
 74%|███████▍  | 1308/1759 [00:32<00:09, 46.30it/s][A
 75%|███████▍  | 1313/1759 [00:32<00:09, 46.47it/s][A
 75%|███████▍  | 1318/1759 [00:32<00:09, 46.38it/s][A
 75%|███████▌  | 1323/1759 [00:32<00:09, 46.45it/s][A
 75%|███████▌  | 1328/1759 [00:32<00:09, 46.53it/s][A
 76%|███████▌  | 1333/1759 [00:32<00:09, 46.51it/s][A
 76%|███████▌  | 1338/1759 [00:32<00:09, 46.44it/s][A
 76%|███████▋  | 1343/1759 [00:33<00:08, 46.40it/s][A
 77%|███████▋  | 1348/1759 [00:33<00:08, 46.39it/s][A
 77%|███████▋  | 1353/1759 [00:33<00:08, 46.44it/s][A
 77%|███████▋  | 1358/1759 [00:33<00:08, 46.41it/s][A
 77%|███████▋  | 1363/1759 [00:33<00:08, 46.46it/s][A
 78%|███████▊  | 1368/1759 [00:33<00:08, 46.59it/s][A
 78%|███████▊  | 1373/1759 [00:33<00:08, 46.60it/s][A
 78%|███████▊  | 1378/1759 [00:33<00:08, 46.54it/s][A
 79%|███████▊  | 1383/1759 [00:33<00:08, 46.54it/s][A
 79%|███████▉  | 1388/1759 [00:34<00:07, 46.45it/s][A
 79%|███████▉  | 1393/1759 [00:34<00:08, 45.34it/s][A
 79%|███████▉  | 1398/1759 [00:34<00:07, 45.75it/s][A
 80%|███████▉  | 1403/1759 [00:34<00:07, 45.96it/s][A
 80%|████████  | 1408/1759 [00:34<00:07, 46.10it/s][A
 80%|████████  | 1413/1759 [00:34<00:07, 46.34it/s][A
 81%|████████  | 1418/1759 [00:34<00:07, 46.45it/s][A
 81%|████████  | 1423/1759 [00:34<00:07, 46.38it/s][A
 81%|████████  | 1428/1759 [00:34<00:07, 46.39it/s][A
 81%|████████▏ | 1433/1759 [00:35<00:07, 46.30it/s][A
 82%|████████▏ | 1438/1759 [00:35<00:06, 46.32it/s][A
 82%|████████▏ | 1443/1759 [00:35<00:06, 46.38it/s][A
 82%|████████▏ | 1448/1759 [00:35<00:06, 46.32it/s][A
 83%|████████▎ | 1453/1759 [00:35<00:06, 46.44it/s][A
 83%|████████▎ | 1458/1759 [00:35<00:06, 46.46it/s][A
 83%|████████▎ | 1463/1759 [00:35<00:06, 46.54it/s][A
 83%|████████▎ | 1468/1759 [00:35<00:06, 46.47it/s][A
 84%|████████▎ | 1473/1759 [00:35<00:06, 46.43it/s][A
 84%|████████▍ | 1478/1759 [00:35<00:06, 46.43it/s][A
 84%|████████▍ | 1483/1759 [00:36<00:05, 46.42it/s][A
 85%|████████▍ | 1488/1759 [00:36<00:05, 46.45it/s][A
 85%|████████▍ | 1493/1759 [00:36<00:05, 46.51it/s][A
 85%|████████▌ | 1498/1759 [00:36<00:05, 46.46it/s][A
 85%|████████▌ | 1503/1759 [00:36<00:05, 46.51it/s][A
 86%|████████▌ | 1508/1759 [00:36<00:05, 46.49it/s][A
 86%|████████▌ | 1513/1759 [00:36<00:05, 46.46it/s][A
 86%|████████▋ | 1518/1759 [00:36<00:05, 46.45it/s][A
 87%|████████▋ | 1523/1759 [00:36<00:05, 46.49it/s][A
 87%|████████▋ | 1528/1759 [00:37<00:04, 46.39it/s][A
 87%|████████▋ | 1533/1759 [00:37<00:05, 44.06it/s][A
 87%|████████▋ | 1538/1759 [00:37<00:04, 44.76it/s][A
 88%|████████▊ | 1543/1759 [00:37<00:04, 45.28it/s][A
 88%|████████▊ | 1548/1759 [00:37<00:04, 45.76it/s][A
 88%|████████▊ | 1553/1759 [00:37<00:04, 46.04it/s][A
 89%|████████▊ | 1558/1759 [00:37<00:04, 46.18it/s][A
 89%|████████▉ | 1563/1759 [00:37<00:04, 46.28it/s][A
 89%|████████▉ | 1568/1759 [00:37<00:04, 46.21it/s][A
 89%|████████▉ | 1573/1759 [00:38<00:04, 46.14it/s][A
 90%|████████▉ | 1578/1759 [00:38<00:03, 46.17it/s][A
 90%|████████▉ | 1583/1759 [00:38<00:03, 46.31it/s][A
 90%|█████████ | 1588/1759 [00:38<00:03, 46.39it/s][A
 91%|█████████ | 1593/1759 [00:38<00:03, 46.51it/s][A
 91%|█████████ | 1598/1759 [00:38<00:03, 46.54it/s][A
 91%|█████████ | 1603/1759 [00:38<00:03, 46.51it/s][A
 91%|█████████▏| 1608/1759 [00:38<00:03, 46.54it/s][A
 92%|█████████▏| 1613/1759 [00:38<00:03, 46.52it/s][A
 92%|█████████▏| 1618/1759 [00:39<00:03, 46.30it/s][A
 92%|█████████▏| 1623/1759 [00:39<00:02, 46.31it/s][A
 93%|█████████▎| 1628/1759 [00:39<00:02, 46.32it/s][A
 93%|█████████▎| 1633/1759 [00:39<00:02, 46.35it/s][A
 93%|█████████▎| 1638/1759 [00:39<00:02, 46.49it/s][A
 93%|█████████▎| 1643/1759 [00:39<00:02, 46.52it/s][A
 94%|█████████▎| 1648/1759 [00:39<00:02, 46.56it/s][A
 94%|█████████▍| 1653/1759 [00:39<00:02, 46.52it/s][A
 94%|█████████▍| 1658/1759 [00:39<00:02, 46.48it/s][A
 95%|█████████▍| 1663/1759 [00:39<00:02, 46.45it/s][A
 95%|█████████▍| 1668/1759 [00:40<00:01, 46.32it/s][A
 95%|█████████▌| 1673/1759 [00:40<00:02, 38.49it/s][A
 95%|█████████▌| 1678/1759 [00:40<00:01, 40.62it/s][A
 96%|█████████▌| 1683/1759 [00:40<00:01, 42.29it/s][A
 96%|█████████▌| 1688/1759 [00:40<00:01, 43.53it/s][A
 96%|█████████▌| 1693/1759 [00:40<00:01, 44.37it/s][A
 97%|█████████▋| 1698/1759 [00:40<00:01, 45.05it/s][A
 97%|█████████▋| 1703/1759 [00:40<00:01, 45.52it/s][A
 97%|█████████▋| 1708/1759 [00:41<00:01, 45.65it/s][A
 97%|█████████▋| 1713/1759 [00:41<00:01, 45.91it/s][A
 98%|█████████▊| 1718/1759 [00:41<00:00, 46.05it/s][A
 98%|█████████▊| 1723/1759 [00:41<00:00, 46.09it/s][A
 98%|█████████▊| 1728/1759 [00:41<00:00, 46.21it/s][A
 99%|█████████▊| 1733/1759 [00:41<00:00, 46.41it/s][A
 99%|█████████▉| 1738/1759 [00:41<00:00, 46.44it/s][A
 99%|█████████▉| 1743/1759 [00:41<00:00, 46.49it/s][A
 99%|█████████▉| 1748/1759 [00:41<00:00, 46.54it/s][A
100%|█████████▉| 1753/1759 [00:41<00:00, 46.46it/s][A
100%|█████████▉| 1758/1759 [00:42<00:00, 46.38it/s][A                                                 
                                                   [A100%|██████████| 390/390 [06:24<00:00,  3.46it/s]
100%|██████████| 1759/1759 [00:42<00:00, 46.38it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:04:51,907 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 18:04:52,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:04:57,557 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:04:57,947 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:04:58,197 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:05:10,553 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:05:10,749 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78 (score: 0.8789819478988647).
                                                 100%|██████████| 390/390 [06:49<00:00,  3.46it/s]100%|██████████| 390/390 [06:49<00:00,  1.05s/it]
[INFO|trainer.py:1894] 2023-08-28 18:05:16,368 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 18:05:16,405 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:05:22,513 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:05:22,539 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:05:22,548 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:05:23,211 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,211 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,211 >>   train_loss               =     0.4986
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,211 >>   train_runtime            = 0:06:49.19
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,211 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,211 >>   train_samples_per_second =     61.084
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:23,212 >>   train_steps_per_second   =      0.953
{'eval_loss': 0.91712486743927, 'eval_runtime': 42.1302, 'eval_samples_per_second': 333.893, 'eval_steps_per_second': 41.751, 'epoch': 4.99}
{'train_runtime': 409.1923, 'train_samples_per_second': 61.084, 'train_steps_per_second': 0.953, 'train_loss': 0.49863019111828927, 'epoch': 4.99}
08/28/2023 18:05:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:05:23,314 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:05:23,314 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 18:05:23,315 >>   Batch size = 8
  0%|          | 0/1759 [00:00<?, ?it/s]  0%|          | 6/1759 [00:00<00:29, 58.68it/s]  1%|          | 12/1759 [00:00<00:33, 51.52it/s]  1%|          | 18/1759 [00:00<00:35, 49.46it/s]  1%|▏         | 23/1759 [00:00<00:35, 48.73it/s]  2%|▏         | 28/1759 [00:00<00:35, 48.25it/s]  2%|▏         | 33/1759 [00:00<00:36, 47.89it/s]  2%|▏         | 38/1759 [00:00<00:36, 47.70it/s]  2%|▏         | 43/1759 [00:00<00:36, 47.53it/s]  3%|▎         | 48/1759 [00:00<00:36, 47.20it/s]  3%|▎         | 53/1759 [00:01<00:36, 47.23it/s]  3%|▎         | 58/1759 [00:01<00:35, 47.26it/s]  4%|▎         | 63/1759 [00:01<00:35, 47.19it/s]  4%|▍         | 68/1759 [00:01<00:35, 47.15it/s]  4%|▍         | 73/1759 [00:01<00:35, 47.05it/s]  4%|▍         | 78/1759 [00:01<00:35, 47.05it/s]  5%|▍         | 83/1759 [00:01<00:35, 47.08it/s]  5%|▌         | 88/1759 [00:01<00:35, 47.11it/s]  5%|▌         | 93/1759 [00:01<00:35, 46.99it/s]  6%|▌         | 98/1759 [00:02<00:35, 46.83it/s]  6%|▌         | 103/1759 [00:02<00:35, 46.99it/s]  6%|▌         | 108/1759 [00:02<00:35, 47.02it/s]  6%|▋         | 113/1759 [00:02<00:35, 46.09it/s]  7%|▋         | 118/1759 [00:02<00:35, 46.48it/s]  7%|▋         | 123/1759 [00:02<00:35, 46.56it/s]  7%|▋         | 128/1759 [00:02<00:34, 46.65it/s]  8%|▊         | 133/1759 [00:02<00:34, 46.78it/s]  8%|▊         | 138/1759 [00:02<00:34, 46.74it/s]  8%|▊         | 143/1759 [00:03<00:34, 46.84it/s]  8%|▊         | 148/1759 [00:03<00:34, 46.95it/s]  9%|▊         | 153/1759 [00:03<00:34, 46.87it/s]  9%|▉         | 158/1759 [00:03<00:34, 46.96it/s]  9%|▉         | 163/1759 [00:03<00:33, 47.04it/s] 10%|▉         | 168/1759 [00:03<00:33, 47.04it/s] 10%|▉         | 173/1759 [00:03<00:33, 46.97it/s] 10%|█         | 178/1759 [00:03<00:33, 47.04it/s] 10%|█         | 183/1759 [00:03<00:33, 46.99it/s] 11%|█         | 188/1759 [00:03<00:33, 46.93it/s] 11%|█         | 193/1759 [00:04<00:33, 46.98it/s] 11%|█▏        | 198/1759 [00:04<00:33, 46.75it/s] 12%|█▏        | 203/1759 [00:04<00:33, 46.82it/s] 12%|█▏        | 208/1759 [00:04<00:33, 46.88it/s] 12%|█▏        | 213/1759 [00:04<00:32, 46.93it/s] 12%|█▏        | 218/1759 [00:04<00:32, 47.02it/s] 13%|█▎        | 223/1759 [00:04<00:32, 47.11it/s] 13%|█▎        | 228/1759 [00:04<00:32, 47.07it/s] 13%|█▎        | 233/1759 [00:04<00:32, 46.98it/s] 14%|█▎        | 238/1759 [00:05<00:32, 47.02it/s] 14%|█▍        | 243/1759 [00:05<00:32, 47.01it/s] 14%|█▍        | 248/1759 [00:05<00:32, 46.93it/s] 14%|█▍        | 253/1759 [00:05<00:31, 47.07it/s] 15%|█▍        | 258/1759 [00:05<00:37, 40.02it/s] 15%|█▍        | 263/1759 [00:05<00:35, 41.99it/s] 15%|█▌        | 268/1759 [00:05<00:34, 43.47it/s] 16%|█▌        | 273/1759 [00:05<00:33, 44.57it/s] 16%|█▌        | 278/1759 [00:05<00:32, 45.39it/s] 16%|█▌        | 283/1759 [00:06<00:32, 45.94it/s] 16%|█▋        | 288/1759 [00:06<00:31, 46.22it/s] 17%|█▋        | 293/1759 [00:06<00:31, 46.55it/s] 17%|█▋        | 298/1759 [00:06<00:31, 46.38it/s] 17%|█▋        | 303/1759 [00:06<00:31, 46.47it/s] 18%|█▊        | 308/1759 [00:06<00:31, 46.61it/s] 18%|█▊        | 313/1759 [00:06<00:30, 46.83it/s] 18%|█▊        | 318/1759 [00:06<00:30, 46.85it/s] 18%|█▊        | 323/1759 [00:06<00:30, 47.03it/s] 19%|█▊        | 328/1759 [00:07<00:30, 47.09it/s] 19%|█▉        | 333/1759 [00:07<00:30, 47.09it/s] 19%|█▉        | 338/1759 [00:07<00:30, 47.17it/s] 19%|█▉        | 343/1759 [00:07<00:30, 47.02it/s] 20%|█▉        | 348/1759 [00:07<00:30, 46.82it/s] 20%|██        | 353/1759 [00:07<00:30, 46.77it/s] 20%|██        | 358/1759 [00:07<00:29, 46.87it/s] 21%|██        | 363/1759 [00:07<00:29, 47.01it/s] 21%|██        | 368/1759 [00:07<00:29, 47.09it/s] 21%|██        | 373/1759 [00:07<00:29, 47.12it/s] 21%|██▏       | 378/1759 [00:08<00:29, 47.01it/s] 22%|██▏       | 383/1759 [00:08<00:29, 47.16it/s] 22%|██▏       | 388/1759 [00:08<00:29, 47.05it/s] 22%|██▏       | 393/1759 [00:08<00:29, 47.06it/s] 23%|██▎       | 398/1759 [00:08<00:33, 41.06it/s] 23%|██▎       | 403/1759 [00:08<00:31, 42.64it/s] 23%|██▎       | 408/1759 [00:08<00:30, 43.73it/s] 23%|██▎       | 413/1759 [00:08<00:30, 44.73it/s] 24%|██▍       | 418/1759 [00:08<00:29, 45.49it/s] 24%|██▍       | 423/1759 [00:09<00:29, 46.06it/s] 24%|██▍       | 428/1759 [00:09<00:28, 46.42it/s] 25%|██▍       | 433/1759 [00:09<00:28, 46.64it/s] 25%|██▍       | 438/1759 [00:09<00:28, 46.40it/s] 25%|██▌       | 443/1759 [00:09<00:28, 46.47it/s] 25%|██▌       | 448/1759 [00:09<00:28, 46.73it/s] 26%|██▌       | 453/1759 [00:09<00:27, 46.84it/s] 26%|██▌       | 458/1759 [00:09<00:27, 46.95it/s] 26%|██▋       | 463/1759 [00:09<00:27, 47.05it/s] 27%|██▋       | 468/1759 [00:10<00:27, 47.05it/s] 27%|██▋       | 473/1759 [00:10<00:27, 47.09it/s] 27%|██▋       | 478/1759 [00:10<00:27, 47.14it/s] 27%|██▋       | 483/1759 [00:10<00:27, 47.00it/s] 28%|██▊       | 488/1759 [00:10<00:27, 46.97it/s] 28%|██▊       | 493/1759 [00:10<00:26, 47.01it/s] 28%|██▊       | 498/1759 [00:10<00:26, 46.98it/s] 29%|██▊       | 503/1759 [00:10<00:26, 46.99it/s] 29%|██▉       | 508/1759 [00:10<00:26, 47.01it/s] 29%|██▉       | 513/1759 [00:11<00:26, 47.12it/s] 29%|██▉       | 518/1759 [00:11<00:26, 47.14it/s] 30%|██▉       | 523/1759 [00:11<00:26, 47.14it/s] 30%|███       | 528/1759 [00:11<00:26, 46.98it/s] 30%|███       | 533/1759 [00:11<00:26, 46.92it/s] 31%|███       | 538/1759 [00:11<00:26, 46.88it/s] 31%|███       | 543/1759 [00:11<00:25, 47.00it/s] 31%|███       | 548/1759 [00:11<00:25, 47.00it/s] 31%|███▏      | 553/1759 [00:11<00:25, 46.96it/s] 32%|███▏      | 558/1759 [00:11<00:25, 46.89it/s] 32%|███▏      | 563/1759 [00:12<00:25, 46.96it/s] 32%|███▏      | 568/1759 [00:12<00:25, 46.99it/s] 33%|███▎      | 573/1759 [00:12<00:25, 47.02it/s] 33%|███▎      | 578/1759 [00:12<00:25, 46.93it/s] 33%|███▎      | 583/1759 [00:12<00:25, 46.83it/s] 33%|███▎      | 588/1759 [00:12<00:25, 46.82it/s] 34%|███▎      | 593/1759 [00:12<00:24, 46.89it/s] 34%|███▍      | 598/1759 [00:12<00:24, 46.87it/s] 34%|███▍      | 603/1759 [00:12<00:24, 47.01it/s] 35%|███▍      | 608/1759 [00:13<00:24, 47.04it/s] 35%|███▍      | 613/1759 [00:13<00:24, 46.94it/s] 35%|███▌      | 618/1759 [00:13<00:24, 46.91it/s] 35%|███▌      | 623/1759 [00:13<00:24, 46.87it/s] 36%|███▌      | 628/1759 [00:13<00:24, 46.82it/s] 36%|███▌      | 633/1759 [00:13<00:24, 46.91it/s] 36%|███▋      | 638/1759 [00:13<00:23, 46.90it/s] 37%|███▋      | 643/1759 [00:13<00:23, 46.87it/s] 37%|███▋      | 648/1759 [00:13<00:23, 46.69it/s] 37%|███▋      | 653/1759 [00:13<00:23, 46.90it/s] 37%|███▋      | 658/1759 [00:14<00:23, 46.94it/s] 38%|███▊      | 663/1759 [00:14<00:23, 46.95it/s] 38%|███▊      | 668/1759 [00:14<00:23, 47.05it/s] 38%|███▊      | 673/1759 [00:14<00:23, 46.93it/s] 39%|███▊      | 678/1759 [00:14<00:22, 47.04it/s] 39%|███▉      | 683/1759 [00:14<00:22, 46.86it/s] 39%|███▉      | 688/1759 [00:14<00:22, 46.78it/s] 39%|███▉      | 693/1759 [00:14<00:22, 46.88it/s] 40%|███▉      | 698/1759 [00:14<00:22, 46.91it/s] 40%|███▉      | 703/1759 [00:15<00:22, 46.87it/s] 40%|████      | 708/1759 [00:15<00:22, 46.81it/s] 41%|████      | 713/1759 [00:15<00:22, 46.83it/s] 41%|████      | 718/1759 [00:15<00:22, 46.83it/s] 41%|████      | 723/1759 [00:15<00:22, 46.91it/s] 41%|████▏     | 728/1759 [00:15<00:21, 46.87it/s] 42%|████▏     | 733/1759 [00:15<00:21, 46.72it/s] 42%|████▏     | 738/1759 [00:15<00:21, 46.79it/s] 42%|████▏     | 743/1759 [00:15<00:21, 46.66it/s] 43%|████▎     | 748/1759 [00:16<00:21, 46.75it/s] 43%|████▎     | 753/1759 [00:16<00:21, 46.80it/s] 43%|████▎     | 758/1759 [00:16<00:21, 46.90it/s] 43%|████▎     | 763/1759 [00:16<00:21, 46.83it/s] 44%|████▎     | 768/1759 [00:16<00:21, 46.93it/s] 44%|████▍     | 773/1759 [00:16<00:21, 46.82it/s] 44%|████▍     | 778/1759 [00:16<00:20, 46.87it/s] 45%|████▍     | 783/1759 [00:16<00:20, 46.83it/s] 45%|████▍     | 788/1759 [00:16<00:20, 46.79it/s] 45%|████▌     | 793/1759 [00:16<00:20, 46.76it/s] 45%|████▌     | 798/1759 [00:17<00:20, 46.87it/s] 46%|████▌     | 803/1759 [00:17<00:20, 46.83it/s] 46%|████▌     | 808/1759 [00:17<00:20, 46.84it/s] 46%|████▌     | 813/1759 [00:17<00:20, 46.82it/s] 47%|████▋     | 818/1759 [00:17<00:20, 46.85it/s] 47%|████▋     | 823/1759 [00:17<00:19, 46.85it/s] 47%|████▋     | 828/1759 [00:17<00:19, 46.90it/s] 47%|████▋     | 833/1759 [00:17<00:19, 46.84it/s] 48%|████▊     | 838/1759 [00:17<00:19, 46.73it/s] 48%|████▊     | 843/1759 [00:18<00:19, 46.84it/s] 48%|████▊     | 848/1759 [00:18<00:19, 46.81it/s] 48%|████▊     | 853/1759 [00:18<00:19, 46.93it/s] 49%|████▉     | 858/1759 [00:18<00:19, 46.82it/s] 49%|████▉     | 863/1759 [00:18<00:19, 46.82it/s] 49%|████▉     | 868/1759 [00:18<00:19, 46.72it/s] 50%|████▉     | 873/1759 [00:18<00:18, 46.76it/s] 50%|████▉     | 878/1759 [00:18<00:18, 46.70it/s] 50%|█████     | 883/1759 [00:18<00:18, 46.76it/s] 50%|█████     | 888/1759 [00:19<00:18, 46.77it/s] 51%|█████     | 893/1759 [00:19<00:18, 46.67it/s] 51%|█████     | 898/1759 [00:19<00:18, 46.58it/s] 51%|█████▏    | 903/1759 [00:19<00:18, 46.63it/s] 52%|█████▏    | 908/1759 [00:19<00:18, 46.71it/s] 52%|█████▏    | 913/1759 [00:19<00:18, 46.65it/s] 52%|█████▏    | 918/1759 [00:19<00:18, 46.72it/s] 52%|█████▏    | 923/1759 [00:19<00:17, 46.69it/s] 53%|█████▎    | 928/1759 [00:19<00:17, 46.66it/s] 53%|█████▎    | 933/1759 [00:19<00:17, 46.71it/s] 53%|█████▎    | 938/1759 [00:20<00:17, 46.82it/s] 54%|█████▎    | 943/1759 [00:20<00:17, 46.88it/s] 54%|█████▍    | 948/1759 [00:20<00:17, 46.90it/s] 54%|█████▍    | 953/1759 [00:20<00:17, 46.85it/s] 54%|█████▍    | 958/1759 [00:20<00:17, 46.77it/s] 55%|█████▍    | 963/1759 [00:20<00:17, 46.76it/s] 55%|█████▌    | 968/1759 [00:20<00:16, 46.81it/s] 55%|█████▌    | 973/1759 [00:20<00:16, 46.68it/s] 56%|█████▌    | 978/1759 [00:20<00:16, 46.72it/s] 56%|█████▌    | 983/1759 [00:21<00:16, 46.78it/s] 56%|█████▌    | 988/1759 [00:21<00:16, 46.68it/s] 56%|█████▋    | 993/1759 [00:21<00:16, 46.80it/s] 57%|█████▋    | 998/1759 [00:21<00:16, 46.86it/s] 57%|█████▋    | 1003/1759 [00:21<00:16, 46.82it/s] 57%|█████▋    | 1008/1759 [00:21<00:16, 46.80it/s] 58%|█████▊    | 1013/1759 [00:21<00:15, 46.75it/s] 58%|█████▊    | 1018/1759 [00:21<00:15, 46.69it/s] 58%|█████▊    | 1023/1759 [00:21<00:15, 46.68it/s] 58%|█████▊    | 1028/1759 [00:21<00:15, 46.79it/s] 59%|█████▊    | 1033/1759 [00:22<00:15, 46.77it/s] 59%|█████▉    | 1038/1759 [00:22<00:16, 44.83it/s] 59%|█████▉    | 1043/1759 [00:22<00:15, 45.48it/s] 60%|█████▉    | 1048/1759 [00:22<00:15, 45.81it/s] 60%|█████▉    | 1053/1759 [00:22<00:15, 46.19it/s] 60%|██████    | 1058/1759 [00:22<00:15, 46.49it/s] 60%|██████    | 1063/1759 [00:22<00:14, 46.54it/s] 61%|██████    | 1068/1759 [00:22<00:14, 46.60it/s] 61%|██████    | 1073/1759 [00:22<00:14, 46.68it/s] 61%|██████▏   | 1078/1759 [00:23<00:14, 46.49it/s] 62%|██████▏   | 1083/1759 [00:23<00:14, 46.48it/s] 62%|██████▏   | 1088/1759 [00:23<00:14, 46.57it/s] 62%|██████▏   | 1093/1759 [00:23<00:14, 46.65it/s] 62%|██████▏   | 1098/1759 [00:23<00:14, 46.73it/s] 63%|██████▎   | 1103/1759 [00:23<00:14, 46.76it/s] 63%|██████▎   | 1108/1759 [00:23<00:13, 46.70it/s] 63%|██████▎   | 1113/1759 [00:23<00:13, 46.66it/s] 64%|██████▎   | 1118/1759 [00:23<00:13, 46.67it/s] 64%|██████▍   | 1123/1759 [00:24<00:13, 46.56it/s] 64%|██████▍   | 1128/1759 [00:24<00:13, 46.59it/s] 64%|██████▍   | 1133/1759 [00:24<00:13, 46.57it/s] 65%|██████▍   | 1138/1759 [00:24<00:13, 46.59it/s] 65%|██████▍   | 1143/1759 [00:24<00:13, 46.51it/s] 65%|██████▌   | 1148/1759 [00:24<00:13, 46.66it/s] 66%|██████▌   | 1153/1759 [00:24<00:12, 46.73it/s] 66%|██████▌   | 1158/1759 [00:24<00:12, 46.71it/s] 66%|██████▌   | 1163/1759 [00:24<00:12, 46.62it/s] 66%|██████▋   | 1168/1759 [00:25<00:12, 46.57it/s] 67%|██████▋   | 1173/1759 [00:25<00:12, 46.36it/s] 67%|██████▋   | 1178/1759 [00:25<00:13, 43.79it/s] 67%|██████▋   | 1183/1759 [00:25<00:12, 44.71it/s] 68%|██████▊   | 1188/1759 [00:25<00:12, 45.35it/s] 68%|██████▊   | 1193/1759 [00:25<00:12, 45.81it/s] 68%|██████▊   | 1198/1759 [00:25<00:12, 46.08it/s] 68%|██████▊   | 1203/1759 [00:25<00:12, 46.15it/s] 69%|██████▊   | 1208/1759 [00:25<00:11, 46.43it/s] 69%|██████▉   | 1213/1759 [00:25<00:11, 46.62it/s] 69%|██████▉   | 1218/1759 [00:26<00:11, 46.21it/s] 70%|██████▉   | 1223/1759 [00:26<00:11, 46.29it/s] 70%|██████▉   | 1228/1759 [00:26<00:11, 46.40it/s] 70%|███████   | 1233/1759 [00:26<00:11, 46.44it/s] 70%|███████   | 1238/1759 [00:26<00:11, 46.59it/s] 71%|███████   | 1243/1759 [00:26<00:11, 46.58it/s] 71%|███████   | 1248/1759 [00:26<00:10, 46.69it/s] 71%|███████   | 1253/1759 [00:26<00:10, 46.62it/s] 72%|███████▏  | 1258/1759 [00:26<00:10, 46.67it/s] 72%|███████▏  | 1263/1759 [00:27<00:10, 46.61it/s] 72%|███████▏  | 1268/1759 [00:27<00:10, 46.49it/s] 72%|███████▏  | 1273/1759 [00:27<00:10, 46.41it/s] 73%|███████▎  | 1278/1759 [00:27<00:10, 46.59it/s] 73%|███████▎  | 1283/1759 [00:27<00:10, 46.59it/s] 73%|███████▎  | 1288/1759 [00:27<00:10, 46.63it/s] 74%|███████▎  | 1293/1759 [00:27<00:09, 46.65it/s] 74%|███████▍  | 1298/1759 [00:27<00:09, 46.54it/s] 74%|███████▍  | 1303/1759 [00:27<00:09, 46.63it/s] 74%|███████▍  | 1308/1759 [00:28<00:09, 46.57it/s] 75%|███████▍  | 1313/1759 [00:28<00:09, 46.50it/s] 75%|███████▍  | 1318/1759 [00:28<00:09, 45.98it/s] 75%|███████▌  | 1323/1759 [00:28<00:09, 46.12it/s] 75%|███████▌  | 1328/1759 [00:28<00:09, 46.25it/s] 76%|███████▌  | 1333/1759 [00:28<00:09, 46.44it/s] 76%|███████▌  | 1338/1759 [00:28<00:09, 46.38it/s] 76%|███████▋  | 1343/1759 [00:28<00:08, 46.52it/s] 77%|███████▋  | 1348/1759 [00:28<00:08, 46.56it/s] 77%|███████▋  | 1353/1759 [00:29<00:08, 46.59it/s] 77%|███████▋  | 1358/1759 [00:29<00:08, 46.49it/s] 77%|███████▋  | 1363/1759 [00:29<00:08, 46.44it/s] 78%|███████▊  | 1368/1759 [00:29<00:08, 46.51it/s] 78%|███████▊  | 1373/1759 [00:29<00:08, 46.62it/s] 78%|███████▊  | 1378/1759 [00:29<00:08, 46.60it/s] 79%|███████▊  | 1383/1759 [00:29<00:08, 46.48it/s] 79%|███████▉  | 1388/1759 [00:29<00:07, 46.60it/s] 79%|███████▉  | 1393/1759 [00:29<00:07, 46.61it/s] 79%|███████▉  | 1398/1759 [00:29<00:07, 46.50it/s] 80%|███████▉  | 1403/1759 [00:30<00:07, 46.41it/s] 80%|████████  | 1408/1759 [00:30<00:07, 46.55it/s] 80%|████████  | 1413/1759 [00:30<00:07, 46.64it/s] 81%|████████  | 1418/1759 [00:30<00:07, 46.65it/s] 81%|████████  | 1423/1759 [00:30<00:07, 46.60it/s] 81%|████████  | 1428/1759 [00:30<00:07, 46.40it/s] 81%|████████▏ | 1433/1759 [00:30<00:07, 46.55it/s] 82%|████████▏ | 1438/1759 [00:30<00:06, 46.60it/s] 82%|████████▏ | 1443/1759 [00:30<00:06, 46.56it/s] 82%|████████▏ | 1448/1759 [00:31<00:06, 46.58it/s] 83%|████████▎ | 1453/1759 [00:31<00:06, 46.46it/s] 83%|████████▎ | 1458/1759 [00:31<00:06, 46.47it/s] 83%|████████▎ | 1463/1759 [00:31<00:06, 46.64it/s] 83%|████████▎ | 1468/1759 [00:31<00:06, 46.62it/s] 84%|████████▎ | 1473/1759 [00:31<00:06, 46.60it/s] 84%|████████▍ | 1478/1759 [00:31<00:06, 46.63it/s] 84%|████████▍ | 1483/1759 [00:31<00:05, 46.44it/s] 85%|████████▍ | 1488/1759 [00:31<00:05, 46.51it/s] 85%|████████▍ | 1493/1759 [00:32<00:05, 46.51it/s] 85%|████████▌ | 1498/1759 [00:32<00:05, 46.50it/s] 85%|████████▌ | 1503/1759 [00:32<00:05, 46.66it/s] 86%|████████▌ | 1508/1759 [00:32<00:05, 46.67it/s] 86%|████████▌ | 1513/1759 [00:32<00:05, 46.48it/s] 86%|████████▋ | 1518/1759 [00:32<00:05, 46.56it/s] 87%|████████▋ | 1523/1759 [00:32<00:05, 46.45it/s] 87%|████████▋ | 1528/1759 [00:32<00:04, 46.50it/s] 87%|████████▋ | 1533/1759 [00:32<00:04, 46.45it/s] 87%|████████▋ | 1538/1759 [00:32<00:04, 46.49it/s] 88%|████████▊ | 1543/1759 [00:33<00:04, 46.60it/s] 88%|████████▊ | 1548/1759 [00:33<00:04, 46.58it/s] 88%|████████▊ | 1553/1759 [00:33<00:04, 46.58it/s] 89%|████████▊ | 1558/1759 [00:33<00:04, 46.57it/s] 89%|████████▉ | 1563/1759 [00:33<00:04, 46.57it/s] 89%|████████▉ | 1568/1759 [00:33<00:04, 46.62it/s] 89%|████████▉ | 1573/1759 [00:33<00:03, 46.59it/s] 90%|████████▉ | 1578/1759 [00:33<00:03, 46.38it/s] 90%|████████▉ | 1583/1759 [00:33<00:03, 46.46it/s] 90%|█████████ | 1588/1759 [00:34<00:03, 46.54it/s] 91%|█████████ | 1593/1759 [00:34<00:03, 46.58it/s] 91%|█████████ | 1598/1759 [00:34<00:03, 44.25it/s] 91%|█████████ | 1603/1759 [00:34<00:03, 44.98it/s] 91%|█████████▏| 1608/1759 [00:34<00:03, 45.46it/s] 92%|█████████▏| 1613/1759 [00:34<00:03, 45.85it/s] 92%|█████████▏| 1618/1759 [00:34<00:03, 46.01it/s] 92%|█████████▏| 1623/1759 [00:34<00:02, 46.24it/s] 93%|█████████▎| 1628/1759 [00:34<00:02, 46.25it/s] 93%|█████████▎| 1633/1759 [00:35<00:02, 46.36it/s] 93%|█████████▎| 1638/1759 [00:35<00:02, 46.22it/s] 93%|█████████▎| 1643/1759 [00:35<00:02, 46.33it/s] 94%|█████████▎| 1648/1759 [00:35<00:02, 46.43it/s] 94%|█████████▍| 1653/1759 [00:35<00:02, 46.56it/s] 94%|█████████▍| 1658/1759 [00:35<00:02, 46.61it/s] 95%|█████████▍| 1663/1759 [00:35<00:02, 46.64it/s] 95%|█████████▍| 1668/1759 [00:35<00:01, 46.56it/s] 95%|█████████▌| 1673/1759 [00:35<00:01, 46.44it/s] 95%|█████████▌| 1678/1759 [00:36<00:01, 46.56it/s] 96%|█████████▌| 1683/1759 [00:36<00:01, 46.51it/s] 96%|█████████▌| 1688/1759 [00:36<00:01, 46.49it/s] 96%|█████████▌| 1693/1759 [00:36<00:01, 46.57it/s] 97%|█████████▋| 1698/1759 [00:36<00:01, 46.51it/s] 97%|█████████▋| 1703/1759 [00:36<00:01, 46.56it/s] 97%|█████████▋| 1708/1759 [00:36<00:01, 46.56it/s] 97%|█████████▋| 1713/1759 [00:36<00:00, 46.49it/s] 98%|█████████▊| 1718/1759 [00:36<00:00, 46.53it/s] 98%|█████████▊| 1723/1759 [00:36<00:00, 46.55it/s] 98%|█████████▊| 1728/1759 [00:37<00:00, 46.55it/s] 99%|█████████▊| 1733/1759 [00:37<00:00, 46.63it/s] 99%|█████████▉| 1738/1759 [00:37<00:00, 46.18it/s] 99%|█████████▉| 1743/1759 [00:37<00:00, 46.24it/s] 99%|█████████▉| 1748/1759 [00:37<00:00, 46.34it/s]100%|█████████▉| 1753/1759 [00:37<00:00, 46.44it/s]100%|█████████▉| 1758/1759 [00:37<00:00, 46.38it/s]100%|██████████| 1759/1759 [00:37<00:00, 46.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:06:01,092 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   eval_loss               =      0.879
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   eval_runtime            = 0:00:37.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   eval_samples            =      14067
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   eval_samples_per_second =    372.364
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   eval_steps_per_second   =     46.562
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:01,092 >>   perplexity              =     2.4084
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:11,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:11,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:11,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:11,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:11,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:06:11,680 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:06:11,681 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:12,258 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:13,311 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:13,311 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:16,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:16,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:16,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:16,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:16,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:06:16,931 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:06:16,932 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:17,490 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:17,646 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:17,646 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'labels': ['country', 'part of', 'platform', 'publisher', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 22024
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22124, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.43it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:10,  1.40it/s]Extractor Predicting: 17it [00:11,  1.40it/s]Extractor Predicting: 18it [00:12,  1.38it/s]Extractor Predicting: 19it [00:12,  1.41it/s]Extractor Predicting: 20it [00:13,  1.40it/s]Extractor Predicting: 21it [00:14,  1.39it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:15,  1.38it/s]Extractor Predicting: 24it [00:16,  1.41it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:17,  1.43it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.50it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.54it/s]Extractor Predicting: 31it [00:21,  1.56it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:23,  1.50it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.52it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.55it/s]Extractor Predicting: 39it [00:26,  1.54it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.57it/s]Extractor Predicting: 45it [00:30,  1.58it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:32,  1.48it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:34,  1.55it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:35,  1.58it/s]Extractor Predicting: 54it [00:36,  1.58it/s]Extractor Predicting: 55it [00:36,  1.59it/s]Extractor Predicting: 56it [00:37,  1.59it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.55it/s]Extractor Predicting: 60it [00:39,  1.57it/s]Extractor Predicting: 61it [00:40,  1.58it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.54it/s]Extractor Predicting: 65it [00:43,  1.53it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:44,  1.52it/s]Extractor Predicting: 68it [00:45,  1.47it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:47,  1.57it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:51,  1.46it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.51it/s]Extractor Predicting: 80it [00:53,  1.52it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:57,  1.53it/s]Extractor Predicting: 87it [00:57,  1.53it/s]Extractor Predicting: 88it [00:58,  1.44it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [01:00,  1.56it/s]Extractor Predicting: 92it [01:01,  1.51it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.62it/s]Extractor Predicting: 98it [01:04,  1.66it/s]Extractor Predicting: 99it [01:05,  1.62it/s]Extractor Predicting: 100it [01:05,  1.70it/s]Extractor Predicting: 101it [01:06,  1.65it/s]Extractor Predicting: 102it [01:07,  1.64it/s]Extractor Predicting: 103it [01:07,  1.65it/s]Extractor Predicting: 104it [01:08,  1.63it/s]Extractor Predicting: 105it [01:08,  1.66it/s]Extractor Predicting: 106it [01:09,  1.64it/s]Extractor Predicting: 107it [01:10,  1.62it/s]Extractor Predicting: 108it [01:10,  1.61it/s]Extractor Predicting: 109it [01:11,  1.61it/s]Extractor Predicting: 110it [01:12,  1.61it/s]Extractor Predicting: 111it [01:12,  1.63it/s]Extractor Predicting: 112it [01:13,  1.64it/s]Extractor Predicting: 113it [01:13,  1.64it/s]Extractor Predicting: 114it [01:14,  1.67it/s]Extractor Predicting: 115it [01:15,  1.66it/s]Extractor Predicting: 116it [01:15,  1.69it/s]Extractor Predicting: 117it [01:16,  1.67it/s]Extractor Predicting: 118it [01:16,  1.68it/s]Extractor Predicting: 119it [01:17,  1.68it/s]Extractor Predicting: 120it [01:17,  1.71it/s]Extractor Predicting: 121it [01:18,  1.68it/s]Extractor Predicting: 122it [01:19,  1.64it/s]Extractor Predicting: 123it [01:19,  1.64it/s]Extractor Predicting: 124it [01:20,  1.62it/s]Extractor Predicting: 125it [01:21,  1.63it/s]Extractor Predicting: 126it [01:21,  1.64it/s]Extractor Predicting: 127it [01:22,  1.70it/s]Extractor Predicting: 128it [01:22,  1.69it/s]Extractor Predicting: 129it [01:23,  1.70it/s]Extractor Predicting: 130it [01:24,  1.66it/s]Extractor Predicting: 131it [01:24,  1.64it/s]Extractor Predicting: 132it [01:25,  1.68it/s]Extractor Predicting: 133it [01:25,  1.63it/s]Extractor Predicting: 134it [01:26,  1.67it/s]Extractor Predicting: 135it [01:27,  1.67it/s]Extractor Predicting: 136it [01:27,  1.66it/s]Extractor Predicting: 137it [01:28,  1.66it/s]Extractor Predicting: 138it [01:28,  1.63it/s]Extractor Predicting: 139it [01:29,  1.62it/s]Extractor Predicting: 140it [01:30,  1.62it/s]Extractor Predicting: 141it [01:30,  1.64it/s]Extractor Predicting: 142it [01:31,  1.67it/s]Extractor Predicting: 143it [01:31,  1.70it/s]Extractor Predicting: 144it [01:32,  1.69it/s]Extractor Predicting: 145it [01:33,  1.63it/s]Extractor Predicting: 146it [01:33,  1.61it/s]Extractor Predicting: 147it [01:34,  1.62it/s]Extractor Predicting: 148it [01:35,  1.60it/s]Extractor Predicting: 149it [01:35,  1.59it/s]Extractor Predicting: 150it [01:36,  1.60it/s]Extractor Predicting: 151it [01:37,  1.54it/s]Extractor Predicting: 152it [01:37,  1.49it/s]Extractor Predicting: 153it [01:38,  1.45it/s]Extractor Predicting: 154it [01:39,  1.46it/s]Extractor Predicting: 155it [01:39,  1.46it/s]Extractor Predicting: 156it [01:40,  1.46it/s]Extractor Predicting: 157it [01:41,  1.49it/s]Extractor Predicting: 158it [01:41,  1.48it/s]Extractor Predicting: 159it [01:42,  1.51it/s]Extractor Predicting: 160it [01:43,  1.54it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:44,  1.51it/s]Extractor Predicting: 163it [01:45,  1.51it/s]Extractor Predicting: 164it [01:45,  1.54it/s]Extractor Predicting: 165it [01:46,  1.53it/s]Extractor Predicting: 166it [01:47,  1.51it/s]Extractor Predicting: 167it [01:47,  1.55it/s]Extractor Predicting: 168it [01:48,  1.53it/s]Extractor Predicting: 169it [01:48,  1.54it/s]Extractor Predicting: 170it [01:49,  1.56it/s]Extractor Predicting: 171it [01:50,  1.59it/s]Extractor Predicting: 172it [01:50,  1.58it/s]Extractor Predicting: 173it [01:51,  1.60it/s]Extractor Predicting: 174it [01:52,  1.59it/s]Extractor Predicting: 175it [01:52,  1.56it/s]Extractor Predicting: 176it [01:53,  1.56it/s]Extractor Predicting: 177it [01:54,  1.53it/s]Extractor Predicting: 178it [01:54,  1.59it/s]Extractor Predicting: 179it [01:55,  1.70it/s]Extractor Predicting: 180it [01:55,  1.56it/s]Extractor Predicting: 181it [01:56,  1.60it/s]Extractor Predicting: 182it [01:57,  1.64it/s]Extractor Predicting: 183it [01:57,  1.57it/s]Extractor Predicting: 184it [01:58,  1.52it/s]Extractor Predicting: 185it [01:59,  1.55it/s]Extractor Predicting: 186it [01:59,  1.53it/s]Extractor Predicting: 187it [02:00,  1.52it/s]Extractor Predicting: 188it [02:01,  1.50it/s]Extractor Predicting: 189it [02:01,  1.51it/s]Extractor Predicting: 190it [02:02,  1.49it/s]Extractor Predicting: 191it [02:03,  1.48it/s]Extractor Predicting: 192it [02:03,  1.50it/s]Extractor Predicting: 193it [02:04,  1.58it/s]Extractor Predicting: 194it [02:04,  1.59it/s]Extractor Predicting: 195it [02:05,  1.60it/s]Extractor Predicting: 196it [02:06,  1.57it/s]Extractor Predicting: 197it [02:06,  1.56it/s]Extractor Predicting: 198it [02:07,  1.52it/s]Extractor Predicting: 199it [02:08,  1.52it/s]Extractor Predicting: 200it [02:08,  1.55it/s]Extractor Predicting: 201it [02:09,  1.55it/s]Extractor Predicting: 202it [02:10,  1.54it/s]Extractor Predicting: 203it [02:10,  1.55it/s]Extractor Predicting: 204it [02:11,  1.56it/s]Extractor Predicting: 205it [02:12,  1.58it/s]Extractor Predicting: 206it [02:12,  1.58it/s]Extractor Predicting: 207it [02:13,  1.57it/s]Extractor Predicting: 208it [02:13,  1.58it/s]Extractor Predicting: 209it [02:14,  1.58it/s]Extractor Predicting: 210it [02:15,  1.56it/s]Extractor Predicting: 211it [02:15,  1.54it/s]Extractor Predicting: 212it [02:16,  1.51it/s]Extractor Predicting: 213it [02:17,  1.50it/s]Extractor Predicting: 214it [02:17,  1.50it/s]Extractor Predicting: 215it [02:18,  1.53it/s]Extractor Predicting: 216it [02:19,  1.53it/s]Extractor Predicting: 217it [02:19,  1.58it/s]Extractor Predicting: 218it [02:20,  1.57it/s]Extractor Predicting: 219it [02:21,  1.57it/s]Extractor Predicting: 220it [02:21,  1.58it/s]Extractor Predicting: 221it [02:22,  1.58it/s]Extractor Predicting: 222it [02:22,  1.58it/s]Extractor Predicting: 223it [02:23,  1.56it/s]Extractor Predicting: 224it [02:24,  1.52it/s]Extractor Predicting: 225it [02:24,  1.53it/s]Extractor Predicting: 226it [02:25,  1.55it/s]Extractor Predicting: 227it [02:26,  1.55it/s]Extractor Predicting: 228it [02:26,  1.53it/s]Extractor Predicting: 229it [02:27,  1.53it/s]Extractor Predicting: 230it [02:28,  1.52it/s]Extractor Predicting: 231it [02:28,  1.51it/s]Extractor Predicting: 232it [02:29,  1.55it/s]Extractor Predicting: 233it [02:30,  1.51it/s]Extractor Predicting: 234it [02:30,  1.50it/s]Extractor Predicting: 235it [02:31,  1.52it/s]Extractor Predicting: 236it [02:32,  1.52it/s]Extractor Predicting: 237it [02:32,  1.53it/s]Extractor Predicting: 238it [02:33,  1.50it/s]Extractor Predicting: 239it [02:34,  1.52it/s]Extractor Predicting: 240it [02:34,  1.53it/s]Extractor Predicting: 241it [02:35,  1.54it/s]Extractor Predicting: 242it [02:36,  1.52it/s]Extractor Predicting: 243it [02:36,  1.54it/s]Extractor Predicting: 244it [02:37,  1.53it/s]Extractor Predicting: 245it [02:38,  1.49it/s]Extractor Predicting: 246it [02:38,  1.52it/s]Extractor Predicting: 247it [02:39,  1.56it/s]Extractor Predicting: 248it [02:40,  1.51it/s]Extractor Predicting: 249it [02:40,  1.50it/s]Extractor Predicting: 250it [02:41,  1.56it/s]Extractor Predicting: 251it [02:41,  1.59it/s]Extractor Predicting: 252it [02:42,  1.62it/s]Extractor Predicting: 253it [02:43,  1.61it/s]Extractor Predicting: 254it [02:43,  1.61it/s]Extractor Predicting: 255it [02:44,  1.58it/s]Extractor Predicting: 256it [02:45,  1.58it/s]Extractor Predicting: 257it [02:45,  1.56it/s]Extractor Predicting: 258it [02:46,  1.58it/s]Extractor Predicting: 259it [02:46,  1.57it/s]Extractor Predicting: 260it [02:47,  1.57it/s]Extractor Predicting: 261it [02:48,  1.60it/s]Extractor Predicting: 262it [02:48,  1.64it/s]Extractor Predicting: 263it [02:49,  1.63it/s]Extractor Predicting: 264it [02:50,  1.64it/s]Extractor Predicting: 265it [02:50,  1.61it/s]Extractor Predicting: 266it [02:51,  1.61it/s]Extractor Predicting: 267it [02:51,  1.60it/s]Extractor Predicting: 268it [02:52,  1.57it/s]Extractor Predicting: 269it [02:53,  1.60it/s]Extractor Predicting: 270it [02:53,  1.58it/s]Extractor Predicting: 271it [02:54,  1.60it/s]Extractor Predicting: 272it [02:55,  1.61it/s]Extractor Predicting: 273it [02:55,  1.60it/s]Extractor Predicting: 274it [02:56,  1.60it/s]Extractor Predicting: 275it [02:56,  1.58it/s]Extractor Predicting: 276it [02:57,  1.60it/s]Extractor Predicting: 277it [02:58,  1.61it/s]Extractor Predicting: 278it [02:58,  1.60it/s]Extractor Predicting: 279it [02:59,  1.58it/s]Extractor Predicting: 280it [03:00,  1.56it/s]Extractor Predicting: 281it [03:00,  1.56it/s]Extractor Predicting: 282it [03:01,  1.50it/s]Extractor Predicting: 283it [03:02,  1.51it/s]Extractor Predicting: 284it [03:02,  1.57it/s]Extractor Predicting: 285it [03:03,  1.56it/s]Extractor Predicting: 286it [03:04,  1.53it/s]Extractor Predicting: 287it [03:04,  1.51it/s]Extractor Predicting: 288it [03:05,  1.54it/s]Extractor Predicting: 289it [03:06,  1.49it/s]Extractor Predicting: 290it [03:06,  1.51it/s]Extractor Predicting: 291it [03:07,  1.53it/s]Extractor Predicting: 292it [03:08,  1.52it/s]Extractor Predicting: 293it [03:08,  1.51it/s]Extractor Predicting: 294it [03:09,  1.53it/s]Extractor Predicting: 295it [03:09,  1.54it/s]Extractor Predicting: 296it [03:10,  1.54it/s]Extractor Predicting: 297it [03:11,  1.32it/s]Extractor Predicting: 298it [03:12,  1.40it/s]Extractor Predicting: 299it [03:12,  1.42it/s]Extractor Predicting: 300it [03:13,  1.43it/s]Extractor Predicting: 301it [03:14,  1.44it/s]Extractor Predicting: 302it [03:14,  1.48it/s]Extractor Predicting: 303it [03:15,  1.50it/s]Extractor Predicting: 304it [03:16,  1.52it/s]Extractor Predicting: 305it [03:16,  1.54it/s]Extractor Predicting: 306it [03:17,  1.51it/s]Extractor Predicting: 307it [03:18,  1.54it/s]Extractor Predicting: 308it [03:18,  1.56it/s]Extractor Predicting: 309it [03:19,  1.53it/s]Extractor Predicting: 310it [03:20,  1.48it/s]Extractor Predicting: 311it [03:20,  1.43it/s]Extractor Predicting: 312it [03:21,  1.40it/s]Extractor Predicting: 313it [03:22,  1.38it/s]Extractor Predicting: 314it [03:23,  1.36it/s]Extractor Predicting: 315it [03:23,  1.37it/s]Extractor Predicting: 316it [03:24,  1.36it/s]Extractor Predicting: 317it [03:25,  1.37it/s]Extractor Predicting: 318it [03:26,  1.37it/s]Extractor Predicting: 319it [03:26,  1.36it/s]Extractor Predicting: 320it [03:27,  1.37it/s]Extractor Predicting: 321it [03:28,  1.36it/s]Extractor Predicting: 322it [03:29,  1.36it/s]Extractor Predicting: 323it [03:29,  1.37it/s]Extractor Predicting: 324it [03:30,  1.38it/s]Extractor Predicting: 325it [03:31,  1.37it/s]Extractor Predicting: 326it [03:31,  1.38it/s]Extractor Predicting: 327it [03:32,  1.43it/s]Extractor Predicting: 328it [03:33,  1.45it/s]Extractor Predicting: 329it [03:33,  1.48it/s]Extractor Predicting: 330it [03:34,  1.53it/s]Extractor Predicting: 331it [03:35,  1.55it/s]Extractor Predicting: 332it [03:35,  1.57it/s]Extractor Predicting: 333it [03:36,  1.57it/s]Extractor Predicting: 334it [03:37,  1.54it/s]Extractor Predicting: 335it [03:37,  1.54it/s]Extractor Predicting: 336it [03:38,  1.50it/s]Extractor Predicting: 337it [03:39,  1.55it/s]Extractor Predicting: 338it [03:39,  1.56it/s]Extractor Predicting: 339it [03:40,  1.55it/s]Extractor Predicting: 340it [03:40,  1.52it/s]Extractor Predicting: 341it [03:41,  1.48it/s]Extractor Predicting: 342it [03:42,  1.47it/s]Extractor Predicting: 343it [03:43,  1.47it/s]Extractor Predicting: 344it [03:43,  1.44it/s]Extractor Predicting: 345it [03:44,  1.43it/s]Extractor Predicting: 346it [03:45,  1.42it/s]Extractor Predicting: 347it [03:45,  1.42it/s]Extractor Predicting: 348it [03:46,  1.43it/s]Extractor Predicting: 349it [03:47,  1.44it/s]Extractor Predicting: 350it [03:48,  1.43it/s]Extractor Predicting: 351it [03:48,  1.45it/s]Extractor Predicting: 352it [03:49,  1.46it/s]Extractor Predicting: 353it [03:50,  1.44it/s]Extractor Predicting: 354it [03:50,  1.44it/s]Extractor Predicting: 355it [03:51,  1.46it/s]Extractor Predicting: 356it [03:52,  1.47it/s]Extractor Predicting: 357it [03:52,  1.48it/s]Extractor Predicting: 358it [03:53,  1.48it/s]Extractor Predicting: 359it [03:54,  1.46it/s]Extractor Predicting: 360it [03:54,  1.48it/s]Extractor Predicting: 361it [03:55,  1.52it/s]Extractor Predicting: 362it [03:56,  1.51it/s]Extractor Predicting: 363it [03:56,  1.51it/s]Extractor Predicting: 364it [03:57,  1.50it/s]Extractor Predicting: 365it [03:58,  1.54it/s]Extractor Predicting: 366it [03:58,  1.51it/s]Extractor Predicting: 367it [03:59,  1.52it/s]Extractor Predicting: 368it [04:00,  1.50it/s]Extractor Predicting: 369it [04:00,  1.50it/s]Extractor Predicting: 370it [04:01,  1.53it/s]Extractor Predicting: 371it [04:02,  1.51it/s]Extractor Predicting: 372it [04:02,  1.48it/s]Extractor Predicting: 373it [04:03,  1.47it/s]Extractor Predicting: 374it [04:04,  1.50it/s]Extractor Predicting: 375it [04:04,  1.52it/s]Extractor Predicting: 376it [04:05,  1.55it/s]Extractor Predicting: 377it [04:05,  1.54it/s]Extractor Predicting: 378it [04:06,  1.54it/s]Extractor Predicting: 379it [04:07,  1.50it/s]Extractor Predicting: 380it [04:07,  1.49it/s]Extractor Predicting: 381it [04:08,  1.50it/s]Extractor Predicting: 382it [04:09,  1.50it/s]Extractor Predicting: 383it [04:09,  1.51it/s]Extractor Predicting: 384it [04:10,  1.52it/s]Extractor Predicting: 385it [04:11,  1.53it/s]Extractor Predicting: 386it [04:11,  1.54it/s]Extractor Predicting: 387it [04:12,  1.52it/s]Extractor Predicting: 388it [04:13,  1.48it/s]Extractor Predicting: 389it [04:13,  1.53it/s]Extractor Predicting: 390it [04:14,  1.54it/s]Extractor Predicting: 391it [04:15,  1.54it/s]Extractor Predicting: 392it [04:15,  1.57it/s]Extractor Predicting: 393it [04:16,  1.56it/s]Extractor Predicting: 394it [04:17,  1.56it/s]Extractor Predicting: 395it [04:17,  1.57it/s]Extractor Predicting: 396it [04:18,  1.55it/s]Extractor Predicting: 397it [04:19,  1.55it/s]Extractor Predicting: 398it [04:19,  1.58it/s]Extractor Predicting: 399it [04:20,  1.58it/s]Extractor Predicting: 400it [04:21,  1.36it/s]Extractor Predicting: 401it [04:21,  1.37it/s]Extractor Predicting: 402it [04:22,  1.38it/s]Extractor Predicting: 403it [04:23,  1.41it/s]Extractor Predicting: 404it [04:23,  1.46it/s]Extractor Predicting: 405it [04:24,  1.49it/s]Extractor Predicting: 406it [04:25,  1.50it/s]Extractor Predicting: 407it [04:25,  1.53it/s]Extractor Predicting: 408it [04:26,  1.58it/s]Extractor Predicting: 409it [04:27,  1.61it/s]Extractor Predicting: 410it [04:27,  1.62it/s]Extractor Predicting: 411it [04:28,  1.61it/s]Extractor Predicting: 412it [04:28,  1.64it/s]Extractor Predicting: 413it [04:29,  1.68it/s]Extractor Predicting: 414it [04:30,  1.70it/s]Extractor Predicting: 415it [04:30,  1.61it/s]Extractor Predicting: 416it [04:31,  1.53it/s]Extractor Predicting: 417it [04:32,  1.47it/s]Extractor Predicting: 418it [04:32,  1.43it/s]Extractor Predicting: 419it [04:33,  1.40it/s]Extractor Predicting: 420it [04:34,  1.40it/s]Extractor Predicting: 421it [04:35,  1.38it/s]Extractor Predicting: 422it [04:35,  1.59it/s]Extractor Predicting: 422it [04:35,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:04,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:04,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:04,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:04,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:04,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:11:05,409 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:11:05,410 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:11:06,041 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:11:07,065 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:11:07,065 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:09,982 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:09,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:09,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:09,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:09,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:11:10,656 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:11:10,657 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:11:10,920 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:11:11,069 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:11:11,070 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.21711899791231734,
  "recall": 0.04435913840904244,
  "score": 0.0736674340357712,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13679
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13779, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.43it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:30,  1.46it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.47it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.46it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:37,  1.46it/s]Extractor Predicting: 58it [00:38,  1.45it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.56it/s]Extractor Predicting: 66it [00:43,  1.56it/s]Extractor Predicting: 67it [00:44,  1.55it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.53it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.54it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:50,  1.54it/s]Extractor Predicting: 77it [00:50,  1.58it/s]Extractor Predicting: 78it [00:51,  1.59it/s]Extractor Predicting: 79it [00:52,  1.57it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.59it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:57,  1.56it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:01,  1.58it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:03,  1.59it/s]Extractor Predicting: 98it [01:04,  1.58it/s]Extractor Predicting: 99it [01:04,  1.59it/s]Extractor Predicting: 100it [01:05,  1.57it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:06,  1.51it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:08,  1.55it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:10,  1.58it/s]Extractor Predicting: 109it [01:11,  1.57it/s]Extractor Predicting: 110it [01:12,  1.59it/s]Extractor Predicting: 111it [01:12,  1.59it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:14,  1.55it/s]Extractor Predicting: 114it [01:14,  1.57it/s]Extractor Predicting: 115it [01:15,  1.56it/s]Extractor Predicting: 116it [01:15,  1.57it/s]Extractor Predicting: 117it [01:16,  1.61it/s]Extractor Predicting: 118it [01:17,  1.58it/s]Extractor Predicting: 119it [01:17,  1.59it/s]Extractor Predicting: 120it [01:18,  1.63it/s]Extractor Predicting: 121it [01:18,  1.62it/s]Extractor Predicting: 122it [01:19,  1.59it/s]Extractor Predicting: 123it [01:20,  1.59it/s]Extractor Predicting: 124it [01:20,  1.57it/s]Extractor Predicting: 125it [01:21,  1.53it/s]Extractor Predicting: 126it [01:22,  1.54it/s]Extractor Predicting: 127it [01:23,  1.44it/s]Extractor Predicting: 128it [01:23,  1.49it/s]Extractor Predicting: 129it [01:24,  1.52it/s]Extractor Predicting: 130it [01:24,  1.51it/s]Extractor Predicting: 131it [01:25,  1.50it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:26,  1.53it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:28,  1.55it/s]Extractor Predicting: 136it [01:28,  1.58it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:30,  1.56it/s]Extractor Predicting: 139it [01:30,  1.55it/s]Extractor Predicting: 140it [01:31,  1.59it/s]Extractor Predicting: 141it [01:31,  1.60it/s]Extractor Predicting: 142it [01:32,  1.62it/s]Extractor Predicting: 143it [01:33,  1.65it/s]Extractor Predicting: 144it [01:33,  1.62it/s]Extractor Predicting: 145it [01:34,  1.63it/s]Extractor Predicting: 146it [01:35,  1.62it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.60it/s]Extractor Predicting: 149it [01:36,  1.61it/s]Extractor Predicting: 150it [01:37,  1.62it/s]Extractor Predicting: 151it [01:38,  1.61it/s]Extractor Predicting: 152it [01:38,  1.63it/s]Extractor Predicting: 153it [01:39,  1.61it/s]Extractor Predicting: 154it [01:39,  1.60it/s]Extractor Predicting: 155it [01:40,  1.60it/s]Extractor Predicting: 156it [01:41,  1.58it/s]Extractor Predicting: 157it [01:41,  1.58it/s]Extractor Predicting: 158it [01:42,  1.59it/s]Extractor Predicting: 159it [01:43,  1.62it/s]Extractor Predicting: 160it [01:43,  1.65it/s]Extractor Predicting: 161it [01:44,  1.58it/s]Extractor Predicting: 162it [01:45,  1.55it/s]Extractor Predicting: 163it [01:45,  1.55it/s]Extractor Predicting: 164it [01:46,  1.60it/s]Extractor Predicting: 165it [01:46,  1.59it/s]Extractor Predicting: 166it [01:47,  1.53it/s]Extractor Predicting: 167it [01:48,  1.50it/s]Extractor Predicting: 168it [01:48,  1.51it/s]Extractor Predicting: 169it [01:49,  1.50it/s]Extractor Predicting: 170it [01:50,  1.46it/s]Extractor Predicting: 170it [01:50,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:09,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:09,013 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:09,013 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:09,013 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:09,013 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:13:09,385 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:13:09,386 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:13:09,665 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:13:10,767 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:13:10,768 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:12,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:12,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:12,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:12,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:12,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:13:12,462 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:13:12,463 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:13:12,730 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:13:12,906 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:13:12,907 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.43386243386243384,
  "recall": 0.08049079754601227,
  "score": 0.1357896915752432,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 3869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 3969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.42it/s]Extractor Predicting: 33it [00:22,  1.48it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:13:36,179 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:13:36,180 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:13:36,185 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:13:36,186 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:13:36,187 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:13:43,023 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:13:43,028 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:13:43,044 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:13:43,045 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:13:43,054 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,062 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,062 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,062 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,062 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:13:43,062 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6352941176470588,
  "recall": 0.05741626794258373,
  "score": 0.10531448074110192,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:13:43,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:43,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:44,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:45,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:45,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:46,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:47,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:48,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:48,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:49,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:50,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:51,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:51,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:52,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:53,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:53,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:54,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:55,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:55,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:56,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:57,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:12, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-28 18:13:58,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:58,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:59,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:00,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:00,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:01,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:02,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:03,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:03,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:04,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:05,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:06,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:07,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:07,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:08,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:09,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:10,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:10,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:11,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:12,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:12,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:01, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:13,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:14,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:15,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:16,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:16,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:17,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:17,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:18,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:18,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:19,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:20,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:20,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:21,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:21,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:22,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:23,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:23,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:24,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:24,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:25,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:42<01:36, 13.83s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:25,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:26,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:27,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:27,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:28,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:29,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:29,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:30,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:31,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:32,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:32,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:33,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:34,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:35,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:36,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:36,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:37,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:38,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:38,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:56<01:22, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:39,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:40,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:40,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:41,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:42,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:43,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:43,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:44,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:45,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:45,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:46,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:46,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:47,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:48,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:49,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:49,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:50,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:51,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:52,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:52,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:10<01:09, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:53,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:54,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:54,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:55,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:56,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:56,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:57,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:58,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:58,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:59,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:59,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:00,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:01,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:01,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:02,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:03,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:03,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:04,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:05,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:05,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:06,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:24<00:55, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 18:15:07,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:08,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:08,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:09,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:10,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:10,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:11,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:12,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:12,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:13,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:14,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:15,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:15,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:16,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:17,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:18,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:19,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:19,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:20,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:21,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:38<00:42, 14.12s/it][WARNING|generation_utils.py:914] 2023-08-28 18:15:21,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:22,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:23,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:24,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:24,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:25,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:26,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:27,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:27,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:28,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:29,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:29,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:30,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:31,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:32,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:32,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:33,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:34,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:34,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:35,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:53<00:28, 14.28s/it][WARNING|generation_utils.py:914] 2023-08-28 18:15:36,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:37,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:37,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:38,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:39,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:40,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:40,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:41,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:42,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:42,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:43,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:44,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:44,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:45,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:46,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:47,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:47,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:48,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:49,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:50,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:07<00:14, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 18:15:50,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:51,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:52,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:53,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:53,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:54,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:55,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:56,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:56,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:57,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:58,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:58,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:15:59,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:00,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:01,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:01,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:02,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:03,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:03,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:04,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:16:05,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:23<00:00, 14.66s/it]Generating: 100%|██████████| 10/10 [02:23<00:00, 14.31s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:12,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:12,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:12,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:12,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:12,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:16:12,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:16:12,397 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:16:13,107 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:16:14,158 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:16:14,158 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:15,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:15,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:15,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:15,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:15,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:16:16,379 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:16:16,380 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:16:16,652 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:16:16,806 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:16:16,806 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : country .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : part of .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : sport .', 'success_rate': 0.9390625, 'errors': {'', "('Paul Martens', 'sport', '', 'It was won by Jacques Rompuy in the 1994 Summer Olympics as well as by Paul Martens .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : continent .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9640625, 'errors': {''}}
['Relation : replaces . Context : Later in the year , the newly created New Labour government announced that they would make a public commitment to the New Deal and take a role in the creation of the new Labour Party , led by Jeremy Corbyn . Head Entity : Jeremy Corbyn , Tail Entity : New Labour Government .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : replaces .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 7413
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7513, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.68it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.66it/s]Extractor Estimating: 9it [00:05,  1.70it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:06,  1.52it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.62it/s]Extractor Estimating: 16it [00:09,  1.65it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:10,  1.72it/s]Extractor Estimating: 19it [00:11,  1.68it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.71it/s]Extractor Estimating: 23it [00:13,  1.65it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:15,  1.64it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.63it/s]Extractor Estimating: 29it [00:17,  1.55it/s]Extractor Estimating: 30it [00:18,  1.61it/s]Extractor Estimating: 31it [00:18,  1.62it/s]Extractor Estimating: 32it [00:19,  1.57it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:20,  1.66it/s]Extractor Estimating: 35it [00:21,  1.62it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:22,  1.56it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:24,  1.55it/s]Extractor Estimating: 40it [00:24,  1.52it/s]Extractor Estimating: 41it [00:25,  1.53it/s]Extractor Estimating: 42it [00:26,  1.54it/s]Extractor Estimating: 43it [00:26,  1.55it/s]Extractor Estimating: 44it [00:27,  1.56it/s]Extractor Estimating: 45it [00:27,  1.58it/s]Extractor Estimating: 46it [00:28,  1.61it/s]Extractor Estimating: 47it [00:29,  1.59it/s]Extractor Estimating: 48it [00:29,  1.65it/s]Extractor Estimating: 49it [00:30,  1.63it/s]Extractor Estimating: 50it [00:31,  1.58it/s]Extractor Estimating: 51it [00:31,  1.67it/s]Extractor Estimating: 52it [00:32,  1.79it/s]Extractor Estimating: 53it [00:32,  1.88it/s]Extractor Estimating: 54it [00:32,  1.99it/s]Extractor Estimating: 55it [00:33,  2.01it/s]Extractor Estimating: 56it [00:33,  2.09it/s]Extractor Estimating: 57it [00:34,  2.03it/s]Extractor Estimating: 58it [00:34,  2.02it/s]Extractor Estimating: 59it [00:35,  2.00it/s]Extractor Estimating: 60it [00:35,  2.06it/s]Extractor Estimating: 61it [00:36,  1.95it/s]Extractor Estimating: 62it [00:36,  1.90it/s]Extractor Estimating: 63it [00:37,  1.92it/s]Extractor Estimating: 64it [00:37,  2.01it/s]Extractor Estimating: 65it [00:38,  2.06it/s]Extractor Estimating: 66it [00:38,  1.90it/s]Extractor Estimating: 67it [00:39,  1.96it/s]Extractor Estimating: 68it [00:39,  2.04it/s]Extractor Estimating: 69it [00:40,  2.02it/s]Extractor Estimating: 70it [00:40,  2.01it/s]Extractor Estimating: 71it [00:41,  2.02it/s]Extractor Estimating: 72it [00:41,  2.03it/s]Extractor Estimating: 73it [00:42,  2.02it/s]Extractor Estimating: 74it [00:42,  2.05it/s]Extractor Estimating: 75it [00:43,  2.04it/s]Extractor Estimating: 76it [00:43,  1.88it/s]Extractor Estimating: 77it [00:44,  1.78it/s]Extractor Estimating: 78it [00:45,  1.70it/s]Extractor Estimating: 79it [00:45,  1.65it/s]Extractor Estimating: 80it [00:46,  1.64it/s]Extractor Estimating: 81it [00:47,  1.62it/s]Extractor Estimating: 82it [00:47,  1.62it/s]Extractor Estimating: 83it [00:48,  1.55it/s]Extractor Estimating: 84it [00:49,  1.59it/s]Extractor Estimating: 85it [00:49,  1.60it/s]Extractor Estimating: 86it [00:50,  1.60it/s]Extractor Estimating: 87it [00:50,  1.59it/s]Extractor Estimating: 88it [00:51,  1.56it/s]Extractor Estimating: 89it [00:52,  1.58it/s]Extractor Estimating: 90it [00:52,  1.56it/s]Extractor Estimating: 91it [00:53,  1.56it/s]Extractor Estimating: 92it [00:54,  1.58it/s]Extractor Estimating: 93it [00:54,  1.55it/s]Extractor Estimating: 94it [00:55,  1.55it/s]Extractor Estimating: 95it [00:56,  1.55it/s]Extractor Estimating: 96it [00:56,  1.55it/s]Extractor Estimating: 97it [00:57,  1.56it/s]Extractor Estimating: 98it [00:58,  1.57it/s]Extractor Estimating: 99it [00:58,  1.52it/s]Extractor Estimating: 100it [00:59,  1.55it/s]Extractor Estimating: 101it [00:59,  1.63it/s]Extractor Estimating: 102it [01:00,  1.63it/s]Extractor Estimating: 103it [01:01,  1.68it/s]Extractor Estimating: 104it [01:01,  1.72it/s]Extractor Estimating: 105it [01:02,  1.69it/s]Extractor Estimating: 106it [01:02,  1.70it/s]Extractor Estimating: 107it [01:03,  1.70it/s]Extractor Estimating: 108it [01:03,  1.71it/s]Extractor Estimating: 109it [01:04,  1.69it/s]Extractor Estimating: 110it [01:05,  1.73it/s]Extractor Estimating: 111it [01:05,  1.74it/s]Extractor Estimating: 112it [01:06,  1.76it/s]Extractor Estimating: 113it [01:06,  1.81it/s]Extractor Estimating: 114it [01:07,  1.81it/s]Extractor Estimating: 115it [01:07,  1.82it/s]Extractor Estimating: 116it [01:08,  1.82it/s]Extractor Estimating: 117it [01:08,  1.80it/s]Extractor Estimating: 118it [01:09,  1.79it/s]Extractor Estimating: 119it [01:10,  1.80it/s]Extractor Estimating: 120it [01:10,  1.71it/s]Extractor Estimating: 121it [01:11,  1.73it/s]Extractor Estimating: 122it [01:11,  1.68it/s]Extractor Estimating: 123it [01:12,  1.66it/s]Extractor Estimating: 124it [01:13,  1.73it/s]Extractor Estimating: 125it [01:13,  1.75it/s]Extractor Estimating: 126it [01:14,  1.70it/s]Extractor Estimating: 127it [01:14,  1.78it/s]Extractor Estimating: 128it [01:15,  1.80it/s]Extractor Estimating: 129it [01:15,  1.70it/s]Extractor Estimating: 130it [01:16,  1.78it/s]Extractor Estimating: 131it [01:17,  1.76it/s]Extractor Estimating: 132it [01:17,  1.78it/s]Extractor Estimating: 133it [01:18,  1.84it/s]Extractor Estimating: 134it [01:18,  1.88it/s]Extractor Estimating: 135it [01:19,  1.92it/s]Extractor Estimating: 136it [01:19,  1.91it/s]Extractor Estimating: 137it [01:20,  1.94it/s]Extractor Estimating: 138it [01:20,  1.91it/s]Extractor Estimating: 139it [01:21,  1.89it/s]Extractor Estimating: 140it [01:21,  1.88it/s]Extractor Estimating: 141it [01:22,  1.88it/s]Extractor Estimating: 142it [01:22,  1.93it/s]Extractor Estimating: 143it [01:23,  1.85it/s]Extractor Estimating: 144it [01:23,  1.89it/s]Extractor Estimating: 145it [01:24,  1.92it/s]Extractor Estimating: 146it [01:24,  1.91it/s]Extractor Estimating: 147it [01:25,  1.89it/s]Extractor Estimating: 148it [01:25,  1.90it/s]Extractor Estimating: 149it [01:26,  1.86it/s]Extractor Estimating: 150it [01:27,  1.84it/s]Extractor Estimating: 151it [01:27,  1.79it/s]Extractor Estimating: 152it [01:28,  1.71it/s]Extractor Estimating: 153it [01:28,  1.67it/s]Extractor Estimating: 154it [01:29,  1.74it/s]Extractor Estimating: 155it [01:30,  1.74it/s]Extractor Estimating: 156it [01:30,  1.59it/s]Extractor Estimating: 157it [01:31,  1.61it/s]Extractor Estimating: 158it [01:31,  1.67it/s]Extractor Estimating: 159it [01:32,  1.63it/s]Extractor Estimating: 160it [01:33,  1.64it/s]Extractor Estimating: 161it [01:33,  1.71it/s]Extractor Estimating: 162it [01:34,  1.70it/s]Extractor Estimating: 163it [01:34,  1.70it/s]Extractor Estimating: 164it [01:35,  1.68it/s]Extractor Estimating: 165it [01:36,  1.64it/s]Extractor Estimating: 166it [01:36,  1.60it/s]Extractor Estimating: 167it [01:37,  1.63it/s]Extractor Estimating: 168it [01:38,  1.62it/s]Extractor Estimating: 169it [01:38,  1.68it/s]Extractor Estimating: 170it [01:39,  1.68it/s]Extractor Estimating: 171it [01:39,  1.64it/s]Extractor Estimating: 172it [01:40,  1.66it/s]Extractor Estimating: 173it [01:40,  1.72it/s]Extractor Estimating: 174it [01:41,  1.80it/s]Extractor Estimating: 175it [01:42,  1.75it/s]Extractor Estimating: 176it [01:42,  1.72it/s]Extractor Estimating: 177it [01:43,  1.58it/s]Extractor Estimating: 178it [01:44,  1.56it/s]Extractor Estimating: 179it [01:44,  1.59it/s]Extractor Estimating: 180it [01:45,  1.62it/s]Extractor Estimating: 181it [01:45,  1.67it/s]Extractor Estimating: 182it [01:46,  1.67it/s]Extractor Estimating: 183it [01:47,  1.64it/s]Extractor Estimating: 184it [01:47,  1.60it/s]Extractor Estimating: 185it [01:48,  1.62it/s]Extractor Estimating: 186it [01:48,  1.64it/s]Extractor Estimating: 187it [01:49,  1.65it/s]Extractor Estimating: 188it [01:50,  1.55it/s]Extractor Estimating: 189it [01:50,  1.59it/s]Extractor Estimating: 190it [01:51,  1.62it/s]Extractor Estimating: 191it [01:51,  1.65it/s]Extractor Estimating: 192it [01:52,  1.59it/s]Extractor Estimating: 193it [01:53,  1.56it/s]Extractor Estimating: 194it [01:53,  1.58it/s]Extractor Estimating: 195it [01:54,  1.61it/s]Extractor Estimating: 196it [01:55,  1.64it/s]Extractor Estimating: 197it [01:55,  1.60it/s]Extractor Estimating: 198it [01:56,  1.58it/s]Extractor Estimating: 199it [01:57,  1.56it/s]Extractor Estimating: 200it [01:57,  1.58it/s]Extractor Estimating: 201it [01:58,  1.60it/s]Extractor Estimating: 202it [01:58,  1.57it/s]Extractor Estimating: 203it [01:59,  1.58it/s]Extractor Estimating: 204it [02:00,  1.54it/s]Extractor Estimating: 205it [02:00,  1.61it/s]Extractor Estimating: 206it [02:01,  1.59it/s]Extractor Estimating: 207it [02:02,  1.59it/s]Extractor Estimating: 208it [02:02,  1.62it/s]Extractor Estimating: 209it [02:03,  1.59it/s]Extractor Estimating: 210it [02:03,  1.63it/s]Extractor Estimating: 211it [02:04,  1.61it/s]Extractor Estimating: 212it [02:05,  1.63it/s]Extractor Estimating: 213it [02:05,  1.64it/s]Extractor Estimating: 214it [02:06,  1.64it/s]Extractor Estimating: 215it [02:07,  1.63it/s]Extractor Estimating: 216it [02:07,  1.60it/s]Extractor Estimating: 217it [02:08,  1.59it/s]Extractor Estimating: 218it [02:08,  1.63it/s]Extractor Estimating: 219it [02:09,  1.61it/s]Extractor Estimating: 220it [02:10,  1.62it/s]Extractor Estimating: 221it [02:10,  1.51it/s]Extractor Estimating: 222it [02:11,  1.56it/s]Extractor Estimating: 223it [02:12,  1.54it/s]Extractor Estimating: 224it [02:12,  1.53it/s]Extractor Estimating: 225it [02:13,  1.56it/s]Extractor Estimating: 226it [02:14,  1.58it/s]Extractor Estimating: 227it [02:14,  1.59it/s]Extractor Estimating: 228it [02:15,  1.57it/s]Extractor Estimating: 229it [02:16,  1.52it/s]Extractor Estimating: 230it [02:16,  1.58it/s]Extractor Estimating: 231it [02:17,  1.59it/s]Extractor Estimating: 232it [02:17,  1.58it/s]Extractor Estimating: 233it [02:18,  1.60it/s]Extractor Estimating: 234it [02:19,  1.61it/s]Extractor Estimating: 235it [02:19,  1.61it/s]Extractor Estimating: 236it [02:20,  1.58it/s]Extractor Estimating: 237it [02:21,  1.57it/s]Extractor Estimating: 238it [02:21,  1.61it/s]Extractor Estimating: 239it [02:22,  1.61it/s]Extractor Estimating: 240it [02:22,  1.65it/s]Extractor Estimating: 241it [02:23,  1.62it/s]Extractor Estimating: 242it [02:24,  1.61it/s]Extractor Estimating: 243it [02:24,  1.62it/s]Extractor Estimating: 244it [02:25,  1.61it/s]Extractor Estimating: 245it [02:25,  1.63it/s]Extractor Estimating: 246it [02:26,  1.68it/s]Extractor Estimating: 247it [02:27,  1.66it/s]Extractor Estimating: 248it [02:27,  1.65it/s]Extractor Estimating: 249it [02:28,  1.62it/s]Extractor Estimating: 250it [02:28,  1.68it/s]Extractor Estimating: 250it [02:28,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:02,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:02,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:02,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:02,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:02,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:19:02,312 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:19:02,313 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:19:02,574 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:19:03,618 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:19:03,618 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:05,786 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:05,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:05,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:05,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:19:05,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:19:06,125 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:19:06,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:19:06,390 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:19:06,537 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:19:06,537 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:24:20,422 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:24:20,452 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4995 mean pseudo reward: 0.9139059764664671
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl'}
train vocab size: 24232
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24332, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24332, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.046, loss:632.4473
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.050, loss:601.3524
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.050, loss:553.8961
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.043, loss:550.0128
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.034, loss:511.3348
>> valid entity prec:0.4122, rec:0.2896, f1:0.3402
>> valid relation prec:0.0480, rec:0.0132, f1:0.0208
>> valid relation with NER prec:0.0480, rec:0.0132, f1:0.0208
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 4.956, loss:510.9994
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.056, loss:490.4314
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.043, loss:508.5462
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.044, loss:446.8869
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.054, loss:488.1263
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4123, rec:0.3297, f1:0.3664
>> valid relation prec:0.0489, rec:0.0147, f1:0.0226
>> valid relation with NER prec:0.0489, rec:0.0147, f1:0.0226
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 4.983, loss:457.6986
g_step 1200, step 155, avg_time 1.029, loss:448.1022
g_step 1300, step 46, avg_time 1.061, loss:434.1111
g_step 1400, step 146, avg_time 1.041, loss:418.6449
g_step 1500, step 37, avg_time 1.045, loss:421.0628
>> valid entity prec:0.4162, rec:0.3063, f1:0.3529
>> valid relation prec:0.0760, rec:0.0251, f1:0.0377
>> valid relation with NER prec:0.0760, rec:0.0251, f1:0.0377
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 4.962, loss:404.3763
g_step 1700, step 28, avg_time 1.046, loss:399.5704
g_step 1800, step 128, avg_time 1.038, loss:393.0862
g_step 1900, step 19, avg_time 1.049, loss:376.6126
g_step 2000, step 119, avg_time 1.045, loss:365.5223
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4170, rec:0.2520, f1:0.3142
>> valid relation prec:0.0665, rec:0.0146, f1:0.0239
>> valid relation with NER prec:0.0665, rec:0.0146, f1:0.0239
g_step 2100, step 10, avg_time 4.948, loss:378.8091
g_step 2200, step 110, avg_time 1.054, loss:341.9213
g_step 2300, step 1, avg_time 1.036, loss:359.7662
g_step 2400, step 101, avg_time 1.041, loss:318.1276
g_step 2500, step 201, avg_time 1.053, loss:355.8961
>> valid entity prec:0.4084, rec:0.3080, f1:0.3512
>> valid relation prec:0.0867, rec:0.0297, f1:0.0442
>> valid relation with NER prec:0.0867, rec:0.0297, f1:0.0442
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 4.981, loss:310.5669
g_step 2700, step 192, avg_time 1.045, loss:340.8114
g_step 2800, step 83, avg_time 1.048, loss:292.6212
g_step 2900, step 183, avg_time 1.051, loss:326.5168
g_step 3000, step 74, avg_time 1.055, loss:281.5829
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.3958, rec:0.3093, f1:0.3472
>> valid relation prec:0.0530, rec:0.0200, f1:0.0290
>> valid relation with NER prec:0.0530, rec:0.0200, f1:0.0290
g_step 3100, step 174, avg_time 4.964, loss:308.6465
g_step 3200, step 65, avg_time 1.025, loss:282.7198
g_step 3300, step 165, avg_time 1.042, loss:287.8189
g_step 3400, step 56, avg_time 1.060, loss:269.9481
g_step 3500, step 156, avg_time 1.054, loss:281.5647
>> valid entity prec:0.4081, rec:0.3521, f1:0.3780
>> valid relation prec:0.1062, rec:0.0459, f1:0.0641
>> valid relation with NER prec:0.1062, rec:0.0459, f1:0.0641
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 47, avg_time 4.983, loss:269.0799
g_step 3700, step 147, avg_time 1.055, loss:261.3711
g_step 3800, step 38, avg_time 1.029, loss:260.5066
g_step 3900, step 138, avg_time 1.046, loss:257.4196
g_step 4000, step 29, avg_time 1.053, loss:257.8061
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4132, rec:0.3092, f1:0.3537
>> valid relation prec:0.0853, rec:0.0310, f1:0.0455
>> valid relation with NER prec:0.0853, rec:0.0310, f1:0.0455
g_step 4100, step 129, avg_time 4.968, loss:241.2575
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:24:20 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:24:20 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-24-20_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:24:21 - WARNING - datasets.builder -   Using custom data configuration default-20a774d5d68c160b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-20a774d5d68c160b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:24:22,370 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:24:22,372 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:24:22,372 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:24:22,373 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:24:22,380 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:22,386 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:24:22,531 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:24:25,764 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:24:25,778 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-20a774d5d68c160b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.27ba/s] 40%|████      | 2/5 [00:00<00:00,  4.14ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.54ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.77ba/s]100%|██████████| 5/5 [00:01<00:00,  4.88ba/s]100%|██████████| 5/5 [00:01<00:00,  4.60ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:03,  4.26ba/s] 13%|█▎        | 2/15 [00:00<00:02,  4.57ba/s] 20%|██        | 3/15 [00:00<00:02,  4.64ba/s] 27%|██▋       | 4/15 [00:00<00:02,  4.70ba/s] 33%|███▎      | 5/15 [00:01<00:02,  4.71ba/s] 40%|████      | 6/15 [00:01<00:02,  3.81ba/s] 47%|████▋     | 7/15 [00:01<00:01,  4.09ba/s] 53%|█████▎    | 8/15 [00:01<00:01,  4.27ba/s] 60%|██████    | 9/15 [00:02<00:01,  4.36ba/s] 67%|██████▋   | 10/15 [00:02<00:01,  4.45ba/s] 73%|███████▎  | 11/15 [00:02<00:00,  4.55ba/s] 80%|████████  | 12/15 [00:02<00:00,  4.59ba/s] 87%|████████▋ | 13/15 [00:02<00:00,  4.66ba/s] 93%|█████████▎| 14/15 [00:03<00:00,  4.69ba/s]100%|██████████| 15/15 [00:03<00:00,  4.77ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.55ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.74ba/s]100%|██████████| 5/5 [00:00<00:00,  8.90ba/s]100%|██████████| 5/5 [00:00<00:00,  8.23ba/s]
  0%|          | 0/15 [00:00<?, ?ba/s]  7%|▋         | 1/15 [00:00<00:01,  8.64ba/s] 13%|█▎        | 2/15 [00:00<00:01,  9.38ba/s] 27%|██▋       | 4/15 [00:00<00:01,  9.91ba/s] 40%|████      | 6/15 [00:00<00:00, 10.14ba/s] 53%|█████▎    | 8/15 [00:00<00:00, 10.15ba/s] 67%|██████▋   | 10/15 [00:00<00:00, 10.28ba/s] 80%|████████  | 12/15 [00:01<00:00,  9.87ba/s] 93%|█████████▎| 14/15 [00:01<00:00,  9.97ba/s]100%|██████████| 15/15 [00:01<00:00, 10.61ba/s]
[INFO|trainer.py:414] 2023-08-28 20:24:32,774 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:24:32,800 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:24:32,800 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 20:24:32,800 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:24:32,800 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:24:32,800 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:24:32,800 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:24:32,801 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.28it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:00<01:53,  3.42it/s]  1%|          | 4/390 [00:01<01:52,  3.44it/s]  1%|▏         | 5/390 [00:01<01:51,  3.46it/s]  2%|▏         | 6/390 [00:01<01:50,  3.46it/s]  2%|▏         | 7/390 [00:02<01:50,  3.47it/s]  2%|▏         | 8/390 [00:02<01:50,  3.47it/s]  2%|▏         | 9/390 [00:02<01:58,  3.23it/s]  3%|▎         | 10/390 [00:02<01:55,  3.30it/s]  3%|▎         | 11/390 [00:03<01:53,  3.35it/s]  3%|▎         | 12/390 [00:03<01:51,  3.39it/s]  3%|▎         | 13/390 [00:03<01:50,  3.41it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:48,  3.45it/s]  4%|▍         | 16/390 [00:04<01:48,  3.46it/s]  4%|▍         | 17/390 [00:04<01:47,  3.46it/s]  5%|▍         | 18/390 [00:05<01:47,  3.47it/s]  5%|▍         | 19/390 [00:05<01:46,  3.47it/s]  5%|▌         | 20/390 [00:05<01:49,  3.39it/s]  5%|▌         | 21/390 [00:06<01:48,  3.41it/s]  6%|▌         | 22/390 [00:06<01:47,  3.43it/s]  6%|▌         | 23/390 [00:06<01:46,  3.44it/s]  6%|▌         | 24/390 [00:07<01:46,  3.45it/s]  6%|▋         | 25/390 [00:07<01:45,  3.46it/s]  7%|▋         | 26/390 [00:07<01:45,  3.46it/s]  7%|▋         | 27/390 [00:07<01:44,  3.47it/s]  7%|▋         | 28/390 [00:08<01:44,  3.46it/s]  7%|▋         | 29/390 [00:08<01:44,  3.46it/s]  8%|▊         | 30/390 [00:08<01:43,  3.47it/s]  8%|▊         | 31/390 [00:09<01:44,  3.44it/s]  8%|▊         | 32/390 [00:09<01:43,  3.45it/s]  8%|▊         | 33/390 [00:09<01:43,  3.46it/s]  9%|▊         | 34/390 [00:09<01:42,  3.46it/s]  9%|▉         | 35/390 [00:10<01:42,  3.46it/s]  9%|▉         | 36/390 [00:10<01:42,  3.46it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:11<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:40,  3.47it/s] 11%|█         | 41/390 [00:11<01:40,  3.47it/s] 11%|█         | 42/390 [00:12<01:41,  3.41it/s] 11%|█         | 43/390 [00:12<01:41,  3.43it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.45it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.45it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.46it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.46it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.46it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.46it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.46it/s] 13%|█▎        | 52/390 [00:15<01:37,  3.46it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.46it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.43it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.46it/s] 16%|█▌        | 62/390 [00:18<01:34,  3.45it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.46it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.46it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.46it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.46it/s] 18%|█▊        | 69/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 74/390 [00:21<01:34,  3.34it/s] 19%|█▉        | 75/390 [00:21<01:33,  3.37it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.40it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:30,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 20:24:55,516 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:24:55,517 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:24:55,517 >>   Batch size = 8

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.91it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.67it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.93it/s][A
  1%|▏         | 23/1759 [00:00<00:35, 48.26it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.75it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.48it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.15it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.79it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.71it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.78it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.81it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.90it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.92it/s][A
  4%|▍         | 73/1759 [00:01<00:35, 46.92it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.80it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.77it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.60it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.66it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.57it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.71it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.76it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.86it/s][A
  7%|▋         | 118/1759 [00:02<00:34, 46.91it/s][A
  7%|▋         | 123/1759 [00:02<00:34, 46.87it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.74it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.66it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.62it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.56it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.65it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.79it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.82it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.88it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.78it/s][A
 10%|▉         | 173/1759 [00:03<00:34, 45.40it/s][A
 10%|█         | 178/1759 [00:03<00:34, 45.77it/s][A
 10%|█         | 183/1759 [00:03<00:34, 46.04it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.23it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.33it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.52it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.69it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.72it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.60it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.59it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.60it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.64it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.58it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.57it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.68it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.78it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.79it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.75it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.66it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.67it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.73it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.62it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.69it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.63it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.64it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.76it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.69it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.69it/s][A
 18%|█▊        | 313/1759 [00:06<00:30, 46.69it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.66it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.55it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.63it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.61it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.45it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.61it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.72it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.75it/s][A
 20%|██        | 358/1759 [00:07<00:29, 46.71it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.70it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.61it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.64it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.61it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.58it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.54it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.64it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.69it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.67it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.72it/s][A
 23%|██▎       | 413/1759 [00:08<00:29, 45.83it/s][A
 24%|██▍       | 418/1759 [00:08<00:29, 46.11it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.10it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.36it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.50it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.48it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.62it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.67it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.59it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.57it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.64it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.59it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.55it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.68it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.66it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.71it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.53it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.62it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.56it/s][A
 29%|██▉       | 508/1759 [00:10<00:28, 43.75it/s][A
 29%|██▉       | 513/1759 [00:11<00:27, 44.86it/s][A
 29%|██▉       | 518/1759 [00:11<00:27, 45.35it/s][A
 30%|██▉       | 523/1759 [00:11<00:27, 45.76it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.08it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.28it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.37it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.32it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.20it/s][A
 31%|███▏      | 553/1759 [00:11<00:26, 46.29it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.35it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.41it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.53it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.54it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.63it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.59it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.54it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.37it/s][A
 34%|███▍      | 598/1759 [00:12<00:25, 46.34it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.41it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.44it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.52it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.66it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.66it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.60it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.44it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.38it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.33it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.40it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.37it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.51it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.60it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.59it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.55it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.54it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.52it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.37it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.44it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.52it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.44it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.55it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.64it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.59it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.59it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.46it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.44it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.44it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.50it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.55it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.39it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.47it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.52it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.52it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.38it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.48it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.44it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.52it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.46it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.44it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.52it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.55it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.53it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.57it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.40it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.46it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.51it/s][A
 48%|████▊     | 838/1759 [00:17<00:19, 46.53it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.60it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.48it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.52it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.58it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.53it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.52it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.39it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.42it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.52it/s][A
 50%|█████     | 888/1759 [00:19<00:19, 45.81it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.02it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.16it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.30it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.33it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.39it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.40it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.34it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.41it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.54it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.54it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.51it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.56it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.55it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.41it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.40it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.34it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.42it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.44it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.44it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.51it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.39it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.49it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.49it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.43it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.52it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.54it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.48it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.46it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.50it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.46it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.49it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.46it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.48it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.47it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.57it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.49it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.50it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.54it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.39it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.49it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.41it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.39it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.52it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.54it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.41it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.53it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.52it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.50it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.41it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:13, 42.95it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:13, 43.95it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:13, 44.73it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 45.33it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 45.75it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.09it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.27it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.37it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 46.09it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 45.94it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.14it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.16it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.33it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.46it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.50it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.62it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.64it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.40it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.23it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.28it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.30it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.28it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.37it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.46it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.57it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.62it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.53it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.33it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.39it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.38it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.39it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.36it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.53it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.62it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.54it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.45it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.48it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.35it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.43it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 45.08it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 45.48it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 45.80it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.13it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.26it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:08, 46.30it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.38it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.36it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.22it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.30it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.38it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.43it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.34it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.42it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.55it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.56it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.48it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.33it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.43it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.49it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.39it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.46it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.38it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.53it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.50it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.50it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.38it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.45it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.44it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.49it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.55it/s][A
 87%|████████▋ | 1533/1759 [00:32<00:04, 46.53it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.51it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.48it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.44it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.37it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.39it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.36it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.48it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.41it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.48it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.53it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.49it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.41it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 45.48it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 45.85it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.00it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.16it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.31it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.28it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.50it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.37it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.35it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.41it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.43it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.30it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.48it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.51it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.50it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.45it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.38it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.41it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.47it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.39it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.49it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.47it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.46it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.49it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.47it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.48it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.44it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.51it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.39it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.43it/s][A
                                                   [A                                                
100%|██████████| 1759/1759 [00:37<00:00, 46.43it/s][A 20%|██        | 78/390 [01:00<01:30,  3.43it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 20:25:33,448 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 20:25:33,496 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:25:36,943 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:25:37,016 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:25:37,036 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:12<1:18:05, 15.07s/it] 21%|██        | 80/390 [01:12<54:56, 10.63s/it]   21%|██        | 81/390 [01:12<38:46,  7.53s/it] 21%|██        | 82/390 [01:13<27:30,  5.36s/it] 21%|██▏       | 83/390 [01:13<19:37,  3.84s/it] 22%|██▏       | 84/390 [01:13<14:08,  2.77s/it] 22%|██▏       | 85/390 [01:13<10:18,  2.03s/it] 22%|██▏       | 86/390 [01:14<07:37,  1.51s/it] 22%|██▏       | 87/390 [01:14<05:45,  1.14s/it] 23%|██▎       | 88/390 [01:14<04:27,  1.13it/s] 23%|██▎       | 89/390 [01:15<03:32,  1.42it/s] 23%|██▎       | 90/390 [01:15<02:54,  1.72it/s] 23%|██▎       | 91/390 [01:15<02:28,  2.01it/s] 24%|██▎       | 92/390 [01:15<02:09,  2.30it/s] 24%|██▍       | 93/390 [01:16<01:55,  2.56it/s] 24%|██▍       | 94/390 [01:16<01:46,  2.78it/s] 24%|██▍       | 95/390 [01:16<01:39,  2.95it/s] 25%|██▍       | 96/390 [01:17<01:35,  3.09it/s] 25%|██▍       | 97/390 [01:17<01:31,  3.19it/s] 25%|██▌       | 98/390 [01:17<01:29,  3.27it/s] 25%|██▌       | 99/390 [01:17<01:27,  3.33it/s] 26%|██▌       | 100/390 [01:18<01:26,  3.36it/s] 26%|██▌       | 101/390 [01:18<01:25,  3.39it/s] 26%|██▌       | 102/390 [01:18<01:25,  3.36it/s] 26%|██▋       | 103/390 [01:19<01:24,  3.39it/s] 27%|██▋       | 104/390 [01:19<01:23,  3.41it/s] 27%|██▋       | 105/390 [01:19<01:23,  3.43it/s] 27%|██▋       | 106/390 [01:20<01:22,  3.44it/s] 27%|██▋       | 107/390 [01:20<01:22,  3.45it/s] 28%|██▊       | 108/390 [01:20<01:21,  3.45it/s] 28%|██▊       | 109/390 [01:20<01:21,  3.46it/s] 28%|██▊       | 110/390 [01:21<01:20,  3.46it/s] 28%|██▊       | 111/390 [01:21<01:20,  3.46it/s] 29%|██▊       | 112/390 [01:21<01:20,  3.46it/s] 29%|██▉       | 113/390 [01:22<01:21,  3.39it/s] 29%|██▉       | 114/390 [01:22<01:20,  3.41it/s] 29%|██▉       | 115/390 [01:22<01:20,  3.43it/s] 30%|██▉       | 116/390 [01:22<01:19,  3.44it/s] 30%|███       | 117/390 [01:23<01:19,  3.45it/s] 30%|███       | 118/390 [01:23<01:18,  3.45it/s] 31%|███       | 119/390 [01:23<01:18,  3.46it/s] 31%|███       | 120/390 [01:24<01:18,  3.46it/s] 31%|███       | 121/390 [01:24<01:17,  3.46it/s] 31%|███▏      | 122/390 [01:24<01:17,  3.46it/s] 32%|███▏      | 123/390 [01:24<01:17,  3.46it/s] 32%|███▏      | 124/390 [01:25<01:17,  3.43it/s] 32%|███▏      | 125/390 [01:25<01:16,  3.44it/s] 32%|███▏      | 126/390 [01:25<01:16,  3.45it/s] 33%|███▎      | 127/390 [01:26<01:16,  3.45it/s] 33%|███▎      | 128/390 [01:26<01:15,  3.45it/s] 33%|███▎      | 129/390 [01:26<01:15,  3.46it/s] 33%|███▎      | 130/390 [01:26<01:15,  3.46it/s] 34%|███▎      | 131/390 [01:27<01:14,  3.46it/s] 34%|███▍      | 132/390 [01:27<01:14,  3.46it/s] 34%|███▍      | 133/390 [01:27<01:14,  3.46it/s] 34%|███▍      | 134/390 [01:28<01:13,  3.46it/s] 35%|███▍      | 135/390 [01:28<01:25,  3.00it/s] 35%|███▍      | 136/390 [01:28<01:21,  3.12it/s] 35%|███▌      | 137/390 [01:29<01:18,  3.22it/s] 35%|███▌      | 138/390 [01:29<01:16,  3.29it/s] 36%|███▌      | 139/390 [01:29<01:15,  3.34it/s] 36%|███▌      | 140/390 [01:30<01:14,  3.37it/s] 36%|███▌      | 141/390 [01:30<01:13,  3.40it/s] 36%|███▋      | 142/390 [01:30<01:12,  3.42it/s] 37%|███▋      | 143/390 [01:30<01:11,  3.43it/s] 37%|███▋      | 144/390 [01:31<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:31<01:11,  3.44it/s] 37%|███▋      | 146/390 [01:31<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:32<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:32<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:32<01:09,  3.45it/s] 38%|███▊      | 150/390 [01:32<01:09,  3.46it/s] 39%|███▊      | 151/390 [01:33<01:10,  3.41it/s] 39%|███▉      | 152/390 [01:33<01:09,  3.42it/s] 39%|███▉      | 153/390 [01:33<01:08,  3.44it/s] 39%|███▉      | 154/390 [01:34<01:08,  3.44it/s] 40%|███▉      | 155/390 [01:34<01:08,  3.45it/s] 40%|████      | 156/390 [01:34<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 20:26:07,513 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:26:07,513 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:26:07,513 >>   Batch size = 8
{'eval_loss': 0.9187746047973633, 'eval_runtime': 37.8835, 'eval_samples_per_second': 371.323, 'eval_steps_per_second': 46.432, 'epoch': 0.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.93it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.57it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.83it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.15it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.71it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.46it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.12it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.64it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.77it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.70it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.70it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.81it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.81it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.75it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.81it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.73it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.52it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.40it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.51it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.58it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.53it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.59it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.69it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.72it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.63it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.58it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.47it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.51it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.47it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.56it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.65it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.67it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.71it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.65it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.56it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.43it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.54it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.53it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.61it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.61it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.58it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.55it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.50it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.41it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.44it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.46it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.49it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.51it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.53it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.58it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.58it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.48it/s][A
 16%|█▌        | 273/1759 [00:05<00:32, 46.43it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.46it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.43it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.45it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.44it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.41it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.58it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.59it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.49it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.49it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.47it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.47it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.48it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.46it/s][A
 19%|█▉        | 343/1759 [00:07<00:31, 44.61it/s][A
 20%|█▉        | 348/1759 [00:07<00:31, 45.41it/s][A
 20%|██        | 353/1759 [00:07<00:30, 45.80it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.05it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.20it/s][A
 21%|██        | 368/1759 [00:07<00:30, 46.17it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.36it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.39it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.18it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.28it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.32it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.41it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.54it/s][A
 23%|██▎       | 408/1759 [00:08<00:28, 46.62it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.44it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.48it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.51it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.34it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.31it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.41it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.46it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.56it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.61it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.49it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.48it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.43it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.40it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.32it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.37it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.53it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.47it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.50it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.51it/s][A
 29%|██▉       | 513/1759 [00:11<00:28, 44.26it/s][A
 30%|██▉       | 519/1759 [00:11<00:27, 45.91it/s][A
 30%|██▉       | 524/1759 [00:11<00:26, 46.15it/s][A
 30%|███       | 529/1759 [00:11<00:26, 46.23it/s][A
 30%|███       | 534/1759 [00:11<00:26, 46.28it/s][A
 31%|███       | 539/1759 [00:11<00:26, 46.48it/s][A
 31%|███       | 544/1759 [00:11<00:26, 46.41it/s][A
 31%|███       | 549/1759 [00:11<00:26, 46.45it/s][A
 31%|███▏      | 554/1759 [00:11<00:25, 46.46it/s][A
 32%|███▏      | 559/1759 [00:12<00:25, 46.27it/s][A
 32%|███▏      | 564/1759 [00:12<00:25, 46.37it/s][A
 32%|███▏      | 569/1759 [00:12<00:25, 46.44it/s][A
 33%|███▎      | 574/1759 [00:12<00:25, 46.41it/s][A
 33%|███▎      | 579/1759 [00:12<00:25, 46.58it/s][A
 33%|███▎      | 584/1759 [00:12<00:25, 46.58it/s][A
 33%|███▎      | 589/1759 [00:12<00:25, 46.55it/s][A
 34%|███▍      | 594/1759 [00:12<00:25, 46.45it/s][A
 34%|███▍      | 599/1759 [00:12<00:24, 46.46it/s][A
 34%|███▍      | 604/1759 [00:13<00:28, 40.37it/s][A
 35%|███▍      | 609/1759 [00:13<00:27, 42.13it/s][A
 35%|███▍      | 614/1759 [00:13<00:26, 43.45it/s][A
 35%|███▌      | 619/1759 [00:13<00:25, 44.26it/s][A
 35%|███▌      | 624/1759 [00:13<00:25, 45.04it/s][A
 36%|███▌      | 629/1759 [00:13<00:24, 45.57it/s][A
 36%|███▌      | 634/1759 [00:13<00:24, 45.91it/s][A
 36%|███▋      | 639/1759 [00:13<00:24, 46.12it/s][A
 37%|███▋      | 644/1759 [00:13<00:24, 45.87it/s][A
 37%|███▋      | 649/1759 [00:14<00:24, 45.76it/s][A
 37%|███▋      | 654/1759 [00:14<00:24, 45.90it/s][A
 37%|███▋      | 659/1759 [00:14<00:23, 46.21it/s][A
 38%|███▊      | 664/1759 [00:14<00:23, 46.26it/s][A
 38%|███▊      | 669/1759 [00:14<00:23, 46.40it/s][A
 38%|███▊      | 674/1759 [00:14<00:23, 46.52it/s][A
 39%|███▊      | 679/1759 [00:14<00:23, 46.54it/s][A
 39%|███▉      | 684/1759 [00:14<00:23, 46.61it/s][A
 39%|███▉      | 689/1759 [00:14<00:23, 46.36it/s][A
 39%|███▉      | 694/1759 [00:14<00:23, 46.14it/s][A
 40%|███▉      | 699/1759 [00:15<00:22, 46.21it/s][A
 40%|████      | 704/1759 [00:15<00:22, 46.32it/s][A
 40%|████      | 709/1759 [00:15<00:22, 46.24it/s][A
 41%|████      | 714/1759 [00:15<00:22, 46.42it/s][A
 41%|████      | 719/1759 [00:15<00:22, 46.53it/s][A
 41%|████      | 724/1759 [00:15<00:22, 46.58it/s][A
 41%|████▏     | 729/1759 [00:15<00:22, 46.56it/s][A
 42%|████▏     | 734/1759 [00:15<00:22, 46.46it/s][A
 42%|████▏     | 739/1759 [00:15<00:21, 46.38it/s][A
 42%|████▏     | 744/1759 [00:16<00:21, 46.18it/s][A
 43%|████▎     | 749/1759 [00:16<00:21, 46.30it/s][A
 43%|████▎     | 754/1759 [00:16<00:21, 46.24it/s][A
 43%|████▎     | 759/1759 [00:16<00:21, 46.34it/s][A
 43%|████▎     | 764/1759 [00:16<00:23, 42.52it/s][A
 44%|████▎     | 769/1759 [00:16<00:22, 43.65it/s][A
 44%|████▍     | 774/1759 [00:16<00:22, 44.53it/s][A
 44%|████▍     | 779/1759 [00:16<00:21, 45.16it/s][A
 45%|████▍     | 784/1759 [00:16<00:21, 45.56it/s][A
 45%|████▍     | 789/1759 [00:17<00:21, 45.89it/s][A
 45%|████▌     | 794/1759 [00:17<00:20, 46.03it/s][A
 45%|████▌     | 799/1759 [00:17<00:20, 46.27it/s][A
 46%|████▌     | 804/1759 [00:17<00:20, 46.13it/s][A
 46%|████▌     | 809/1759 [00:17<00:20, 46.15it/s][A
 46%|████▋     | 814/1759 [00:17<00:20, 46.23it/s][A
 47%|████▋     | 819/1759 [00:17<00:20, 46.42it/s][A
 47%|████▋     | 824/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 829/1759 [00:17<00:20, 46.46it/s][A
 47%|████▋     | 834/1759 [00:18<00:19, 46.57it/s][A
 48%|████▊     | 839/1759 [00:18<00:19, 46.47it/s][A
 48%|████▊     | 844/1759 [00:18<00:19, 46.49it/s][A
 48%|████▊     | 849/1759 [00:18<00:19, 46.39it/s][A
 49%|████▊     | 854/1759 [00:18<00:19, 46.30it/s][A
 49%|████▉     | 859/1759 [00:18<00:19, 46.38it/s][A
 49%|████▉     | 864/1759 [00:18<00:19, 46.36it/s][A
 49%|████▉     | 869/1759 [00:18<00:19, 46.41it/s][A
 50%|████▉     | 874/1759 [00:18<00:19, 46.52it/s][A
 50%|████▉     | 879/1759 [00:18<00:18, 46.52it/s][A
 50%|█████     | 884/1759 [00:19<00:18, 46.43it/s][A
 51%|█████     | 889/1759 [00:19<00:18, 46.45it/s][A
 51%|█████     | 894/1759 [00:19<00:18, 46.44it/s][A
 51%|█████     | 899/1759 [00:19<00:18, 46.36it/s][A
 51%|█████▏    | 904/1759 [00:19<00:19, 43.84it/s][A
 52%|█████▏    | 909/1759 [00:19<00:19, 44.68it/s][A
 52%|█████▏    | 914/1759 [00:19<00:18, 45.21it/s][A
 52%|█████▏    | 919/1759 [00:19<00:18, 45.66it/s][A
 53%|█████▎    | 924/1759 [00:19<00:18, 46.02it/s][A
 53%|█████▎    | 929/1759 [00:20<00:17, 46.12it/s][A
 53%|█████▎    | 934/1759 [00:20<00:17, 46.13it/s][A
 53%|█████▎    | 939/1759 [00:20<00:17, 46.33it/s][A
 54%|█████▎    | 944/1759 [00:20<00:17, 46.03it/s][A
 54%|█████▍    | 949/1759 [00:20<00:17, 46.15it/s][A
 54%|█████▍    | 954/1759 [00:20<00:17, 46.25it/s][A
 55%|█████▍    | 959/1759 [00:20<00:17, 46.08it/s][A
 55%|█████▍    | 964/1759 [00:20<00:17, 46.52it/s][A
 55%|█████▌    | 969/1759 [00:20<00:16, 46.59it/s][A
 55%|█████▌    | 974/1759 [00:21<00:16, 46.55it/s][A
 56%|█████▌    | 979/1759 [00:21<00:16, 46.60it/s][A
 56%|█████▌    | 984/1759 [00:21<00:16, 46.53it/s][A
 56%|█████▌    | 989/1759 [00:21<00:16, 46.28it/s][A
 57%|█████▋    | 994/1759 [00:21<00:16, 46.28it/s][A
 57%|█████▋    | 999/1759 [00:21<00:16, 46.35it/s][A
 57%|█████▋    | 1004/1759 [00:21<00:16, 46.30it/s][A
 57%|█████▋    | 1009/1759 [00:21<00:16, 46.36it/s][A
 58%|█████▊    | 1014/1759 [00:21<00:16, 46.54it/s][A
 58%|█████▊    | 1019/1759 [00:22<00:15, 46.52it/s][A
 58%|█████▊    | 1024/1759 [00:22<00:15, 46.57it/s][A
 58%|█████▊    | 1029/1759 [00:22<00:15, 46.56it/s][A
 59%|█████▉    | 1034/1759 [00:22<00:15, 46.38it/s][A
 59%|█████▉    | 1039/1759 [00:22<00:15, 46.27it/s][A
 59%|█████▉    | 1044/1759 [00:22<00:15, 46.27it/s][A
 60%|█████▉    | 1049/1759 [00:22<00:15, 46.24it/s][A
 60%|█████▉    | 1054/1759 [00:22<00:15, 46.34it/s][A
 60%|██████    | 1059/1759 [00:22<00:15, 46.37it/s][A
 60%|██████    | 1064/1759 [00:22<00:14, 46.45it/s][A
 61%|██████    | 1069/1759 [00:23<00:14, 46.58it/s][A
 61%|██████    | 1074/1759 [00:23<00:14, 46.55it/s][A
 61%|██████▏   | 1079/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1084/1759 [00:23<00:14, 46.32it/s][A
 62%|██████▏   | 1089/1759 [00:23<00:14, 46.27it/s][A
 62%|██████▏   | 1094/1759 [00:23<00:14, 46.34it/s][A
 62%|██████▏   | 1099/1759 [00:23<00:14, 46.37it/s][A
 63%|██████▎   | 1104/1759 [00:23<00:14, 46.33it/s][A
 63%|██████▎   | 1109/1759 [00:23<00:14, 46.40it/s][A
 63%|██████▎   | 1114/1759 [00:24<00:13, 46.45it/s][A
 64%|██████▎   | 1119/1759 [00:24<00:13, 46.48it/s][A
 64%|██████▍   | 1124/1759 [00:24<00:13, 46.46it/s][A
 64%|██████▍   | 1129/1759 [00:24<00:13, 46.47it/s][A
 64%|██████▍   | 1134/1759 [00:24<00:13, 46.46it/s][A
 65%|██████▍   | 1139/1759 [00:24<00:13, 46.38it/s][A
 65%|██████▌   | 1144/1759 [00:24<00:13, 46.39it/s][A
 65%|██████▌   | 1149/1759 [00:24<00:13, 46.41it/s][A
 66%|██████▌   | 1154/1759 [00:24<00:13, 46.41it/s][A
 66%|██████▌   | 1159/1759 [00:25<00:12, 46.43it/s][A
 66%|██████▌   | 1164/1759 [00:25<00:12, 46.34it/s][A
 66%|██████▋   | 1169/1759 [00:25<00:12, 46.47it/s][A
 67%|██████▋   | 1174/1759 [00:25<00:12, 46.48it/s][A
 67%|██████▋   | 1179/1759 [00:25<00:12, 46.44it/s][A
 67%|██████▋   | 1184/1759 [00:25<00:12, 44.73it/s][A
 68%|██████▊   | 1189/1759 [00:25<00:12, 45.24it/s][A
 68%|██████▊   | 1194/1759 [00:25<00:12, 45.54it/s][A
 68%|██████▊   | 1199/1759 [00:25<00:12, 45.90it/s][A
 68%|██████▊   | 1204/1759 [00:26<00:12, 46.04it/s][A
 69%|██████▊   | 1209/1759 [00:26<00:11, 46.18it/s][A
 69%|██████▉   | 1214/1759 [00:26<00:11, 46.24it/s][A
 69%|██████▉   | 1219/1759 [00:26<00:11, 46.27it/s][A
 70%|██████▉   | 1224/1759 [00:26<00:11, 46.31it/s][A
 70%|██████▉   | 1229/1759 [00:26<00:11, 46.33it/s][A
 70%|███████   | 1234/1759 [00:26<00:11, 46.34it/s][A
 70%|███████   | 1239/1759 [00:26<00:11, 46.42it/s][A
 71%|███████   | 1244/1759 [00:26<00:11, 46.38it/s][A
 71%|███████   | 1249/1759 [00:26<00:10, 46.42it/s][A
 71%|███████▏  | 1254/1759 [00:27<00:10, 46.46it/s][A
 72%|███████▏  | 1259/1759 [00:27<00:10, 46.37it/s][A
 72%|███████▏  | 1264/1759 [00:27<00:10, 46.46it/s][A
 72%|███████▏  | 1269/1759 [00:27<00:10, 46.44it/s][A
 72%|███████▏  | 1274/1759 [00:27<00:10, 46.44it/s][A
 73%|███████▎  | 1279/1759 [00:27<00:10, 46.40it/s][A
 73%|███████▎  | 1284/1759 [00:27<00:10, 46.40it/s][A
 73%|███████▎  | 1289/1759 [00:27<00:10, 46.50it/s][A
 74%|███████▎  | 1294/1759 [00:27<00:10, 46.46it/s][A
 74%|███████▍  | 1299/1759 [00:28<00:09, 46.43it/s][A
 74%|███████▍  | 1304/1759 [00:28<00:09, 46.49it/s][A
 74%|███████▍  | 1309/1759 [00:28<00:09, 46.48it/s][A
 75%|███████▍  | 1314/1759 [00:28<00:09, 46.49it/s][A
 75%|███████▍  | 1319/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1324/1759 [00:28<00:09, 46.36it/s][A
 76%|███████▌  | 1329/1759 [00:28<00:09, 46.39it/s][A
 76%|███████▌  | 1334/1759 [00:28<00:09, 46.41it/s][A
 76%|███████▌  | 1339/1759 [00:28<00:09, 46.25it/s][A
 76%|███████▋  | 1344/1759 [00:29<00:08, 46.27it/s][A
 77%|███████▋  | 1349/1759 [00:29<00:08, 46.26it/s][A
 77%|███████▋  | 1354/1759 [00:29<00:08, 46.41it/s][A
 77%|███████▋  | 1359/1759 [00:29<00:08, 46.45it/s][A
 78%|███████▊  | 1364/1759 [00:29<00:08, 46.41it/s][A
 78%|███████▊  | 1369/1759 [00:29<00:08, 46.37it/s][A
 78%|███████▊  | 1374/1759 [00:29<00:08, 46.29it/s][A
 78%|███████▊  | 1379/1759 [00:29<00:08, 46.38it/s][A
 79%|███████▊  | 1384/1759 [00:29<00:08, 46.42it/s][A
 79%|███████▉  | 1389/1759 [00:30<00:07, 46.44it/s][A
 79%|███████▉  | 1394/1759 [00:30<00:07, 46.40it/s][A
 80%|███████▉  | 1399/1759 [00:30<00:07, 46.46it/s][A
 80%|███████▉  | 1404/1759 [00:30<00:07, 46.47it/s][A
 80%|████████  | 1409/1759 [00:30<00:07, 46.47it/s][A
 80%|████████  | 1414/1759 [00:30<00:07, 46.44it/s][A
 81%|████████  | 1419/1759 [00:30<00:07, 46.42it/s][A
 81%|████████  | 1424/1759 [00:30<00:07, 46.42it/s][A
 81%|████████  | 1429/1759 [00:30<00:07, 46.45it/s][A
 82%|████████▏ | 1434/1759 [00:30<00:06, 46.44it/s][A
 82%|████████▏ | 1439/1759 [00:31<00:06, 46.32it/s][A
 82%|████████▏ | 1444/1759 [00:31<00:06, 46.50it/s][A
 82%|████████▏ | 1449/1759 [00:31<00:06, 46.42it/s][A
 83%|████████▎ | 1454/1759 [00:31<00:06, 46.42it/s][A
 83%|████████▎ | 1459/1759 [00:31<00:06, 46.46it/s][A
 83%|████████▎ | 1464/1759 [00:31<00:06, 46.43it/s][A
 84%|████████▎ | 1469/1759 [00:31<00:06, 46.41it/s][A
 84%|████████▍ | 1474/1759 [00:31<00:06, 46.41it/s][A
 84%|████████▍ | 1479/1759 [00:31<00:06, 46.41it/s][A
 84%|████████▍ | 1484/1759 [00:32<00:05, 46.30it/s][A
 85%|████████▍ | 1489/1759 [00:32<00:05, 46.33it/s][A
 85%|████████▍ | 1494/1759 [00:32<00:05, 46.47it/s][A
 85%|████████▌ | 1499/1759 [00:32<00:05, 46.46it/s][A
 86%|████████▌ | 1504/1759 [00:32<00:05, 46.41it/s][A
 86%|████████▌ | 1509/1759 [00:32<00:05, 46.51it/s][A
 86%|████████▌ | 1514/1759 [00:32<00:05, 46.48it/s][A
 86%|████████▋ | 1519/1759 [00:32<00:05, 46.39it/s][A
 87%|████████▋ | 1524/1759 [00:32<00:05, 46.49it/s][A
 87%|████████▋ | 1529/1759 [00:33<00:04, 46.47it/s][A
 87%|████████▋ | 1534/1759 [00:33<00:04, 46.44it/s][A
 87%|████████▋ | 1539/1759 [00:33<00:04, 46.33it/s][A
 88%|████████▊ | 1544/1759 [00:33<00:04, 46.35it/s][A
 88%|████████▊ | 1549/1759 [00:33<00:04, 46.41it/s][A
 88%|████████▊ | 1554/1759 [00:33<00:04, 46.41it/s][A
 89%|████████▊ | 1559/1759 [00:33<00:04, 46.40it/s][A
 89%|████████▉ | 1564/1759 [00:33<00:04, 46.44it/s][A
 89%|████████▉ | 1569/1759 [00:33<00:04, 46.49it/s][A
 89%|████████▉ | 1574/1759 [00:33<00:03, 46.44it/s][A
 90%|████████▉ | 1579/1759 [00:34<00:03, 46.44it/s][A
 90%|█████████ | 1584/1759 [00:34<00:03, 46.34it/s][A
 90%|█████████ | 1589/1759 [00:34<00:03, 46.32it/s][A
 91%|█████████ | 1594/1759 [00:34<00:03, 46.39it/s][A
 91%|█████████ | 1599/1759 [00:34<00:03, 46.48it/s][A
 91%|█████████ | 1604/1759 [00:34<00:03, 46.37it/s][A
 91%|█████████▏| 1609/1759 [00:34<00:03, 46.41it/s][A
 92%|█████████▏| 1614/1759 [00:34<00:03, 46.45it/s][A
 92%|█████████▏| 1619/1759 [00:34<00:03, 46.39it/s][A
 92%|█████████▏| 1624/1759 [00:35<00:02, 46.50it/s][A
 93%|█████████▎| 1629/1759 [00:35<00:02, 46.41it/s][A
 93%|█████████▎| 1634/1759 [00:35<00:02, 46.38it/s][A
 93%|█████████▎| 1639/1759 [00:35<00:02, 46.40it/s][A
 93%|█████████▎| 1644/1759 [00:35<00:02, 46.48it/s][A
 94%|█████████▎| 1649/1759 [00:35<00:02, 46.36it/s][A
 94%|█████████▍| 1654/1759 [00:35<00:02, 46.35it/s][A
 94%|█████████▍| 1659/1759 [00:35<00:02, 46.39it/s][A
 95%|█████████▍| 1664/1759 [00:35<00:02, 46.41it/s][A
 95%|█████████▍| 1669/1759 [00:36<00:01, 46.44it/s][A
 95%|█████████▌| 1674/1759 [00:36<00:01, 46.41it/s][A
 95%|█████████▌| 1679/1759 [00:36<00:01, 46.40it/s][A
 96%|█████████▌| 1684/1759 [00:36<00:01, 46.42it/s][A
 96%|█████████▌| 1689/1759 [00:36<00:01, 46.51it/s][A
 96%|█████████▋| 1694/1759 [00:36<00:01, 46.44it/s][A
 97%|█████████▋| 1699/1759 [00:36<00:01, 46.37it/s][A
 97%|█████████▋| 1704/1759 [00:36<00:01, 46.41it/s][A
 97%|█████████▋| 1709/1759 [00:36<00:01, 46.41it/s][A
 97%|█████████▋| 1714/1759 [00:37<00:00, 46.42it/s][A
 98%|█████████▊| 1719/1759 [00:37<00:00, 46.32it/s][A
 98%|█████████▊| 1724/1759 [00:37<00:00, 46.33it/s][A
 98%|█████████▊| 1729/1759 [00:37<00:00, 46.42it/s][A
 99%|█████████▊| 1734/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▉| 1739/1759 [00:37<00:00, 46.44it/s][A
 99%|█████████▉| 1744/1759 [00:37<00:00, 46.46it/s][A
 99%|█████████▉| 1749/1759 [00:37<00:00, 46.46it/s][A
100%|█████████▉| 1754/1759 [00:37<00:00, 46.45it/s][A
100%|██████████| 1759/1759 [00:37<00:00, 46.46it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.46it/s][A 40%|████      | 156/390 [02:12<01:07,  3.45it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 20:26:45,536 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 20:26:45,577 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:26:50,080 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:26:50,144 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:26:50,195 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:29<1:04:20, 16.57s/it] 41%|████      | 158/390 [02:29<45:11, 11.69s/it]   41%|████      | 159/390 [02:29<31:49,  8.27s/it] 41%|████      | 160/390 [02:30<22:31,  5.87s/it] 41%|████▏     | 161/390 [02:30<16:01,  4.20s/it] 42%|████▏     | 162/390 [02:30<11:29,  3.03s/it] 42%|████▏     | 163/390 [02:30<08:20,  2.20s/it] 42%|████▏     | 164/390 [02:31<06:08,  1.63s/it] 42%|████▏     | 165/390 [02:31<04:36,  1.23s/it] 43%|████▎     | 166/390 [02:31<03:31,  1.06it/s] 43%|████▎     | 167/390 [02:32<02:46,  1.34it/s] 43%|████▎     | 168/390 [02:32<02:15,  1.64it/s] 43%|████▎     | 169/390 [02:32<01:53,  1.94it/s] 44%|████▎     | 170/390 [02:32<01:38,  2.24it/s] 44%|████▍     | 171/390 [02:33<01:27,  2.50it/s] 44%|████▍     | 172/390 [02:33<01:19,  2.73it/s] 44%|████▍     | 173/390 [02:33<01:14,  2.91it/s] 45%|████▍     | 174/390 [02:34<01:10,  3.06it/s] 45%|████▍     | 175/390 [02:34<01:07,  3.17it/s] 45%|████▌     | 176/390 [02:34<01:06,  3.22it/s] 45%|████▌     | 177/390 [02:35<01:04,  3.29it/s] 46%|████▌     | 178/390 [02:35<01:03,  3.34it/s] 46%|████▌     | 179/390 [02:35<01:02,  3.37it/s] 46%|████▌     | 180/390 [02:35<01:01,  3.40it/s] 46%|████▋     | 181/390 [02:36<01:01,  3.42it/s] 47%|████▋     | 182/390 [02:36<01:00,  3.43it/s] 47%|████▋     | 183/390 [02:36<01:00,  3.43it/s] 47%|████▋     | 184/390 [02:37<00:59,  3.44it/s] 47%|████▋     | 185/390 [02:37<00:59,  3.44it/s] 48%|████▊     | 186/390 [02:37<00:59,  3.45it/s] 48%|████▊     | 187/390 [02:37<00:59,  3.43it/s] 48%|████▊     | 188/390 [02:38<00:58,  3.44it/s] 48%|████▊     | 189/390 [02:38<00:58,  3.44it/s] 49%|████▊     | 190/390 [02:38<00:57,  3.45it/s] 49%|████▉     | 191/390 [02:39<00:57,  3.45it/s] 49%|████▉     | 192/390 [02:39<00:57,  3.45it/s] 49%|████▉     | 193/390 [02:39<00:57,  3.45it/s] 50%|████▉     | 194/390 [02:39<00:56,  3.46it/s] 50%|█████     | 195/390 [02:40<00:56,  3.46it/s] 50%|█████     | 196/390 [02:40<00:56,  3.46it/s] 51%|█████     | 197/390 [02:40<00:55,  3.45it/s] 51%|█████     | 198/390 [02:41<00:55,  3.45it/s] 51%|█████     | 199/390 [02:41<00:55,  3.45it/s] 51%|█████▏    | 200/390 [02:41<00:54,  3.46it/s] 52%|█████▏    | 201/390 [02:41<00:54,  3.45it/s] 52%|█████▏    | 202/390 [02:42<00:54,  3.46it/s] 52%|█████▏    | 203/390 [02:42<00:54,  3.46it/s] 52%|█████▏    | 204/390 [02:42<00:53,  3.46it/s] 53%|█████▎    | 205/390 [02:43<00:53,  3.45it/s] 53%|█████▎    | 206/390 [02:43<00:53,  3.46it/s] 53%|█████▎    | 207/390 [02:43<00:52,  3.46it/s] 53%|█████▎    | 208/390 [02:43<00:52,  3.46it/s] 54%|█████▎    | 209/390 [02:44<00:52,  3.43it/s] 54%|█████▍    | 210/390 [02:44<00:52,  3.44it/s] 54%|█████▍    | 211/390 [02:44<00:51,  3.44it/s] 54%|█████▍    | 212/390 [02:45<00:51,  3.45it/s] 55%|█████▍    | 213/390 [02:45<00:51,  3.45it/s] 55%|█████▍    | 214/390 [02:45<00:50,  3.45it/s] 55%|█████▌    | 215/390 [02:46<00:51,  3.42it/s] 55%|█████▌    | 216/390 [02:46<00:50,  3.43it/s] 56%|█████▌    | 217/390 [02:46<00:50,  3.44it/s] 56%|█████▌    | 218/390 [02:46<00:49,  3.44it/s] 56%|█████▌    | 219/390 [02:47<00:49,  3.44it/s] 56%|█████▋    | 220/390 [02:47<00:49,  3.44it/s] 57%|█████▋    | 221/390 [02:47<00:51,  3.30it/s] 57%|█████▋    | 222/390 [02:48<00:50,  3.33it/s] 57%|█████▋    | 223/390 [02:48<00:49,  3.37it/s] 57%|█████▋    | 224/390 [02:48<00:48,  3.39it/s] 58%|█████▊    | 225/390 [02:48<00:48,  3.41it/s] 58%|█████▊    | 226/390 [02:49<00:47,  3.42it/s] 58%|█████▊    | 227/390 [02:49<00:47,  3.43it/s] 58%|█████▊    | 228/390 [02:49<00:47,  3.44it/s] 59%|█████▊    | 229/390 [02:50<00:46,  3.45it/s] 59%|█████▉    | 230/390 [02:50<00:46,  3.45it/s] 59%|█████▉    | 231/390 [02:50<00:46,  3.42it/s] 59%|█████▉    | 232/390 [02:51<00:46,  3.43it/s] 60%|█████▉    | 233/390 [02:51<00:45,  3.43it/s] 60%|██████    | 234/390 [02:51<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 20:27:24,443 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:27:24,444 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:27:24,444 >>   Batch size = 8
{'eval_loss': 0.9328827261924744, 'eval_runtime': 37.9991, 'eval_samples_per_second': 370.193, 'eval_steps_per_second': 46.291, 'epoch': 1.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 56.90it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.56it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.84it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.13it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.70it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.43it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.15it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.77it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.66it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.80it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.78it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.75it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.78it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.76it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.79it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.72it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.49it/s][A
  5%|▌         | 93/1759 [00:01<00:38, 42.89it/s][A
  6%|▌         | 98/1759 [00:02<00:37, 43.95it/s][A
  6%|▌         | 103/1759 [00:02<00:36, 44.86it/s][A
  6%|▌         | 108/1759 [00:02<00:36, 45.45it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 45.85it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.20it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.47it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.52it/s][A
  8%|▊         | 133/1759 [00:02<00:35, 46.17it/s][A
  8%|▊         | 138/1759 [00:02<00:35, 46.07it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.26it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.44it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.54it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.65it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.70it/s][A
 10%|▉         | 168/1759 [00:03<00:33, 46.80it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.84it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.56it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.46it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.46it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.42it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.59it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.62it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.66it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.70it/s][A
 12%|█▏        | 218/1759 [00:04<00:32, 46.76it/s][A
 13%|█▎        | 223/1759 [00:04<00:33, 46.52it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.51it/s][A
 13%|█▎        | 233/1759 [00:05<00:33, 44.95it/s][A
 14%|█▎        | 238/1759 [00:05<00:33, 45.55it/s][A
 14%|█▍        | 243/1759 [00:05<00:33, 45.88it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.21it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.46it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.59it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.59it/s][A
 15%|█▌        | 268/1759 [00:05<00:32, 46.56it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.48it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.44it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.49it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.61it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.60it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.62it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.68it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.60it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.47it/s][A
 18%|█▊        | 318/1759 [00:06<00:31, 46.48it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.34it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.46it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.47it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.52it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.58it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.64it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.72it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.51it/s][A
 21%|██        | 363/1759 [00:07<00:30, 46.49it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.48it/s][A
 21%|██        | 373/1759 [00:08<00:29, 46.47it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.50it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.51it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.57it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.67it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.49it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.54it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.55it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.44it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.51it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.49it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.40it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.42it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.64it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.54it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.61it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.55it/s][A
 26%|██▌       | 458/1759 [00:09<00:28, 46.44it/s][A
 26%|██▋       | 463/1759 [00:09<00:27, 46.48it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.47it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.53it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.50it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.47it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.53it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.54it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.58it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.57it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.36it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.46it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.49it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.48it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.52it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.45it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.56it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.56it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.49it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.54it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.41it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.51it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.54it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.51it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.51it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.55it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.48it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.56it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.49it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.50it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.48it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.49it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.56it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.50it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 45.29it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 45.73it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 45.97it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.14it/s][A
 37%|███▋      | 648/1759 [00:13<00:24, 46.22it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.24it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.34it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.39it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.35it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.27it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.33it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.43it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.50it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.49it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.49it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.46it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.46it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.32it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.39it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.44it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.45it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.51it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.50it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.57it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.54it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.38it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.41it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.45it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.47it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.48it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.45it/s][A
 45%|████▍     | 783/1759 [00:16<00:20, 46.54it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.50it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.53it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.51it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.34it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.48it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.49it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.41it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.48it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.45it/s][A
 47%|████▋     | 833/1759 [00:17<00:19, 46.53it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.56it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.46it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.37it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.45it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.47it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.51it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.45it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.52it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.50it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.50it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.57it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.42it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.48it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.04it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.25it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.32it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.37it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.42it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.46it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.45it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.43it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.41it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.45it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.45it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.40it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.45it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.50it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.40it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.51it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.45it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.47it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.44it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.46it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.52it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.35it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.48it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.44it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.48it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.23it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.36it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.39it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.44it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.47it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.48it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.51it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.44it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.42it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.46it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:13, 46.46it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.49it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.48it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.41it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.52it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.48it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.36it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.47it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.43it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.48it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.49it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.43it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.39it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.47it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.46it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.50it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.48it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:11, 46.44it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.37it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.46it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.38it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.44it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.48it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.50it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.49it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.41it/s][A
 71%|███████   | 1248/1759 [00:26<00:10, 46.47it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.48it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.50it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.31it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.43it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.42it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.48it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.49it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.42it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.43it/s][A
 74%|███████▍  | 1298/1759 [00:27<00:09, 46.40it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.48it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.38it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.48it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.45it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.46it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 44.95it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 45.47it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 45.83it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:09, 46.03it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.12it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.17it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.28it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.37it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.21it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.31it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.44it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.47it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.52it/s][A
 79%|███████▉  | 1393/1759 [00:29<00:07, 46.37it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.44it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.41it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.40it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.35it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.36it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.43it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.55it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.55it/s][A
 82%|████████▏ | 1438/1759 [00:30<00:06, 46.51it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.46it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.46it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.38it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.37it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.05it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.24it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.41it/s][A
 84%|████████▍ | 1483/1759 [00:31<00:05, 46.48it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.50it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.49it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.48it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.45it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.34it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.32it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.37it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.39it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.49it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.48it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.51it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.55it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.42it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.40it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.43it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.44it/s][A
 90%|████████▉ | 1578/1759 [00:33<00:03, 46.47it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.51it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.51it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.50it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.39it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.36it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 45.50it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 45.90it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.14it/s][A
 92%|█████████▏| 1623/1759 [00:34<00:02, 46.14it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.36it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.43it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.39it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.48it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.31it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.27it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.35it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.40it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.35it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.40it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.49it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.41it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.44it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.48it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.40it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.40it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.50it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.40it/s][A
 98%|█████████▊| 1718/1759 [00:36<00:00, 46.47it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.56it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.43it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.51it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.49it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.42it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 42.87it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 44.00it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 44.67it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 44.67it/s][A 60%|██████    | 234/390 [03:29<00:45,  3.44it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 20:28:02,440 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 20:28:02,477 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:28:08,024 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:28:08,118 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:28:08,193 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:45<42:27, 16.43s/it] 61%|██████    | 236/390 [03:45<29:45, 11.59s/it] 61%|██████    | 237/390 [03:46<20:55,  8.21s/it] 61%|██████    | 238/390 [03:46<14:46,  5.83s/it] 61%|██████▏   | 239/390 [03:46<10:29,  4.17s/it] 62%|██████▏   | 240/390 [03:47<07:30,  3.00s/it] 62%|██████▏   | 241/390 [03:47<05:26,  2.19s/it] 62%|██████▏   | 242/390 [03:47<03:59,  1.62s/it] 62%|██████▏   | 243/390 [03:48<03:09,  1.29s/it] 63%|██████▎   | 244/390 [03:48<02:24,  1.01it/s] 63%|██████▎   | 245/390 [03:48<01:52,  1.28it/s] 63%|██████▎   | 246/390 [03:49<01:31,  1.58it/s] 63%|██████▎   | 247/390 [03:49<01:15,  1.89it/s] 64%|██████▎   | 248/390 [03:49<01:05,  2.18it/s] 64%|██████▍   | 249/390 [03:49<00:57,  2.46it/s] 64%|██████▍   | 250/390 [03:50<00:52,  2.69it/s] 64%|██████▍   | 251/390 [03:50<00:48,  2.89it/s] 65%|██████▍   | 252/390 [03:50<00:45,  3.04it/s] 65%|██████▍   | 253/390 [03:51<00:43,  3.16it/s] 65%|██████▌   | 254/390 [03:51<00:41,  3.25it/s] 65%|██████▌   | 255/390 [03:51<00:40,  3.31it/s] 66%|██████▌   | 256/390 [03:52<00:39,  3.36it/s] 66%|██████▌   | 257/390 [03:52<00:39,  3.33it/s] 66%|██████▌   | 258/390 [03:52<00:39,  3.37it/s] 66%|██████▋   | 259/390 [03:52<00:38,  3.40it/s] 67%|██████▋   | 260/390 [03:53<00:38,  3.42it/s] 67%|██████▋   | 261/390 [03:53<00:37,  3.43it/s] 67%|██████▋   | 262/390 [03:53<00:37,  3.44it/s] 67%|██████▋   | 263/390 [03:54<00:36,  3.45it/s] 68%|██████▊   | 264/390 [03:54<00:36,  3.46it/s] 68%|██████▊   | 265/390 [03:54<00:36,  3.46it/s] 68%|██████▊   | 266/390 [03:54<00:35,  3.46it/s] 68%|██████▊   | 267/390 [03:55<00:35,  3.46it/s] 69%|██████▊   | 268/390 [03:55<00:35,  3.45it/s] 69%|██████▉   | 269/390 [03:55<00:35,  3.45it/s] 69%|██████▉   | 270/390 [03:56<00:34,  3.46it/s] 69%|██████▉   | 271/390 [03:56<00:34,  3.46it/s] 70%|██████▉   | 272/390 [03:56<00:34,  3.46it/s] 70%|███████   | 273/390 [03:56<00:33,  3.46it/s] 70%|███████   | 274/390 [03:57<00:33,  3.47it/s] 71%|███████   | 275/390 [03:57<00:33,  3.47it/s] 71%|███████   | 276/390 [03:57<00:32,  3.47it/s] 71%|███████   | 277/390 [03:58<00:32,  3.47it/s] 71%|███████▏  | 278/390 [03:58<00:32,  3.47it/s] 72%|███████▏  | 279/390 [03:58<00:32,  3.43it/s] 72%|███████▏  | 280/390 [03:58<00:32,  3.44it/s] 72%|███████▏  | 281/390 [03:59<00:31,  3.45it/s] 72%|███████▏  | 282/390 [03:59<00:31,  3.45it/s] 73%|███████▎  | 283/390 [03:59<00:30,  3.46it/s] 73%|███████▎  | 284/390 [04:00<00:30,  3.46it/s] 73%|███████▎  | 285/390 [04:00<00:30,  3.46it/s] 73%|███████▎  | 286/390 [04:00<00:30,  3.46it/s] 74%|███████▎  | 287/390 [04:00<00:29,  3.46it/s] 74%|███████▍  | 288/390 [04:01<00:29,  3.46it/s] 74%|███████▍  | 289/390 [04:01<00:29,  3.46it/s] 74%|███████▍  | 290/390 [04:01<00:29,  3.43it/s] 75%|███████▍  | 291/390 [04:02<00:28,  3.44it/s] 75%|███████▍  | 292/390 [04:02<00:28,  3.45it/s] 75%|███████▌  | 293/390 [04:02<00:28,  3.45it/s] 75%|███████▌  | 294/390 [04:03<00:27,  3.45it/s] 76%|███████▌  | 295/390 [04:03<00:27,  3.45it/s] 76%|███████▌  | 296/390 [04:03<00:27,  3.46it/s] 76%|███████▌  | 297/390 [04:03<00:26,  3.46it/s] 76%|███████▋  | 298/390 [04:04<00:26,  3.46it/s] 77%|███████▋  | 299/390 [04:04<00:26,  3.46it/s] 77%|███████▋  | 300/390 [04:04<00:25,  3.46it/s] 77%|███████▋  | 301/390 [04:05<00:25,  3.46it/s] 77%|███████▋  | 302/390 [04:05<00:25,  3.46it/s] 78%|███████▊  | 303/390 [04:05<00:25,  3.46it/s] 78%|███████▊  | 304/390 [04:05<00:24,  3.46it/s] 78%|███████▊  | 305/390 [04:06<00:24,  3.46it/s] 78%|███████▊  | 306/390 [04:06<00:24,  3.46it/s] 79%|███████▊  | 307/390 [04:06<00:24,  3.45it/s] 79%|███████▉  | 308/390 [04:07<00:23,  3.45it/s] 79%|███████▉  | 309/390 [04:07<00:23,  3.46it/s] 79%|███████▉  | 310/390 [04:07<00:23,  3.46it/s] 80%|███████▉  | 311/390 [04:07<00:22,  3.46it/s] 80%|████████  | 312/390 [04:08<00:22,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 20:28:41,074 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:28:41,074 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:28:41,074 >>   Batch size = 8
{'eval_loss': 0.9541685581207275, 'eval_runtime': 37.9276, 'eval_samples_per_second': 370.891, 'eval_steps_per_second': 46.378, 'epoch': 2.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.30it/s][A
  1%|          | 12/1759 [00:00<00:34, 50.46it/s][A
  1%|          | 18/1759 [00:00<00:35, 48.74it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 48.07it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.73it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.52it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 47.16it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.78it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.67it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.61it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.67it/s][A
  4%|▎         | 63/1759 [00:01<00:41, 40.95it/s][A
  4%|▍         | 68/1759 [00:01<00:39, 42.50it/s][A
  4%|▍         | 73/1759 [00:01<00:38, 43.61it/s][A
  4%|▍         | 78/1759 [00:01<00:37, 44.55it/s][A
  5%|▍         | 83/1759 [00:01<00:36, 45.31it/s][A
  5%|▌         | 88/1759 [00:01<00:36, 45.82it/s][A
  5%|▌         | 93/1759 [00:02<00:36, 46.14it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.40it/s][A
  6%|▌         | 103/1759 [00:02<00:36, 45.95it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.15it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.36it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.50it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.57it/s][A
  7%|▋         | 128/1759 [00:02<00:35, 46.58it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.62it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.76it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.66it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.52it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.50it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.52it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.60it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.63it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.68it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.77it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.75it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.66it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.64it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.54it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.59it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.65it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.57it/s][A
 12%|█▏        | 218/1759 [00:04<00:33, 46.64it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.64it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.73it/s][A
 13%|█▎        | 233/1759 [00:05<00:32, 46.83it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.66it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.63it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.65it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.57it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.63it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.44it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.62it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.69it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.72it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.71it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.66it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.71it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.54it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.58it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.61it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.61it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.66it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.63it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.70it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.64it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.52it/s][A
 19%|█▉        | 343/1759 [00:07<00:37, 38.13it/s][A
 20%|█▉        | 348/1759 [00:07<00:35, 40.30it/s][A
 20%|██        | 353/1759 [00:07<00:33, 42.03it/s][A
 20%|██        | 358/1759 [00:07<00:32, 43.45it/s][A
 21%|██        | 363/1759 [00:07<00:31, 44.33it/s][A
 21%|██        | 368/1759 [00:07<00:30, 45.08it/s][A
 21%|██        | 373/1759 [00:08<00:30, 45.57it/s][A
 21%|██▏       | 378/1759 [00:08<00:30, 45.91it/s][A
 22%|██▏       | 383/1759 [00:08<00:30, 45.78it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 45.91it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 45.97it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.23it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.38it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.49it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.48it/s][A
 24%|██▍       | 418/1759 [00:09<00:28, 46.47it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.57it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.54it/s][A
 25%|██▍       | 433/1759 [00:09<00:28, 46.53it/s][A
 25%|██▍       | 438/1759 [00:09<00:28, 46.50it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 46.41it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 46.56it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.56it/s][A
 26%|██▌       | 458/1759 [00:09<00:27, 46.58it/s][A
 26%|██▋       | 463/1759 [00:10<00:27, 46.60it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.52it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.54it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.45it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.46it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.54it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.53it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.44it/s][A
 29%|██▊       | 503/1759 [00:10<00:26, 46.61it/s][A
 29%|██▉       | 508/1759 [00:11<00:26, 46.54it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.51it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.52it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.46it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.52it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.55it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.42it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.51it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.54it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.45it/s][A
 32%|███▏      | 558/1759 [00:12<00:25, 46.50it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.51it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.50it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.49it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.50it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.49it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.51it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.61it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.48it/s][A
 34%|███▍      | 603/1759 [00:13<00:24, 46.42it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.54it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.49it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.51it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.52it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.45it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.44it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.51it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.45it/s][A
 37%|███▋      | 648/1759 [00:14<00:23, 46.51it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.38it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.52it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.52it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.42it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.49it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.49it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.46it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.55it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.53it/s][A
 40%|███▉      | 698/1759 [00:15<00:22, 46.41it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.41it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.40it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.48it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.49it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.43it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.48it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.55it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.56it/s][A
 42%|████▏     | 743/1759 [00:16<00:21, 46.53it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.47it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.45it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.43it/s][A
 43%|████▎     | 763/1759 [00:16<00:22, 44.82it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 45.42it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 45.74it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 45.95it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.20it/s][A
 45%|████▍     | 788/1759 [00:17<00:20, 46.24it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.33it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.26it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.12it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.26it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.44it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.44it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.52it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.49it/s][A
 47%|████▋     | 833/1759 [00:18<00:19, 46.51it/s][A
 48%|████▊     | 838/1759 [00:18<00:19, 46.50it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.47it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.38it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.38it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.42it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.45it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.49it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.46it/s][A
 50%|████▉     | 878/1759 [00:18<00:18, 46.47it/s][A
 50%|█████     | 883/1759 [00:19<00:18, 46.48it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.51it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.51it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.33it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.01it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.11it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.27it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.34it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.38it/s][A
 53%|█████▎    | 928/1759 [00:20<00:17, 46.36it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.42it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.49it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.38it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.36it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.44it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.42it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.51it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.47it/s][A
 55%|█████▌    | 973/1759 [00:21<00:16, 46.36it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.47it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.49it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.35it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.42it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.50it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.46it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.51it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.49it/s][A
 58%|█████▊    | 1023/1759 [00:22<00:15, 46.48it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.47it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.45it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.43it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.35it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.42it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.54it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.49it/s][A
 60%|██████    | 1063/1759 [00:22<00:14, 46.49it/s][A
 61%|██████    | 1068/1759 [00:23<00:14, 46.55it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.49it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.43it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.51it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.46it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.47it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.41it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.48it/s][A
 63%|██████▎   | 1113/1759 [00:24<00:13, 46.48it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.46it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.51it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.51it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.50it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.43it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.34it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.41it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.44it/s][A
 66%|██████▌   | 1158/1759 [00:25<00:12, 46.51it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.43it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.46it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.50it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.46it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.43it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.48it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.38it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.48it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 44.36it/s][A
 69%|██████▊   | 1208/1759 [00:26<00:12, 44.94it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:12, 45.48it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 45.83it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.03it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.23it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.27it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.18it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.16it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.17it/s][A
 71%|███████   | 1253/1759 [00:27<00:10, 46.32it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.42it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.43it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.58it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:10, 46.50it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:10, 46.52it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:10, 46.51it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 46.34it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 46.26it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:09, 46.32it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:09, 46.43it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 46.51it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 46.44it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 46.52it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 46.54it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.52it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.46it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.38it/s][A
 76%|███████▋  | 1343/1759 [00:29<00:08, 46.36it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.27it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.39it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.47it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.39it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.48it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.41it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.37it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.40it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.42it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.41it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.33it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.32it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.47it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.53it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.46it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.46it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.48it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.36it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.43it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.45it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.41it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.44it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.48it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.51it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.51it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.56it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.43it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.39it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.46it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.42it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.39it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.44it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.42it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.52it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.46it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.47it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.50it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.41it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.46it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.49it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.36it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.45it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.44it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.48it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.53it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:03, 46.52it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.54it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.51it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.43it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.33it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.30it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.24it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.43it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.41it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.49it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.47it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.51it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.52it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.42it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.42it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.32it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.37it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.47it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.48it/s][A
 95%|█████████▍| 1668/1759 [00:36<00:01, 46.49it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.51it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.51it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.53it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.50it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.42it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.44it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.46it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.43it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.43it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.27it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.39it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.38it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.29it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.33it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.34it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.49it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.49it/s][A 80%|████████  | 312/390 [04:46<00:22,  3.46it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 20:29:19,115 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 20:29:19,146 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:29:23,120 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:29:23,140 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:29:23,185 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [05:01<20:44, 16.16s/it] 81%|████████  | 314/390 [05:01<14:27, 11.41s/it] 81%|████████  | 315/390 [05:02<10:05,  8.07s/it] 81%|████████  | 316/390 [05:02<07:04,  5.74s/it] 81%|████████▏ | 317/390 [05:02<04:59,  4.10s/it] 82%|████████▏ | 318/390 [05:02<03:33,  2.96s/it] 82%|████████▏ | 319/390 [05:03<02:33,  2.16s/it] 82%|████████▏ | 320/390 [05:03<01:51,  1.60s/it] 82%|████████▏ | 321/390 [05:03<01:23,  1.20s/it] 83%|████████▎ | 322/390 [05:04<01:03,  1.08it/s] 83%|████████▎ | 323/390 [05:04<00:49,  1.36it/s] 83%|████████▎ | 324/390 [05:04<00:39,  1.66it/s] 83%|████████▎ | 325/390 [05:04<00:33,  1.95it/s] 84%|████████▎ | 326/390 [05:05<00:28,  2.25it/s] 84%|████████▍ | 327/390 [05:05<00:25,  2.51it/s] 84%|████████▍ | 328/390 [05:05<00:22,  2.74it/s] 84%|████████▍ | 329/390 [05:06<00:20,  2.92it/s] 85%|████████▍ | 330/390 [05:06<00:19,  3.07it/s] 85%|████████▍ | 331/390 [05:06<00:18,  3.17it/s] 85%|████████▌ | 332/390 [05:06<00:17,  3.26it/s] 85%|████████▌ | 333/390 [05:07<00:17,  3.31it/s] 86%|████████▌ | 334/390 [05:07<00:16,  3.36it/s] 86%|████████▌ | 335/390 [05:07<00:16,  3.39it/s] 86%|████████▌ | 336/390 [05:08<00:15,  3.41it/s] 86%|████████▋ | 337/390 [05:08<00:15,  3.41it/s] 87%|████████▋ | 338/390 [05:08<00:15,  3.42it/s] 87%|████████▋ | 339/390 [05:08<00:14,  3.44it/s] 87%|████████▋ | 340/390 [05:09<00:14,  3.44it/s] 87%|████████▋ | 341/390 [05:09<00:14,  3.45it/s] 88%|████████▊ | 342/390 [05:09<00:13,  3.45it/s] 88%|████████▊ | 343/390 [05:10<00:13,  3.45it/s] 88%|████████▊ | 344/390 [05:10<00:13,  3.46it/s] 88%|████████▊ | 345/390 [05:10<00:13,  3.46it/s] 89%|████████▊ | 346/390 [05:11<00:12,  3.46it/s] 89%|████████▉ | 347/390 [05:11<00:12,  3.46it/s] 89%|████████▉ | 348/390 [05:11<00:12,  3.43it/s] 89%|████████▉ | 349/390 [05:11<00:11,  3.44it/s] 90%|████████▉ | 350/390 [05:12<00:11,  3.45it/s] 90%|█████████ | 351/390 [05:12<00:11,  3.45it/s] 90%|█████████ | 352/390 [05:12<00:10,  3.46it/s] 91%|█████████ | 353/390 [05:13<00:10,  3.46it/s] 91%|█████████ | 354/390 [05:13<00:10,  3.45it/s] 91%|█████████ | 355/390 [05:13<00:10,  3.46it/s] 91%|█████████▏| 356/390 [05:13<00:09,  3.45it/s] 92%|█████████▏| 357/390 [05:14<00:09,  3.46it/s] 92%|█████████▏| 358/390 [05:14<00:09,  3.46it/s] 92%|█████████▏| 359/390 [05:14<00:09,  3.31it/s] 92%|█████████▏| 360/390 [05:15<00:08,  3.35it/s] 93%|█████████▎| 361/390 [05:15<00:08,  3.38it/s] 93%|█████████▎| 362/390 [05:15<00:08,  3.41it/s] 93%|█████████▎| 363/390 [05:15<00:07,  3.42it/s] 93%|█████████▎| 364/390 [05:16<00:07,  3.43it/s] 94%|█████████▎| 365/390 [05:16<00:07,  3.44it/s] 94%|█████████▍| 366/390 [05:16<00:06,  3.44it/s] 94%|█████████▍| 367/390 [05:17<00:06,  3.45it/s] 94%|█████████▍| 368/390 [05:17<00:06,  3.45it/s] 95%|█████████▍| 369/390 [05:17<00:06,  3.45it/s] 95%|█████████▍| 370/390 [05:17<00:05,  3.43it/s] 95%|█████████▌| 371/390 [05:18<00:05,  3.44it/s] 95%|█████████▌| 372/390 [05:18<00:05,  3.45it/s] 96%|█████████▌| 373/390 [05:18<00:04,  3.45it/s] 96%|█████████▌| 374/390 [05:19<00:04,  3.45it/s] 96%|█████████▌| 375/390 [05:19<00:04,  3.45it/s] 96%|█████████▋| 376/390 [05:19<00:04,  3.45it/s] 97%|█████████▋| 377/390 [05:20<00:03,  3.45it/s] 97%|█████████▋| 378/390 [05:20<00:03,  3.46it/s] 97%|█████████▋| 379/390 [05:20<00:03,  3.46it/s] 97%|█████████▋| 380/390 [05:20<00:02,  3.46it/s] 98%|█████████▊| 381/390 [05:21<00:02,  3.37it/s] 98%|█████████▊| 382/390 [05:21<00:02,  3.40it/s] 98%|█████████▊| 383/390 [05:21<00:02,  3.41it/s] 98%|█████████▊| 384/390 [05:22<00:01,  3.43it/s] 99%|█████████▊| 385/390 [05:22<00:01,  3.44it/s] 99%|█████████▉| 386/390 [05:22<00:01,  3.44it/s] 99%|█████████▉| 387/390 [05:22<00:00,  3.45it/s] 99%|█████████▉| 388/390 [05:23<00:00,  3.45it/s]100%|█████████▉| 389/390 [05:23<00:00,  3.45it/s]100%|██████████| 390/390 [05:23<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 20:29:56,615 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:29:56,616 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:29:56,616 >>   Batch size = 8
{'eval_loss': 0.9596394300460815, 'eval_runtime': 37.9847, 'eval_samples_per_second': 370.333, 'eval_steps_per_second': 46.308, 'epoch': 3.99}

  0%|          | 0/1759 [00:00<?, ?it/s][A
  0%|          | 6/1759 [00:00<00:30, 57.13it/s][A
  1%|          | 12/1759 [00:00<00:35, 49.40it/s][A
  1%|          | 18/1759 [00:00<00:36, 48.08it/s][A
  1%|▏         | 23/1759 [00:00<00:36, 47.70it/s][A
  2%|▏         | 28/1759 [00:00<00:36, 47.33it/s][A
  2%|▏         | 33/1759 [00:00<00:36, 47.24it/s][A
  2%|▏         | 38/1759 [00:00<00:36, 46.95it/s][A
  2%|▏         | 43/1759 [00:00<00:36, 46.82it/s][A
  3%|▎         | 48/1759 [00:01<00:36, 46.79it/s][A
  3%|▎         | 53/1759 [00:01<00:36, 46.86it/s][A
  3%|▎         | 58/1759 [00:01<00:36, 46.79it/s][A
  4%|▎         | 63/1759 [00:01<00:36, 46.78it/s][A
  4%|▍         | 68/1759 [00:01<00:36, 46.84it/s][A
  4%|▍         | 73/1759 [00:01<00:36, 46.82it/s][A
  4%|▍         | 78/1759 [00:01<00:35, 46.77it/s][A
  5%|▍         | 83/1759 [00:01<00:35, 46.82it/s][A
  5%|▌         | 88/1759 [00:01<00:35, 46.65it/s][A
  5%|▌         | 93/1759 [00:01<00:35, 46.64it/s][A
  6%|▌         | 98/1759 [00:02<00:35, 46.69it/s][A
  6%|▌         | 103/1759 [00:02<00:35, 46.73it/s][A
  6%|▌         | 108/1759 [00:02<00:35, 46.65it/s][A
  6%|▋         | 113/1759 [00:02<00:35, 46.73it/s][A
  7%|▋         | 118/1759 [00:02<00:35, 46.72it/s][A
  7%|▋         | 123/1759 [00:02<00:35, 46.73it/s][A
  7%|▋         | 128/1759 [00:02<00:34, 46.72it/s][A
  8%|▊         | 133/1759 [00:02<00:34, 46.63it/s][A
  8%|▊         | 138/1759 [00:02<00:34, 46.55it/s][A
  8%|▊         | 143/1759 [00:03<00:34, 46.63it/s][A
  8%|▊         | 148/1759 [00:03<00:34, 46.64it/s][A
  9%|▊         | 153/1759 [00:03<00:34, 46.66it/s][A
  9%|▉         | 158/1759 [00:03<00:34, 46.64it/s][A
  9%|▉         | 163/1759 [00:03<00:34, 46.66it/s][A
 10%|▉         | 168/1759 [00:03<00:34, 46.56it/s][A
 10%|▉         | 173/1759 [00:03<00:33, 46.70it/s][A
 10%|█         | 178/1759 [00:03<00:33, 46.63it/s][A
 10%|█         | 183/1759 [00:03<00:33, 46.59it/s][A
 11%|█         | 188/1759 [00:04<00:33, 46.53it/s][A
 11%|█         | 193/1759 [00:04<00:33, 46.65it/s][A
 11%|█▏        | 198/1759 [00:04<00:33, 46.67it/s][A
 12%|█▏        | 203/1759 [00:04<00:33, 46.60it/s][A
 12%|█▏        | 208/1759 [00:04<00:33, 46.66it/s][A
 12%|█▏        | 213/1759 [00:04<00:33, 46.67it/s][A
 12%|█▏        | 218/1759 [00:04<00:32, 46.71it/s][A
 13%|█▎        | 223/1759 [00:04<00:32, 46.66it/s][A
 13%|█▎        | 228/1759 [00:04<00:32, 46.72it/s][A
 13%|█▎        | 233/1759 [00:04<00:32, 46.71it/s][A
 14%|█▎        | 238/1759 [00:05<00:32, 46.73it/s][A
 14%|█▍        | 243/1759 [00:05<00:32, 46.76it/s][A
 14%|█▍        | 248/1759 [00:05<00:32, 46.68it/s][A
 14%|█▍        | 253/1759 [00:05<00:32, 46.68it/s][A
 15%|█▍        | 258/1759 [00:05<00:32, 46.55it/s][A
 15%|█▍        | 263/1759 [00:05<00:32, 46.45it/s][A
 15%|█▌        | 268/1759 [00:05<00:31, 46.64it/s][A
 16%|█▌        | 273/1759 [00:05<00:31, 46.59it/s][A
 16%|█▌        | 278/1759 [00:05<00:31, 46.62it/s][A
 16%|█▌        | 283/1759 [00:06<00:31, 46.54it/s][A
 16%|█▋        | 288/1759 [00:06<00:31, 46.62it/s][A
 17%|█▋        | 293/1759 [00:06<00:31, 46.67it/s][A
 17%|█▋        | 298/1759 [00:06<00:31, 46.68it/s][A
 17%|█▋        | 303/1759 [00:06<00:31, 46.56it/s][A
 18%|█▊        | 308/1759 [00:06<00:31, 46.53it/s][A
 18%|█▊        | 313/1759 [00:06<00:31, 46.50it/s][A
 18%|█▊        | 318/1759 [00:06<00:30, 46.55it/s][A
 18%|█▊        | 323/1759 [00:06<00:30, 46.55it/s][A
 19%|█▊        | 328/1759 [00:07<00:30, 46.60it/s][A
 19%|█▉        | 333/1759 [00:07<00:30, 46.58it/s][A
 19%|█▉        | 338/1759 [00:07<00:30, 46.57it/s][A
 19%|█▉        | 343/1759 [00:07<00:30, 46.64it/s][A
 20%|█▉        | 348/1759 [00:07<00:30, 46.60it/s][A
 20%|██        | 353/1759 [00:07<00:30, 46.58it/s][A
 20%|██        | 358/1759 [00:07<00:30, 46.60it/s][A
 21%|██        | 363/1759 [00:07<00:29, 46.56it/s][A
 21%|██        | 368/1759 [00:07<00:29, 46.44it/s][A
 21%|██        | 373/1759 [00:07<00:29, 46.56it/s][A
 21%|██▏       | 378/1759 [00:08<00:29, 46.61it/s][A
 22%|██▏       | 383/1759 [00:08<00:29, 46.62it/s][A
 22%|██▏       | 388/1759 [00:08<00:29, 46.70it/s][A
 22%|██▏       | 393/1759 [00:08<00:29, 46.70it/s][A
 23%|██▎       | 398/1759 [00:08<00:29, 46.64it/s][A
 23%|██▎       | 403/1759 [00:08<00:29, 46.63it/s][A
 23%|██▎       | 408/1759 [00:08<00:29, 46.56it/s][A
 23%|██▎       | 413/1759 [00:08<00:28, 46.57it/s][A
 24%|██▍       | 418/1759 [00:08<00:28, 46.66it/s][A
 24%|██▍       | 423/1759 [00:09<00:28, 46.66it/s][A
 24%|██▍       | 428/1759 [00:09<00:28, 46.61it/s][A
 25%|██▍       | 433/1759 [00:09<00:29, 44.26it/s][A
 25%|██▍       | 438/1759 [00:09<00:29, 45.00it/s][A
 25%|██▌       | 443/1759 [00:09<00:28, 45.60it/s][A
 25%|██▌       | 448/1759 [00:09<00:28, 45.96it/s][A
 26%|██▌       | 453/1759 [00:09<00:28, 46.08it/s][A
 26%|██▌       | 458/1759 [00:09<00:28, 46.20it/s][A
 26%|██▋       | 463/1759 [00:09<00:28, 46.19it/s][A
 27%|██▋       | 468/1759 [00:10<00:27, 46.33it/s][A
 27%|██▋       | 473/1759 [00:10<00:27, 46.19it/s][A
 27%|██▋       | 478/1759 [00:10<00:27, 46.16it/s][A
 27%|██▋       | 483/1759 [00:10<00:27, 46.32it/s][A
 28%|██▊       | 488/1759 [00:10<00:27, 46.47it/s][A
 28%|██▊       | 493/1759 [00:10<00:27, 46.55it/s][A
 28%|██▊       | 498/1759 [00:10<00:27, 46.54it/s][A
 29%|██▊       | 503/1759 [00:10<00:27, 46.51it/s][A
 29%|██▉       | 508/1759 [00:10<00:26, 46.53it/s][A
 29%|██▉       | 513/1759 [00:11<00:26, 46.50it/s][A
 29%|██▉       | 518/1759 [00:11<00:26, 46.36it/s][A
 30%|██▉       | 523/1759 [00:11<00:26, 46.28it/s][A
 30%|███       | 528/1759 [00:11<00:26, 46.41it/s][A
 30%|███       | 533/1759 [00:11<00:26, 46.51it/s][A
 31%|███       | 538/1759 [00:11<00:26, 46.59it/s][A
 31%|███       | 543/1759 [00:11<00:26, 46.53it/s][A
 31%|███       | 548/1759 [00:11<00:26, 46.50it/s][A
 31%|███▏      | 553/1759 [00:11<00:25, 46.44it/s][A
 32%|███▏      | 558/1759 [00:11<00:25, 46.46it/s][A
 32%|███▏      | 563/1759 [00:12<00:25, 46.34it/s][A
 32%|███▏      | 568/1759 [00:12<00:25, 46.32it/s][A
 33%|███▎      | 573/1759 [00:12<00:25, 46.33it/s][A
 33%|███▎      | 578/1759 [00:12<00:25, 46.40it/s][A
 33%|███▎      | 583/1759 [00:12<00:25, 46.46it/s][A
 33%|███▎      | 588/1759 [00:12<00:25, 46.46it/s][A
 34%|███▎      | 593/1759 [00:12<00:25, 46.38it/s][A
 34%|███▍      | 598/1759 [00:12<00:24, 46.48it/s][A
 34%|███▍      | 603/1759 [00:12<00:24, 46.44it/s][A
 35%|███▍      | 608/1759 [00:13<00:24, 46.41it/s][A
 35%|███▍      | 613/1759 [00:13<00:24, 46.31it/s][A
 35%|███▌      | 618/1759 [00:13<00:24, 46.28it/s][A
 35%|███▌      | 623/1759 [00:13<00:24, 46.41it/s][A
 36%|███▌      | 628/1759 [00:13<00:24, 46.43it/s][A
 36%|███▌      | 633/1759 [00:13<00:24, 46.48it/s][A
 36%|███▋      | 638/1759 [00:13<00:24, 46.36it/s][A
 37%|███▋      | 643/1759 [00:13<00:24, 46.32it/s][A
 37%|███▋      | 648/1759 [00:13<00:23, 46.36it/s][A
 37%|███▋      | 653/1759 [00:14<00:23, 46.29it/s][A
 37%|███▋      | 658/1759 [00:14<00:23, 46.32it/s][A
 38%|███▊      | 663/1759 [00:14<00:23, 46.36it/s][A
 38%|███▊      | 668/1759 [00:14<00:23, 46.35it/s][A
 38%|███▊      | 673/1759 [00:14<00:23, 46.49it/s][A
 39%|███▊      | 678/1759 [00:14<00:23, 46.53it/s][A
 39%|███▉      | 683/1759 [00:14<00:23, 46.46it/s][A
 39%|███▉      | 688/1759 [00:14<00:23, 46.50it/s][A
 39%|███▉      | 693/1759 [00:14<00:22, 46.40it/s][A
 40%|███▉      | 698/1759 [00:14<00:22, 46.31it/s][A
 40%|███▉      | 703/1759 [00:15<00:22, 46.38it/s][A
 40%|████      | 708/1759 [00:15<00:22, 46.33it/s][A
 41%|████      | 713/1759 [00:15<00:22, 46.38it/s][A
 41%|████      | 718/1759 [00:15<00:22, 46.49it/s][A
 41%|████      | 723/1759 [00:15<00:22, 46.53it/s][A
 41%|████▏     | 728/1759 [00:15<00:22, 46.47it/s][A
 42%|████▏     | 733/1759 [00:15<00:22, 46.43it/s][A
 42%|████▏     | 738/1759 [00:15<00:21, 46.44it/s][A
 42%|████▏     | 743/1759 [00:15<00:21, 46.39it/s][A
 43%|████▎     | 748/1759 [00:16<00:21, 46.26it/s][A
 43%|████▎     | 753/1759 [00:16<00:21, 46.40it/s][A
 43%|████▎     | 758/1759 [00:16<00:21, 46.39it/s][A
 43%|████▎     | 763/1759 [00:16<00:21, 46.40it/s][A
 44%|████▎     | 768/1759 [00:16<00:21, 46.53it/s][A
 44%|████▍     | 773/1759 [00:16<00:21, 46.44it/s][A
 44%|████▍     | 778/1759 [00:16<00:21, 46.51it/s][A
 45%|████▍     | 783/1759 [00:16<00:21, 46.47it/s][A
 45%|████▍     | 788/1759 [00:16<00:20, 46.40it/s][A
 45%|████▌     | 793/1759 [00:17<00:20, 46.34it/s][A
 45%|████▌     | 798/1759 [00:17<00:20, 46.39it/s][A
 46%|████▌     | 803/1759 [00:17<00:20, 46.47it/s][A
 46%|████▌     | 808/1759 [00:17<00:20, 46.56it/s][A
 46%|████▌     | 813/1759 [00:17<00:20, 46.41it/s][A
 47%|████▋     | 818/1759 [00:17<00:20, 46.46it/s][A
 47%|████▋     | 823/1759 [00:17<00:20, 46.43it/s][A
 47%|████▋     | 828/1759 [00:17<00:20, 46.39it/s][A
 47%|████▋     | 833/1759 [00:17<00:20, 45.70it/s][A
 48%|████▊     | 838/1759 [00:18<00:20, 45.97it/s][A
 48%|████▊     | 843/1759 [00:18<00:19, 46.09it/s][A
 48%|████▊     | 848/1759 [00:18<00:19, 46.10it/s][A
 48%|████▊     | 853/1759 [00:18<00:19, 46.25it/s][A
 49%|████▉     | 858/1759 [00:18<00:19, 46.32it/s][A
 49%|████▉     | 863/1759 [00:18<00:19, 46.43it/s][A
 49%|████▉     | 868/1759 [00:18<00:19, 46.48it/s][A
 50%|████▉     | 873/1759 [00:18<00:19, 46.27it/s][A
 50%|████▉     | 878/1759 [00:18<00:19, 46.24it/s][A
 50%|█████     | 883/1759 [00:18<00:18, 46.33it/s][A
 50%|█████     | 888/1759 [00:19<00:18, 46.34it/s][A
 51%|█████     | 893/1759 [00:19<00:18, 46.40it/s][A
 51%|█████     | 898/1759 [00:19<00:18, 46.40it/s][A
 51%|█████▏    | 903/1759 [00:19<00:18, 46.49it/s][A
 52%|█████▏    | 908/1759 [00:19<00:18, 46.53it/s][A
 52%|█████▏    | 913/1759 [00:19<00:18, 46.45it/s][A
 52%|█████▏    | 918/1759 [00:19<00:18, 46.42it/s][A
 52%|█████▏    | 923/1759 [00:19<00:18, 46.37it/s][A
 53%|█████▎    | 928/1759 [00:19<00:17, 46.33it/s][A
 53%|█████▎    | 933/1759 [00:20<00:17, 46.41it/s][A
 53%|█████▎    | 938/1759 [00:20<00:17, 46.39it/s][A
 54%|█████▎    | 943/1759 [00:20<00:17, 46.36it/s][A
 54%|█████▍    | 948/1759 [00:20<00:17, 46.41it/s][A
 54%|█████▍    | 953/1759 [00:20<00:17, 46.49it/s][A
 54%|█████▍    | 958/1759 [00:20<00:17, 46.46it/s][A
 55%|█████▍    | 963/1759 [00:20<00:17, 46.39it/s][A
 55%|█████▌    | 968/1759 [00:20<00:17, 46.36it/s][A
 55%|█████▌    | 973/1759 [00:20<00:16, 46.27it/s][A
 56%|█████▌    | 978/1759 [00:21<00:16, 46.41it/s][A
 56%|█████▌    | 983/1759 [00:21<00:16, 46.43it/s][A
 56%|█████▌    | 988/1759 [00:21<00:16, 46.45it/s][A
 56%|█████▋    | 993/1759 [00:21<00:16, 46.46it/s][A
 57%|█████▋    | 998/1759 [00:21<00:16, 46.33it/s][A
 57%|█████▋    | 1003/1759 [00:21<00:16, 46.38it/s][A
 57%|█████▋    | 1008/1759 [00:21<00:16, 46.32it/s][A
 58%|█████▊    | 1013/1759 [00:21<00:16, 46.33it/s][A
 58%|█████▊    | 1018/1759 [00:21<00:15, 46.42it/s][A
 58%|█████▊    | 1023/1759 [00:21<00:15, 46.35it/s][A
 58%|█████▊    | 1028/1759 [00:22<00:15, 46.36it/s][A
 59%|█████▊    | 1033/1759 [00:22<00:15, 46.38it/s][A
 59%|█████▉    | 1038/1759 [00:22<00:15, 46.39it/s][A
 59%|█████▉    | 1043/1759 [00:22<00:15, 46.46it/s][A
 60%|█████▉    | 1048/1759 [00:22<00:15, 46.34it/s][A
 60%|█████▉    | 1053/1759 [00:22<00:15, 46.30it/s][A
 60%|██████    | 1058/1759 [00:22<00:15, 46.29it/s][A
 60%|██████    | 1063/1759 [00:22<00:15, 46.34it/s][A
 61%|██████    | 1068/1759 [00:22<00:14, 46.43it/s][A
 61%|██████    | 1073/1759 [00:23<00:14, 46.39it/s][A
 61%|██████▏   | 1078/1759 [00:23<00:14, 46.33it/s][A
 62%|██████▏   | 1083/1759 [00:23<00:14, 46.37it/s][A
 62%|██████▏   | 1088/1759 [00:23<00:14, 46.39it/s][A
 62%|██████▏   | 1093/1759 [00:23<00:14, 46.45it/s][A
 62%|██████▏   | 1098/1759 [00:23<00:14, 46.39it/s][A
 63%|██████▎   | 1103/1759 [00:23<00:14, 46.23it/s][A
 63%|██████▎   | 1108/1759 [00:23<00:14, 46.32it/s][A
 63%|██████▎   | 1113/1759 [00:23<00:14, 45.79it/s][A
 64%|██████▎   | 1118/1759 [00:24<00:13, 46.09it/s][A
 64%|██████▍   | 1123/1759 [00:24<00:13, 46.28it/s][A
 64%|██████▍   | 1128/1759 [00:24<00:13, 46.40it/s][A
 64%|██████▍   | 1133/1759 [00:24<00:13, 46.45it/s][A
 65%|██████▍   | 1138/1759 [00:24<00:13, 46.40it/s][A
 65%|██████▍   | 1143/1759 [00:24<00:13, 46.42it/s][A
 65%|██████▌   | 1148/1759 [00:24<00:13, 46.34it/s][A
 66%|██████▌   | 1153/1759 [00:24<00:13, 46.28it/s][A
 66%|██████▌   | 1158/1759 [00:24<00:12, 46.27it/s][A
 66%|██████▌   | 1163/1759 [00:25<00:12, 46.35it/s][A
 66%|██████▋   | 1168/1759 [00:25<00:12, 46.37it/s][A
 67%|██████▋   | 1173/1759 [00:25<00:12, 46.43it/s][A
 67%|██████▋   | 1178/1759 [00:25<00:12, 46.45it/s][A
 67%|██████▋   | 1183/1759 [00:25<00:12, 46.47it/s][A
 68%|██████▊   | 1188/1759 [00:25<00:12, 46.37it/s][A
 68%|██████▊   | 1193/1759 [00:25<00:12, 46.34it/s][A
 68%|██████▊   | 1198/1759 [00:25<00:12, 46.26it/s][A
 68%|██████▊   | 1203/1759 [00:25<00:12, 46.30it/s][A
 69%|██████▊   | 1208/1759 [00:25<00:11, 46.40it/s][A
 69%|██████▉   | 1213/1759 [00:26<00:11, 46.44it/s][A
 69%|██████▉   | 1218/1759 [00:26<00:11, 46.47it/s][A
 70%|██████▉   | 1223/1759 [00:26<00:11, 46.50it/s][A
 70%|██████▉   | 1228/1759 [00:26<00:11, 46.51it/s][A
 70%|███████   | 1233/1759 [00:26<00:11, 46.40it/s][A
 70%|███████   | 1238/1759 [00:26<00:11, 46.30it/s][A
 71%|███████   | 1243/1759 [00:26<00:11, 46.31it/s][A
 71%|███████   | 1248/1759 [00:26<00:11, 46.31it/s][A
 71%|███████   | 1253/1759 [00:26<00:10, 46.35it/s][A
 72%|███████▏  | 1258/1759 [00:27<00:10, 46.43it/s][A
 72%|███████▏  | 1263/1759 [00:27<00:10, 46.54it/s][A
 72%|███████▏  | 1268/1759 [00:27<00:10, 46.47it/s][A
 72%|███████▏  | 1273/1759 [00:27<00:12, 37.68it/s][A
 73%|███████▎  | 1278/1759 [00:27<00:12, 39.96it/s][A
 73%|███████▎  | 1283/1759 [00:27<00:11, 41.78it/s][A
 73%|███████▎  | 1288/1759 [00:27<00:10, 43.11it/s][A
 74%|███████▎  | 1293/1759 [00:27<00:10, 44.05it/s][A
 74%|███████▍  | 1298/1759 [00:28<00:10, 44.88it/s][A
 74%|███████▍  | 1303/1759 [00:28<00:10, 45.46it/s][A
 74%|███████▍  | 1308/1759 [00:28<00:09, 45.77it/s][A
 75%|███████▍  | 1313/1759 [00:28<00:09, 45.48it/s][A
 75%|███████▍  | 1318/1759 [00:28<00:09, 45.61it/s][A
 75%|███████▌  | 1323/1759 [00:28<00:09, 45.82it/s][A
 75%|███████▌  | 1328/1759 [00:28<00:09, 46.01it/s][A
 76%|███████▌  | 1333/1759 [00:28<00:09, 46.26it/s][A
 76%|███████▌  | 1338/1759 [00:28<00:09, 46.31it/s][A
 76%|███████▋  | 1343/1759 [00:28<00:08, 46.45it/s][A
 77%|███████▋  | 1348/1759 [00:29<00:08, 46.55it/s][A
 77%|███████▋  | 1353/1759 [00:29<00:08, 46.55it/s][A
 77%|███████▋  | 1358/1759 [00:29<00:08, 46.29it/s][A
 77%|███████▋  | 1363/1759 [00:29<00:08, 46.27it/s][A
 78%|███████▊  | 1368/1759 [00:29<00:08, 46.19it/s][A
 78%|███████▊  | 1373/1759 [00:29<00:08, 46.33it/s][A
 78%|███████▊  | 1378/1759 [00:29<00:08, 46.39it/s][A
 79%|███████▊  | 1383/1759 [00:29<00:08, 46.41it/s][A
 79%|███████▉  | 1388/1759 [00:29<00:07, 46.40it/s][A
 79%|███████▉  | 1393/1759 [00:30<00:07, 46.40it/s][A
 79%|███████▉  | 1398/1759 [00:30<00:07, 46.40it/s][A
 80%|███████▉  | 1403/1759 [00:30<00:07, 46.29it/s][A
 80%|████████  | 1408/1759 [00:30<00:07, 46.11it/s][A
 80%|████████  | 1413/1759 [00:30<00:07, 46.04it/s][A
 81%|████████  | 1418/1759 [00:30<00:07, 46.16it/s][A
 81%|████████  | 1423/1759 [00:30<00:07, 46.32it/s][A
 81%|████████  | 1428/1759 [00:30<00:07, 46.47it/s][A
 81%|████████▏ | 1433/1759 [00:30<00:07, 46.53it/s][A
 82%|████████▏ | 1438/1759 [00:31<00:06, 46.41it/s][A
 82%|████████▏ | 1443/1759 [00:31<00:06, 46.43it/s][A
 82%|████████▏ | 1448/1759 [00:31<00:06, 46.40it/s][A
 83%|████████▎ | 1453/1759 [00:31<00:06, 46.30it/s][A
 83%|████████▎ | 1458/1759 [00:31<00:06, 46.16it/s][A
 83%|████████▎ | 1463/1759 [00:31<00:06, 46.23it/s][A
 83%|████████▎ | 1468/1759 [00:31<00:06, 46.19it/s][A
 84%|████████▎ | 1473/1759 [00:31<00:06, 46.43it/s][A
 84%|████████▍ | 1478/1759 [00:31<00:06, 46.55it/s][A
 84%|████████▍ | 1483/1759 [00:32<00:05, 46.51it/s][A
 85%|████████▍ | 1488/1759 [00:32<00:05, 46.44it/s][A
 85%|████████▍ | 1493/1759 [00:32<00:05, 46.41it/s][A
 85%|████████▌ | 1498/1759 [00:32<00:05, 46.38it/s][A
 85%|████████▌ | 1503/1759 [00:32<00:05, 46.32it/s][A
 86%|████████▌ | 1508/1759 [00:32<00:05, 46.23it/s][A
 86%|████████▌ | 1513/1759 [00:32<00:05, 46.35it/s][A
 86%|████████▋ | 1518/1759 [00:32<00:05, 46.45it/s][A
 87%|████████▋ | 1523/1759 [00:32<00:05, 46.50it/s][A
 87%|████████▋ | 1528/1759 [00:32<00:04, 46.49it/s][A
 87%|████████▋ | 1533/1759 [00:33<00:04, 46.51it/s][A
 87%|████████▋ | 1538/1759 [00:33<00:04, 46.46it/s][A
 88%|████████▊ | 1543/1759 [00:33<00:04, 46.41it/s][A
 88%|████████▊ | 1548/1759 [00:33<00:04, 46.43it/s][A
 88%|████████▊ | 1553/1759 [00:33<00:04, 46.38it/s][A
 89%|████████▊ | 1558/1759 [00:33<00:04, 46.31it/s][A
 89%|████████▉ | 1563/1759 [00:33<00:04, 46.42it/s][A
 89%|████████▉ | 1568/1759 [00:33<00:04, 46.40it/s][A
 89%|████████▉ | 1573/1759 [00:33<00:04, 46.39it/s][A
 90%|████████▉ | 1578/1759 [00:34<00:03, 46.43it/s][A
 90%|████████▉ | 1583/1759 [00:34<00:03, 46.29it/s][A
 90%|█████████ | 1588/1759 [00:34<00:03, 46.41it/s][A
 91%|█████████ | 1593/1759 [00:34<00:03, 46.39it/s][A
 91%|█████████ | 1598/1759 [00:34<00:03, 46.24it/s][A
 91%|█████████ | 1603/1759 [00:34<00:03, 46.35it/s][A
 91%|█████████▏| 1608/1759 [00:34<00:03, 46.36it/s][A
 92%|█████████▏| 1613/1759 [00:34<00:03, 46.35it/s][A
 92%|█████████▏| 1618/1759 [00:34<00:03, 46.40it/s][A
 92%|█████████▏| 1623/1759 [00:35<00:02, 46.31it/s][A
 93%|█████████▎| 1628/1759 [00:35<00:02, 46.25it/s][A
 93%|█████████▎| 1633/1759 [00:35<00:02, 46.30it/s][A
 93%|█████████▎| 1638/1759 [00:35<00:02, 46.37it/s][A
 93%|█████████▎| 1643/1759 [00:35<00:02, 46.36it/s][A
 94%|█████████▎| 1648/1759 [00:35<00:02, 46.38it/s][A
 94%|█████████▍| 1653/1759 [00:35<00:02, 46.30it/s][A
 94%|█████████▍| 1658/1759 [00:35<00:02, 46.33it/s][A
 95%|█████████▍| 1663/1759 [00:35<00:02, 46.40it/s][A
 95%|█████████▍| 1668/1759 [00:35<00:01, 46.40it/s][A
 95%|█████████▌| 1673/1759 [00:36<00:01, 46.27it/s][A
 95%|█████████▌| 1678/1759 [00:36<00:01, 46.30it/s][A
 96%|█████████▌| 1683/1759 [00:36<00:01, 46.19it/s][A
 96%|█████████▌| 1688/1759 [00:36<00:01, 46.26it/s][A
 96%|█████████▌| 1693/1759 [00:36<00:01, 46.29it/s][A
 97%|█████████▋| 1698/1759 [00:36<00:01, 46.27it/s][A
 97%|█████████▋| 1703/1759 [00:36<00:01, 46.38it/s][A
 97%|█████████▋| 1708/1759 [00:36<00:01, 46.41it/s][A
 97%|█████████▋| 1713/1759 [00:36<00:00, 46.36it/s][A
 98%|█████████▊| 1718/1759 [00:37<00:00, 46.31it/s][A
 98%|█████████▊| 1723/1759 [00:37<00:00, 46.30it/s][A
 98%|█████████▊| 1728/1759 [00:37<00:00, 46.36it/s][A
 99%|█████████▊| 1733/1759 [00:37<00:00, 46.33it/s][A
 99%|█████████▉| 1738/1759 [00:37<00:00, 46.39it/s][A
 99%|█████████▉| 1743/1759 [00:37<00:00, 46.36it/s][A
 99%|█████████▉| 1748/1759 [00:37<00:00, 46.29it/s][A
100%|█████████▉| 1753/1759 [00:37<00:00, 46.45it/s][A
100%|█████████▉| 1758/1759 [00:37<00:00, 46.44it/s][A
                                                   [A                                                 
100%|██████████| 1759/1759 [00:37<00:00, 46.44it/s][A100%|██████████| 390/390 [06:01<00:00,  3.45it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 20:30:34,643 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 20:30:34,672 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:30:39,199 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:30:39,241 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:30:39,255 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:30:47,126 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:30:47,132 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78 (score: 0.9187746047973633).
                                                 100%|██████████| 390/390 [06:19<00:00,  3.45it/s]100%|██████████| 390/390 [06:19<00:00,  1.03it/s]
[INFO|trainer.py:1894] 2023-08-28 20:30:52,691 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 20:30:52,728 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:30:58,019 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:30:58,045 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:30:58,059 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:30:58,539 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   train_loss               =     0.3884
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   train_runtime            = 0:06:19.87
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   train_samples_per_second =     65.812
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:30:58,540 >>   train_steps_per_second   =      1.027
{'eval_loss': 0.96617192029953, 'eval_runtime': 37.9714, 'eval_samples_per_second': 370.463, 'eval_steps_per_second': 46.324, 'epoch': 4.99}
{'train_runtime': 379.8712, 'train_samples_per_second': 65.812, 'train_steps_per_second': 1.027, 'train_loss': 0.3884469056740785, 'epoch': 4.99}
08/28/2023 20:30:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:30:58,598 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:30:58,598 >>   Num examples = 14067
[INFO|trainer.py:2145] 2023-08-28 20:30:58,598 >>   Batch size = 8
  0%|          | 0/1759 [00:00<?, ?it/s]  0%|          | 6/1759 [00:00<00:29, 58.46it/s]  1%|          | 12/1759 [00:00<00:33, 51.50it/s]  1%|          | 18/1759 [00:00<00:35, 49.45it/s]  1%|▏         | 23/1759 [00:00<00:35, 48.68it/s]  2%|▏         | 28/1759 [00:00<00:35, 48.16it/s]  2%|▏         | 33/1759 [00:00<00:36, 47.85it/s]  2%|▏         | 38/1759 [00:00<00:36, 47.59it/s]  2%|▏         | 43/1759 [00:00<00:36, 47.47it/s]  3%|▎         | 48/1759 [00:00<00:36, 47.17it/s]  3%|▎         | 53/1759 [00:01<00:36, 47.06it/s]  3%|▎         | 58/1759 [00:01<00:36, 47.16it/s]  4%|▎         | 63/1759 [00:01<00:35, 47.20it/s]  4%|▍         | 68/1759 [00:01<00:35, 47.19it/s]  4%|▍         | 73/1759 [00:01<00:35, 47.16it/s]  4%|▍         | 78/1759 [00:01<00:35, 47.20it/s]  5%|▍         | 83/1759 [00:01<00:35, 47.17it/s]  5%|▌         | 88/1759 [00:01<00:35, 47.01it/s]  5%|▌         | 93/1759 [00:01<00:35, 46.93it/s]  6%|▌         | 98/1759 [00:02<00:35, 46.94it/s]  6%|▌         | 103/1759 [00:02<00:35, 46.89it/s]  6%|▌         | 108/1759 [00:02<00:35, 46.98it/s]  6%|▋         | 113/1759 [00:02<00:35, 46.97it/s]  7%|▋         | 118/1759 [00:02<00:34, 46.99it/s]  7%|▋         | 123/1759 [00:02<00:37, 43.07it/s]  7%|▋         | 128/1759 [00:02<00:36, 44.18it/s]  8%|▊         | 133/1759 [00:02<00:36, 45.10it/s]  8%|▊         | 138/1759 [00:02<00:35, 45.72it/s]  8%|▊         | 143/1759 [00:03<00:34, 46.20it/s]  8%|▊         | 148/1759 [00:03<00:34, 46.42it/s]  9%|▊         | 153/1759 [00:03<00:34, 46.64it/s]  9%|▉         | 158/1759 [00:03<00:34, 46.73it/s]  9%|▉         | 163/1759 [00:03<00:34, 46.40it/s] 10%|▉         | 168/1759 [00:03<00:34, 46.42it/s] 10%|▉         | 173/1759 [00:03<00:34, 46.53it/s] 10%|█         | 178/1759 [00:03<00:33, 46.70it/s] 10%|█         | 183/1759 [00:03<00:33, 46.80it/s] 11%|█         | 188/1759 [00:04<00:33, 46.95it/s] 11%|█         | 193/1759 [00:04<00:33, 47.09it/s] 11%|█▏        | 198/1759 [00:04<00:33, 47.07it/s] 12%|█▏        | 203/1759 [00:04<00:33, 46.99it/s] 12%|█▏        | 208/1759 [00:04<00:33, 46.75it/s] 12%|█▏        | 213/1759 [00:04<00:33, 46.62it/s] 12%|█▏        | 218/1759 [00:04<00:33, 46.62it/s] 13%|█▎        | 223/1759 [00:04<00:32, 46.73it/s] 13%|█▎        | 228/1759 [00:04<00:32, 46.78it/s] 13%|█▎        | 233/1759 [00:04<00:32, 46.97it/s] 14%|█▎        | 238/1759 [00:05<00:32, 47.05it/s] 14%|█▍        | 243/1759 [00:05<00:32, 47.11it/s] 14%|█▍        | 248/1759 [00:05<00:32, 46.97it/s] 14%|█▍        | 253/1759 [00:05<00:32, 46.93it/s] 15%|█▍        | 258/1759 [00:05<00:32, 46.65it/s] 15%|█▍        | 263/1759 [00:05<00:31, 46.78it/s] 15%|█▌        | 268/1759 [00:05<00:31, 46.79it/s] 16%|█▌        | 273/1759 [00:05<00:31, 46.74it/s] 16%|█▌        | 278/1759 [00:05<00:31, 46.92it/s] 16%|█▌        | 283/1759 [00:06<00:31, 47.04it/s] 16%|█▋        | 288/1759 [00:06<00:31, 47.04it/s] 17%|█▋        | 293/1759 [00:06<00:31, 47.02it/s] 17%|█▋        | 298/1759 [00:06<00:31, 46.88it/s] 17%|█▋        | 303/1759 [00:06<00:31, 46.66it/s] 18%|█▊        | 308/1759 [00:06<00:31, 46.78it/s] 18%|█▊        | 313/1759 [00:06<00:30, 46.79it/s] 18%|█▊        | 318/1759 [00:06<00:30, 46.64it/s] 18%|█▊        | 323/1759 [00:06<00:30, 46.83it/s] 19%|█▊        | 328/1759 [00:06<00:30, 46.95it/s] 19%|█▉        | 333/1759 [00:07<00:30, 46.94it/s] 19%|█▉        | 338/1759 [00:07<00:30, 46.99it/s] 19%|█▉        | 343/1759 [00:07<00:30, 46.78it/s] 20%|█▉        | 348/1759 [00:07<00:30, 46.76it/s] 20%|██        | 353/1759 [00:07<00:30, 46.75it/s] 20%|██        | 358/1759 [00:07<00:29, 46.76it/s] 21%|██        | 363/1759 [00:07<00:29, 46.76it/s] 21%|██        | 368/1759 [00:07<00:29, 46.84it/s] 21%|██        | 373/1759 [00:07<00:29, 46.88it/s] 21%|██▏       | 378/1759 [00:08<00:29, 46.92it/s] 22%|██▏       | 383/1759 [00:08<00:29, 46.96it/s] 22%|██▏       | 388/1759 [00:08<00:29, 46.93it/s] 22%|██▏       | 393/1759 [00:08<00:29, 46.72it/s] 23%|██▎       | 398/1759 [00:08<00:29, 46.69it/s] 23%|██▎       | 403/1759 [00:08<00:29, 46.72it/s] 23%|██▎       | 408/1759 [00:08<00:29, 46.29it/s] 23%|██▎       | 413/1759 [00:08<00:28, 46.53it/s] 24%|██▍       | 418/1759 [00:08<00:28, 46.68it/s] 24%|██▍       | 423/1759 [00:09<00:28, 46.85it/s] 24%|██▍       | 428/1759 [00:09<00:28, 46.94it/s] 25%|██▍       | 433/1759 [00:09<00:28, 46.95it/s] 25%|██▍       | 438/1759 [00:09<00:28, 46.83it/s] 25%|██▌       | 443/1759 [00:09<00:28, 46.71it/s] 25%|██▌       | 448/1759 [00:09<00:28, 46.70it/s] 26%|██▌       | 453/1759 [00:09<00:27, 46.82it/s] 26%|██▌       | 458/1759 [00:09<00:27, 46.89it/s] 26%|██▋       | 463/1759 [00:09<00:27, 46.94it/s] 27%|██▋       | 468/1759 [00:09<00:27, 47.08it/s] 27%|██▋       | 473/1759 [00:10<00:27, 47.01it/s] 27%|██▋       | 478/1759 [00:10<00:27, 46.95it/s] 27%|██▋       | 483/1759 [00:10<00:27, 46.93it/s] 28%|██▊       | 488/1759 [00:10<00:27, 46.89it/s] 28%|██▊       | 493/1759 [00:10<00:27, 46.86it/s] 28%|██▊       | 498/1759 [00:10<00:26, 46.81it/s] 29%|██▊       | 503/1759 [00:10<00:26, 46.82it/s] 29%|██▉       | 508/1759 [00:10<00:26, 46.91it/s] 29%|██▉       | 513/1759 [00:10<00:26, 47.01it/s] 29%|██▉       | 518/1759 [00:11<00:26, 47.03it/s] 30%|██▉       | 523/1759 [00:11<00:26, 47.00it/s] 30%|███       | 528/1759 [00:11<00:26, 46.95it/s] 30%|███       | 533/1759 [00:11<00:26, 46.91it/s] 31%|███       | 538/1759 [00:11<00:26, 46.80it/s] 31%|███       | 543/1759 [00:11<00:25, 46.86it/s] 31%|███       | 548/1759 [00:11<00:25, 46.77it/s] 31%|███▏      | 553/1759 [00:11<00:25, 46.87it/s] 32%|███▏      | 558/1759 [00:11<00:25, 46.94it/s] 32%|███▏      | 563/1759 [00:12<00:25, 46.98it/s] 32%|███▏      | 568/1759 [00:12<00:25, 46.97it/s] 33%|███▎      | 573/1759 [00:12<00:25, 46.94it/s] 33%|███▎      | 578/1759 [00:12<00:25, 46.96it/s] 33%|███▎      | 583/1759 [00:12<00:25, 46.92it/s] 33%|███▎      | 588/1759 [00:12<00:24, 46.91it/s] 34%|███▎      | 593/1759 [00:12<00:24, 46.72it/s] 34%|███▍      | 598/1759 [00:12<00:24, 46.76it/s] 34%|███▍      | 603/1759 [00:12<00:24, 46.81it/s] 35%|███▍      | 608/1759 [00:12<00:24, 46.89it/s] 35%|███▍      | 613/1759 [00:13<00:24, 46.89it/s] 35%|███▌      | 618/1759 [00:13<00:24, 46.89it/s] 35%|███▌      | 623/1759 [00:13<00:24, 46.78it/s] 36%|███▌      | 628/1759 [00:13<00:24, 46.72it/s] 36%|███▌      | 633/1759 [00:13<00:24, 46.80it/s] 36%|███▋      | 638/1759 [00:13<00:23, 46.86it/s] 37%|███▋      | 643/1759 [00:13<00:23, 46.68it/s] 37%|███▋      | 648/1759 [00:13<00:23, 46.75it/s] 37%|███▋      | 653/1759 [00:13<00:23, 46.87it/s] 37%|███▋      | 658/1759 [00:14<00:23, 46.72it/s] 38%|███▊      | 663/1759 [00:14<00:23, 46.85it/s] 38%|███▊      | 668/1759 [00:14<00:23, 46.87it/s] 38%|███▊      | 673/1759 [00:14<00:23, 46.20it/s] 39%|███▊      | 678/1759 [00:14<00:23, 46.45it/s] 39%|███▉      | 683/1759 [00:14<00:23, 46.46it/s] 39%|███▉      | 688/1759 [00:14<00:22, 46.64it/s] 39%|███▉      | 693/1759 [00:14<00:22, 46.69it/s] 40%|███▉      | 698/1759 [00:14<00:22, 46.59it/s] 40%|███▉      | 703/1759 [00:15<00:22, 46.79it/s] 40%|████      | 708/1759 [00:15<00:22, 46.87it/s] 41%|████      | 713/1759 [00:15<00:22, 46.77it/s] 41%|████      | 718/1759 [00:15<00:22, 46.85it/s] 41%|████      | 723/1759 [00:15<00:22, 46.89it/s] 41%|████▏     | 728/1759 [00:15<00:22, 46.73it/s] 42%|████▏     | 733/1759 [00:15<00:21, 46.79it/s] 42%|████▏     | 738/1759 [00:15<00:21, 46.80it/s] 42%|████▏     | 743/1759 [00:15<00:21, 46.79it/s] 43%|████▎     | 748/1759 [00:15<00:21, 46.87it/s] 43%|████▎     | 753/1759 [00:16<00:21, 46.85it/s] 43%|████▎     | 758/1759 [00:16<00:21, 46.81it/s] 43%|████▎     | 763/1759 [00:16<00:21, 46.87it/s] 44%|████▎     | 768/1759 [00:16<00:21, 46.86it/s] 44%|████▍     | 773/1759 [00:16<00:21, 46.85it/s] 44%|████▍     | 778/1759 [00:16<00:20, 46.84it/s] 45%|████▍     | 783/1759 [00:16<00:20, 46.68it/s] 45%|████▍     | 788/1759 [00:16<00:20, 46.80it/s] 45%|████▌     | 793/1759 [00:16<00:20, 46.79it/s] 45%|████▌     | 798/1759 [00:17<00:20, 46.81it/s] 46%|████▌     | 803/1759 [00:17<00:20, 46.72it/s] 46%|████▌     | 808/1759 [00:17<00:20, 46.83it/s] 46%|████▌     | 813/1759 [00:17<00:20, 46.82it/s] 47%|████▋     | 818/1759 [00:17<00:20, 46.28it/s] 47%|████▋     | 823/1759 [00:17<00:20, 46.48it/s] 47%|████▋     | 828/1759 [00:17<00:19, 46.56it/s] 47%|████▋     | 833/1759 [00:17<00:19, 46.61it/s] 48%|████▊     | 838/1759 [00:17<00:19, 46.62it/s] 48%|████▊     | 843/1759 [00:17<00:19, 46.69it/s] 48%|████▊     | 848/1759 [00:18<00:19, 46.75it/s] 48%|████▊     | 853/1759 [00:18<00:19, 46.69it/s] 49%|████▉     | 858/1759 [00:18<00:19, 46.61it/s] 49%|████▉     | 863/1759 [00:18<00:19, 46.77it/s] 49%|████▉     | 868/1759 [00:18<00:19, 46.83it/s] 50%|████▉     | 873/1759 [00:18<00:18, 46.84it/s] 50%|████▉     | 878/1759 [00:18<00:18, 46.82it/s] 50%|█████     | 883/1759 [00:18<00:18, 46.75it/s] 50%|█████     | 888/1759 [00:18<00:18, 46.68it/s] 51%|█████     | 893/1759 [00:19<00:18, 46.77it/s] 51%|█████     | 898/1759 [00:19<00:18, 46.76it/s] 51%|█████▏    | 903/1759 [00:19<00:18, 46.73it/s] 52%|█████▏    | 908/1759 [00:19<00:18, 46.68it/s] 52%|█████▏    | 913/1759 [00:19<00:18, 46.74it/s] 52%|█████▏    | 918/1759 [00:19<00:17, 46.84it/s] 52%|█████▏    | 923/1759 [00:19<00:17, 46.89it/s] 53%|█████▎    | 928/1759 [00:19<00:17, 46.80it/s] 53%|█████▎    | 933/1759 [00:19<00:17, 46.80it/s] 53%|█████▎    | 938/1759 [00:20<00:17, 46.78it/s] 54%|█████▎    | 943/1759 [00:20<00:17, 46.74it/s] 54%|█████▍    | 948/1759 [00:20<00:17, 46.86it/s] 54%|█████▍    | 953/1759 [00:20<00:17, 46.79it/s] 54%|█████▍    | 958/1759 [00:20<00:17, 46.69it/s] 55%|█████▍    | 963/1759 [00:20<00:17, 46.74it/s] 55%|█████▌    | 968/1759 [00:20<00:16, 46.84it/s] 55%|█████▌    | 973/1759 [00:20<00:16, 46.80it/s] 56%|█████▌    | 978/1759 [00:20<00:16, 46.81it/s] 56%|█████▌    | 983/1759 [00:20<00:16, 46.68it/s] 56%|█████▌    | 988/1759 [00:21<00:16, 46.54it/s] 56%|█████▋    | 993/1759 [00:21<00:16, 46.82it/s] 57%|█████▋    | 998/1759 [00:21<00:16, 46.90it/s] 57%|█████▋    | 1003/1759 [00:21<00:16, 46.81it/s] 57%|█████▋    | 1008/1759 [00:21<00:16, 46.83it/s] 58%|█████▊    | 1013/1759 [00:21<00:15, 46.88it/s] 58%|█████▊    | 1018/1759 [00:21<00:15, 46.83it/s] 58%|█████▊    | 1023/1759 [00:21<00:15, 46.83it/s] 58%|█████▊    | 1028/1759 [00:21<00:15, 46.70it/s] 59%|█████▊    | 1033/1759 [00:22<00:15, 46.70it/s] 59%|█████▉    | 1038/1759 [00:22<00:15, 46.70it/s] 59%|█████▉    | 1043/1759 [00:22<00:15, 46.69it/s] 60%|█████▉    | 1048/1759 [00:22<00:15, 46.69it/s] 60%|█████▉    | 1053/1759 [00:22<00:15, 46.84it/s] 60%|██████    | 1058/1759 [00:22<00:14, 46.76it/s] 60%|██████    | 1063/1759 [00:22<00:14, 46.76it/s] 61%|██████    | 1068/1759 [00:22<00:14, 46.81it/s] 61%|██████    | 1073/1759 [00:22<00:14, 46.68it/s] 61%|██████▏   | 1078/1759 [00:23<00:14, 46.72it/s] 62%|██████▏   | 1083/1759 [00:23<00:14, 46.75it/s] 62%|██████▏   | 1088/1759 [00:23<00:14, 46.65it/s] 62%|██████▏   | 1093/1759 [00:23<00:14, 46.59it/s] 62%|██████▏   | 1098/1759 [00:23<00:14, 46.68it/s] 63%|██████▎   | 1103/1759 [00:23<00:14, 46.80it/s] 63%|██████▎   | 1108/1759 [00:23<00:13, 46.83it/s] 63%|██████▎   | 1113/1759 [00:23<00:13, 46.64it/s] 64%|██████▎   | 1118/1759 [00:23<00:13, 46.64it/s] 64%|██████▍   | 1123/1759 [00:23<00:13, 46.69it/s] 64%|██████▍   | 1128/1759 [00:24<00:13, 46.63it/s] 64%|██████▍   | 1133/1759 [00:24<00:13, 46.61it/s] 65%|██████▍   | 1138/1759 [00:24<00:13, 46.67it/s] 65%|██████▍   | 1143/1759 [00:24<00:13, 46.64it/s] 65%|██████▌   | 1148/1759 [00:24<00:13, 46.76it/s] 66%|██████▌   | 1153/1759 [00:24<00:12, 46.76it/s] 66%|██████▌   | 1158/1759 [00:24<00:12, 46.74it/s] 66%|██████▌   | 1163/1759 [00:24<00:12, 46.76it/s] 66%|██████▋   | 1168/1759 [00:24<00:12, 46.65it/s] 67%|██████▋   | 1173/1759 [00:25<00:12, 46.53it/s] 67%|██████▋   | 1178/1759 [00:25<00:12, 46.60it/s] 67%|██████▋   | 1183/1759 [00:25<00:12, 46.64it/s] 68%|██████▊   | 1188/1759 [00:25<00:12, 45.58it/s] 68%|██████▊   | 1193/1759 [00:25<00:12, 46.64it/s] 68%|██████▊   | 1198/1759 [00:25<00:12, 46.75it/s] 68%|██████▊   | 1203/1759 [00:25<00:11, 46.75it/s] 69%|██████▊   | 1208/1759 [00:25<00:11, 46.70it/s] 69%|██████▉   | 1213/1759 [00:25<00:11, 46.68it/s] 69%|██████▉   | 1218/1759 [00:26<00:11, 46.64it/s] 70%|██████▉   | 1223/1759 [00:26<00:11, 46.56it/s] 70%|██████▉   | 1228/1759 [00:26<00:11, 46.53it/s] 70%|███████   | 1233/1759 [00:26<00:11, 46.61it/s] 70%|███████   | 1238/1759 [00:26<00:11, 46.64it/s] 71%|███████   | 1243/1759 [00:26<00:12, 40.73it/s] 71%|███████   | 1248/1759 [00:26<00:12, 42.45it/s] 71%|███████   | 1253/1759 [00:26<00:11, 43.63it/s] 72%|███████▏  | 1258/1759 [00:26<00:11, 44.61it/s] 72%|███████▏  | 1263/1759 [00:27<00:10, 45.33it/s] 72%|███████▏  | 1268/1759 [00:27<00:10, 45.78it/s] 72%|███████▏  | 1273/1759 [00:27<00:10, 46.10it/s] 73%|███████▎  | 1278/1759 [00:27<00:10, 46.31it/s] 73%|███████▎  | 1283/1759 [00:27<00:10, 46.01it/s] 73%|███████▎  | 1288/1759 [00:27<00:10, 46.14it/s] 74%|███████▎  | 1293/1759 [00:27<00:10, 46.32it/s] 74%|███████▍  | 1298/1759 [00:27<00:09, 46.28it/s] 74%|███████▍  | 1303/1759 [00:27<00:09, 46.54it/s] 74%|███████▍  | 1308/1759 [00:28<00:09, 46.72it/s] 75%|███████▍  | 1313/1759 [00:28<00:09, 46.82it/s] 75%|███████▍  | 1318/1759 [00:28<00:09, 46.83it/s] 75%|███████▌  | 1323/1759 [00:28<00:09, 46.75it/s] 75%|███████▌  | 1328/1759 [00:28<00:09, 46.54it/s] 76%|███████▌  | 1333/1759 [00:28<00:09, 46.53it/s] 76%|███████▌  | 1338/1759 [00:28<00:09, 46.53it/s] 76%|███████▋  | 1343/1759 [00:28<00:08, 46.43it/s] 77%|███████▋  | 1348/1759 [00:28<00:08, 46.62it/s] 77%|███████▋  | 1353/1759 [00:28<00:08, 46.77it/s] 77%|███████▋  | 1358/1759 [00:29<00:08, 46.78it/s] 77%|███████▋  | 1363/1759 [00:29<00:08, 46.85it/s] 78%|███████▊  | 1368/1759 [00:29<00:08, 46.62it/s] 78%|███████▊  | 1373/1759 [00:29<00:08, 46.57it/s] 78%|███████▊  | 1378/1759 [00:29<00:08, 46.46it/s] 79%|███████▊  | 1383/1759 [00:29<00:08, 46.49it/s] 79%|███████▉  | 1388/1759 [00:29<00:07, 46.56it/s] 79%|███████▉  | 1393/1759 [00:29<00:07, 46.63it/s] 79%|███████▉  | 1398/1759 [00:29<00:07, 46.66it/s] 80%|███████▉  | 1403/1759 [00:30<00:07, 46.74it/s] 80%|████████  | 1408/1759 [00:30<00:07, 46.84it/s] 80%|████████  | 1413/1759 [00:30<00:07, 46.65it/s] 81%|████████  | 1418/1759 [00:30<00:07, 46.53it/s] 81%|████████  | 1423/1759 [00:30<00:07, 46.50it/s] 81%|████████  | 1428/1759 [00:30<00:07, 46.53it/s] 81%|████████▏ | 1433/1759 [00:30<00:07, 46.52it/s] 82%|████████▏ | 1438/1759 [00:30<00:06, 46.56it/s] 82%|████████▏ | 1443/1759 [00:30<00:06, 46.72it/s] 82%|████████▏ | 1448/1759 [00:31<00:06, 46.70it/s] 83%|████████▎ | 1453/1759 [00:31<00:06, 46.78it/s] 83%|████████▎ | 1458/1759 [00:31<00:06, 46.80it/s] 83%|████████▎ | 1463/1759 [00:31<00:06, 46.73it/s] 83%|████████▎ | 1468/1759 [00:31<00:06, 46.58it/s] 84%|████████▎ | 1473/1759 [00:31<00:06, 46.41it/s] 84%|████████▍ | 1478/1759 [00:31<00:06, 46.45it/s] 84%|████████▍ | 1483/1759 [00:31<00:05, 46.54it/s] 85%|████████▍ | 1488/1759 [00:31<00:05, 46.62it/s] 85%|████████▍ | 1493/1759 [00:31<00:05, 46.65it/s] 85%|████████▌ | 1498/1759 [00:32<00:05, 46.67it/s] 85%|████████▌ | 1503/1759 [00:32<00:05, 46.74it/s] 86%|████████▌ | 1508/1759 [00:32<00:05, 46.69it/s] 86%|████████▌ | 1513/1759 [00:32<00:05, 46.66it/s] 86%|████████▋ | 1518/1759 [00:32<00:05, 46.56it/s] 87%|████████▋ | 1523/1759 [00:32<00:05, 46.49it/s] 87%|████████▋ | 1528/1759 [00:32<00:04, 46.43it/s] 87%|████████▋ | 1533/1759 [00:32<00:04, 46.57it/s] 87%|████████▋ | 1538/1759 [00:32<00:04, 46.63it/s] 88%|████████▊ | 1543/1759 [00:33<00:04, 46.77it/s] 88%|████████▊ | 1548/1759 [00:33<00:04, 46.71it/s] 88%|████████▊ | 1553/1759 [00:33<00:04, 46.74it/s] 89%|████████▊ | 1558/1759 [00:33<00:04, 46.66it/s] 89%|████████▉ | 1563/1759 [00:33<00:04, 46.56it/s] 89%|████████▉ | 1568/1759 [00:33<00:04, 46.55it/s] 89%|████████▉ | 1573/1759 [00:33<00:03, 46.52it/s] 90%|████████▉ | 1578/1759 [00:33<00:03, 46.64it/s] 90%|████████▉ | 1583/1759 [00:33<00:03, 46.68it/s] 90%|█████████ | 1588/1759 [00:34<00:03, 46.61it/s] 91%|█████████ | 1593/1759 [00:34<00:03, 46.70it/s] 91%|█████████ | 1598/1759 [00:34<00:03, 46.72it/s] 91%|█████████ | 1603/1759 [00:34<00:03, 46.66it/s] 91%|█████████▏| 1608/1759 [00:34<00:03, 46.62it/s] 92%|█████████▏| 1613/1759 [00:34<00:03, 46.60it/s] 92%|█████████▏| 1618/1759 [00:34<00:03, 46.59it/s] 92%|█████████▏| 1623/1759 [00:34<00:02, 46.64it/s] 93%|█████████▎| 1628/1759 [00:34<00:02, 46.58it/s] 93%|█████████▎| 1633/1759 [00:34<00:02, 46.65it/s] 93%|█████████▎| 1638/1759 [00:35<00:02, 46.72it/s] 93%|█████████▎| 1643/1759 [00:35<00:02, 46.66it/s] 94%|█████████▎| 1648/1759 [00:35<00:02, 46.64it/s] 94%|█████████▍| 1653/1759 [00:35<00:02, 46.51it/s] 94%|█████████▍| 1658/1759 [00:35<00:02, 46.48it/s] 95%|█████████▍| 1663/1759 [00:35<00:02, 46.53it/s] 95%|█████████▍| 1668/1759 [00:35<00:01, 46.55it/s] 95%|█████████▌| 1673/1759 [00:35<00:01, 46.62it/s] 95%|█████████▌| 1678/1759 [00:35<00:01, 46.48it/s] 96%|█████████▌| 1683/1759 [00:36<00:01, 46.63it/s] 96%|█████████▌| 1688/1759 [00:36<00:01, 46.71it/s] 96%|█████████▌| 1693/1759 [00:36<00:01, 46.63it/s] 97%|█████████▋| 1698/1759 [00:36<00:01, 46.63it/s] 97%|█████████▋| 1703/1759 [00:36<00:01, 46.61it/s] 97%|█████████▋| 1708/1759 [00:36<00:01, 46.59it/s] 97%|█████████▋| 1713/1759 [00:36<00:00, 46.53it/s] 98%|█████████▊| 1718/1759 [00:36<00:00, 46.62it/s] 98%|█████████▊| 1723/1759 [00:36<00:00, 46.61it/s] 98%|█████████▊| 1728/1759 [00:37<00:00, 46.64it/s] 99%|█████████▊| 1733/1759 [00:37<00:00, 46.63it/s] 99%|█████████▉| 1738/1759 [00:37<00:00, 46.56it/s] 99%|█████████▉| 1743/1759 [00:37<00:00, 46.60it/s] 99%|█████████▉| 1748/1759 [00:37<00:00, 46.55it/s]100%|█████████▉| 1753/1759 [00:37<00:00, 46.52it/s]100%|█████████▉| 1758/1759 [00:37<00:00, 46.57it/s]100%|██████████| 1759/1759 [00:37<00:00, 46.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:31:36,303 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,303 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,303 >>   eval_loss               =     0.9188
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,303 >>   eval_runtime            = 0:00:37.70
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,304 >>   eval_samples            =      14067
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,304 >>   eval_samples_per_second =    373.077
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,304 >>   eval_steps_per_second   =     46.651
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:31:36,304 >>   perplexity              =     2.5062
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:45,277 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:45,282 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:45,282 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:45,282 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:45,282 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:31:45,661 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:31:45,663 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:31:45,981 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:31:47,052 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:31:47,052 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:48,421 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:48,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:48,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:48,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:31:48,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:31:49,242 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:31:49,243 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:31:49,510 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:31:49,707 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:31:49,707 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/dev.jsonl', 'labels': ['country', 'part of', 'platform', 'publisher', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 22024
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22124, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:10,  1.38it/s]Extractor Predicting: 17it [00:11,  1.39it/s]Extractor Predicting: 18it [00:12,  1.39it/s]Extractor Predicting: 19it [00:12,  1.41it/s]Extractor Predicting: 20it [00:13,  1.40it/s]Extractor Predicting: 21it [00:14,  1.39it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:15,  1.39it/s]Extractor Predicting: 24it [00:16,  1.42it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.43it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.49it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:20,  1.54it/s]Extractor Predicting: 31it [00:21,  1.56it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.55it/s]Extractor Predicting: 34it [00:23,  1.55it/s]Extractor Predicting: 35it [00:23,  1.56it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:26,  1.54it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.57it/s]Extractor Predicting: 45it [00:30,  1.58it/s]Extractor Predicting: 46it [00:30,  1.59it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.54it/s]Extractor Predicting: 52it [00:34,  1.56it/s]Extractor Predicting: 53it [00:35,  1.57it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:36,  1.58it/s]Extractor Predicting: 56it [00:37,  1.58it/s]Extractor Predicting: 57it [00:37,  1.56it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.53it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:40,  1.56it/s]Extractor Predicting: 62it [00:41,  1.53it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:43,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.47it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:46,  1.56it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:51,  1.55it/s]Extractor Predicting: 79it [00:52,  1.53it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.56it/s]Extractor Predicting: 84it [00:55,  1.61it/s]Extractor Predicting: 85it [00:56,  1.59it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:57,  1.53it/s]Extractor Predicting: 88it [00:58,  1.44it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.55it/s]Extractor Predicting: 92it [01:00,  1.50it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:03,  1.61it/s]Extractor Predicting: 98it [01:04,  1.65it/s]Extractor Predicting: 99it [01:05,  1.62it/s]Extractor Predicting: 100it [01:05,  1.70it/s]Extractor Predicting: 101it [01:06,  1.66it/s]Extractor Predicting: 102it [01:06,  1.64it/s]Extractor Predicting: 103it [01:07,  1.65it/s]Extractor Predicting: 104it [01:08,  1.65it/s]Extractor Predicting: 105it [01:08,  1.68it/s]Extractor Predicting: 106it [01:09,  1.65it/s]Extractor Predicting: 107it [01:10,  1.62it/s]Extractor Predicting: 108it [01:10,  1.61it/s]Extractor Predicting: 109it [01:11,  1.61it/s]Extractor Predicting: 110it [01:11,  1.62it/s]Extractor Predicting: 111it [01:12,  1.63it/s]Extractor Predicting: 112it [01:13,  1.62it/s]Extractor Predicting: 113it [01:13,  1.63it/s]Extractor Predicting: 114it [01:14,  1.67it/s]Extractor Predicting: 115it [01:14,  1.66it/s]Extractor Predicting: 116it [01:15,  1.69it/s]Extractor Predicting: 117it [01:16,  1.68it/s]Extractor Predicting: 118it [01:16,  1.67it/s]Extractor Predicting: 119it [01:17,  1.68it/s]Extractor Predicting: 120it [01:17,  1.71it/s]Extractor Predicting: 121it [01:18,  1.69it/s]Extractor Predicting: 122it [01:19,  1.65it/s]Extractor Predicting: 123it [01:19,  1.64it/s]Extractor Predicting: 124it [01:20,  1.63it/s]Extractor Predicting: 125it [01:20,  1.64it/s]Extractor Predicting: 126it [01:21,  1.64it/s]Extractor Predicting: 127it [01:22,  1.70it/s]Extractor Predicting: 128it [01:22,  1.69it/s]Extractor Predicting: 129it [01:23,  1.70it/s]Extractor Predicting: 130it [01:23,  1.69it/s]Extractor Predicting: 131it [01:24,  1.67it/s]Extractor Predicting: 132it [01:25,  1.69it/s]Extractor Predicting: 133it [01:25,  1.65it/s]Extractor Predicting: 134it [01:26,  1.68it/s]Extractor Predicting: 135it [01:26,  1.68it/s]Extractor Predicting: 136it [01:27,  1.67it/s]Extractor Predicting: 137it [01:28,  1.66it/s]Extractor Predicting: 138it [01:28,  1.64it/s]Extractor Predicting: 139it [01:29,  1.64it/s]Extractor Predicting: 140it [01:29,  1.63it/s]Extractor Predicting: 141it [01:30,  1.65it/s]Extractor Predicting: 142it [01:31,  1.67it/s]Extractor Predicting: 143it [01:31,  1.70it/s]Extractor Predicting: 144it [01:32,  1.70it/s]Extractor Predicting: 145it [01:32,  1.64it/s]Extractor Predicting: 146it [01:33,  1.62it/s]Extractor Predicting: 147it [01:34,  1.62it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:35,  1.59it/s]Extractor Predicting: 150it [01:36,  1.59it/s]Extractor Predicting: 151it [01:36,  1.54it/s]Extractor Predicting: 152it [01:37,  1.48it/s]Extractor Predicting: 153it [01:38,  1.45it/s]Extractor Predicting: 154it [01:38,  1.46it/s]Extractor Predicting: 155it [01:39,  1.46it/s]Extractor Predicting: 156it [01:40,  1.46it/s]Extractor Predicting: 157it [01:40,  1.50it/s]Extractor Predicting: 158it [01:41,  1.48it/s]Extractor Predicting: 159it [01:42,  1.52it/s]Extractor Predicting: 160it [01:42,  1.55it/s]Extractor Predicting: 161it [01:43,  1.57it/s]Extractor Predicting: 162it [01:44,  1.56it/s]Extractor Predicting: 163it [01:44,  1.54it/s]Extractor Predicting: 164it [01:45,  1.39it/s]Extractor Predicting: 165it [01:46,  1.42it/s]Extractor Predicting: 166it [01:47,  1.43it/s]Extractor Predicting: 167it [01:47,  1.48it/s]Extractor Predicting: 168it [01:48,  1.48it/s]Extractor Predicting: 169it [01:48,  1.51it/s]Extractor Predicting: 170it [01:49,  1.55it/s]Extractor Predicting: 171it [01:50,  1.58it/s]Extractor Predicting: 172it [01:50,  1.56it/s]Extractor Predicting: 173it [01:51,  1.59it/s]Extractor Predicting: 174it [01:52,  1.59it/s]Extractor Predicting: 175it [01:52,  1.56it/s]Extractor Predicting: 176it [01:53,  1.55it/s]Extractor Predicting: 177it [01:54,  1.52it/s]Extractor Predicting: 178it [01:54,  1.59it/s]Extractor Predicting: 179it [01:55,  1.70it/s]Extractor Predicting: 180it [01:55,  1.79it/s]Extractor Predicting: 181it [01:56,  1.77it/s]Extractor Predicting: 182it [01:56,  1.77it/s]Extractor Predicting: 183it [01:57,  1.65it/s]Extractor Predicting: 184it [01:58,  1.57it/s]Extractor Predicting: 185it [01:58,  1.58it/s]Extractor Predicting: 186it [01:59,  1.56it/s]Extractor Predicting: 187it [02:00,  1.55it/s]Extractor Predicting: 188it [02:00,  1.53it/s]Extractor Predicting: 189it [02:01,  1.53it/s]Extractor Predicting: 190it [02:02,  1.51it/s]Extractor Predicting: 191it [02:02,  1.49it/s]Extractor Predicting: 192it [02:03,  1.52it/s]Extractor Predicting: 193it [02:03,  1.59it/s]Extractor Predicting: 194it [02:04,  1.61it/s]Extractor Predicting: 195it [02:05,  1.61it/s]Extractor Predicting: 196it [02:05,  1.59it/s]Extractor Predicting: 197it [02:06,  1.57it/s]Extractor Predicting: 198it [02:07,  1.53it/s]Extractor Predicting: 199it [02:07,  1.53it/s]Extractor Predicting: 200it [02:08,  1.54it/s]Extractor Predicting: 201it [02:09,  1.55it/s]Extractor Predicting: 202it [02:09,  1.54it/s]Extractor Predicting: 203it [02:10,  1.55it/s]Extractor Predicting: 204it [02:11,  1.57it/s]Extractor Predicting: 205it [02:11,  1.58it/s]Extractor Predicting: 206it [02:12,  1.58it/s]Extractor Predicting: 207it [02:12,  1.57it/s]Extractor Predicting: 208it [02:13,  1.59it/s]Extractor Predicting: 209it [02:14,  1.58it/s]Extractor Predicting: 210it [02:14,  1.60it/s]Extractor Predicting: 211it [02:15,  1.57it/s]Extractor Predicting: 212it [02:16,  1.53it/s]Extractor Predicting: 213it [02:16,  1.52it/s]Extractor Predicting: 214it [02:17,  1.52it/s]Extractor Predicting: 215it [02:18,  1.54it/s]Extractor Predicting: 216it [02:18,  1.53it/s]Extractor Predicting: 217it [02:19,  1.59it/s]Extractor Predicting: 218it [02:19,  1.60it/s]Extractor Predicting: 219it [02:20,  1.59it/s]Extractor Predicting: 220it [02:21,  1.59it/s]Extractor Predicting: 221it [02:21,  1.59it/s]Extractor Predicting: 222it [02:22,  1.59it/s]Extractor Predicting: 223it [02:23,  1.57it/s]Extractor Predicting: 224it [02:23,  1.53it/s]Extractor Predicting: 225it [02:24,  1.54it/s]Extractor Predicting: 226it [02:25,  1.56it/s]Extractor Predicting: 227it [02:25,  1.56it/s]Extractor Predicting: 228it [02:26,  1.53it/s]Extractor Predicting: 229it [02:27,  1.53it/s]Extractor Predicting: 230it [02:27,  1.52it/s]Extractor Predicting: 231it [02:28,  1.51it/s]Extractor Predicting: 232it [02:29,  1.55it/s]Extractor Predicting: 233it [02:29,  1.51it/s]Extractor Predicting: 234it [02:30,  1.50it/s]Extractor Predicting: 235it [02:31,  1.52it/s]Extractor Predicting: 236it [02:31,  1.53it/s]Extractor Predicting: 237it [02:32,  1.53it/s]Extractor Predicting: 238it [02:33,  1.50it/s]Extractor Predicting: 239it [02:33,  1.53it/s]Extractor Predicting: 240it [02:34,  1.53it/s]Extractor Predicting: 241it [02:34,  1.54it/s]Extractor Predicting: 242it [02:35,  1.48it/s]Extractor Predicting: 243it [02:36,  1.51it/s]Extractor Predicting: 244it [02:36,  1.52it/s]Extractor Predicting: 245it [02:37,  1.48it/s]Extractor Predicting: 246it [02:38,  1.52it/s]Extractor Predicting: 247it [02:38,  1.55it/s]Extractor Predicting: 248it [02:39,  1.51it/s]Extractor Predicting: 249it [02:40,  1.50it/s]Extractor Predicting: 250it [02:40,  1.56it/s]Extractor Predicting: 251it [02:41,  1.58it/s]Extractor Predicting: 252it [02:42,  1.61it/s]Extractor Predicting: 253it [02:42,  1.61it/s]Extractor Predicting: 254it [02:43,  1.61it/s]Extractor Predicting: 255it [02:43,  1.59it/s]Extractor Predicting: 256it [02:44,  1.58it/s]Extractor Predicting: 257it [02:45,  1.56it/s]Extractor Predicting: 258it [02:45,  1.58it/s]Extractor Predicting: 259it [02:46,  1.57it/s]Extractor Predicting: 260it [02:47,  1.58it/s]Extractor Predicting: 261it [02:47,  1.61it/s]Extractor Predicting: 262it [02:48,  1.64it/s]Extractor Predicting: 263it [02:48,  1.64it/s]Extractor Predicting: 264it [02:49,  1.65it/s]Extractor Predicting: 265it [02:50,  1.64it/s]Extractor Predicting: 266it [02:50,  1.63it/s]Extractor Predicting: 267it [02:51,  1.60it/s]Extractor Predicting: 268it [02:52,  1.57it/s]Extractor Predicting: 269it [02:52,  1.60it/s]Extractor Predicting: 270it [02:53,  1.59it/s]Extractor Predicting: 271it [02:53,  1.60it/s]Extractor Predicting: 272it [02:54,  1.62it/s]Extractor Predicting: 273it [02:55,  1.61it/s]Extractor Predicting: 274it [02:55,  1.62it/s]Extractor Predicting: 275it [02:56,  1.60it/s]Extractor Predicting: 276it [02:57,  1.62it/s]Extractor Predicting: 277it [02:57,  1.62it/s]Extractor Predicting: 278it [02:58,  1.61it/s]Extractor Predicting: 279it [02:58,  1.59it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [03:00,  1.56it/s]Extractor Predicting: 282it [03:00,  1.54it/s]Extractor Predicting: 283it [03:01,  1.54it/s]Extractor Predicting: 284it [03:02,  1.59it/s]Extractor Predicting: 285it [03:02,  1.57it/s]Extractor Predicting: 286it [03:03,  1.54it/s]Extractor Predicting: 287it [03:04,  1.34it/s]Extractor Predicting: 288it [03:05,  1.41it/s]Extractor Predicting: 289it [03:05,  1.40it/s]Extractor Predicting: 290it [03:06,  1.44it/s]Extractor Predicting: 291it [03:07,  1.47it/s]Extractor Predicting: 292it [03:07,  1.47it/s]Extractor Predicting: 293it [03:08,  1.48it/s]Extractor Predicting: 294it [03:09,  1.50it/s]Extractor Predicting: 295it [03:09,  1.51it/s]Extractor Predicting: 296it [03:10,  1.51it/s]Extractor Predicting: 297it [03:11,  1.51it/s]Extractor Predicting: 298it [03:11,  1.54it/s]Extractor Predicting: 299it [03:12,  1.51it/s]Extractor Predicting: 300it [03:13,  1.49it/s]Extractor Predicting: 301it [03:13,  1.49it/s]Extractor Predicting: 302it [03:14,  1.51it/s]Extractor Predicting: 303it [03:15,  1.53it/s]Extractor Predicting: 304it [03:15,  1.54it/s]Extractor Predicting: 305it [03:16,  1.56it/s]Extractor Predicting: 306it [03:16,  1.53it/s]Extractor Predicting: 307it [03:17,  1.56it/s]Extractor Predicting: 308it [03:18,  1.57it/s]Extractor Predicting: 309it [03:18,  1.55it/s]Extractor Predicting: 310it [03:19,  1.49it/s]Extractor Predicting: 311it [03:20,  1.45it/s]Extractor Predicting: 312it [03:21,  1.43it/s]Extractor Predicting: 313it [03:21,  1.41it/s]Extractor Predicting: 314it [03:22,  1.38it/s]Extractor Predicting: 315it [03:23,  1.36it/s]Extractor Predicting: 316it [03:24,  1.36it/s]Extractor Predicting: 317it [03:24,  1.37it/s]Extractor Predicting: 318it [03:25,  1.37it/s]Extractor Predicting: 319it [03:26,  1.36it/s]Extractor Predicting: 320it [03:26,  1.37it/s]Extractor Predicting: 321it [03:27,  1.35it/s]Extractor Predicting: 322it [03:28,  1.35it/s]Extractor Predicting: 323it [03:29,  1.36it/s]Extractor Predicting: 324it [03:29,  1.38it/s]Extractor Predicting: 325it [03:30,  1.37it/s]Extractor Predicting: 326it [03:31,  1.38it/s]Extractor Predicting: 327it [03:31,  1.44it/s]Extractor Predicting: 328it [03:32,  1.45it/s]Extractor Predicting: 329it [03:33,  1.48it/s]Extractor Predicting: 330it [03:33,  1.53it/s]Extractor Predicting: 331it [03:34,  1.53it/s]Extractor Predicting: 332it [03:35,  1.56it/s]Extractor Predicting: 333it [03:35,  1.55it/s]Extractor Predicting: 334it [03:36,  1.53it/s]Extractor Predicting: 335it [03:37,  1.54it/s]Extractor Predicting: 336it [03:37,  1.50it/s]Extractor Predicting: 337it [03:38,  1.55it/s]Extractor Predicting: 338it [03:39,  1.56it/s]Extractor Predicting: 339it [03:39,  1.55it/s]Extractor Predicting: 340it [03:40,  1.52it/s]Extractor Predicting: 341it [03:41,  1.48it/s]Extractor Predicting: 342it [03:41,  1.48it/s]Extractor Predicting: 343it [03:42,  1.48it/s]Extractor Predicting: 344it [03:43,  1.45it/s]Extractor Predicting: 345it [03:43,  1.44it/s]Extractor Predicting: 346it [03:44,  1.41it/s]Extractor Predicting: 347it [03:45,  1.42it/s]Extractor Predicting: 348it [03:46,  1.43it/s]Extractor Predicting: 349it [03:46,  1.45it/s]Extractor Predicting: 350it [03:47,  1.43it/s]Extractor Predicting: 351it [03:48,  1.44it/s]Extractor Predicting: 352it [03:48,  1.45it/s]Extractor Predicting: 353it [03:49,  1.44it/s]Extractor Predicting: 354it [03:50,  1.44it/s]Extractor Predicting: 355it [03:50,  1.47it/s]Extractor Predicting: 356it [03:51,  1.43it/s]Extractor Predicting: 357it [03:52,  1.45it/s]Extractor Predicting: 358it [03:52,  1.46it/s]Extractor Predicting: 359it [03:53,  1.45it/s]Extractor Predicting: 360it [03:54,  1.48it/s]Extractor Predicting: 361it [03:54,  1.52it/s]Extractor Predicting: 362it [03:55,  1.51it/s]Extractor Predicting: 363it [03:56,  1.51it/s]Extractor Predicting: 364it [03:56,  1.51it/s]Extractor Predicting: 365it [03:57,  1.55it/s]Extractor Predicting: 366it [03:58,  1.52it/s]Extractor Predicting: 367it [03:58,  1.52it/s]Extractor Predicting: 368it [03:59,  1.51it/s]Extractor Predicting: 369it [04:00,  1.50it/s]Extractor Predicting: 370it [04:00,  1.53it/s]Extractor Predicting: 371it [04:01,  1.50it/s]Extractor Predicting: 372it [04:02,  1.50it/s]Extractor Predicting: 373it [04:02,  1.48it/s]Extractor Predicting: 374it [04:03,  1.51it/s]Extractor Predicting: 375it [04:04,  1.52it/s]Extractor Predicting: 376it [04:04,  1.55it/s]Extractor Predicting: 377it [04:05,  1.54it/s]Extractor Predicting: 378it [04:06,  1.54it/s]Extractor Predicting: 379it [04:06,  1.50it/s]Extractor Predicting: 380it [04:07,  1.50it/s]Extractor Predicting: 381it [04:08,  1.33it/s]Extractor Predicting: 382it [04:09,  1.37it/s]Extractor Predicting: 383it [04:09,  1.41it/s]Extractor Predicting: 384it [04:10,  1.44it/s]Extractor Predicting: 385it [04:10,  1.47it/s]Extractor Predicting: 386it [04:11,  1.48it/s]Extractor Predicting: 387it [04:12,  1.48it/s]Extractor Predicting: 388it [04:13,  1.47it/s]Extractor Predicting: 389it [04:13,  1.52it/s]Extractor Predicting: 390it [04:14,  1.53it/s]Extractor Predicting: 391it [04:14,  1.53it/s]Extractor Predicting: 392it [04:15,  1.56it/s]Extractor Predicting: 393it [04:16,  1.54it/s]Extractor Predicting: 394it [04:16,  1.55it/s]Extractor Predicting: 395it [04:17,  1.56it/s]Extractor Predicting: 396it [04:18,  1.53it/s]Extractor Predicting: 397it [04:18,  1.54it/s]Extractor Predicting: 398it [04:19,  1.56it/s]Extractor Predicting: 399it [04:20,  1.56it/s]Extractor Predicting: 400it [04:20,  1.52it/s]Extractor Predicting: 401it [04:21,  1.47it/s]Extractor Predicting: 402it [04:22,  1.45it/s]Extractor Predicting: 403it [04:22,  1.46it/s]Extractor Predicting: 404it [04:23,  1.49it/s]Extractor Predicting: 405it [04:24,  1.50it/s]Extractor Predicting: 406it [04:24,  1.49it/s]Extractor Predicting: 407it [04:25,  1.53it/s]Extractor Predicting: 408it [04:26,  1.58it/s]Extractor Predicting: 409it [04:26,  1.60it/s]Extractor Predicting: 410it [04:27,  1.62it/s]Extractor Predicting: 411it [04:27,  1.62it/s]Extractor Predicting: 412it [04:28,  1.62it/s]Extractor Predicting: 413it [04:29,  1.66it/s]Extractor Predicting: 414it [04:29,  1.69it/s]Extractor Predicting: 415it [04:30,  1.61it/s]Extractor Predicting: 416it [04:31,  1.54it/s]Extractor Predicting: 417it [04:31,  1.47it/s]Extractor Predicting: 418it [04:32,  1.44it/s]Extractor Predicting: 419it [04:33,  1.42it/s]Extractor Predicting: 420it [04:33,  1.42it/s]Extractor Predicting: 421it [04:34,  1.40it/s]Extractor Predicting: 422it [04:35,  1.59it/s]Extractor Predicting: 422it [04:35,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:36,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:36,255 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:36,255 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:36,255 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:36,255 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:36:36,943 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:36:36,944 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:36:37,523 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:36:38,569 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:36:38,569 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:41,607 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:41,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:41,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:41,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:41,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:36:42,329 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:36:42,330 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:36:42,899 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:36:43,053 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:36:43,053 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.11966719773411223,
  "recall": 0.04805573327646264,
  "score": 0.0685737472103875,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13679
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13779, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:26,  1.49it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:30,  1.46it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.47it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.46it/s]Extractor Predicting: 55it [00:36,  1.45it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:37,  1.46it/s]Extractor Predicting: 58it [00:38,  1.45it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.56it/s]Extractor Predicting: 66it [00:43,  1.56it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:45,  1.55it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.53it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:50,  1.57it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:52,  1.56it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:53,  1.58it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:57,  1.55it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:01,  1.57it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.55it/s]Extractor Predicting: 97it [01:03,  1.59it/s]Extractor Predicting: 98it [01:04,  1.45it/s]Extractor Predicting: 99it [01:05,  1.48it/s]Extractor Predicting: 100it [01:05,  1.49it/s]Extractor Predicting: 101it [01:06,  1.47it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:07,  1.50it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:09,  1.52it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.53it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.57it/s]Extractor Predicting: 111it [01:13,  1.57it/s]Extractor Predicting: 112it [01:13,  1.54it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:15,  1.55it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.55it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.56it/s]Extractor Predicting: 119it [01:18,  1.57it/s]Extractor Predicting: 120it [01:18,  1.60it/s]Extractor Predicting: 121it [01:19,  1.60it/s]Extractor Predicting: 122it [01:20,  1.56it/s]Extractor Predicting: 123it [01:20,  1.56it/s]Extractor Predicting: 124it [01:21,  1.55it/s]Extractor Predicting: 125it [01:22,  1.51it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.55it/s]Extractor Predicting: 128it [01:24,  1.57it/s]Extractor Predicting: 129it [01:24,  1.57it/s]Extractor Predicting: 130it [01:25,  1.55it/s]Extractor Predicting: 131it [01:25,  1.52it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.55it/s]Extractor Predicting: 134it [01:27,  1.54it/s]Extractor Predicting: 135it [01:28,  1.55it/s]Extractor Predicting: 136it [01:29,  1.59it/s]Extractor Predicting: 137it [01:29,  1.59it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:31,  1.55it/s]Extractor Predicting: 140it [01:31,  1.59it/s]Extractor Predicting: 141it [01:32,  1.60it/s]Extractor Predicting: 142it [01:32,  1.62it/s]Extractor Predicting: 143it [01:33,  1.65it/s]Extractor Predicting: 144it [01:34,  1.62it/s]Extractor Predicting: 145it [01:34,  1.63it/s]Extractor Predicting: 146it [01:35,  1.62it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.61it/s]Extractor Predicting: 149it [01:37,  1.62it/s]Extractor Predicting: 150it [01:37,  1.62it/s]Extractor Predicting: 151it [01:38,  1.62it/s]Extractor Predicting: 152it [01:39,  1.61it/s]Extractor Predicting: 153it [01:39,  1.60it/s]Extractor Predicting: 154it [01:40,  1.60it/s]Extractor Predicting: 155it [01:40,  1.60it/s]Extractor Predicting: 156it [01:41,  1.60it/s]Extractor Predicting: 157it [01:42,  1.59it/s]Extractor Predicting: 158it [01:42,  1.60it/s]Extractor Predicting: 159it [01:43,  1.63it/s]Extractor Predicting: 160it [01:44,  1.65it/s]Extractor Predicting: 161it [01:44,  1.58it/s]Extractor Predicting: 162it [01:45,  1.55it/s]Extractor Predicting: 163it [01:46,  1.56it/s]Extractor Predicting: 164it [01:46,  1.61it/s]Extractor Predicting: 165it [01:47,  1.60it/s]Extractor Predicting: 166it [01:47,  1.53it/s]Extractor Predicting: 167it [01:48,  1.50it/s]Extractor Predicting: 168it [01:49,  1.48it/s]Extractor Predicting: 169it [01:50,  1.49it/s]Extractor Predicting: 170it [01:50,  1.45it/s]Extractor Predicting: 170it [01:50,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:42,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:42,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:42,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:42,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:42,452 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:38:43,227 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:38:43,228 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:38:43,851 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:38:45,121 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:38:45,121 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:47,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:47,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:47,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:47,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:47,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:38:48,088 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:38:48,089 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:38:48,530 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:38:48,744 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:38:48,745 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3192771084337349,
  "recall": 0.10404907975460123,
  "score": 0.1569498426799926,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 3869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 3969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.35it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.44it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:15,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.51it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.40it/s]Extractor Predicting: 33it [00:22,  1.47it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6238532110091743,
  "recall": 0.10845295055821372,
  "score": 0.1847826086956522,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/', 'labels': ['continent', 'owned by', 'performer', 'producer', 'replaces'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:16, 16.87s/it]Extractor Predicting: 2it [00:18,  7.99s/it]Extractor Predicting: 3it [00:19,  4.63s/it]Extractor Predicting: 4it [00:19,  3.05s/it]Extractor Predicting: 5it [00:20,  2.18s/it]Extractor Predicting: 6it [00:21,  1.67s/it]Extractor Predicting: 7it [00:21,  1.32s/it]Extractor Predicting: 8it [00:22,  1.09s/it]Extractor Predicting: 9it [00:23,  1.05it/s]Extractor Predicting: 10it [00:23,  1.18it/s]Extractor Predicting: 11it [00:24,  1.27it/s]Extractor Predicting: 12it [00:25,  1.32it/s]Extractor Predicting: 13it [00:25,  1.37it/s]Extractor Predicting: 14it [00:26,  1.41it/s]Extractor Predicting: 15it [00:26,  1.45it/s]Extractor Predicting: 16it [00:27,  1.46it/s]Extractor Predicting: 17it [00:28,  1.19it/s]Extractor Predicting: 18it [00:29,  1.30it/s]Extractor Predicting: 19it [00:30,  1.35it/s]Extractor Predicting: 20it [00:30,  1.39it/s]Extractor Predicting: 21it [00:31,  1.43it/s]Extractor Predicting: 22it [00:32,  1.45it/s]Extractor Predicting: 23it [00:32,  1.44it/s]Extractor Predicting: 24it [00:33,  1.47it/s]Extractor Predicting: 25it [00:34,  1.51it/s]Extractor Predicting: 26it [00:34,  1.54it/s]Extractor Predicting: 27it [00:35,  1.56it/s]Extractor Predicting: 28it [00:36,  1.55it/s]Extractor Predicting: 29it [00:36,  1.57it/s]Extractor Predicting: 30it [00:37,  1.54it/s]Extractor Predicting: 31it [00:37,  1.56it/s]Extractor Predicting: 32it [00:38,  1.54it/s]Extractor Predicting: 33it [00:39,  1.45it/s]Extractor Predicting: 34it [00:40,  1.47it/s]Extractor Predicting: 35it [00:40,  1.48it/s]Extractor Predicting: 36it [00:41,  1.53it/s]Extractor Predicting: 37it [00:41,  1.60it/s]Extractor Predicting: 38it [00:42,  1.57it/s]Extractor Predicting: 39it [00:43,  1.60it/s]Extractor Predicting: 40it [00:43,  1.58it/s]Extractor Predicting: 41it [00:44,  1.57it/s]Extractor Predicting: 42it [00:45,  1.57it/s]Extractor Predicting: 43it [00:45,  1.60it/s]Extractor Predicting: 44it [00:46,  1.59it/s]Extractor Predicting: 45it [00:46,  1.61it/s]Extractor Predicting: 46it [00:47,  1.58it/s]Extractor Predicting: 47it [00:48,  1.56it/s]Extractor Predicting: 48it [00:50,  1.01it/s]Extractor Predicting: 49it [00:50,  1.14it/s]Extractor Predicting: 50it [00:51,  1.26it/s]Extractor Predicting: 51it [00:51,  1.34it/s]Extractor Predicting: 52it [00:52,  1.44it/s]Extractor Predicting: 53it [00:53,  1.47it/s]Extractor Predicting: 54it [00:53,  1.51it/s]Extractor Predicting: 55it [00:54,  1.53it/s]Extractor Predicting: 56it [00:54,  1.55it/s]Extractor Predicting: 57it [00:55,  1.57it/s]Extractor Predicting: 58it [00:56,  1.52it/s]Extractor Predicting: 59it [00:56,  1.54it/s]Extractor Predicting: 60it [00:57,  1.53it/s]Extractor Predicting: 61it [00:58,  1.58it/s]Extractor Predicting: 62it [00:58,  1.59it/s]Extractor Predicting: 63it [00:59,  1.56it/s]Extractor Predicting: 64it [01:00,  1.56it/s]Extractor Predicting: 65it [01:00,  1.55it/s]Extractor Predicting: 66it [01:01,  1.57it/s]Extractor Predicting: 67it [01:02,  1.52it/s]Extractor Predicting: 68it [01:02,  1.49it/s]Extractor Predicting: 69it [01:03,  1.48it/s]Extractor Predicting: 70it [01:04,  1.52it/s]Extractor Predicting: 71it [01:04,  1.52it/s]Extractor Predicting: 72it [01:05,  1.48it/s]Extractor Predicting: 73it [01:06,  1.47it/s]Extractor Predicting: 74it [01:07,  1.29it/s]Extractor Predicting: 75it [01:07,  1.36it/s]Extractor Predicting: 76it [01:08,  1.42it/s]Extractor Predicting: 77it [01:09,  1.45it/s]Extractor Predicting: 78it [01:09,  1.48it/s]Extractor Predicting: 79it [01:10,  1.50it/s]Extractor Predicting: 80it [01:11,  1.50it/s]Extractor Predicting: 81it [01:11,  1.52it/s]Extractor Predicting: 82it [01:12,  1.57it/s]Extractor Predicting: 83it [01:12,  1.53it/s]Extractor Predicting: 84it [01:13,  1.54it/s]Extractor Predicting: 85it [01:14,  1.53it/s]Extractor Predicting: 86it [01:14,  1.54it/s]Extractor Predicting: 87it [01:15,  1.57it/s]Extractor Predicting: 88it [01:16,  1.58it/s]Extractor Predicting: 89it [01:16,  1.57it/s]Extractor Predicting: 90it [01:17,  1.55it/s]Extractor Predicting: 91it [01:18,  1.56it/s]Extractor Predicting: 92it [01:18,  1.58it/s]Extractor Predicting: 93it [01:19,  1.58it/s]Extractor Predicting: 94it [01:19,  1.55it/s]Extractor Predicting: 95it [01:20,  1.54it/s]Extractor Predicting: 96it [01:21,  1.51it/s]Extractor Predicting: 97it [01:21,  1.52it/s]Extractor Predicting: 98it [01:22,  1.51it/s]Extractor Predicting: 99it [01:23,  1.51it/s]Extractor Predicting: 100it [01:23,  1.52it/s]Extractor Predicting: 101it [01:24,  1.48it/s]Extractor Predicting: 102it [01:25,  1.48it/s]Extractor Predicting: 103it [01:26,  1.49it/s]Extractor Predicting: 104it [01:26,  1.52it/s]Extractor Predicting: 105it [01:27,  1.50it/s]Extractor Predicting: 106it [01:27,  1.54it/s]Extractor Predicting: 107it [01:28,  1.55it/s]Extractor Predicting: 108it [01:29,  1.57it/s]Extractor Predicting: 109it [01:29,  1.56it/s]Extractor Predicting: 110it [01:30,  1.55it/s]Extractor Predicting: 111it [01:31,  1.51it/s]Extractor Predicting: 112it [01:31,  1.56it/s]Extractor Predicting: 113it [01:32,  1.53it/s]Extractor Predicting: 114it [01:33,  1.52it/s]Extractor Predicting: 115it [01:33,  1.56it/s]Extractor Predicting: 116it [01:34,  1.56it/s]Extractor Predicting: 117it [01:35,  1.56it/s]Extractor Predicting: 118it [01:35,  1.50it/s]Extractor Predicting: 119it [01:36,  1.54it/s]Extractor Predicting: 120it [01:37,  1.54it/s]Extractor Predicting: 121it [01:37,  1.50it/s]Extractor Predicting: 122it [01:38,  1.52it/s]Extractor Predicting: 123it [01:38,  1.55it/s]Extractor Predicting: 124it [01:39,  1.54it/s]Extractor Predicting: 125it [01:41,  1.11it/s]Extractor Predicting: 126it [01:41,  1.22it/s]Extractor Predicting: 127it [01:42,  1.25it/s]Extractor Predicting: 128it [01:43,  1.21it/s]Extractor Predicting: 129it [01:44,  1.27it/s]Extractor Predicting: 130it [01:44,  1.31it/s]Extractor Predicting: 131it [01:45,  1.31it/s]Extractor Predicting: 132it [01:46,  1.33it/s]Extractor Predicting: 133it [01:46,  1.37it/s]Extractor Predicting: 134it [01:47,  1.42it/s]Extractor Predicting: 135it [01:48,  1.43it/s]Extractor Predicting: 136it [01:48,  1.44it/s]Extractor Predicting: 137it [01:49,  1.48it/s]Extractor Predicting: 138it [01:50,  1.43it/s]Extractor Predicting: 139it [01:51,  1.43it/s]Extractor Predicting: 140it [01:51,  1.43it/s]Extractor Predicting: 141it [01:52,  1.44it/s]Extractor Predicting: 142it [01:53,  1.44it/s]Extractor Predicting: 143it [01:53,  1.44it/s]Extractor Predicting: 144it [01:54,  1.45it/s]Extractor Predicting: 145it [01:55,  1.43it/s]Extractor Predicting: 146it [01:55,  1.45it/s]Extractor Predicting: 147it [01:56,  1.46it/s]Extractor Predicting: 148it [01:57,  1.41it/s]Extractor Predicting: 149it [01:57,  1.46it/s]Extractor Predicting: 149it [01:57,  1.26it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.245662100456621,
  "recall": 0.06660064372369398,
  "score": 0.10479158550837554,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:06,  1.76it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:08,  1.81it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:09,  1.83it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.73it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:12,  1.75it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.79it/s]Extractor Predicting: 26it [00:14,  1.62it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:16,  1.75it/s]Extractor Predicting: 29it [00:16,  1.75it/s]Extractor Predicting: 30it [00:17,  1.77it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:18,  1.79it/s]Extractor Predicting: 33it [00:18,  1.75it/s]Extractor Predicting: 34it [00:19,  1.72it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:20,  1.76it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:22,  1.72it/s]Extractor Predicting: 40it [00:22,  1.70it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.56it/s]Extractor Predicting: 46it [00:26,  1.55it/s]Extractor Predicting: 47it [00:27,  1.59it/s]Extractor Predicting: 48it [00:27,  1.62it/s]Extractor Predicting: 49it [00:28,  1.66it/s]Extractor Predicting: 50it [00:29,  1.68it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.66it/s]Extractor Predicting: 53it [00:30,  1.68it/s]Extractor Predicting: 54it [00:31,  1.69it/s]Extractor Predicting: 55it [00:32,  1.67it/s]Extractor Predicting: 56it [00:32,  1.62it/s]Extractor Predicting: 57it [00:33,  1.62it/s]Extractor Predicting: 58it [00:33,  1.65it/s]Extractor Predicting: 59it [00:34,  1.62it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:35,  1.63it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:36,  1.63it/s]Extractor Predicting: 64it [00:37,  1.66it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:39,  1.68it/s]Extractor Predicting: 69it [00:40,  1.64it/s]Extractor Predicting: 70it [00:41,  1.62it/s]Extractor Predicting: 71it [00:41,  1.62it/s]Extractor Predicting: 72it [00:42,  1.64it/s]Extractor Predicting: 73it [00:43,  1.63it/s]Extractor Predicting: 74it [00:43,  1.64it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:44,  1.64it/s]Extractor Predicting: 77it [00:45,  1.64it/s]Extractor Predicting: 78it [00:46,  1.59it/s]Extractor Predicting: 79it [00:46,  1.62it/s]Extractor Predicting: 80it [00:47,  1.67it/s]Extractor Predicting: 81it [00:47,  1.63it/s]Extractor Predicting: 82it [00:48,  1.61it/s]Extractor Predicting: 83it [00:49,  1.61it/s]Extractor Predicting: 84it [00:49,  1.64it/s]Extractor Predicting: 85it [00:50,  1.62it/s]Extractor Predicting: 86it [00:51,  1.49it/s]Extractor Predicting: 87it [00:51,  1.55it/s]Extractor Predicting: 88it [00:52,  1.60it/s]Extractor Predicting: 89it [00:52,  1.66it/s]Extractor Predicting: 90it [00:53,  1.67it/s]Extractor Predicting: 91it [00:54,  1.67it/s]Extractor Predicting: 92it [00:54,  1.67it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.68it/s]Extractor Predicting: 95it [00:56,  1.67it/s]Extractor Predicting: 96it [00:57,  1.71it/s]Extractor Predicting: 97it [00:57,  1.74it/s]Extractor Predicting: 98it [00:58,  1.72it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:59,  1.73it/s]Extractor Predicting: 101it [00:59,  1.72it/s]Extractor Predicting: 102it [01:00,  1.65it/s]Extractor Predicting: 103it [01:01,  1.65it/s]Extractor Predicting: 104it [01:01,  1.69it/s]Extractor Predicting: 105it [01:02,  1.64it/s]Extractor Predicting: 106it [01:02,  1.67it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:04,  1.64it/s]Extractor Predicting: 109it [01:04,  1.64it/s]Extractor Predicting: 110it [01:05,  1.67it/s]Extractor Predicting: 111it [01:05,  1.65it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.69it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:08,  1.72it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:10,  1.77it/s]Extractor Predicting: 119it [01:10,  1.78it/s]Extractor Predicting: 120it [01:11,  1.76it/s]Extractor Predicting: 121it [01:11,  1.76it/s]Extractor Predicting: 122it [01:12,  1.75it/s]Extractor Predicting: 123it [01:12,  1.72it/s]Extractor Predicting: 124it [01:13,  1.74it/s]Extractor Predicting: 125it [01:14,  1.73it/s]Extractor Predicting: 126it [01:14,  1.71it/s]Extractor Predicting: 127it [01:15,  1.67it/s]Extractor Predicting: 128it [01:15,  1.63it/s]Extractor Predicting: 129it [01:16,  1.61it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:17,  1.57it/s]Extractor Predicting: 132it [01:18,  1.59it/s]Extractor Predicting: 133it [01:19,  1.60it/s]Extractor Predicting: 134it [01:19,  1.64it/s]Extractor Predicting: 135it [01:20,  1.58it/s]Extractor Predicting: 136it [01:20,  1.57it/s]Extractor Predicting: 137it [01:21,  1.56it/s]Extractor Predicting: 138it [01:22,  1.58it/s]Extractor Predicting: 139it [01:22,  1.58it/s]Extractor Predicting: 140it [01:23,  1.54it/s]Extractor Predicting: 141it [01:24,  1.55it/s]Extractor Predicting: 142it [01:24,  1.56it/s]Extractor Predicting: 143it [01:25,  1.59it/s]Extractor Predicting: 144it [01:26,  1.57it/s]Extractor Predicting: 145it [01:26,  1.55it/s]Extractor Predicting: 146it [01:27,  1.55it/s]Extractor Predicting: 147it [01:28,  1.57it/s]Extractor Predicting: 148it [01:28,  1.58it/s]Extractor Predicting: 149it [01:29,  1.61it/s]Extractor Predicting: 150it [01:29,  1.60it/s]Extractor Predicting: 151it [01:30,  1.59it/s]Extractor Predicting: 152it [01:31,  1.59it/s]Extractor Predicting: 153it [01:31,  1.53it/s]Extractor Predicting: 154it [01:32,  1.53it/s]Extractor Predicting: 155it [01:33,  1.56it/s]Extractor Predicting: 156it [01:33,  1.62it/s]Extractor Predicting: 157it [01:34,  1.67it/s]Extractor Predicting: 158it [01:34,  1.69it/s]Extractor Predicting: 159it [01:35,  1.69it/s]Extractor Predicting: 160it [01:35,  1.71it/s]Extractor Predicting: 161it [01:36,  1.75it/s]Extractor Predicting: 162it [01:37,  1.71it/s]Extractor Predicting: 163it [01:37,  1.69it/s]Extractor Predicting: 164it [01:38,  1.71it/s]Extractor Predicting: 165it [01:38,  1.75it/s]Extractor Predicting: 166it [01:39,  1.74it/s]Extractor Predicting: 167it [01:39,  1.80it/s]Extractor Predicting: 168it [01:40,  1.81it/s]Extractor Predicting: 169it [01:41,  1.83it/s]Extractor Predicting: 170it [01:41,  1.77it/s]Extractor Predicting: 171it [01:42,  1.69it/s]Extractor Predicting: 172it [01:42,  1.66it/s]Extractor Predicting: 173it [01:43,  1.73it/s]Extractor Predicting: 173it [01:43,  1.67it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2526690391459075,
  "recall": 0.05137481910274964,
  "score": 0.08538785327720985,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.44it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:10,  1.75it/s]Extractor Predicting: 17it [00:10,  1.84it/s]Extractor Predicting: 18it [00:11,  1.88it/s]Extractor Predicting: 19it [00:11,  1.89it/s]Extractor Predicting: 20it [00:12,  1.92it/s]Extractor Predicting: 21it [00:12,  1.96it/s]Extractor Predicting: 22it [00:13,  1.92it/s]Extractor Predicting: 23it [00:13,  1.91it/s]Extractor Predicting: 24it [00:14,  1.90it/s]Extractor Predicting: 25it [00:14,  1.90it/s]Extractor Predicting: 26it [00:15,  1.94it/s]Extractor Predicting: 27it [00:15,  1.89it/s]Extractor Predicting: 28it [00:16,  1.91it/s]Extractor Predicting: 29it [00:16,  1.92it/s]Extractor Predicting: 30it [00:17,  1.95it/s]Extractor Predicting: 31it [00:17,  1.97it/s]Extractor Predicting: 32it [00:18,  1.96it/s]Extractor Predicting: 33it [00:18,  1.97it/s]Extractor Predicting: 34it [00:19,  1.96it/s]Extractor Predicting: 35it [00:19,  1.94it/s]Extractor Predicting: 36it [00:20,  1.89it/s]Extractor Predicting: 37it [00:21,  1.85it/s]Extractor Predicting: 38it [00:21,  1.92it/s]Extractor Predicting: 39it [00:21,  1.92it/s]Extractor Predicting: 40it [00:22,  1.92it/s]Extractor Predicting: 41it [00:23,  1.91it/s]Extractor Predicting: 42it [00:23,  1.94it/s]Extractor Predicting: 43it [00:24,  1.90it/s]Extractor Predicting: 44it [00:24,  1.74it/s]Extractor Predicting: 45it [00:25,  1.64it/s]Extractor Predicting: 46it [00:26,  1.57it/s]Extractor Predicting: 47it [00:26,  1.53it/s]Extractor Predicting: 48it [00:27,  1.44it/s]Extractor Predicting: 49it [00:28,  1.46it/s]Extractor Predicting: 50it [00:28,  1.47it/s]Extractor Predicting: 51it [00:29,  1.47it/s]Extractor Predicting: 52it [00:30,  1.47it/s]Extractor Predicting: 53it [00:31,  1.47it/s]Extractor Predicting: 54it [00:31,  1.46it/s]Extractor Predicting: 55it [00:32,  1.49it/s]Extractor Predicting: 56it [00:33,  1.46it/s]Extractor Predicting: 57it [00:33,  1.46it/s]Extractor Predicting: 58it [00:34,  1.47it/s]Extractor Predicting: 59it [00:35,  1.51it/s]Extractor Predicting: 59it [00:35,  1.68it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7274633123689728,
  "recall": 0.22437762689945037,
  "score": 0.3429701013096121,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:12, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:18, 17.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:52<02:07, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:10<01:46, 17.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:26<01:26, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:43<01:08, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:01<00:51, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:15<00:32, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:28<00:15, 15.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:45<00:00, 15.80s/it]Generating: 100%|██████████| 10/10 [02:45<00:00, 16.55s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : given name .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('', 'given name', 'James Kirkwood', 'James Kirkwood ( born March 11 , 1984 in Houston , Texas ) is an American professional ice hockey goaltender currently playing for the Boston Bruins of the National Hockey League .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 447, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 514, 'raw': 704}
{'target': 600, 'success': 537, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : Later in the year ( October 18 &mdash; June 6 , 1914 ) at the end of World War I , he sailed around the eastern part of the Great Rift Valley , passing through Baja California &mdash; South . Head Entity : Baja California , Tail Entity : the south .\n']
['Relation : lowest point . Context : Later in the year ( October 18 &mdash; June 6 , 1914 ) at the end of World War I , he sailed around the eastern part of the Great Rift Valley , passing through Baja California &mdash; South . Head Entity : Baja California , Tail Entity : the south .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to enter the imperial territory of China . Head Entity : Liu Bang , Tail Entity : emperor territory .\n']
['Relation : lowest point . Context : Later in the year ( October 18 &mdash; June 6 , 1914 ) at the end of World War I , he sailed around the eastern part of the Great Rift Valley , passing through Baja California &mdash; South . Head Entity : Baja California , Tail Entity : the south .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to enter the imperial territory of China . Head Entity : Liu Bang , Tail Entity : emperor territory .\n', 'Relation : lowest point . Context : This was the third studio album by the British alternative rock band The Stooges , released on 11 September 2000 on Capitol Records . Head Entity : Capitol Records , Tail Entity : 17 September 2000 .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 338, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 520, 'raw': 736}
{'target': 600, 'success': 537, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 583, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.6967592592592593, 'errors': {'', "('New York Rangers', 'lowest point', '', 'He played in nine NHL seasons , being traded to the New York Rangers for Sidney Crosby , a draft pick also on the same team , and the same number of games .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8151041666666666, 'errors': {'', "('Baltic Sea', 'mouth of the watercourse', '', 'It is found in the Baltic Sea and on the north coast of western Russia .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.842391304347826, 'errors': {'', "('most common names', 'is a list of', '', 'It is the largest and oldest list of the most common names in the English language .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 11130
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11230, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.20it/s]Extractor Estimating: 2it [00:01,  1.34it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.59it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.56it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:09,  1.57it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:11,  1.02s/it]Extractor Estimating: 17it [00:12,  1.12it/s]Extractor Estimating: 18it [00:12,  1.23it/s]Extractor Estimating: 19it [00:13,  1.27it/s]Extractor Estimating: 20it [00:14,  1.31it/s]Extractor Estimating: 21it [00:14,  1.37it/s]Extractor Estimating: 22it [00:15,  1.41it/s]Extractor Estimating: 23it [00:16,  1.51it/s]Extractor Estimating: 24it [00:16,  1.44it/s]Extractor Estimating: 25it [00:17,  1.48it/s]Extractor Estimating: 26it [00:18,  1.55it/s]Extractor Estimating: 27it [00:18,  1.51it/s]Extractor Estimating: 28it [00:19,  1.54it/s]Extractor Estimating: 29it [00:20,  1.34it/s]Extractor Estimating: 30it [00:21,  1.40it/s]Extractor Estimating: 31it [00:21,  1.51it/s]Extractor Estimating: 32it [00:22,  1.52it/s]Extractor Estimating: 33it [00:22,  1.59it/s]Extractor Estimating: 34it [00:23,  1.61it/s]Extractor Estimating: 35it [00:24,  1.61it/s]Extractor Estimating: 36it [00:24,  1.61it/s]Extractor Estimating: 37it [00:25,  1.56it/s]Extractor Estimating: 38it [00:25,  1.62it/s]Extractor Estimating: 39it [00:26,  1.56it/s]Extractor Estimating: 40it [00:27,  1.53it/s]Extractor Estimating: 41it [00:27,  1.58it/s]Extractor Estimating: 42it [00:28,  1.65it/s]Extractor Estimating: 43it [00:29,  1.64it/s]Extractor Estimating: 44it [00:29,  1.63it/s]Extractor Estimating: 45it [00:30,  1.36it/s]Extractor Estimating: 46it [00:31,  1.45it/s]Extractor Estimating: 47it [00:31,  1.52it/s]Extractor Estimating: 48it [00:32,  1.52it/s]Extractor Estimating: 49it [00:33,  1.59it/s]Extractor Estimating: 50it [00:33,  1.61it/s]Extractor Estimating: 51it [00:34,  1.61it/s]Extractor Estimating: 52it [00:34,  1.58it/s]Extractor Estimating: 53it [00:35,  1.58it/s]Extractor Estimating: 54it [00:36,  1.64it/s]Extractor Estimating: 55it [00:36,  1.62it/s]Extractor Estimating: 56it [00:37,  1.60it/s]Extractor Estimating: 57it [00:38,  1.63it/s]Extractor Estimating: 58it [00:38,  1.59it/s]Extractor Estimating: 59it [00:39,  1.57it/s]Extractor Estimating: 60it [00:39,  1.60it/s]Extractor Estimating: 61it [00:40,  1.55it/s]Extractor Estimating: 62it [00:41,  1.53it/s]Extractor Estimating: 63it [00:42,  1.50it/s]Extractor Estimating: 64it [00:42,  1.50it/s]Extractor Estimating: 65it [00:43,  1.44it/s]Extractor Estimating: 66it [00:44,  1.46it/s]Extractor Estimating: 67it [00:44,  1.51it/s]Extractor Estimating: 68it [00:45,  1.54it/s]Extractor Estimating: 69it [00:45,  1.54it/s]Extractor Estimating: 70it [00:46,  1.51it/s]Extractor Estimating: 71it [00:47,  1.51it/s]Extractor Estimating: 72it [00:48,  1.50it/s]Extractor Estimating: 73it [00:48,  1.50it/s]Extractor Estimating: 74it [00:49,  1.53it/s]Extractor Estimating: 75it [00:49,  1.53it/s]Extractor Estimating: 76it [00:50,  1.54it/s]Extractor Estimating: 77it [00:51,  1.56it/s]Extractor Estimating: 78it [00:51,  1.51it/s]Extractor Estimating: 79it [00:52,  1.50it/s]Extractor Estimating: 80it [00:53,  1.51it/s]Extractor Estimating: 81it [00:53,  1.54it/s]Extractor Estimating: 82it [00:54,  1.52it/s]Extractor Estimating: 83it [00:55,  1.52it/s]Extractor Estimating: 84it [00:55,  1.51it/s]Extractor Estimating: 85it [00:56,  1.48it/s]Extractor Estimating: 86it [00:57,  1.50it/s]Extractor Estimating: 87it [00:58,  1.39it/s]Extractor Estimating: 88it [00:58,  1.41it/s]Extractor Estimating: 89it [00:59,  1.40it/s]Extractor Estimating: 90it [01:00,  1.38it/s]Extractor Estimating: 91it [01:00,  1.41it/s]Extractor Estimating: 92it [01:01,  1.41it/s]Extractor Estimating: 93it [01:02,  1.46it/s]Extractor Estimating: 94it [01:03,  1.42it/s]Extractor Estimating: 95it [01:03,  1.48it/s]Extractor Estimating: 96it [01:04,  1.46it/s]Extractor Estimating: 97it [01:04,  1.48it/s]Extractor Estimating: 98it [01:05,  1.48it/s]Extractor Estimating: 99it [01:06,  1.43it/s]Extractor Estimating: 100it [01:07,  1.44it/s]Extractor Estimating: 101it [01:07,  1.48it/s]Extractor Estimating: 102it [01:08,  1.53it/s]Extractor Estimating: 103it [01:08,  1.55it/s]Extractor Estimating: 104it [01:09,  1.57it/s]Extractor Estimating: 105it [01:10,  1.63it/s]Extractor Estimating: 106it [01:10,  1.58it/s]Extractor Estimating: 107it [01:11,  1.59it/s]Extractor Estimating: 108it [01:12,  1.58it/s]Extractor Estimating: 109it [01:12,  1.58it/s]Extractor Estimating: 110it [01:13,  1.61it/s]Extractor Estimating: 111it [01:13,  1.62it/s]Extractor Estimating: 112it [01:14,  1.65it/s]Extractor Estimating: 113it [01:15,  1.64it/s]Extractor Estimating: 114it [01:15,  1.66it/s]Extractor Estimating: 115it [01:16,  1.71it/s]Extractor Estimating: 116it [01:16,  1.71it/s]Extractor Estimating: 117it [01:17,  1.74it/s]Extractor Estimating: 118it [01:18,  1.68it/s]Extractor Estimating: 119it [01:18,  1.67it/s]Extractor Estimating: 120it [01:19,  1.68it/s]Extractor Estimating: 121it [01:19,  1.65it/s]Extractor Estimating: 122it [01:20,  1.65it/s]Extractor Estimating: 123it [01:20,  1.70it/s]Extractor Estimating: 124it [01:21,  1.72it/s]Extractor Estimating: 125it [01:22,  1.70it/s]Extractor Estimating: 126it [01:22,  1.69it/s]Extractor Estimating: 127it [01:23,  1.67it/s]Extractor Estimating: 128it [01:24,  1.63it/s]Extractor Estimating: 129it [01:24,  1.61it/s]Extractor Estimating: 130it [01:25,  1.57it/s]Extractor Estimating: 131it [01:25,  1.56it/s]Extractor Estimating: 132it [01:26,  1.53it/s]Extractor Estimating: 133it [01:27,  1.56it/s]Extractor Estimating: 134it [01:27,  1.55it/s]Extractor Estimating: 135it [01:28,  1.55it/s]Extractor Estimating: 136it [01:29,  1.58it/s]Extractor Estimating: 137it [01:29,  1.57it/s]Extractor Estimating: 138it [01:30,  1.60it/s]Extractor Estimating: 139it [01:31,  1.63it/s]Extractor Estimating: 140it [01:31,  1.54it/s]Extractor Estimating: 141it [01:32,  1.53it/s]Extractor Estimating: 142it [01:33,  1.52it/s]Extractor Estimating: 143it [01:33,  1.52it/s]Extractor Estimating: 144it [01:34,  1.46it/s]Extractor Estimating: 145it [01:35,  1.51it/s]Extractor Estimating: 146it [01:35,  1.50it/s]Extractor Estimating: 147it [01:36,  1.53it/s]Extractor Estimating: 148it [01:37,  1.53it/s]Extractor Estimating: 149it [01:37,  1.50it/s]Extractor Estimating: 150it [01:38,  1.58it/s]Extractor Estimating: 151it [01:38,  1.61it/s]Extractor Estimating: 152it [01:39,  1.65it/s]Extractor Estimating: 153it [01:40,  1.58it/s]Extractor Estimating: 154it [01:40,  1.62it/s]Extractor Estimating: 155it [01:41,  1.68it/s]Extractor Estimating: 156it [01:41,  1.64it/s]Extractor Estimating: 157it [01:42,  1.67it/s]Extractor Estimating: 158it [01:43,  1.63it/s]Extractor Estimating: 159it [01:43,  1.65it/s]Extractor Estimating: 160it [01:44,  1.63it/s]Extractor Estimating: 161it [01:44,  1.67it/s]Extractor Estimating: 162it [01:45,  1.67it/s]Extractor Estimating: 163it [01:46,  1.69it/s]Extractor Estimating: 164it [01:46,  1.70it/s]Extractor Estimating: 165it [01:47,  1.72it/s]Extractor Estimating: 166it [01:47,  1.64it/s]Extractor Estimating: 167it [01:48,  1.66it/s]Extractor Estimating: 168it [01:49,  1.16it/s]Extractor Estimating: 169it [01:50,  1.26it/s]Extractor Estimating: 170it [01:51,  1.38it/s]Extractor Estimating: 171it [01:51,  1.42it/s]Extractor Estimating: 172it [01:52,  1.48it/s]Extractor Estimating: 173it [01:52,  1.58it/s]Extractor Estimating: 174it [01:53,  1.64it/s]Extractor Estimating: 175it [01:54,  1.66it/s]Extractor Estimating: 176it [01:54,  1.67it/s]Extractor Estimating: 177it [01:55,  1.74it/s]Extractor Estimating: 178it [01:55,  1.71it/s]Extractor Estimating: 179it [01:56,  1.76it/s]Extractor Estimating: 180it [01:56,  1.73it/s]Extractor Estimating: 181it [01:57,  1.75it/s]Extractor Estimating: 182it [01:58,  1.76it/s]Extractor Estimating: 183it [01:58,  1.74it/s]Extractor Estimating: 184it [01:59,  1.72it/s]Extractor Estimating: 185it [01:59,  1.73it/s]Extractor Estimating: 186it [02:00,  1.75it/s]Extractor Estimating: 187it [02:00,  1.80it/s]Extractor Estimating: 188it [02:01,  1.81it/s]Extractor Estimating: 189it [02:02,  1.74it/s]Extractor Estimating: 190it [02:02,  1.73it/s]Extractor Estimating: 191it [02:03,  1.76it/s]Extractor Estimating: 192it [02:03,  1.72it/s]Extractor Estimating: 193it [02:04,  1.74it/s]Extractor Estimating: 194it [02:04,  1.72it/s]Extractor Estimating: 195it [02:05,  1.73it/s]Extractor Estimating: 196it [02:06,  1.71it/s]Extractor Estimating: 197it [02:06,  1.71it/s]Extractor Estimating: 198it [02:07,  1.67it/s]Extractor Estimating: 199it [02:07,  1.64it/s]Extractor Estimating: 200it [02:08,  1.67it/s]Extractor Estimating: 201it [02:09,  1.64it/s]Extractor Estimating: 202it [02:09,  1.60it/s]Extractor Estimating: 203it [02:10,  1.67it/s]Extractor Estimating: 204it [02:11,  1.65it/s]Extractor Estimating: 205it [02:11,  1.68it/s]Extractor Estimating: 206it [02:12,  1.67it/s]Extractor Estimating: 207it [02:12,  1.65it/s]Extractor Estimating: 208it [02:13,  1.66it/s]Extractor Estimating: 209it [02:13,  1.72it/s]Extractor Estimating: 210it [02:14,  1.76it/s]Extractor Estimating: 211it [02:15,  1.70it/s]Extractor Estimating: 212it [02:15,  1.70it/s]Extractor Estimating: 213it [02:16,  1.70it/s]Extractor Estimating: 214it [02:16,  1.67it/s]Extractor Estimating: 215it [02:17,  1.65it/s]Extractor Estimating: 216it [02:18,  1.65it/s]Extractor Estimating: 217it [02:18,  1.63it/s]Extractor Estimating: 218it [02:19,  1.62it/s]Extractor Estimating: 219it [02:19,  1.67it/s]Extractor Estimating: 220it [02:20,  1.67it/s]Extractor Estimating: 221it [02:21,  1.68it/s]Extractor Estimating: 222it [02:21,  1.71it/s]Extractor Estimating: 223it [02:22,  1.67it/s]Extractor Estimating: 224it [02:22,  1.67it/s]Extractor Estimating: 225it [02:23,  1.74it/s]Extractor Estimating: 226it [02:24,  1.57it/s]Extractor Estimating: 227it [02:24,  1.54it/s]Extractor Estimating: 228it [02:25,  1.59it/s]Extractor Estimating: 229it [02:26,  1.58it/s]Extractor Estimating: 230it [02:26,  1.55it/s]Extractor Estimating: 231it [02:27,  1.51it/s]Extractor Estimating: 232it [02:28,  1.53it/s]Extractor Estimating: 233it [02:28,  1.59it/s]Extractor Estimating: 234it [02:29,  1.60it/s]Extractor Estimating: 235it [02:29,  1.59it/s]Extractor Estimating: 236it [02:30,  1.59it/s]Extractor Estimating: 237it [02:31,  1.60it/s]Extractor Estimating: 238it [02:31,  1.59it/s]Extractor Estimating: 239it [02:32,  1.57it/s]Extractor Estimating: 240it [02:33,  1.57it/s]Extractor Estimating: 241it [02:33,  1.57it/s]Extractor Estimating: 242it [02:34,  1.58it/s]Extractor Estimating: 243it [02:35,  1.60it/s]Extractor Estimating: 244it [02:35,  1.61it/s]Extractor Estimating: 245it [02:36,  1.60it/s]Extractor Estimating: 246it [02:36,  1.57it/s]Extractor Estimating: 247it [02:37,  1.55it/s]Extractor Estimating: 248it [02:38,  1.59it/s]Extractor Estimating: 249it [02:38,  1.62it/s]Extractor Estimating: 250it [02:39,  1.62it/s]Extractor Estimating: 250it [02:39,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4969 mean pseudo reward: 0.9391386590160568
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 22636
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22736, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22736, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.351, loss:961.3613
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.058, loss:912.9904
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.077, loss:896.1599
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.054, loss:913.1542
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.071, loss:853.1268
>> valid entity prec:0.5713, rec:0.4367, f1:0.4950
>> valid relation prec:0.3674, rec:0.0374, f1:0.0679
>> valid relation with NER prec:0.3674, rec:0.0374, f1:0.0679
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.502, loss:871.3261
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.058, loss:837.7338
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.077, loss:895.3231
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.053, loss:810.1719
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.063, loss:856.5855
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5717, rec:0.4878, f1:0.5264
>> valid relation prec:0.3066, rec:0.0473, f1:0.0820
>> valid relation with NER prec:0.3066, rec:0.0473, f1:0.0820
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.485, loss:795.9177
g_step 1200, step 160, avg_time 1.062, loss:827.6691
g_step 1300, step 52, avg_time 1.052, loss:765.1998
g_step 1400, step 152, avg_time 1.053, loss:744.7770
g_step 1500, step 44, avg_time 1.062, loss:766.7544
>> valid entity prec:0.4933, rec:0.3924, f1:0.4371
>> valid relation prec:0.1861, rec:0.0273, f1:0.0476
>> valid relation with NER prec:0.1861, rec:0.0273, f1:0.0476
g_step 1600, step 144, avg_time 2.486, loss:731.4169
g_step 1700, step 36, avg_time 1.046, loss:722.2102
g_step 1800, step 136, avg_time 1.056, loss:705.6442
g_step 1900, step 28, avg_time 1.038, loss:718.6945
g_step 2000, step 128, avg_time 1.057, loss:676.9506
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5285, rec:0.4112, f1:0.4625
>> valid relation prec:0.2799, rec:0.0407, f1:0.0710
>> valid relation with NER prec:0.2799, rec:0.0407, f1:0.0710
g_step 2100, step 20, avg_time 2.492, loss:668.2267
g_step 2200, step 120, avg_time 1.060, loss:649.2888
g_step 2300, step 12, avg_time 1.052, loss:646.9551
g_step 2400, step 112, avg_time 1.065, loss:623.9008
g_step 2500, step 4, avg_time 1.045, loss:630.2616
>> valid entity prec:0.5478, rec:0.5016, f1:0.5237
>> valid relation prec:0.2936, rec:0.0526, f1:0.0892
>> valid relation with NER prec:0.2936, rec:0.0526, f1:0.0892
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.508, loss:595.7678
g_step 2700, step 204, avg_time 1.056, loss:607.2156
g_step 2800, step 96, avg_time 1.053, loss:575.4723
g_step 2900, step 196, avg_time 1.047, loss:575.6107
g_step 3000, step 88, avg_time 1.050, loss:561.2986
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4992, rec:0.5120, f1:0.5055
>> valid relation prec:0.2415, rec:0.0736, f1:0.1128
>> valid relation with NER prec:0.2415, rec:0.0736, f1:0.1128
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.507, loss:560.8018
g_step 3200, step 80, avg_time 1.056, loss:523.0032
g_step 3300, step 180, avg_time 1.061, loss:529.6542
g_step 3400, step 72, avg_time 1.046, loss:508.7893
g_step 3500, step 172, avg_time 1.057, loss:516.4477
>> valid entity prec:0.5305, rec:0.3570, f1:0.4268
>> valid relation prec:0.2002, rec:0.0424, f1:0.0700
>> valid relation with NER prec:0.2002, rec:0.0424, f1:0.0700
g_step 3600, step 64, avg_time 2.484, loss:483.1148
g_step 3700, step 164, avg_time 1.067, loss:504.8659
g_step 3800, step 56, avg_time 1.054, loss:475.8125
g_step 3900, step 156, avg_time 1.046, loss:469.4265
g_step 4000, step 48, avg_time 1.055, loss:455.7239
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5227, rec:0.4207, f1:0.4662
>> valid relation prec:0.1984, rec:0.0625, f1:0.0950
>> valid relation with NER prec:0.1984, rec:0.0625, f1:0.0950
g_step 4100, step 148, avg_time 2.489, loss:458.8618
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:24:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:24:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-24-48_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:24:49 - WARNING - datasets.builder -   Using custom data configuration default-5923f7106cb9e9ca
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5923f7106cb9e9ca/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:24:49,730 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:24:49,731 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:24:49,731 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:24:49,732 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:24:49,740 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,743 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,744 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:24:49,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:24:49,892 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:24:53,129 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:24:53,135 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5923f7106cb9e9ca/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 22:24:53 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1497c35960e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.79ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.71ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.12ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.33ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.45ba/s]100%|██████████| 6/6 [00:01<00:00,  4.96ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.72ba/s] 40%|████      | 2/5 [00:00<00:00,  3.52ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.87ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.06ba/s]100%|██████████| 5/5 [00:01<00:00,  4.69ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.91ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.16ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.71ba/s]100%|██████████| 6/6 [00:00<00:00, 11.22ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.78ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.57ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.00ba/s]100%|██████████| 5/5 [00:00<00:00, 10.57ba/s]
[INFO|trainer.py:414] 2023-08-28 22:24:57,007 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:24:57,024 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:24:57,024 >>   Num examples = 5004
[INFO|trainer.py:1149] 2023-08-28 22:24:57,024 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:24:57,025 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:24:57,025 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:24:57,025 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:24:57,025 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:59,  3.26it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:51,  3.45it/s]  1%|▏         | 5/390 [00:01<01:51,  3.46it/s]  2%|▏         | 6/390 [00:01<01:50,  3.47it/s]  2%|▏         | 7/390 [00:02<01:50,  3.48it/s]  2%|▏         | 8/390 [00:02<01:49,  3.47it/s]  2%|▏         | 9/390 [00:02<01:49,  3.47it/s]  3%|▎         | 10/390 [00:02<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:48,  3.48it/s]  3%|▎         | 12/390 [00:03<01:48,  3.48it/s]  3%|▎         | 13/390 [00:03<01:48,  3.48it/s]  4%|▎         | 14/390 [00:04<01:47,  3.48it/s]  4%|▍         | 15/390 [00:04<01:47,  3.49it/s]  4%|▍         | 16/390 [00:04<01:47,  3.49it/s]  4%|▍         | 17/390 [00:04<01:47,  3.48it/s]  5%|▍         | 18/390 [00:05<01:46,  3.48it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:05<01:47,  3.44it/s]  5%|▌         | 21/390 [00:06<01:46,  3.45it/s]  6%|▌         | 22/390 [00:06<01:46,  3.46it/s]  6%|▌         | 23/390 [00:06<01:45,  3.47it/s]  6%|▌         | 24/390 [00:06<01:45,  3.47it/s]  6%|▋         | 25/390 [00:07<01:45,  3.47it/s]  7%|▋         | 26/390 [00:07<01:44,  3.48it/s]  7%|▋         | 27/390 [00:07<01:44,  3.48it/s]  7%|▋         | 28/390 [00:08<01:44,  3.48it/s]  7%|▋         | 29/390 [00:08<01:43,  3.48it/s]  8%|▊         | 30/390 [00:08<01:43,  3.48it/s]  8%|▊         | 31/390 [00:08<01:43,  3.45it/s]  8%|▊         | 32/390 [00:09<01:43,  3.46it/s]  8%|▊         | 33/390 [00:09<01:43,  3.46it/s]  9%|▊         | 34/390 [00:09<01:42,  3.47it/s]  9%|▉         | 35/390 [00:10<01:42,  3.47it/s]  9%|▉         | 36/390 [00:10<01:41,  3.47it/s]  9%|▉         | 37/390 [00:10<01:41,  3.48it/s] 10%|▉         | 38/390 [00:10<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:40,  3.47it/s] 11%|█         | 41/390 [00:11<01:40,  3.48it/s] 11%|█         | 42/390 [00:12<01:40,  3.46it/s] 11%|█         | 43/390 [00:12<01:40,  3.46it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.47it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.47it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.48it/s] 13%|█▎        | 52/390 [00:14<01:37,  3.47it/s] 14%|█▎        | 53/390 [00:15<01:39,  3.40it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.42it/s] 14%|█▍        | 55/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.45it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.46it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.46it/s] 15%|█▌        | 59/390 [00:17<01:35,  3.46it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.47it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:18<01:39,  3.28it/s] 17%|█▋        | 65/390 [00:18<01:37,  3.34it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.38it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.40it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.42it/s] 18%|█▊        | 69/390 [00:19<01:33,  3.44it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.45it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.45it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.46it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 75/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 76/390 [00:21<01:31,  3.43it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.44it/s] 20%|██        | 78/390 [00:22<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 22:25:19,641 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:25:19,642 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:25:19,642 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.10it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.62it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.82it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.01it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.54it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.39it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.22it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.93it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.78it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.80it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.85it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.86it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.84it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.94it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.78it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.83it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.75it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.60it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.69it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.72it/s][A
 21%|██▏       | 108/505 [00:02<00:09, 40.43it/s][A
 22%|██▏       | 113/505 [00:02<00:09, 42.13it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 43.53it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 44.50it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 45.11it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 45.72it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.06it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.34it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.05it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.15it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.29it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.49it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.57it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.79it/s][A
 35%|███▌      | 178/505 [00:03<00:06, 46.81it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.76it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.82it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.56it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.48it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.56it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.56it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.68it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.77it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.84it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.87it/s][A
 46%|████▌     | 233/505 [00:05<00:05, 46.80it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.51it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.56it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.47it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.51it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.68it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.70it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.76it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.87it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.87it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.67it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.73it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.63it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.49it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.60it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.58it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.67it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.70it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.75it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.76it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.70it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.58it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.53it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.44it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.52it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.61it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.66it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.69it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.72it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.70it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.63it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 45.02it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 45.51it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 45.87it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.17it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.30it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.42it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.52it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.50it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.34it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.34it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.29it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.55it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.62it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.60it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.76it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.78it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.65it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.56it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.45it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.43it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.49it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.63it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.62it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.66it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:30,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:25:30,568 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 22:25:30,592 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:25:35,681 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:25:35,696 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:25:35,706 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:48<41:01,  7.92s/it] 21%|██        | 80/390 [00:48<29:05,  5.63s/it] 21%|██        | 81/390 [00:48<20:44,  4.03s/it] 21%|██        | 82/390 [00:49<14:55,  2.91s/it] 21%|██▏       | 83/390 [00:49<10:50,  2.12s/it] 22%|██▏       | 84/390 [00:49<08:00,  1.57s/it] 22%|██▏       | 85/390 [00:50<06:01,  1.19s/it] 22%|██▏       | 86/390 [00:50<04:39,  1.09it/s] 22%|██▏       | 87/390 [00:50<03:40,  1.37it/s] 23%|██▎       | 88/390 [00:50<03:00,  1.68it/s] 23%|██▎       | 89/390 [00:51<02:36,  1.92it/s] 23%|██▎       | 90/390 [00:51<02:15,  2.21it/s] 23%|██▎       | 91/390 [00:51<02:01,  2.46it/s] 24%|██▎       | 92/390 [00:52<01:50,  2.70it/s] 24%|██▍       | 93/390 [00:52<01:42,  2.89it/s] 24%|██▍       | 94/390 [00:52<01:37,  3.05it/s] 24%|██▍       | 95/390 [00:52<01:33,  3.16it/s] 25%|██▍       | 96/390 [00:53<01:30,  3.25it/s] 25%|██▍       | 97/390 [00:53<01:28,  3.31it/s] 25%|██▌       | 98/390 [00:53<01:26,  3.36it/s] 25%|██▌       | 99/390 [00:54<01:25,  3.39it/s] 26%|██▌       | 100/390 [00:54<01:24,  3.42it/s] 26%|██▌       | 101/390 [00:54<01:24,  3.44it/s] 26%|██▌       | 102/390 [00:54<01:23,  3.45it/s] 26%|██▋       | 103/390 [00:55<01:23,  3.46it/s] 27%|██▋       | 104/390 [00:55<01:22,  3.46it/s] 27%|██▋       | 105/390 [00:55<01:23,  3.41it/s] 27%|██▋       | 106/390 [00:56<01:22,  3.43it/s] 27%|██▋       | 107/390 [00:56<01:22,  3.44it/s] 28%|██▊       | 108/390 [00:56<01:21,  3.45it/s] 28%|██▊       | 109/390 [00:57<01:21,  3.46it/s] 28%|██▊       | 110/390 [00:57<01:20,  3.46it/s] 28%|██▊       | 111/390 [00:57<01:20,  3.47it/s] 29%|██▊       | 112/390 [00:57<01:20,  3.47it/s] 29%|██▉       | 113/390 [00:58<01:19,  3.47it/s] 29%|██▉       | 114/390 [00:58<01:19,  3.47it/s] 29%|██▉       | 115/390 [00:58<01:19,  3.47it/s] 30%|██▉       | 116/390 [00:59<01:21,  3.37it/s] 30%|███       | 117/390 [00:59<01:20,  3.40it/s] 30%|███       | 118/390 [00:59<01:19,  3.42it/s] 31%|███       | 119/390 [00:59<01:18,  3.44it/s] 31%|███       | 120/390 [01:00<01:18,  3.45it/s] 31%|███       | 121/390 [01:00<01:17,  3.46it/s] 31%|███▏      | 122/390 [01:00<01:17,  3.46it/s] 32%|███▏      | 123/390 [01:01<01:17,  3.47it/s] 32%|███▏      | 124/390 [01:01<01:16,  3.47it/s] 32%|███▏      | 125/390 [01:01<01:16,  3.47it/s] 32%|███▏      | 126/390 [01:01<01:16,  3.47it/s] 33%|███▎      | 127/390 [01:02<01:16,  3.43it/s] 33%|███▎      | 128/390 [01:02<01:16,  3.44it/s] 33%|███▎      | 129/390 [01:02<01:15,  3.45it/s] 33%|███▎      | 130/390 [01:03<01:15,  3.46it/s] 34%|███▎      | 131/390 [01:03<01:14,  3.46it/s] 34%|███▍      | 132/390 [01:03<01:14,  3.46it/s] 34%|███▍      | 133/390 [01:03<01:14,  3.46it/s] 34%|███▍      | 134/390 [01:04<01:13,  3.46it/s] 35%|███▍      | 135/390 [01:04<01:13,  3.47it/s] 35%|███▍      | 136/390 [01:04<01:13,  3.47it/s] 35%|███▌      | 137/390 [01:05<01:12,  3.47it/s] 35%|███▌      | 138/390 [01:05<01:14,  3.38it/s] 36%|███▌      | 139/390 [01:05<01:13,  3.41it/s] 36%|███▌      | 140/390 [01:06<01:12,  3.42it/s] 36%|███▌      | 141/390 [01:06<01:12,  3.44it/s] 36%|███▋      | 142/390 [01:06<01:11,  3.45it/s] 37%|███▋      | 143/390 [01:06<01:11,  3.46it/s] 37%|███▋      | 144/390 [01:07<01:11,  3.46it/s] 37%|███▋      | 145/390 [01:07<01:10,  3.46it/s] 37%|███▋      | 146/390 [01:07<01:10,  3.46it/s] 38%|███▊      | 147/390 [01:08<01:10,  3.47it/s] 38%|███▊      | 148/390 [01:08<01:09,  3.47it/s] 38%|███▊      | 149/390 [01:08<01:17,  3.12it/s] 38%|███▊      | 150/390 [01:09<01:14,  3.22it/s] 39%|███▊      | 151/390 [01:09<01:12,  3.29it/s] 39%|███▉      | 152/390 [01:09<01:11,  3.34it/s] 39%|███▉      | 153/390 [01:09<01:10,  3.38it/s] 39%|███▉      | 154/390 [01:10<01:09,  3.40it/s] 40%|███▉      | 155/390 [01:10<01:08,  3.43it/s] 40%|████      | 156/390 [01:10<01:08,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 22:26:07,800 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:26:07,800 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:26:07,800 >>   Batch size = 8
{'eval_loss': 1.0413570404052734, 'eval_runtime': 10.9007, 'eval_samples_per_second': 370.528, 'eval_steps_per_second': 46.327, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.55it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.85it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.82it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.20it/s][A
  6%|▌         | 28/505 [00:00<00:09, 47.72it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.22it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.90it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.65it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.65it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.68it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.70it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.81it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.91it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.79it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.78it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.62it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.49it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.47it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.50it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.65it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.76it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.71it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.80it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.74it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.59it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.50it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.50it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.51it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.59it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.65it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.62it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.75it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.73it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.65it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.61it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.59it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.44it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.51it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.66it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.66it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.58it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.66it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.63it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.61it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.60it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.58it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.57it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.48it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.50it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.61it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.63it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.60it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.52it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.49it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.53it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.50it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.60it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.39it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.50it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.52it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 43.31it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 44.22it/s][A
 64%|██████▍   | 323/505 [00:06<00:04, 44.96it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 45.51it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 45.90it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.08it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.22it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.32it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.12it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.15it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.24it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.36it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.51it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.56it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.68it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.50it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.55it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.44it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.33it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.41it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.44it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.50it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.58it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.60it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.69it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.57it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.32it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.37it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.38it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.41it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.43it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.44it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.55it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.56it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.62it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.61it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.41it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.42it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.43it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:21<01:08,  3.44it/s]
100%|██████████| 505/505 [00:10<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:26:18,703 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 22:26:18,725 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:26:23,024 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:26:23,055 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:26:23,073 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:35<29:33,  7.61s/it] 41%|████      | 158/390 [01:35<20:56,  5.42s/it] 41%|████      | 159/390 [01:36<14:55,  3.88s/it] 41%|████      | 160/390 [01:36<10:44,  2.80s/it] 41%|████▏     | 161/390 [01:36<07:48,  2.05s/it] 42%|████▏     | 162/390 [01:36<05:46,  1.52s/it] 42%|████▏     | 163/390 [01:37<04:20,  1.15s/it] 42%|████▏     | 164/390 [01:37<03:21,  1.12it/s] 42%|████▏     | 165/390 [01:37<02:39,  1.41it/s] 43%|████▎     | 166/390 [01:38<02:10,  1.71it/s] 43%|████▎     | 167/390 [01:38<01:50,  2.02it/s] 43%|████▎     | 168/390 [01:38<01:35,  2.31it/s] 43%|████▎     | 169/390 [01:38<01:27,  2.54it/s] 44%|████▎     | 170/390 [01:39<01:19,  2.76it/s] 44%|████▍     | 171/390 [01:39<01:14,  2.94it/s] 44%|████▍     | 172/390 [01:39<01:10,  3.08it/s] 44%|████▍     | 173/390 [01:40<01:07,  3.19it/s] 45%|████▍     | 174/390 [01:40<01:05,  3.28it/s] 45%|████▍     | 175/390 [01:40<01:04,  3.33it/s] 45%|████▌     | 176/390 [01:40<01:03,  3.38it/s] 45%|████▌     | 177/390 [01:41<01:02,  3.40it/s] 46%|████▌     | 178/390 [01:41<01:01,  3.42it/s] 46%|████▌     | 179/390 [01:41<01:01,  3.44it/s] 46%|████▌     | 180/390 [01:42<01:01,  3.44it/s] 46%|████▋     | 181/390 [01:42<01:00,  3.45it/s] 47%|████▋     | 182/390 [01:42<01:00,  3.46it/s] 47%|████▋     | 183/390 [01:42<00:59,  3.47it/s] 47%|████▋     | 184/390 [01:43<00:59,  3.47it/s] 47%|████▋     | 185/390 [01:43<00:59,  3.47it/s] 48%|████▊     | 186/390 [01:43<00:58,  3.47it/s] 48%|████▊     | 187/390 [01:44<00:58,  3.47it/s] 48%|████▊     | 188/390 [01:44<00:58,  3.47it/s] 48%|████▊     | 189/390 [01:44<00:57,  3.47it/s] 49%|████▊     | 190/390 [01:44<00:57,  3.47it/s] 49%|████▉     | 191/390 [01:45<01:01,  3.25it/s] 49%|████▉     | 192/390 [01:45<00:59,  3.31it/s] 49%|████▉     | 193/390 [01:45<00:58,  3.36it/s] 50%|████▉     | 194/390 [01:46<00:57,  3.39it/s] 50%|█████     | 195/390 [01:46<00:57,  3.42it/s] 50%|█████     | 196/390 [01:46<00:56,  3.43it/s] 51%|█████     | 197/390 [01:47<00:56,  3.45it/s] 51%|█████     | 198/390 [01:47<00:55,  3.45it/s] 51%|█████     | 199/390 [01:47<00:55,  3.46it/s] 51%|█████▏    | 200/390 [01:47<00:54,  3.46it/s] 52%|█████▏    | 201/390 [01:48<00:54,  3.47it/s] 52%|█████▏    | 202/390 [01:48<00:54,  3.43it/s] 52%|█████▏    | 203/390 [01:48<00:54,  3.44it/s] 52%|█████▏    | 204/390 [01:49<00:53,  3.45it/s] 53%|█████▎    | 205/390 [01:49<00:53,  3.46it/s] 53%|█████▎    | 206/390 [01:49<00:53,  3.46it/s] 53%|█████▎    | 207/390 [01:49<00:52,  3.46it/s] 53%|█████▎    | 208/390 [01:50<00:52,  3.47it/s] 54%|█████▎    | 209/390 [01:50<00:52,  3.43it/s] 54%|█████▍    | 210/390 [01:50<00:52,  3.44it/s] 54%|█████▍    | 211/390 [01:51<00:51,  3.45it/s] 54%|█████▍    | 212/390 [01:51<00:57,  3.09it/s] 55%|█████▍    | 213/390 [01:51<00:57,  3.09it/s] 55%|█████▍    | 214/390 [01:52<00:55,  3.19it/s] 55%|█████▌    | 215/390 [01:52<00:53,  3.27it/s] 55%|█████▌    | 216/390 [01:52<00:52,  3.33it/s] 56%|█████▌    | 217/390 [01:52<00:51,  3.37it/s] 56%|█████▌    | 218/390 [01:53<00:50,  3.40it/s] 56%|█████▌    | 219/390 [01:53<00:50,  3.42it/s] 56%|█████▋    | 220/390 [01:53<00:49,  3.43it/s] 57%|█████▋    | 221/390 [01:54<00:49,  3.44it/s] 57%|█████▋    | 222/390 [01:54<00:48,  3.45it/s] 57%|█████▋    | 223/390 [01:54<00:48,  3.46it/s] 57%|█████▋    | 224/390 [01:54<00:48,  3.46it/s] 58%|█████▊    | 225/390 [01:55<00:47,  3.46it/s] 58%|█████▊    | 226/390 [01:55<00:47,  3.46it/s] 58%|█████▊    | 227/390 [01:55<00:47,  3.47it/s] 58%|█████▊    | 228/390 [01:56<00:46,  3.47it/s] 59%|█████▊    | 229/390 [01:56<00:46,  3.47it/s] 59%|█████▉    | 230/390 [01:56<00:46,  3.47it/s] 59%|█████▉    | 231/390 [01:56<00:45,  3.47it/s] 59%|█████▉    | 232/390 [01:57<00:45,  3.45it/s] 60%|█████▉    | 233/390 [01:57<00:45,  3.46it/s] 60%|██████    | 234/390 [01:57<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 22:26:54,933 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:26:54,933 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:26:54,933 >>   Batch size = 8
{'eval_loss': 1.0559051036834717, 'eval_runtime': 10.8833, 'eval_samples_per_second': 371.119, 'eval_steps_per_second': 46.401, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.33it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.56it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.92it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.23it/s][A
  6%|▌         | 28/505 [00:00<00:09, 47.71it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.28it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.85it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.47it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.58it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.71it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.70it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.82it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.86it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.85it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.74it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.54it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.39it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.38it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.54it/s][A
 20%|██        | 103/505 [00:02<00:09, 43.98it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 44.87it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 45.48it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 45.89it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.15it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.29it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.32it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.32it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.16it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.30it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.31it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.38it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.55it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.60it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.60it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.61it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.50it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.45it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.37it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.42it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.46it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.54it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.59it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.64it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.55it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.52it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.53it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.42it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.33it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.39it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.46it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.54it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.54it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.52it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.53it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.41it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.45it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.40it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.45it/s][A
 59%|█████▉    | 298/505 [00:06<00:05, 41.30it/s][A
 60%|██████    | 303/505 [00:06<00:04, 42.80it/s][A
 61%|██████    | 308/505 [00:06<00:04, 43.82it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 44.67it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 45.29it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 45.66it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 45.98it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.13it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.06it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.23it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.26it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.30it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.41it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.50it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.57it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.65it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.50it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.42it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.38it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.40it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.43it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.34it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.41it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.56it/s][A
 83%|████████▎ | 418/505 [00:09<00:01, 46.57it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.63it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.55it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.47it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 45.86it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.03it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.12it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.15it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.32it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.45it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.55it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.44it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.32it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.27it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.40it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.44it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.33it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.30it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [02:08<00:45,  3.46it/s]
100%|██████████| 505/505 [00:10<00:00, 46.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:27:05,890 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 22:27:05,910 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:27:09,639 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:27:09,691 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:27:09,715 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:21<18:39,  7.22s/it] 61%|██████    | 236/390 [02:21<13:12,  5.14s/it] 61%|██████    | 237/390 [02:21<09:24,  3.69s/it] 61%|██████    | 238/390 [02:22<06:45,  2.67s/it] 61%|██████▏   | 239/390 [02:22<04:54,  1.95s/it] 62%|██████▏   | 240/390 [02:22<03:38,  1.45s/it] 62%|██████▏   | 241/390 [02:22<02:44,  1.10s/it] 62%|██████▏   | 242/390 [02:23<02:07,  1.16it/s] 62%|██████▏   | 243/390 [02:23<01:41,  1.45it/s] 63%|██████▎   | 244/390 [02:23<01:22,  1.76it/s] 63%|██████▎   | 245/390 [02:24<01:10,  2.07it/s] 63%|██████▎   | 246/390 [02:24<01:01,  2.35it/s] 63%|██████▎   | 247/390 [02:24<00:55,  2.57it/s] 64%|██████▎   | 248/390 [02:25<00:50,  2.79it/s] 64%|██████▍   | 249/390 [02:25<00:47,  2.96it/s] 64%|██████▍   | 250/390 [02:25<00:45,  3.10it/s] 64%|██████▍   | 251/390 [02:25<00:43,  3.20it/s] 65%|██████▍   | 252/390 [02:26<00:42,  3.28it/s] 65%|██████▍   | 253/390 [02:26<00:41,  3.34it/s] 65%|██████▌   | 254/390 [02:26<00:40,  3.38it/s] 65%|██████▌   | 255/390 [02:27<00:39,  3.40it/s] 66%|██████▌   | 256/390 [02:27<00:39,  3.43it/s] 66%|██████▌   | 257/390 [02:27<00:38,  3.44it/s] 66%|██████▌   | 258/390 [02:27<00:39,  3.33it/s] 66%|██████▋   | 259/390 [02:28<00:38,  3.37it/s] 67%|██████▋   | 260/390 [02:28<00:38,  3.40it/s] 67%|██████▋   | 261/390 [02:28<00:37,  3.42it/s] 67%|██████▋   | 262/390 [02:29<00:37,  3.43it/s] 67%|██████▋   | 263/390 [02:29<00:36,  3.45it/s] 68%|██████▊   | 264/390 [02:29<00:36,  3.45it/s] 68%|██████▊   | 265/390 [02:29<00:36,  3.46it/s] 68%|██████▊   | 266/390 [02:30<00:35,  3.46it/s] 68%|██████▊   | 267/390 [02:30<00:35,  3.46it/s] 69%|██████▊   | 268/390 [02:30<00:35,  3.46it/s] 69%|██████▉   | 269/390 [02:31<00:35,  3.46it/s] 69%|██████▉   | 270/390 [02:31<00:34,  3.46it/s] 69%|██████▉   | 271/390 [02:31<00:34,  3.46it/s] 70%|██████▉   | 272/390 [02:31<00:34,  3.46it/s] 70%|███████   | 273/390 [02:32<00:33,  3.47it/s] 70%|███████   | 274/390 [02:32<00:33,  3.47it/s] 71%|███████   | 275/390 [02:32<00:33,  3.47it/s] 71%|███████   | 276/390 [02:33<00:32,  3.46it/s] 71%|███████   | 277/390 [02:33<00:32,  3.47it/s] 71%|███████▏  | 278/390 [02:33<00:32,  3.47it/s] 72%|███████▏  | 279/390 [02:34<00:31,  3.47it/s] 72%|███████▏  | 280/390 [02:34<00:32,  3.43it/s] 72%|███████▏  | 281/390 [02:34<00:31,  3.44it/s] 72%|███████▏  | 282/390 [02:34<00:31,  3.45it/s] 73%|███████▎  | 283/390 [02:35<00:30,  3.45it/s] 73%|███████▎  | 284/390 [02:35<00:30,  3.46it/s] 73%|███████▎  | 285/390 [02:35<00:30,  3.46it/s] 73%|███████▎  | 286/390 [02:36<00:30,  3.46it/s] 74%|███████▎  | 287/390 [02:36<00:29,  3.46it/s] 74%|███████▍  | 288/390 [02:36<00:29,  3.46it/s] 74%|███████▍  | 289/390 [02:36<00:29,  3.47it/s] 74%|███████▍  | 290/390 [02:37<00:28,  3.47it/s] 75%|███████▍  | 291/390 [02:37<00:28,  3.46it/s] 75%|███████▍  | 292/390 [02:37<00:28,  3.46it/s] 75%|███████▌  | 293/390 [02:38<00:28,  3.46it/s] 75%|███████▌  | 294/390 [02:38<00:27,  3.47it/s] 76%|███████▌  | 295/390 [02:38<00:27,  3.47it/s] 76%|███████▌  | 296/390 [02:38<00:27,  3.47it/s] 76%|███████▌  | 297/390 [02:39<00:26,  3.47it/s] 76%|███████▋  | 298/390 [02:39<00:26,  3.47it/s] 77%|███████▋  | 299/390 [02:39<00:26,  3.47it/s] 77%|███████▋  | 300/390 [02:40<00:25,  3.47it/s] 77%|███████▋  | 301/390 [02:40<00:26,  3.38it/s] 77%|███████▋  | 302/390 [02:40<00:25,  3.41it/s] 78%|███████▊  | 303/390 [02:40<00:25,  3.42it/s] 78%|███████▊  | 304/390 [02:41<00:25,  3.44it/s] 78%|███████▊  | 305/390 [02:41<00:24,  3.45it/s] 78%|███████▊  | 306/390 [02:41<00:24,  3.45it/s] 79%|███████▊  | 307/390 [02:42<00:24,  3.46it/s] 79%|███████▉  | 308/390 [02:42<00:23,  3.46it/s] 79%|███████▉  | 309/390 [02:42<00:23,  3.46it/s] 79%|███████▉  | 310/390 [02:42<00:23,  3.46it/s] 80%|███████▉  | 311/390 [02:43<00:22,  3.46it/s] 80%|████████  | 312/390 [02:43<00:22,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 22:27:40,629 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:27:40,629 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:27:40,629 >>   Batch size = 8
{'eval_loss': 1.065059781074524, 'eval_runtime': 10.9377, 'eval_samples_per_second': 369.273, 'eval_steps_per_second': 46.171, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.25it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.44it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.80it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.04it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.65it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.33it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.93it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.62it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.73it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.74it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.75it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.82it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.81it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.82it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.86it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.72it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.59it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.65it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.62it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.63it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.70it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.64it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.72it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.65it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.57it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.27it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.36it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.47it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.51it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.52it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.58it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.61it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.63it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.64it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.48it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.51it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.57it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.58it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.63it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.59it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.54it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.55it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.56it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.43it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.48it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.50it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.52it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.52it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.53it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.59it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.55it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.52it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.49it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.47it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.45it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.49it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.48it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.51it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.46it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.54it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.53it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.49it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.54it/s][A
 64%|██████▍   | 323/505 [00:06<00:04, 44.63it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 45.27it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 45.71it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 45.98it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.09it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.28it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.29it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.37it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 44.71it/s][A
 73%|███████▎  | 368/505 [00:07<00:03, 39.81it/s][A
 74%|███████▍  | 373/505 [00:08<00:03, 41.72it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 43.11it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 44.08it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 44.92it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 45.46it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 45.77it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.00it/s][A
 81%|████████  | 408/505 [00:08<00:02, 45.75it/s][A
 82%|████████▏ | 413/505 [00:08<00:02, 45.84it/s][A
 83%|████████▎ | 418/505 [00:09<00:01, 45.91it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.18it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.29it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.44it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.58it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.59it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.60it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.39it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.22it/s][A
 92%|█████████▏| 463/505 [00:10<00:00, 46.19it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.28it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.31it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.47it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.58it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.58it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.64it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.60it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.41it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:54<00:22,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:27:51,627 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 22:27:51,673 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:27:56,449 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:27:56,463 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:27:56,478 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:08<09:49,  7.66s/it] 81%|████████  | 314/390 [03:08<06:54,  5.45s/it] 81%|████████  | 315/390 [03:09<04:52,  3.90s/it] 81%|████████  | 316/390 [03:09<03:28,  2.82s/it] 81%|████████▏ | 317/390 [03:09<02:30,  2.06s/it] 82%|████████▏ | 318/390 [03:09<01:49,  1.53s/it] 82%|████████▏ | 319/390 [03:10<01:22,  1.16s/it] 82%|████████▏ | 320/390 [03:10<01:02,  1.12it/s] 82%|████████▏ | 321/390 [03:10<00:49,  1.40it/s] 83%|████████▎ | 322/390 [03:11<00:39,  1.71it/s] 83%|████████▎ | 323/390 [03:11<00:33,  2.02it/s] 83%|████████▎ | 324/390 [03:11<00:28,  2.31it/s] 83%|████████▎ | 325/390 [03:11<00:25,  2.57it/s] 84%|████████▎ | 326/390 [03:12<00:22,  2.79it/s] 84%|████████▍ | 327/390 [03:12<00:21,  2.96it/s] 84%|████████▍ | 328/390 [03:12<00:20,  3.10it/s] 84%|████████▍ | 329/390 [03:13<00:19,  3.20it/s] 85%|████████▍ | 330/390 [03:13<00:18,  3.28it/s] 85%|████████▍ | 331/390 [03:13<00:17,  3.34it/s] 85%|████████▌ | 332/390 [03:13<00:17,  3.38it/s] 85%|████████▌ | 333/390 [03:14<00:16,  3.39it/s] 86%|████████▌ | 334/390 [03:14<00:16,  3.41it/s] 86%|████████▌ | 335/390 [03:14<00:16,  3.43it/s] 86%|████████▌ | 336/390 [03:15<00:15,  3.44it/s] 86%|████████▋ | 337/390 [03:15<00:15,  3.45it/s] 87%|████████▋ | 338/390 [03:15<00:15,  3.46it/s] 87%|████████▋ | 339/390 [03:15<00:14,  3.46it/s] 87%|████████▋ | 340/390 [03:16<00:14,  3.46it/s] 87%|████████▋ | 341/390 [03:16<00:14,  3.47it/s] 88%|████████▊ | 342/390 [03:16<00:13,  3.47it/s] 88%|████████▊ | 343/390 [03:17<00:13,  3.47it/s] 88%|████████▊ | 344/390 [03:17<00:13,  3.46it/s] 88%|████████▊ | 345/390 [03:17<00:13,  3.46it/s] 89%|████████▊ | 346/390 [03:17<00:12,  3.46it/s] 89%|████████▉ | 347/390 [03:18<00:12,  3.46it/s] 89%|████████▉ | 348/390 [03:18<00:12,  3.47it/s] 89%|████████▉ | 349/390 [03:18<00:11,  3.47it/s] 90%|████████▉ | 350/390 [03:19<00:11,  3.47it/s] 90%|█████████ | 351/390 [03:19<00:11,  3.47it/s] 90%|█████████ | 352/390 [03:19<00:10,  3.47it/s] 91%|█████████ | 353/390 [03:19<00:10,  3.47it/s] 91%|█████████ | 354/390 [03:20<00:10,  3.47it/s] 91%|█████████ | 355/390 [03:20<00:10,  3.46it/s] 91%|█████████▏| 356/390 [03:20<00:09,  3.46it/s] 92%|█████████▏| 357/390 [03:21<00:09,  3.47it/s] 92%|█████████▏| 358/390 [03:21<00:09,  3.47it/s] 92%|█████████▏| 359/390 [03:21<00:08,  3.47it/s] 92%|█████████▏| 360/390 [03:21<00:08,  3.47it/s] 93%|█████████▎| 361/390 [03:22<00:08,  3.47it/s] 93%|█████████▎| 362/390 [03:22<00:08,  3.47it/s] 93%|█████████▎| 363/390 [03:22<00:07,  3.47it/s] 93%|█████████▎| 364/390 [03:23<00:07,  3.47it/s] 94%|█████████▎| 365/390 [03:23<00:07,  3.47it/s] 94%|█████████▍| 366/390 [03:23<00:06,  3.45it/s] 94%|█████████▍| 367/390 [03:23<00:06,  3.46it/s] 94%|█████████▍| 368/390 [03:24<00:06,  3.46it/s] 95%|█████████▍| 369/390 [03:24<00:06,  3.46it/s] 95%|█████████▍| 370/390 [03:24<00:05,  3.46it/s] 95%|█████████▌| 371/390 [03:25<00:05,  3.47it/s] 95%|█████████▌| 372/390 [03:25<00:05,  3.47it/s] 96%|█████████▌| 373/390 [03:25<00:04,  3.46it/s] 96%|█████████▌| 374/390 [03:26<00:04,  3.46it/s] 96%|█████████▌| 375/390 [03:26<00:04,  3.47it/s] 96%|█████████▋| 376/390 [03:26<00:04,  3.47it/s] 97%|█████████▋| 377/390 [03:26<00:03,  3.45it/s] 97%|█████████▋| 378/390 [03:27<00:03,  3.46it/s] 97%|█████████▋| 379/390 [03:27<00:03,  3.46it/s] 97%|█████████▋| 380/390 [03:27<00:02,  3.46it/s] 98%|█████████▊| 381/390 [03:28<00:02,  3.46it/s] 98%|█████████▊| 382/390 [03:28<00:02,  3.47it/s] 98%|█████████▊| 383/390 [03:28<00:02,  3.47it/s] 98%|█████████▊| 384/390 [03:28<00:01,  3.47it/s] 99%|█████████▊| 385/390 [03:29<00:01,  3.47it/s] 99%|█████████▉| 386/390 [03:29<00:01,  3.47it/s] 99%|█████████▉| 387/390 [03:29<00:00,  3.47it/s] 99%|█████████▉| 388/390 [03:30<00:00,  3.46it/s]100%|█████████▉| 389/390 [03:30<00:00,  3.46it/s]100%|██████████| 390/390 [03:30<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 22:28:27,665 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:28:27,665 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:28:27,665 >>   Batch size = 8
{'eval_loss': 1.0718557834625244, 'eval_runtime': 10.9414, 'eval_samples_per_second': 369.147, 'eval_steps_per_second': 46.155, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.11it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.57it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.81it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.12it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.62it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.40it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.19it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.80it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.71it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.80it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.83it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.83it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.82it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.79it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.70it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.66it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.50it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.52it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.59it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.60it/s][A
 21%|██▏       | 108/505 [00:02<00:09, 44.05it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 44.76it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 45.37it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 45.83it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.14it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.29it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.38it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.34it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.31it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.35it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.34it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.47it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.57it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.59it/s][A
 35%|███▌      | 178/505 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.75it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.60it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.48it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.44it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.46it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.41it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.58it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.56it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.64it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.71it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.61it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.53it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.52it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 45.43it/s][A
 50%|█████     | 253/505 [00:05<00:05, 45.87it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.09it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.22it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.26it/s][A
 54%|█████▍    | 273/505 [00:05<00:05, 46.36it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.40it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.34it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.30it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.32it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.35it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.46it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.53it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.58it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.52it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.50it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.47it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.49it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.41it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.43it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.40it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.44it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.55it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.63it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.55it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.51it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.49it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.48it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.47it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.35it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.36it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.40it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.56it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.45it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.47it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.46it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.45it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.47it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.41it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.40it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.43it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.47it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.52it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.55it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.51it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.50it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.50it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.50it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.42it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.46it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.48it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.40it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:41<00:00,  3.46it/s]
100%|██████████| 505/505 [00:10<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:28:38,598 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 22:28:38,642 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:28:42,997 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:28:43,030 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:28:43,043 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:28:50,717 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:28:50,722 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78 (score: 1.0413570404052734).
                                                 100%|██████████| 390/390 [03:57<00:00,  3.46it/s]100%|██████████| 390/390 [03:57<00:00,  1.64it/s]
[INFO|trainer.py:1894] 2023-08-28 22:28:54,816 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 22:28:54,840 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:28:58,828 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:28:58,847 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:28:58,867 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:28:59,077 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   train_loss               =     0.6458
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   train_runtime            = 0:03:57.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   train_samples            =       5004
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   train_samples_per_second =    105.231
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:28:59,077 >>   train_steps_per_second   =       1.64
{'eval_loss': 1.0755308866500854, 'eval_runtime': 10.8792, 'eval_samples_per_second': 371.26, 'eval_steps_per_second': 46.419, 'epoch': 4.99}
{'train_runtime': 237.763, 'train_samples_per_second': 105.231, 'train_steps_per_second': 1.64, 'train_loss': 0.6458410801031651, 'epoch': 4.99}
08/28/2023 22:28:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:28:59,120 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:28:59,120 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 22:28:59,120 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 58.60it/s]  2%|▏         | 12/505 [00:00<00:09, 51.21it/s]  4%|▎         | 18/505 [00:00<00:09, 49.37it/s]  5%|▍         | 23/505 [00:00<00:09, 48.62it/s]  6%|▌         | 28/505 [00:00<00:09, 48.13it/s]  7%|▋         | 33/505 [00:00<00:09, 47.82it/s]  8%|▊         | 38/505 [00:00<00:09, 47.67it/s]  9%|▊         | 43/505 [00:00<00:09, 47.46it/s] 10%|▉         | 48/505 [00:00<00:09, 47.07it/s] 10%|█         | 53/505 [00:01<00:09, 47.01it/s] 11%|█▏        | 58/505 [00:01<00:09, 46.92it/s] 12%|█▏        | 63/505 [00:01<00:09, 46.93it/s] 13%|█▎        | 68/505 [00:01<00:09, 46.98it/s] 14%|█▍        | 73/505 [00:01<00:09, 47.01it/s] 15%|█▌        | 78/505 [00:01<00:09, 47.03it/s] 16%|█▋        | 83/505 [00:01<00:08, 47.04it/s] 17%|█▋        | 88/505 [00:01<00:08, 46.85it/s] 18%|█▊        | 93/505 [00:01<00:08, 46.83it/s] 19%|█▉        | 98/505 [00:02<00:08, 46.83it/s] 20%|██        | 103/505 [00:02<00:08, 46.78it/s] 21%|██▏       | 108/505 [00:02<00:08, 46.79it/s] 22%|██▏       | 113/505 [00:02<00:08, 46.87it/s] 23%|██▎       | 118/505 [00:02<00:08, 46.91it/s] 24%|██▍       | 123/505 [00:02<00:08, 46.95it/s] 25%|██▌       | 128/505 [00:02<00:08, 46.96it/s] 26%|██▋       | 133/505 [00:02<00:07, 46.79it/s] 27%|██▋       | 138/505 [00:02<00:07, 46.79it/s] 28%|██▊       | 143/505 [00:03<00:07, 46.81it/s] 29%|██▉       | 148/505 [00:03<00:07, 46.74it/s] 30%|███       | 153/505 [00:03<00:07, 46.68it/s] 31%|███▏      | 158/505 [00:03<00:07, 46.79it/s] 32%|███▏      | 163/505 [00:03<00:07, 46.80it/s] 33%|███▎      | 168/505 [00:03<00:07, 46.92it/s] 34%|███▍      | 173/505 [00:03<00:07, 46.93it/s] 35%|███▌      | 178/505 [00:03<00:06, 46.73it/s] 36%|███▌      | 183/505 [00:03<00:06, 46.81it/s] 37%|███▋      | 188/505 [00:03<00:06, 46.81it/s] 38%|███▊      | 193/505 [00:04<00:06, 46.75it/s] 39%|███▉      | 198/505 [00:04<00:06, 46.70it/s] 40%|████      | 203/505 [00:04<00:06, 46.76it/s] 41%|████      | 208/505 [00:04<00:06, 46.81it/s] 42%|████▏     | 213/505 [00:04<00:06, 46.92it/s] 43%|████▎     | 218/505 [00:04<00:06, 46.90it/s] 44%|████▍     | 223/505 [00:04<00:06, 46.75it/s] 45%|████▌     | 228/505 [00:04<00:05, 46.80it/s] 46%|████▌     | 233/505 [00:04<00:05, 46.80it/s] 47%|████▋     | 238/505 [00:05<00:05, 46.76it/s] 48%|████▊     | 243/505 [00:05<00:05, 46.74it/s] 49%|████▉     | 248/505 [00:05<00:05, 46.76it/s] 50%|█████     | 253/505 [00:05<00:05, 46.84it/s] 51%|█████     | 258/505 [00:05<00:05, 46.81it/s] 52%|█████▏    | 263/505 [00:05<00:05, 46.87it/s] 53%|█████▎    | 268/505 [00:05<00:05, 46.90it/s] 54%|█████▍    | 273/505 [00:05<00:04, 46.95it/s] 55%|█████▌    | 278/505 [00:05<00:04, 46.99it/s] 56%|█████▌    | 283/505 [00:06<00:04, 46.91it/s] 57%|█████▋    | 288/505 [00:06<00:04, 46.89it/s] 58%|█████▊    | 293/505 [00:06<00:04, 46.78it/s] 59%|█████▉    | 298/505 [00:06<00:04, 46.90it/s] 60%|██████    | 303/505 [00:06<00:04, 46.97it/s] 61%|██████    | 308/505 [00:06<00:04, 46.95it/s] 62%|██████▏   | 313/505 [00:06<00:04, 46.80it/s] 63%|██████▎   | 318/505 [00:06<00:03, 46.93it/s] 64%|██████▍   | 323/505 [00:06<00:03, 46.95it/s] 65%|██████▍   | 328/505 [00:06<00:03, 46.88it/s] 66%|██████▌   | 333/505 [00:07<00:03, 46.92it/s] 67%|██████▋   | 338/505 [00:07<00:03, 46.89it/s] 68%|██████▊   | 343/505 [00:07<00:03, 46.85it/s] 69%|██████▉   | 348/505 [00:07<00:03, 46.95it/s] 70%|██████▉   | 353/505 [00:07<00:03, 46.98it/s] 71%|███████   | 358/505 [00:07<00:03, 46.75it/s] 72%|███████▏  | 363/505 [00:07<00:03, 46.91it/s] 73%|███████▎  | 368/505 [00:07<00:02, 46.91it/s] 74%|███████▍  | 373/505 [00:07<00:02, 46.88it/s] 75%|███████▍  | 378/505 [00:08<00:02, 46.92it/s] 76%|███████▌  | 383/505 [00:08<00:02, 46.88it/s] 77%|███████▋  | 388/505 [00:08<00:02, 46.84it/s] 78%|███████▊  | 393/505 [00:08<00:02, 46.85it/s] 79%|███████▉  | 398/505 [00:08<00:02, 46.86it/s] 80%|███████▉  | 403/505 [00:08<00:02, 46.88it/s] 81%|████████  | 408/505 [00:08<00:02, 46.93it/s] 82%|████████▏ | 413/505 [00:08<00:01, 46.90it/s] 83%|████████▎ | 418/505 [00:08<00:01, 46.85it/s] 84%|████████▍ | 423/505 [00:08<00:01, 46.89it/s] 85%|████████▍ | 428/505 [00:09<00:01, 46.89it/s] 86%|████████▌ | 433/505 [00:09<00:01, 46.56it/s] 87%|████████▋ | 438/505 [00:09<00:01, 46.79it/s] 88%|████████▊ | 443/505 [00:09<00:01, 46.64it/s] 89%|████████▊ | 448/505 [00:09<00:01, 46.80it/s] 90%|████████▉ | 453/505 [00:09<00:01, 46.89it/s] 91%|█████████ | 458/505 [00:09<00:01, 46.91it/s] 92%|█████████▏| 463/505 [00:09<00:00, 46.86it/s] 93%|█████████▎| 468/505 [00:09<00:00, 46.80it/s] 94%|█████████▎| 473/505 [00:10<00:00, 46.83it/s] 95%|█████████▍| 478/505 [00:10<00:00, 46.77it/s] 96%|█████████▌| 483/505 [00:10<00:00, 46.80it/s] 97%|█████████▋| 488/505 [00:10<00:00, 46.72it/s] 98%|█████████▊| 493/505 [00:10<00:00, 46.77it/s] 99%|█████████▊| 498/505 [00:10<00:00, 46.89it/s]100%|█████████▉| 503/505 [00:10<00:00, 46.89it/s]100%|██████████| 505/505 [00:10<00:00, 46.94it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:29:09,901 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   eval_loss               =     1.0414
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   eval_runtime            = 0:00:10.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   eval_samples_per_second =    374.637
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   eval_steps_per_second   =     46.841
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:29:09,901 >>   perplexity              =     2.8331
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:16,621 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:16,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:16,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:16,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:16,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:29:16,952 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:29:16,953 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:29:17,219 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:29:18,284 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:29:18,284 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:20,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:20,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:20,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:20,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:20,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:29:21,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:29:21,271 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:29:21,557 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:29:21,723 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:29:21,724 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.48it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.55it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.55it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:35,  1.55it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:37,  1.55it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.36it/s]Extractor Predicting: 69it [00:46,  1.37it/s]Extractor Predicting: 70it [00:46,  1.40it/s]Extractor Predicting: 71it [00:47,  1.42it/s]Extractor Predicting: 72it [00:48,  1.40it/s]Extractor Predicting: 73it [00:49,  1.40it/s]Extractor Predicting: 74it [00:49,  1.36it/s]Extractor Predicting: 75it [00:50,  1.39it/s]Extractor Predicting: 76it [00:51,  1.42it/s]Extractor Predicting: 77it [00:51,  1.42it/s]Extractor Predicting: 78it [00:52,  1.43it/s]Extractor Predicting: 79it [00:53,  1.44it/s]Extractor Predicting: 80it [00:53,  1.44it/s]Extractor Predicting: 81it [00:54,  1.45it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:55,  1.44it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:57,  1.45it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:59,  1.51it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.49it/s]Extractor Predicting: 92it [01:01,  1.52it/s]Extractor Predicting: 93it [01:02,  1.51it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:04,  1.46it/s]Extractor Predicting: 97it [01:05,  1.46it/s]Extractor Predicting: 98it [01:06,  1.46it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.43it/s]Extractor Predicting: 102it [01:08,  1.43it/s]Extractor Predicting: 103it [01:09,  1.45it/s]Extractor Predicting: 104it [01:10,  1.48it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:11,  1.49it/s]Extractor Predicting: 107it [01:12,  1.49it/s]Extractor Predicting: 108it [01:12,  1.51it/s]Extractor Predicting: 109it [01:13,  1.51it/s]Extractor Predicting: 110it [01:14,  1.49it/s]Extractor Predicting: 111it [01:14,  1.46it/s]Extractor Predicting: 112it [01:15,  1.50it/s]Extractor Predicting: 113it [01:16,  1.49it/s]Extractor Predicting: 114it [01:16,  1.41it/s]Extractor Predicting: 115it [01:17,  1.47it/s]Extractor Predicting: 116it [01:18,  1.48it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.44it/s]Extractor Predicting: 119it [01:20,  1.47it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:23,  1.49it/s]Extractor Predicting: 125it [01:24,  1.37it/s]Extractor Predicting: 126it [01:25,  1.41it/s]Extractor Predicting: 127it [01:25,  1.37it/s]Extractor Predicting: 128it [01:26,  1.35it/s]Extractor Predicting: 129it [01:27,  1.36it/s]Extractor Predicting: 130it [01:28,  1.36it/s]Extractor Predicting: 131it [01:29,  1.34it/s]Extractor Predicting: 132it [01:29,  1.34it/s]Extractor Predicting: 133it [01:30,  1.35it/s]Extractor Predicting: 134it [01:31,  1.40it/s]Extractor Predicting: 135it [01:31,  1.40it/s]Extractor Predicting: 136it [01:32,  1.41it/s]Extractor Predicting: 137it [01:33,  1.43it/s]Extractor Predicting: 138it [01:33,  1.39it/s]Extractor Predicting: 139it [01:34,  1.39it/s]Extractor Predicting: 140it [01:35,  1.38it/s]Extractor Predicting: 141it [01:36,  1.38it/s]Extractor Predicting: 142it [01:36,  1.39it/s]Extractor Predicting: 143it [01:37,  1.39it/s]Extractor Predicting: 144it [01:38,  1.39it/s]Extractor Predicting: 145it [01:39,  1.38it/s]Extractor Predicting: 146it [01:39,  1.39it/s]Extractor Predicting: 147it [01:40,  1.40it/s]Extractor Predicting: 148it [01:41,  1.36it/s]Extractor Predicting: 149it [01:41,  1.40it/s]Extractor Predicting: 149it [01:41,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:11,500 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:11,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:11,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:11,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:11,504 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:31:11,823 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:31:11,824 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:31:12,109 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:31:13,122 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:31:13,122 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:14,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:14,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:14,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:14,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:31:14,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:31:14,842 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:31:14,845 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:31:15,136 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:31:15,301 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:31:15,301 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2764976958525346,
  "recall": 0.07427581084426839,
  "score": 0.11709601873536302,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:09,  1.74it/s]Extractor Predicting: 16it [00:09,  1.74it/s]Extractor Predicting: 17it [00:10,  1.73it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.57it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.62it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:15,  1.65it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:18,  1.71it/s]Extractor Predicting: 31it [00:18,  1.73it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.64it/s]Extractor Predicting: 45it [00:27,  1.51it/s]Extractor Predicting: 46it [00:28,  1.50it/s]Extractor Predicting: 47it [00:28,  1.53it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:29,  1.59it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.62it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:34,  1.55it/s]Extractor Predicting: 57it [00:34,  1.56it/s]Extractor Predicting: 58it [00:35,  1.57it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:36,  1.56it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:38,  1.57it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:40,  1.61it/s]Extractor Predicting: 67it [00:41,  1.60it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:42,  1.57it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:43,  1.55it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:45,  1.55it/s]Extractor Predicting: 75it [00:46,  1.58it/s]Extractor Predicting: 76it [00:46,  1.56it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.52it/s]Extractor Predicting: 79it [00:48,  1.55it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:50,  1.54it/s]Extractor Predicting: 83it [00:51,  1.54it/s]Extractor Predicting: 84it [00:52,  1.56it/s]Extractor Predicting: 85it [00:52,  1.55it/s]Extractor Predicting: 86it [00:53,  1.53it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:56,  1.59it/s]Extractor Predicting: 92it [00:57,  1.60it/s]Extractor Predicting: 93it [00:57,  1.63it/s]Extractor Predicting: 94it [00:58,  1.64it/s]Extractor Predicting: 95it [00:58,  1.62it/s]Extractor Predicting: 96it [00:59,  1.65it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:00,  1.64it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:01,  1.66it/s]Extractor Predicting: 101it [01:02,  1.64it/s]Extractor Predicting: 102it [01:03,  1.60it/s]Extractor Predicting: 103it [01:03,  1.59it/s]Extractor Predicting: 104it [01:04,  1.63it/s]Extractor Predicting: 105it [01:05,  1.57it/s]Extractor Predicting: 106it [01:05,  1.59it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:07,  1.44it/s]Extractor Predicting: 109it [01:07,  1.47it/s]Extractor Predicting: 110it [01:08,  1.51it/s]Extractor Predicting: 111it [01:09,  1.53it/s]Extractor Predicting: 112it [01:09,  1.54it/s]Extractor Predicting: 113it [01:10,  1.58it/s]Extractor Predicting: 114it [01:11,  1.57it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:13,  1.68it/s]Extractor Predicting: 120it [01:14,  1.68it/s]Extractor Predicting: 121it [01:15,  1.67it/s]Extractor Predicting: 122it [01:15,  1.66it/s]Extractor Predicting: 123it [01:16,  1.63it/s]Extractor Predicting: 124it [01:17,  1.65it/s]Extractor Predicting: 125it [01:17,  1.65it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.60it/s]Extractor Predicting: 128it [01:19,  1.58it/s]Extractor Predicting: 129it [01:20,  1.55it/s]Extractor Predicting: 130it [01:20,  1.53it/s]Extractor Predicting: 131it [01:21,  1.51it/s]Extractor Predicting: 132it [01:22,  1.53it/s]Extractor Predicting: 133it [01:22,  1.54it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:24,  1.52it/s]Extractor Predicting: 136it [01:24,  1.51it/s]Extractor Predicting: 137it [01:25,  1.51it/s]Extractor Predicting: 138it [01:26,  1.52it/s]Extractor Predicting: 139it [01:26,  1.52it/s]Extractor Predicting: 140it [01:27,  1.49it/s]Extractor Predicting: 141it [01:28,  1.50it/s]Extractor Predicting: 142it [01:28,  1.50it/s]Extractor Predicting: 143it [01:29,  1.52it/s]Extractor Predicting: 144it [01:30,  1.51it/s]Extractor Predicting: 145it [01:30,  1.50it/s]Extractor Predicting: 146it [01:31,  1.50it/s]Extractor Predicting: 147it [01:32,  1.52it/s]Extractor Predicting: 148it [01:32,  1.51it/s]Extractor Predicting: 149it [01:33,  1.54it/s]Extractor Predicting: 150it [01:34,  1.53it/s]Extractor Predicting: 151it [01:34,  1.52it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:36,  1.50it/s]Extractor Predicting: 155it [01:37,  1.51it/s]Extractor Predicting: 156it [01:38,  1.56it/s]Extractor Predicting: 157it [01:38,  1.62it/s]Extractor Predicting: 158it [01:39,  1.62it/s]Extractor Predicting: 159it [01:39,  1.62it/s]Extractor Predicting: 160it [01:40,  1.64it/s]Extractor Predicting: 161it [01:40,  1.68it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:42,  1.60it/s]Extractor Predicting: 164it [01:42,  1.63it/s]Extractor Predicting: 165it [01:43,  1.67it/s]Extractor Predicting: 166it [01:44,  1.66it/s]Extractor Predicting: 167it [01:44,  1.72it/s]Extractor Predicting: 168it [01:45,  1.72it/s]Extractor Predicting: 169it [01:45,  1.75it/s]Extractor Predicting: 170it [01:46,  1.69it/s]Extractor Predicting: 171it [01:47,  1.62it/s]Extractor Predicting: 172it [01:47,  1.59it/s]Extractor Predicting: 173it [01:48,  1.64it/s]Extractor Predicting: 173it [01:48,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:10,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:10,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:10,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:10,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:10,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:33:11,077 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:33:11,078 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:33:11,345 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:33:12,374 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:33:12,375 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:14,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:14,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:14,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:14,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:14,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:33:15,038 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:33:15,039 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:33:15,310 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:33:15,480 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:33:15,480 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25529865125240847,
  "recall": 0.0639170284611674,
  "score": 0.10223765432098765,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:06,  1.43it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:08,  1.43it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.58it/s]Extractor Predicting: 16it [00:10,  1.66it/s]Extractor Predicting: 17it [00:11,  1.75it/s]Extractor Predicting: 18it [00:11,  1.78it/s]Extractor Predicting: 19it [00:12,  1.79it/s]Extractor Predicting: 20it [00:12,  1.81it/s]Extractor Predicting: 21it [00:13,  1.85it/s]Extractor Predicting: 22it [00:13,  1.81it/s]Extractor Predicting: 23it [00:14,  1.81it/s]Extractor Predicting: 24it [00:14,  1.80it/s]Extractor Predicting: 25it [00:15,  1.80it/s]Extractor Predicting: 26it [00:16,  1.84it/s]Extractor Predicting: 27it [00:16,  1.80it/s]Extractor Predicting: 28it [00:17,  1.82it/s]Extractor Predicting: 29it [00:17,  1.82it/s]Extractor Predicting: 30it [00:18,  1.85it/s]Extractor Predicting: 31it [00:18,  1.88it/s]Extractor Predicting: 32it [00:19,  1.86it/s]Extractor Predicting: 33it [00:19,  1.87it/s]Extractor Predicting: 34it [00:20,  1.86it/s]Extractor Predicting: 35it [00:20,  1.86it/s]Extractor Predicting: 36it [00:21,  1.81it/s]Extractor Predicting: 37it [00:22,  1.77it/s]Extractor Predicting: 38it [00:22,  1.83it/s]Extractor Predicting: 39it [00:23,  1.82it/s]Extractor Predicting: 40it [00:23,  1.83it/s]Extractor Predicting: 41it [00:24,  1.82it/s]Extractor Predicting: 42it [00:24,  1.86it/s]Extractor Predicting: 43it [00:25,  1.82it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:27,  1.51it/s]Extractor Predicting: 47it [00:28,  1.47it/s]Extractor Predicting: 48it [00:28,  1.46it/s]Extractor Predicting: 49it [00:29,  1.46it/s]Extractor Predicting: 50it [00:30,  1.45it/s]Extractor Predicting: 51it [00:30,  1.44it/s]Extractor Predicting: 52it [00:31,  1.43it/s]Extractor Predicting: 53it [00:32,  1.42it/s]Extractor Predicting: 54it [00:33,  1.41it/s]Extractor Predicting: 55it [00:33,  1.43it/s]Extractor Predicting: 56it [00:34,  1.41it/s]Extractor Predicting: 57it [00:35,  1.40it/s]Extractor Predicting: 58it [00:35,  1.41it/s]Extractor Predicting: 59it [00:36,  1.67it/s]Extractor Predicting: 59it [00:36,  1.62it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:33:53,018 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:33:53,020 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:33:53,025 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:33:53,026 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:33:53,029 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:33:57,713 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:33:57,713 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:33:57,756 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:33:57,757 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:33:57,766 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:33:57,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7508571428571429,
  "recall": 0.21241513094083414,
  "score": 0.3311491935483871,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:33:58,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:58,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:59,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:00,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:01,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:01,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:02,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:03,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:04,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:05,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:06,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:07,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:08,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:08,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:09,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:10,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:11,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:12,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:12,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:13,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:26, 16.30s/it][WARNING|generation_utils.py:914] 2023-08-28 22:34:14,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:14,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:15,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:16,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:17,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:17,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:18,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:19,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:19,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:20,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:22,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:22,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:23,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:24,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:24,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:25,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:26,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:27,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:27,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:28,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:29,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:29,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:30,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:31,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:34<02:17, 17.13s/it][WARNING|generation_utils.py:914] 2023-08-28 22:34:32,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:32,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:33,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:34,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:35,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:36,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:37,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:38,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:38,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:39,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:40,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:40,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:41,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:42,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:43,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:43,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:44,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:45,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:46,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:47,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:48,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:49,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:49,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:52<02:04, 17.78s/it][WARNING|generation_utils.py:914] 2023-08-28 22:34:50,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:51,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:52,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:53,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:53,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:54,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:55,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:56,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:57,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:58,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:58,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:34:59,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:00,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:01,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:01,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:02,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:03,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:04,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:04,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:05,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:06,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:07,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:45, 17.56s/it][WARNING|generation_utils.py:914] 2023-08-28 22:35:07,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:08,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:09,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:09,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:10,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:11,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:12,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:12,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:13,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:14,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:14,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:15,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:16,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:17,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:17,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:18,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:19,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:19,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:20,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:21,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:21,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:22,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:25<01:24, 16.85s/it][WARNING|generation_utils.py:914] 2023-08-28 22:35:23,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:24,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:24,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:25,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:26,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:27,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:27,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:28,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:29,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:30,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:30,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:31,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:32,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:33,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:33,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:35,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:36,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:36,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:37,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:38,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:39,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:39,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:40,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:43<01:08, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 22:35:41,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:42,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:42,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:43,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:43,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:44,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:45,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:46,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:46,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:47,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:47,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:48,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:49,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:49,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:50,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:51,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:52,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:52,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:53,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:54,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:54,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:55,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:58<00:49, 16.50s/it][WARNING|generation_utils.py:914] 2023-08-28 22:35:56,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:56,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:57,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:58,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:35:59,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:00,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:00,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:01,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:01,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:02,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:03,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:03,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:04,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:05,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:05,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:06,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:07,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:07,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:08,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:09,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:09,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:12<00:31, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-28 22:36:10,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:10,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:11,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:12,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:12,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:13,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:14,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:15,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:15,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:16,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:17,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:17,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:18,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:19,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:20,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:20,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:21,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:22,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:23,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:25<00:14, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-28 22:36:23,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:24,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:25,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:25,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:26,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:27,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:27,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:28,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:29,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:30,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:30,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:31,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:32,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:32,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:33,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:35,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:35,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:36,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:37,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:37,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:38,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:36:39,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:42<00:00, 15.44s/it]Generating: 100%|██████████| 10/10 [02:42<00:00, 16.22s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:45,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:45,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:45,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:45,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:45,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:36:45,593 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:36:45,594 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:36:45,860 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:36:46,951 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:36:46,951 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:48,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:48,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:48,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:48,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:36:48,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:36:49,259 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:36:49,260 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:36:49,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:36:49,706 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:36:49,706 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : given name .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('an elector', 'given name', '', 'He was an elector for the Democratic Party at the state Legislature from 1874 to 1885 and he served as an official in the Attorney General of Oklahoma from 1886 until 1894 .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 568, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 623, 'raw': 800}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.77875, 'errors': {'', "('Egypt', 'languages spoken, written or signed', '', 'The language of Egypt , spoken by the 1,000 or more people of the Ancient Egypt n nation of Egypt .')", "('Brazil', 'languages spoken, written or signed', '', 'As of 2013 , the language was mostly spoken in the western part of the country , predominantly in Brazil and the Philippines .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 558, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.7525, 'errors': {'', "('White Sox', 'lowest point', '', 'On September 16 , 2015 , the White Sox traded catcher Jose Verne to the Milwaukee Brewers for pitcher Yu Darvish and several minor league relief pieces .')", "('final champion', 'lowest point', '', 'In the second round , they defeated the eventual champions , the eventual champions , the final champion , the undefeated champions of the world .')", "('Tampa Bay Lightning', 'lowest point', '', 'He played three years as a forward , but was traded in a blockbuster deal that sent him to the Tampa Bay Lightning for Chris Kunitz and Joe Meeks .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8764204545454546, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , together with Jack White ’s Dont Stop Loving Me , for Bizarre Love . Head Entity : Dont Stop Loving Me , Tail Entity : pop .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Warner Bros.', 'genre', '', 'It was recorded in 1979 on the Warner Bros. label .')", "('Candlelit Club', 'genre', '', 'The game , named after Arthurian legend , is a combat strategy game created by Paul Verhoeven for the Sony PlayStation 2 as part of his Candlelit Club series .')", "('A Nightmare on Elm Street', 'genre', '', 'He has produced such albums as Faking It and A Nightmare on Elm Street ( 1988 ) , a remake of the original , with Dave Matthews .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Wikipedia', 'is a list of', '', 'It is an extension to the English Wikipedia , currently the second oldest .')", "('Association of American Philosophers', 'is a list of', '', 'The lists of religions and philosophies have been published by the Association of American Philosophers since 1874 .')", "('Tom Jones', 'is a list of', '', 'It is a compilation of short , extended , and musical works composed by jazz artists such as Tom Jones , Jerry Garcia , and George Harrison .')", "('Wikimedia Foundation', 'is a list of', '', 'It is a reference list of the languages in the Wikimedia Foundation shared with all the worlds languages in order of importance .')", "('operating system', 'is a list of', '', 'It is a list of supported operating system s and is maintained by a single individual .')", "('Central African Republic', 'is a list of', '', 'The list of members is an official list of organizations that may be held by the Central African Republic in .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.94375, 'errors': {''}}
['Relation : member of . Context : Later in the year , the Parliament of the United Kingdom established the First Charter of Rights and Freedoms , which laid down the general rights , duties and responsibilities of a Member of Parliament . Head Entity : The Parliament of the United Kingdom , Tail Entity : Parliament of the United Kingdom .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 10056
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10156, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.51it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.56it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.53it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.51it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:09,  1.49it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.57it/s]Extractor Estimating: 21it [00:13,  1.61it/s]Extractor Estimating: 22it [00:14,  1.53it/s]Extractor Estimating: 23it [00:15,  1.51it/s]Extractor Estimating: 24it [00:15,  1.53it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.66it/s]Extractor Estimating: 28it [00:18,  1.70it/s]Extractor Estimating: 29it [00:18,  1.73it/s]Extractor Estimating: 30it [00:19,  1.71it/s]Extractor Estimating: 31it [00:19,  1.58it/s]Extractor Estimating: 32it [00:20,  1.62it/s]Extractor Estimating: 33it [00:21,  1.64it/s]Extractor Estimating: 34it [00:21,  1.66it/s]Extractor Estimating: 35it [00:22,  1.58it/s]Extractor Estimating: 36it [00:23,  1.59it/s]Extractor Estimating: 37it [00:23,  1.64it/s]Extractor Estimating: 38it [00:24,  1.64it/s]Extractor Estimating: 39it [00:24,  1.69it/s]Extractor Estimating: 40it [00:25,  1.64it/s]Extractor Estimating: 41it [00:26,  1.62it/s]Extractor Estimating: 42it [00:26,  1.64it/s]Extractor Estimating: 43it [00:27,  1.61it/s]Extractor Estimating: 44it [00:27,  1.67it/s]Extractor Estimating: 45it [00:28,  1.68it/s]Extractor Estimating: 46it [00:28,  1.70it/s]Extractor Estimating: 47it [00:29,  1.67it/s]Extractor Estimating: 48it [00:30,  1.68it/s]Extractor Estimating: 49it [00:30,  1.69it/s]Extractor Estimating: 50it [00:31,  1.56it/s]Extractor Estimating: 51it [00:32,  1.56it/s]Extractor Estimating: 52it [00:32,  1.52it/s]Extractor Estimating: 53it [00:33,  1.51it/s]Extractor Estimating: 54it [00:34,  1.53it/s]Extractor Estimating: 55it [00:34,  1.55it/s]Extractor Estimating: 56it [00:35,  1.56it/s]Extractor Estimating: 57it [00:36,  1.60it/s]Extractor Estimating: 58it [00:36,  1.55it/s]Extractor Estimating: 59it [00:37,  1.57it/s]Extractor Estimating: 60it [00:37,  1.59it/s]Extractor Estimating: 61it [00:38,  1.58it/s]Extractor Estimating: 62it [00:39,  1.62it/s]Extractor Estimating: 63it [00:39,  1.61it/s]Extractor Estimating: 64it [00:40,  1.60it/s]Extractor Estimating: 65it [00:41,  1.62it/s]Extractor Estimating: 66it [00:41,  1.62it/s]Extractor Estimating: 67it [00:42,  1.57it/s]Extractor Estimating: 68it [00:43,  1.53it/s]Extractor Estimating: 69it [00:43,  1.58it/s]Extractor Estimating: 70it [00:44,  1.58it/s]Extractor Estimating: 71it [00:44,  1.59it/s]Extractor Estimating: 72it [00:45,  1.63it/s]Extractor Estimating: 73it [00:46,  1.58it/s]Extractor Estimating: 74it [00:46,  1.55it/s]Extractor Estimating: 75it [00:47,  1.55it/s]Extractor Estimating: 76it [00:48,  1.57it/s]Extractor Estimating: 77it [00:48,  1.56it/s]Extractor Estimating: 78it [00:49,  1.55it/s]Extractor Estimating: 79it [00:49,  1.55it/s]Extractor Estimating: 80it [00:50,  1.56it/s]Extractor Estimating: 81it [00:51,  1.59it/s]Extractor Estimating: 82it [00:51,  1.51it/s]Extractor Estimating: 83it [00:52,  1.51it/s]Extractor Estimating: 84it [00:53,  1.48it/s]Extractor Estimating: 85it [00:53,  1.50it/s]Extractor Estimating: 86it [00:54,  1.51it/s]Extractor Estimating: 87it [00:55,  1.48it/s]Extractor Estimating: 88it [00:55,  1.49it/s]Extractor Estimating: 89it [00:56,  1.51it/s]Extractor Estimating: 90it [00:57,  1.53it/s]Extractor Estimating: 91it [00:57,  1.56it/s]Extractor Estimating: 92it [00:58,  1.59it/s]Extractor Estimating: 93it [00:59,  1.51it/s]Extractor Estimating: 94it [00:59,  1.51it/s]Extractor Estimating: 95it [01:00,  1.50it/s]Extractor Estimating: 96it [01:01,  1.53it/s]Extractor Estimating: 97it [01:01,  1.58it/s]Extractor Estimating: 98it [01:02,  1.59it/s]Extractor Estimating: 99it [01:03,  1.54it/s]Extractor Estimating: 100it [01:03,  1.57it/s]Extractor Estimating: 101it [01:04,  1.61it/s]Extractor Estimating: 102it [01:04,  1.60it/s]Extractor Estimating: 103it [01:05,  1.59it/s]Extractor Estimating: 104it [01:06,  1.61it/s]Extractor Estimating: 105it [01:06,  1.61it/s]Extractor Estimating: 106it [01:07,  1.60it/s]Extractor Estimating: 107it [01:08,  1.58it/s]Extractor Estimating: 108it [01:08,  1.58it/s]Extractor Estimating: 109it [01:09,  1.59it/s]Extractor Estimating: 110it [01:09,  1.59it/s]Extractor Estimating: 111it [01:10,  1.61it/s]Extractor Estimating: 112it [01:11,  1.59it/s]Extractor Estimating: 113it [01:11,  1.58it/s]Extractor Estimating: 114it [01:12,  1.54it/s]Extractor Estimating: 115it [01:13,  1.59it/s]Extractor Estimating: 116it [01:13,  1.47it/s]Extractor Estimating: 117it [01:14,  1.50it/s]Extractor Estimating: 118it [01:15,  1.56it/s]Extractor Estimating: 119it [01:15,  1.52it/s]Extractor Estimating: 120it [01:16,  1.56it/s]Extractor Estimating: 121it [01:17,  1.57it/s]Extractor Estimating: 122it [01:17,  1.52it/s]Extractor Estimating: 123it [01:18,  1.57it/s]Extractor Estimating: 124it [01:18,  1.57it/s]Extractor Estimating: 125it [01:19,  1.59it/s]Extractor Estimating: 126it [01:20,  1.57it/s]Extractor Estimating: 127it [01:20,  1.53it/s]Extractor Estimating: 128it [01:21,  1.55it/s]Extractor Estimating: 129it [01:22,  1.52it/s]Extractor Estimating: 130it [01:22,  1.51it/s]Extractor Estimating: 131it [01:23,  1.50it/s]Extractor Estimating: 132it [01:24,  1.53it/s]Extractor Estimating: 133it [01:24,  1.57it/s]Extractor Estimating: 134it [01:25,  1.60it/s]Extractor Estimating: 135it [01:26,  1.58it/s]Extractor Estimating: 136it [01:26,  1.57it/s]Extractor Estimating: 137it [01:27,  1.51it/s]Extractor Estimating: 138it [01:28,  1.53it/s]Extractor Estimating: 139it [01:28,  1.55it/s]Extractor Estimating: 140it [01:29,  1.56it/s]Extractor Estimating: 141it [01:29,  1.57it/s]Extractor Estimating: 142it [01:30,  1.49it/s]Extractor Estimating: 143it [01:31,  1.55it/s]Extractor Estimating: 144it [01:31,  1.56it/s]Extractor Estimating: 145it [01:32,  1.53it/s]Extractor Estimating: 146it [01:33,  1.54it/s]Extractor Estimating: 147it [01:33,  1.55it/s]Extractor Estimating: 148it [01:34,  1.53it/s]Extractor Estimating: 149it [01:35,  1.51it/s]Extractor Estimating: 150it [01:35,  1.50it/s]Extractor Estimating: 151it [01:36,  1.56it/s]Extractor Estimating: 152it [01:36,  1.65it/s]Extractor Estimating: 153it [01:37,  1.70it/s]Extractor Estimating: 154it [01:38,  1.75it/s]Extractor Estimating: 155it [01:38,  1.73it/s]Extractor Estimating: 156it [01:39,  1.71it/s]Extractor Estimating: 157it [01:39,  1.75it/s]Extractor Estimating: 158it [01:40,  1.75it/s]Extractor Estimating: 159it [01:40,  1.79it/s]Extractor Estimating: 160it [01:41,  1.90it/s]Extractor Estimating: 161it [01:41,  1.87it/s]Extractor Estimating: 162it [01:42,  1.80it/s]Extractor Estimating: 163it [01:43,  1.80it/s]Extractor Estimating: 164it [01:43,  1.80it/s]Extractor Estimating: 165it [01:44,  1.79it/s]Extractor Estimating: 166it [01:44,  1.78it/s]Extractor Estimating: 167it [01:45,  1.71it/s]Extractor Estimating: 168it [01:45,  1.70it/s]Extractor Estimating: 169it [01:46,  1.78it/s]Extractor Estimating: 170it [01:47,  1.72it/s]Extractor Estimating: 171it [01:47,  1.75it/s]Extractor Estimating: 172it [01:48,  1.71it/s]Extractor Estimating: 173it [01:48,  1.73it/s]Extractor Estimating: 174it [01:49,  1.75it/s]Extractor Estimating: 175it [01:49,  1.76it/s]Extractor Estimating: 176it [01:50,  1.78it/s]Extractor Estimating: 177it [01:51,  1.79it/s]Extractor Estimating: 178it [01:51,  1.64it/s]Extractor Estimating: 179it [01:52,  1.63it/s]Extractor Estimating: 180it [01:53,  1.64it/s]Extractor Estimating: 181it [01:53,  1.61it/s]Extractor Estimating: 182it [01:54,  1.64it/s]Extractor Estimating: 183it [01:54,  1.68it/s]Extractor Estimating: 184it [01:55,  1.71it/s]Extractor Estimating: 185it [01:55,  1.73it/s]Extractor Estimating: 186it [01:56,  1.71it/s]Extractor Estimating: 187it [01:57,  1.74it/s]Extractor Estimating: 188it [01:57,  1.75it/s]Extractor Estimating: 189it [01:58,  1.79it/s]Extractor Estimating: 190it [01:58,  1.74it/s]Extractor Estimating: 191it [01:59,  1.70it/s]Extractor Estimating: 192it [02:00,  1.66it/s]Extractor Estimating: 193it [02:00,  1.68it/s]Extractor Estimating: 194it [02:01,  1.71it/s]Extractor Estimating: 195it [02:01,  1.71it/s]Extractor Estimating: 196it [02:02,  1.71it/s]Extractor Estimating: 197it [02:02,  1.72it/s]Extractor Estimating: 198it [02:03,  1.75it/s]Extractor Estimating: 199it [02:04,  1.73it/s]Extractor Estimating: 200it [02:04,  1.61it/s]Extractor Estimating: 201it [02:05,  1.69it/s]Extractor Estimating: 202it [02:05,  1.76it/s]Extractor Estimating: 203it [02:06,  1.70it/s]Extractor Estimating: 204it [02:07,  1.74it/s]Extractor Estimating: 205it [02:07,  1.74it/s]Extractor Estimating: 206it [02:08,  1.77it/s]Extractor Estimating: 207it [02:08,  1.59it/s]Extractor Estimating: 208it [02:09,  1.68it/s]Extractor Estimating: 209it [02:09,  1.72it/s]Extractor Estimating: 210it [02:10,  1.75it/s]Extractor Estimating: 211it [02:11,  1.70it/s]Extractor Estimating: 212it [02:11,  1.72it/s]Extractor Estimating: 213it [02:12,  1.74it/s]Extractor Estimating: 214it [02:12,  1.76it/s]Extractor Estimating: 215it [02:13,  1.80it/s]Extractor Estimating: 216it [02:13,  1.82it/s]Extractor Estimating: 217it [02:14,  1.81it/s]Extractor Estimating: 218it [02:15,  1.77it/s]Extractor Estimating: 219it [02:15,  1.77it/s]Extractor Estimating: 220it [02:16,  1.68it/s]Extractor Estimating: 221it [02:16,  1.70it/s]Extractor Estimating: 222it [02:17,  1.72it/s]Extractor Estimating: 223it [02:17,  1.77it/s]Extractor Estimating: 224it [02:18,  1.74it/s]Extractor Estimating: 225it [02:19,  1.68it/s]Extractor Estimating: 226it [02:19,  1.63it/s]Extractor Estimating: 227it [02:20,  1.64it/s]Extractor Estimating: 228it [02:21,  1.64it/s]Extractor Estimating: 229it [02:21,  1.66it/s]Extractor Estimating: 230it [02:22,  1.63it/s]Extractor Estimating: 231it [02:22,  1.63it/s]Extractor Estimating: 232it [02:23,  1.65it/s]Extractor Estimating: 233it [02:24,  1.64it/s]Extractor Estimating: 234it [02:24,  1.60it/s]Extractor Estimating: 235it [02:25,  1.58it/s]Extractor Estimating: 236it [02:26,  1.55it/s]Extractor Estimating: 237it [02:26,  1.55it/s]Extractor Estimating: 238it [02:27,  1.55it/s]Extractor Estimating: 239it [02:27,  1.56it/s]Extractor Estimating: 240it [02:28,  1.57it/s]Extractor Estimating: 241it [02:29,  1.54it/s]Extractor Estimating: 242it [02:29,  1.51it/s]Extractor Estimating: 243it [02:30,  1.55it/s]Extractor Estimating: 244it [02:31,  1.55it/s]Extractor Estimating: 245it [02:31,  1.53it/s]Extractor Estimating: 246it [02:32,  1.55it/s]Extractor Estimating: 247it [02:33,  1.55it/s]Extractor Estimating: 248it [02:33,  1.58it/s]Extractor Estimating: 249it [02:34,  1.54it/s]Extractor Estimating: 250it [02:35,  1.56it/s]Extractor Estimating: 250it [02:35,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:41,230 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:41,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:41,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:41,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:41,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:39:41,541 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:39:41,542 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:39:41,812 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:39:42,842 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:39:42,842 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,009 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:39:45,349 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:39:45,350 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:39:46,057 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:39:46,304 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:39:46,304 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:13:05,255 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:13:05,278 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4983 mean pseudo reward: 0.9577079680164249
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 20891
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20991, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20991, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.051, loss:530.7566
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.065, loss:510.1206
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.063, loss:481.3942
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.070, loss:500.6081
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.071, loss:462.3705
>> valid entity prec:0.5495, rec:0.4251, f1:0.4794
>> valid relation prec:0.2705, rec:0.0590, f1:0.0969
>> valid relation with NER prec:0.2705, rec:0.0590, f1:0.0969
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.514, loss:467.1268
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.053, loss:457.5679
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.069, loss:483.6107
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.073, loss:453.1122
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.060, loss:502.1260
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5360, rec:0.3936, f1:0.4538
>> valid relation prec:0.2500, rec:0.0357, f1:0.0625
>> valid relation with NER prec:0.2500, rec:0.0357, f1:0.0625
g_step 1100, step 60, avg_time 2.496, loss:452.9762
g_step 1200, step 160, avg_time 1.078, loss:471.1591
g_step 1300, step 52, avg_time 1.060, loss:457.1555
g_step 1400, step 152, avg_time 1.072, loss:441.4314
g_step 1500, step 44, avg_time 1.065, loss:435.7071
>> valid entity prec:0.4717, rec:0.4539, f1:0.4626
>> valid relation prec:0.2351, rec:0.0558, f1:0.0902
>> valid relation with NER prec:0.2351, rec:0.0558, f1:0.0902
g_step 1600, step 144, avg_time 2.513, loss:436.5452
g_step 1700, step 36, avg_time 1.067, loss:423.6727
g_step 1800, step 136, avg_time 1.069, loss:409.9557
g_step 1900, step 28, avg_time 1.064, loss:397.7015
g_step 2000, step 128, avg_time 1.065, loss:398.5029
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5153, rec:0.5147, f1:0.5150
>> valid relation prec:0.2275, rec:0.0513, f1:0.0837
>> valid relation with NER prec:0.2275, rec:0.0513, f1:0.0837
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.526, loss:381.3937
g_step 2200, step 120, avg_time 1.072, loss:373.2581
g_step 2300, step 12, avg_time 1.058, loss:369.9186
g_step 2400, step 112, avg_time 1.062, loss:343.3991
g_step 2500, step 4, avg_time 1.072, loss:373.4167
>> valid entity prec:0.5247, rec:0.3582, f1:0.4257
>> valid relation prec:0.2698, rec:0.0421, f1:0.0729
>> valid relation with NER prec:0.2698, rec:0.0421, f1:0.0729
g_step 2600, step 104, avg_time 2.522, loss:331.3330
g_step 2700, step 204, avg_time 1.067, loss:375.5696
g_step 2800, step 96, avg_time 1.067, loss:314.0820
g_step 2900, step 196, avg_time 1.068, loss:347.7371
g_step 3000, step 88, avg_time 1.068, loss:302.1533
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5094, rec:0.4546, f1:0.4804
>> valid relation prec:0.2418, rec:0.0694, f1:0.1079
>> valid relation with NER prec:0.2418, rec:0.0694, f1:0.1079
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.515, loss:331.4369
g_step 3200, step 80, avg_time 1.076, loss:293.8848
g_step 3300, step 180, avg_time 1.063, loss:307.2111
g_step 3400, step 72, avg_time 1.074, loss:285.5444
g_step 3500, step 172, avg_time 1.061, loss:299.7179
>> valid entity prec:0.4935, rec:0.4406, f1:0.4655
>> valid relation prec:0.2190, rec:0.0612, f1:0.0957
>> valid relation with NER prec:0.2190, rec:0.0612, f1:0.0957
g_step 3600, step 64, avg_time 2.530, loss:282.9886
g_step 3700, step 164, avg_time 1.050, loss:293.1736
g_step 3800, step 56, avg_time 1.078, loss:283.8227
g_step 3900, step 156, avg_time 1.067, loss:275.3540
g_step 4000, step 48, avg_time 1.074, loss:271.4750
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5043, rec:0.4494, f1:0.4752
>> valid relation prec:0.2028, rec:0.0674, f1:0.1012
>> valid relation with NER prec:0.2028, rec:0.0674, f1:0.1012
g_step 4100, step 148, avg_time 2.515, loss:265.6426
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:13:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:13:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-13-05_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:13:06 - WARNING - datasets.builder -   Using custom data configuration default-d9970188354cf49a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d9970188354cf49a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:13:06,475 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:13:06,476 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:13:06,476 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:13:06,477 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:13:06,489 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:06,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:13:06,613 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:13:09,789 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:13:09,795 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d9970188354cf49a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.41ba/s] 40%|████      | 2/5 [00:00<00:00,  3.43ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.98ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.25ba/s]100%|██████████| 5/5 [00:01<00:00,  4.40ba/s]100%|██████████| 5/5 [00:01<00:00,  4.01ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.63ba/s] 40%|████      | 2/5 [00:00<00:00,  4.03ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.25ba/s]100%|██████████| 5/5 [00:00<00:00,  5.11ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.05ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.62ba/s]100%|██████████| 5/5 [00:00<00:00,  9.99ba/s]100%|██████████| 5/5 [00:00<00:00,  9.78ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.01ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.57ba/s]100%|██████████| 5/5 [00:00<00:00, 12.97ba/s]100%|██████████| 5/5 [00:00<00:00, 11.85ba/s]
[INFO|trainer.py:414] 2023-08-29 00:13:13,463 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:13:13,476 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:13:13,476 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-29 00:13:13,476 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:13:13,476 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:13:13,476 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:13:13,476 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:13:13,476 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:55,  3.37it/s]  1%|          | 2/390 [00:00<01:52,  3.44it/s]  1%|          | 3/390 [00:00<01:51,  3.46it/s]  1%|          | 4/390 [00:01<01:51,  3.47it/s]  1%|▏         | 5/390 [00:01<01:50,  3.48it/s]  2%|▏         | 6/390 [00:01<01:50,  3.48it/s]  2%|▏         | 7/390 [00:02<01:50,  3.48it/s]  2%|▏         | 8/390 [00:02<01:49,  3.48it/s]  2%|▏         | 9/390 [00:02<01:49,  3.48it/s]  3%|▎         | 10/390 [00:02<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:49,  3.48it/s]  3%|▎         | 12/390 [00:03<01:48,  3.48it/s]  3%|▎         | 13/390 [00:03<01:48,  3.48it/s]  4%|▎         | 14/390 [00:04<01:47,  3.48it/s]  4%|▍         | 15/390 [00:04<01:47,  3.48it/s]  4%|▍         | 16/390 [00:04<01:47,  3.49it/s]  4%|▍         | 17/390 [00:04<01:47,  3.48it/s]  5%|▍         | 18/390 [00:05<01:46,  3.48it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:05<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:46,  3.46it/s]  6%|▌         | 22/390 [00:06<01:46,  3.46it/s]  6%|▌         | 23/390 [00:06<01:45,  3.47it/s]  6%|▌         | 24/390 [00:06<01:45,  3.47it/s]  6%|▋         | 25/390 [00:07<01:45,  3.47it/s]  7%|▋         | 26/390 [00:07<01:44,  3.47it/s]  7%|▋         | 27/390 [00:07<01:44,  3.47it/s]  7%|▋         | 28/390 [00:08<01:44,  3.47it/s]  7%|▋         | 29/390 [00:08<01:43,  3.47it/s]  8%|▊         | 30/390 [00:08<01:43,  3.47it/s]  8%|▊         | 31/390 [00:08<01:43,  3.47it/s]  8%|▊         | 32/390 [00:09<01:43,  3.45it/s]  8%|▊         | 33/390 [00:09<01:43,  3.45it/s]  9%|▊         | 34/390 [00:09<01:42,  3.46it/s]  9%|▉         | 35/390 [00:10<01:42,  3.46it/s]  9%|▉         | 36/390 [00:10<01:42,  3.47it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:10<01:41,  3.47it/s] 10%|█         | 39/390 [00:11<01:41,  3.47it/s] 10%|█         | 40/390 [00:11<01:40,  3.47it/s] 11%|█         | 41/390 [00:11<01:40,  3.47it/s] 11%|█         | 42/390 [00:12<01:40,  3.47it/s] 11%|█         | 43/390 [00:12<01:40,  3.46it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.47it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.47it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.47it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 52/390 [00:14<01:37,  3.47it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.47it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.46it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.46it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.47it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.47it/s] 15%|█▌        | 59/390 [00:16<01:35,  3.47it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.47it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.47it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.46it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.47it/s] 18%|█▊        | 69/390 [00:19<01:32,  3.47it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.47it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.46it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.47it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.45it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.46it/s] 20%|██        | 78/390 [00:22<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 00:13:36,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:13:36,003 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:13:36,003 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.75it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.47it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.73it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.14it/s][A
  6%|▌         | 28/505 [00:00<00:09, 47.76it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.48it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.21it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.72it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.74it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.80it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.75it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.77it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.86it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.91it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.92it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.85it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.58it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.57it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.64it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.64it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.63it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.73it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.77it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.86it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.79it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.62it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.57it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.52it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.69it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.74it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.67it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.69it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.82it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.75it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.70it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.57it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.51it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.56it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.60it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.59it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.61it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.69it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.65it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.59it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.49it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.49it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.52it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.54it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.62it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.69it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.65it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.61it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.55it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.58it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.54it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.56it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.42it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.59it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.63it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.55it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.68it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.59it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.54it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.58it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.54it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.57it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.56it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.53it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.60it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.56it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.55it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.57it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.58it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.47it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.55it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.53it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.51it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.49it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.51it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.50it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.61it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.62it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.61it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.51it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.57it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.53it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.49it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.57it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.48it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.52it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.62it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.56it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.59it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.58it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.50it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.47it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.52it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.48it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.53it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:30,  3.46it/s]
100%|██████████| 505/505 [00:10<00:00, 46.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:13:46,859 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-29 00:13:46,877 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:13:49,215 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:13:49,233 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:13:49,245 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:40<29:21,  5.66s/it] 21%|██        | 80/390 [00:40<20:56,  4.05s/it] 21%|██        | 81/390 [00:41<15:03,  2.92s/it] 21%|██        | 82/390 [00:41<10:56,  2.13s/it] 21%|██▏       | 83/390 [00:41<08:04,  1.58s/it] 22%|██▏       | 84/390 [00:42<06:04,  1.19s/it] 22%|██▏       | 85/390 [00:42<04:40,  1.09it/s] 22%|██▏       | 86/390 [00:42<03:42,  1.37it/s] 22%|██▏       | 87/390 [00:43<03:01,  1.67it/s] 23%|██▎       | 88/390 [00:43<02:32,  1.98it/s] 23%|██▎       | 89/390 [00:43<02:12,  2.27it/s] 23%|██▎       | 90/390 [00:43<01:58,  2.53it/s] 23%|██▎       | 91/390 [00:44<01:48,  2.75it/s] 24%|██▎       | 92/390 [00:44<01:41,  2.93it/s] 24%|██▍       | 93/390 [00:44<01:36,  3.08it/s] 24%|██▍       | 94/390 [00:45<01:32,  3.19it/s] 24%|██▍       | 95/390 [00:45<01:30,  3.26it/s] 25%|██▍       | 96/390 [00:45<01:28,  3.32it/s] 25%|██▍       | 97/390 [00:45<01:27,  3.37it/s] 25%|██▌       | 98/390 [00:46<01:25,  3.40it/s] 25%|██▌       | 99/390 [00:46<01:25,  3.42it/s] 26%|██▌       | 100/390 [00:46<01:24,  3.43it/s] 26%|██▌       | 101/390 [00:47<01:23,  3.44it/s] 26%|██▌       | 102/390 [00:47<01:23,  3.44it/s] 26%|██▋       | 103/390 [00:47<01:23,  3.45it/s] 27%|██▋       | 104/390 [00:47<01:22,  3.45it/s] 27%|██▋       | 105/390 [00:48<01:22,  3.46it/s] 27%|██▋       | 106/390 [00:48<01:22,  3.46it/s] 27%|██▋       | 107/390 [00:48<01:21,  3.46it/s] 28%|██▊       | 108/390 [00:49<01:21,  3.46it/s] 28%|██▊       | 109/390 [00:49<01:21,  3.46it/s] 28%|██▊       | 110/390 [00:49<01:20,  3.46it/s] 28%|██▊       | 111/390 [00:49<01:20,  3.47it/s] 29%|██▊       | 112/390 [00:50<01:20,  3.47it/s] 29%|██▉       | 113/390 [00:50<01:20,  3.46it/s] 29%|██▉       | 114/390 [00:50<01:19,  3.46it/s] 29%|██▉       | 115/390 [00:51<01:20,  3.42it/s] 30%|██▉       | 116/390 [00:51<01:19,  3.43it/s] 30%|███       | 117/390 [00:51<01:19,  3.44it/s] 30%|███       | 118/390 [00:51<01:20,  3.38it/s] 31%|███       | 119/390 [00:52<01:19,  3.40it/s] 31%|███       | 120/390 [00:52<01:18,  3.42it/s] 31%|███       | 121/390 [00:52<01:18,  3.43it/s] 31%|███▏      | 122/390 [00:53<01:17,  3.44it/s] 32%|███▏      | 123/390 [00:53<01:17,  3.45it/s] 32%|███▏      | 124/390 [00:53<01:17,  3.45it/s] 32%|███▏      | 125/390 [00:54<01:16,  3.45it/s] 32%|███▏      | 126/390 [00:54<01:16,  3.45it/s] 33%|███▎      | 127/390 [00:54<01:16,  3.46it/s] 33%|███▎      | 128/390 [00:54<01:15,  3.46it/s] 33%|███▎      | 129/390 [00:55<01:15,  3.46it/s] 33%|███▎      | 130/390 [00:55<01:15,  3.46it/s] 34%|███▎      | 131/390 [00:55<01:14,  3.46it/s] 34%|███▍      | 132/390 [00:56<01:14,  3.45it/s] 34%|███▍      | 133/390 [00:56<01:14,  3.45it/s] 34%|███▍      | 134/390 [00:56<01:14,  3.45it/s] 35%|███▍      | 135/390 [00:56<01:13,  3.45it/s] 35%|███▍      | 136/390 [00:57<01:13,  3.45it/s] 35%|███▌      | 137/390 [00:57<01:13,  3.45it/s] 35%|███▌      | 138/390 [00:57<01:12,  3.46it/s] 36%|███▌      | 139/390 [00:58<01:12,  3.46it/s] 36%|███▌      | 140/390 [00:58<01:12,  3.46it/s] 36%|███▌      | 141/390 [00:58<01:12,  3.46it/s] 36%|███▋      | 142/390 [00:58<01:11,  3.46it/s] 37%|███▋      | 143/390 [00:59<01:11,  3.46it/s] 37%|███▋      | 144/390 [00:59<01:11,  3.46it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.45it/s] 37%|███▋      | 146/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:00<01:09,  3.45it/s] 38%|███▊      | 150/390 [01:01<01:09,  3.45it/s] 39%|███▊      | 151/390 [01:01<01:09,  3.45it/s] 39%|███▉      | 152/390 [01:01<01:08,  3.45it/s] 39%|███▉      | 153/390 [01:02<01:08,  3.45it/s] 39%|███▉      | 154/390 [01:02<01:08,  3.45it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.46it/s] 40%|████      | 156/390 [01:02<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:14:16,501 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:14:16,501 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:14:16,501 >>   Batch size = 8
{'eval_loss': 1.0567213296890259, 'eval_runtime': 10.8374, 'eval_samples_per_second': 372.69, 'eval_steps_per_second': 46.598, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.79it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.38it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.59it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.93it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.56it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.23it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.93it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.57it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.54it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.64it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.60it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.62it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.70it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.61it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.57it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.57it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.43it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.41it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.37it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.45it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.45it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.50it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.57it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.63it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.56it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.50it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.39it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.43it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.53it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.50it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.53it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.51it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.52it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.56it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.52it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.44it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.44it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.46it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.51it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.49it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.43it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.51it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.55it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.53it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.44it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.48it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.49it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.43it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.47it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.46it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.39it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.45it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.49it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.51it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.56it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.53it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.51it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.46it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.49it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.50it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.43it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.47it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.52it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.45it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.40it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.46it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.49it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.54it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.52it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.42it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.47it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.48it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.45it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.48it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.42it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.50it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.45it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.45it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.54it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.49it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.52it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.42it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.39it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.44it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.44it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.44it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.45it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.49it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.51it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.48it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.48it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.47it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.47it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.43it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.46it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.40it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.46it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.46it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.44it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.50it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:13<01:07,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:14:27,388 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:14:27,402 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:14:29,745 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:14:29,764 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:14:29,775 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:21<22:05,  5.69s/it] 41%|████      | 158/390 [01:21<15:43,  4.07s/it] 41%|████      | 159/390 [01:21<11:17,  2.93s/it] 41%|████      | 160/390 [01:22<08:12,  2.14s/it] 41%|████▏     | 161/390 [01:22<06:02,  1.58s/it] 42%|████▏     | 162/390 [01:22<04:32,  1.20s/it] 42%|████▏     | 163/390 [01:22<03:29,  1.08it/s] 42%|████▏     | 164/390 [01:23<02:45,  1.36it/s] 42%|████▏     | 165/390 [01:23<02:14,  1.67it/s] 43%|████▎     | 166/390 [01:23<01:53,  1.98it/s] 43%|████▎     | 167/390 [01:24<01:38,  2.27it/s] 43%|████▎     | 168/390 [01:24<01:27,  2.53it/s] 43%|████▎     | 169/390 [01:24<01:20,  2.76it/s] 44%|████▎     | 170/390 [01:25<01:14,  2.94it/s] 44%|████▍     | 171/390 [01:25<01:11,  3.08it/s] 44%|████▍     | 172/390 [01:25<01:08,  3.19it/s] 44%|████▍     | 173/390 [01:25<01:06,  3.27it/s] 45%|████▍     | 174/390 [01:26<01:04,  3.33it/s] 45%|████▍     | 175/390 [01:26<01:03,  3.37it/s] 45%|████▌     | 176/390 [01:26<01:03,  3.40it/s] 45%|████▌     | 177/390 [01:27<01:02,  3.42it/s] 46%|████▌     | 178/390 [01:27<01:01,  3.42it/s] 46%|████▌     | 179/390 [01:27<01:01,  3.44it/s] 46%|████▌     | 180/390 [01:27<01:00,  3.45it/s] 46%|████▋     | 181/390 [01:28<01:00,  3.46it/s] 47%|████▋     | 182/390 [01:28<01:00,  3.46it/s] 47%|████▋     | 183/390 [01:28<00:59,  3.46it/s] 47%|████▋     | 184/390 [01:29<00:59,  3.46it/s] 47%|████▋     | 185/390 [01:29<00:59,  3.46it/s] 48%|████▊     | 186/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 187/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 188/390 [01:30<00:58,  3.46it/s] 48%|████▊     | 189/390 [01:30<00:58,  3.45it/s] 49%|████▊     | 190/390 [01:30<00:57,  3.46it/s] 49%|████▉     | 191/390 [01:31<00:57,  3.46it/s] 49%|████▉     | 192/390 [01:31<00:57,  3.46it/s] 49%|████▉     | 193/390 [01:31<00:56,  3.46it/s] 50%|████▉     | 194/390 [01:31<00:56,  3.46it/s] 50%|█████     | 195/390 [01:32<00:56,  3.46it/s] 50%|█████     | 196/390 [01:32<00:56,  3.46it/s] 51%|█████     | 197/390 [01:32<00:55,  3.46it/s] 51%|█████     | 198/390 [01:33<00:55,  3.46it/s] 51%|█████     | 199/390 [01:33<00:55,  3.46it/s] 51%|█████▏    | 200/390 [01:33<00:55,  3.45it/s] 52%|█████▏    | 201/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 202/390 [01:34<00:54,  3.46it/s] 52%|█████▏    | 203/390 [01:34<00:54,  3.46it/s] 52%|█████▏    | 204/390 [01:34<00:53,  3.46it/s] 53%|█████▎    | 205/390 [01:35<00:53,  3.46it/s] 53%|█████▎    | 206/390 [01:35<00:53,  3.46it/s] 53%|█████▎    | 207/390 [01:35<00:52,  3.46it/s] 53%|█████▎    | 208/390 [01:35<00:52,  3.46it/s] 54%|█████▎    | 209/390 [01:36<00:52,  3.46it/s] 54%|█████▍    | 210/390 [01:36<00:52,  3.46it/s] 54%|█████▍    | 211/390 [01:36<00:51,  3.45it/s] 54%|█████▍    | 212/390 [01:37<00:51,  3.45it/s] 55%|█████▍    | 213/390 [01:37<00:51,  3.46it/s] 55%|█████▍    | 214/390 [01:37<00:50,  3.46it/s] 55%|█████▌    | 215/390 [01:38<00:50,  3.46it/s] 55%|█████▌    | 216/390 [01:38<00:50,  3.46it/s] 56%|█████▌    | 217/390 [01:38<00:49,  3.46it/s] 56%|█████▌    | 218/390 [01:38<00:49,  3.46it/s] 56%|█████▌    | 219/390 [01:39<00:49,  3.46it/s] 56%|█████▋    | 220/390 [01:39<00:49,  3.46it/s] 57%|█████▋    | 221/390 [01:39<00:48,  3.46it/s] 57%|█████▋    | 222/390 [01:40<00:49,  3.42it/s] 57%|█████▋    | 223/390 [01:40<00:48,  3.44it/s] 57%|█████▋    | 224/390 [01:40<00:48,  3.44it/s] 58%|█████▊    | 225/390 [01:40<00:47,  3.45it/s] 58%|█████▊    | 226/390 [01:41<00:47,  3.45it/s] 58%|█████▊    | 227/390 [01:41<00:47,  3.45it/s] 58%|█████▊    | 228/390 [01:41<00:46,  3.45it/s] 59%|█████▊    | 229/390 [01:42<00:46,  3.46it/s] 59%|█████▉    | 230/390 [01:42<00:46,  3.46it/s] 59%|█████▉    | 231/390 [01:42<00:45,  3.46it/s] 59%|█████▉    | 232/390 [01:42<00:45,  3.46it/s] 60%|█████▉    | 233/390 [01:43<00:45,  3.45it/s] 60%|██████    | 234/390 [01:43<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:14:57,034 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:14:57,034 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:14:57,034 >>   Batch size = 8
{'eval_loss': 1.0724915266036987, 'eval_runtime': 10.8666, 'eval_samples_per_second': 371.691, 'eval_steps_per_second': 46.473, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.41it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.36it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.65it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.81it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.47it/s][A
  7%|▋         | 33/505 [00:00<00:10, 47.11it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.94it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.59it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.44it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.63it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.55it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.57it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.61it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.52it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.54it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.50it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.42it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.42it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.44it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.45it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.46it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.44it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.58it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.48it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.48it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.51it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.45it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.44it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.41it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.42it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.56it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.60it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.51it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.44it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.46it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.47it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.41it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.42it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.34it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.40it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.51it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.52it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.48it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.51it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.43it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.43it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.39it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.33it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.44it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.39it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.48it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.52it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.43it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.46it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.49it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.40it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.46it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.47it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.42it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.43it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.43it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.39it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.44it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.50it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.45it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.42it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.46it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.42it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.48it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.43it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.41it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.42it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.46it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.42it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.48it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.49it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.42it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 43.29it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 44.32it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 45.04it/s][A
 81%|████████  | 408/505 [00:08<00:02, 45.49it/s][A
 82%|████████▏ | 413/505 [00:08<00:02, 45.91it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.13it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.15it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.34it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.24it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 45.98it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.04it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.14it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.37it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.48it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.54it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.53it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.58it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.41it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.23it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.03it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.12it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.22it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.40it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:54<00:45,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:15:07,958 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 00:15:07,974 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:15:10,090 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:15:10,104 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:15:10,114 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:02<15:31,  6.01s/it] 61%|██████    | 236/390 [02:03<11:01,  4.29s/it] 61%|██████    | 237/390 [02:03<07:53,  3.09s/it] 61%|██████    | 238/390 [02:03<05:42,  2.25s/it] 61%|██████▏   | 239/390 [02:04<04:11,  1.66s/it] 62%|██████▏   | 240/390 [02:04<03:07,  1.25s/it] 62%|██████▏   | 241/390 [02:04<02:23,  1.04it/s] 62%|██████▏   | 242/390 [02:04<01:52,  1.32it/s] 62%|██████▏   | 243/390 [02:05<01:30,  1.62it/s] 63%|██████▎   | 244/390 [02:05<01:15,  1.93it/s] 63%|██████▎   | 245/390 [02:05<01:05,  2.22it/s] 63%|██████▎   | 246/390 [02:06<00:57,  2.49it/s] 63%|██████▎   | 247/390 [02:06<00:52,  2.71it/s] 64%|██████▎   | 248/390 [02:06<00:48,  2.90it/s] 64%|██████▍   | 249/390 [02:06<00:46,  3.05it/s] 64%|██████▍   | 250/390 [02:07<00:44,  3.17it/s] 64%|██████▍   | 251/390 [02:07<00:42,  3.25it/s] 65%|██████▍   | 252/390 [02:07<00:41,  3.31it/s] 65%|██████▍   | 253/390 [02:08<00:40,  3.36it/s] 65%|██████▌   | 254/390 [02:08<00:40,  3.39it/s] 65%|██████▌   | 255/390 [02:08<00:39,  3.41it/s] 66%|██████▌   | 256/390 [02:08<00:39,  3.43it/s] 66%|██████▌   | 257/390 [02:09<00:38,  3.44it/s] 66%|██████▌   | 258/390 [02:09<00:38,  3.44it/s] 66%|██████▋   | 259/390 [02:09<00:37,  3.45it/s] 67%|██████▋   | 260/390 [02:10<00:37,  3.46it/s] 67%|██████▋   | 261/390 [02:10<00:37,  3.46it/s] 67%|██████▋   | 262/390 [02:10<00:36,  3.47it/s] 67%|██████▋   | 263/390 [02:10<00:36,  3.46it/s] 68%|██████▊   | 264/390 [02:11<00:36,  3.47it/s] 68%|██████▊   | 265/390 [02:11<00:36,  3.47it/s] 68%|██████▊   | 266/390 [02:11<00:35,  3.47it/s] 68%|██████▊   | 267/390 [02:12<00:35,  3.46it/s] 69%|██████▊   | 268/390 [02:12<00:35,  3.46it/s] 69%|██████▉   | 269/390 [02:12<00:35,  3.45it/s] 69%|██████▉   | 270/390 [02:12<00:34,  3.45it/s] 69%|██████▉   | 271/390 [02:13<00:34,  3.46it/s] 70%|██████▉   | 272/390 [02:13<00:34,  3.46it/s] 70%|███████   | 273/390 [02:13<00:33,  3.46it/s] 70%|███████   | 274/390 [02:14<00:33,  3.46it/s] 71%|███████   | 275/390 [02:14<00:33,  3.46it/s] 71%|███████   | 276/390 [02:14<00:32,  3.46it/s] 71%|███████   | 277/390 [02:14<00:32,  3.46it/s] 71%|███████▏  | 278/390 [02:15<00:32,  3.46it/s] 72%|███████▏  | 279/390 [02:15<00:32,  3.46it/s] 72%|███████▏  | 280/390 [02:15<00:31,  3.45it/s] 72%|███████▏  | 281/390 [02:16<00:31,  3.45it/s] 72%|███████▏  | 282/390 [02:16<00:31,  3.46it/s] 73%|███████▎  | 283/390 [02:16<00:30,  3.46it/s] 73%|███████▎  | 284/390 [02:17<00:30,  3.46it/s] 73%|███████▎  | 285/390 [02:17<00:30,  3.46it/s] 73%|███████▎  | 286/390 [02:17<00:30,  3.46it/s] 74%|███████▎  | 287/390 [02:17<00:29,  3.46it/s] 74%|███████▍  | 288/390 [02:18<00:29,  3.46it/s] 74%|███████▍  | 289/390 [02:18<00:29,  3.46it/s] 74%|███████▍  | 290/390 [02:18<00:28,  3.46it/s] 75%|███████▍  | 291/390 [02:19<00:28,  3.44it/s] 75%|███████▍  | 292/390 [02:19<00:28,  3.44it/s] 75%|███████▌  | 293/390 [02:19<00:28,  3.45it/s] 75%|███████▌  | 294/390 [02:19<00:27,  3.45it/s] 76%|███████▌  | 295/390 [02:20<00:27,  3.45it/s] 76%|███████▌  | 296/390 [02:20<00:27,  3.46it/s] 76%|███████▌  | 297/390 [02:20<00:26,  3.46it/s] 76%|███████▋  | 298/390 [02:21<00:26,  3.46it/s] 77%|███████▋  | 299/390 [02:21<00:26,  3.46it/s] 77%|███████▋  | 300/390 [02:21<00:26,  3.46it/s] 77%|███████▋  | 301/390 [02:21<00:25,  3.46it/s] 77%|███████▋  | 302/390 [02:22<00:25,  3.45it/s] 78%|███████▊  | 303/390 [02:22<00:25,  3.45it/s] 78%|███████▊  | 304/390 [02:22<00:24,  3.45it/s] 78%|███████▊  | 305/390 [02:23<00:24,  3.46it/s] 78%|███████▊  | 306/390 [02:23<00:24,  3.46it/s] 79%|███████▊  | 307/390 [02:23<00:23,  3.46it/s] 79%|███████▉  | 308/390 [02:23<00:23,  3.46it/s] 79%|███████▉  | 309/390 [02:24<00:23,  3.46it/s] 79%|███████▉  | 310/390 [02:24<00:23,  3.46it/s] 80%|███████▉  | 311/390 [02:24<00:22,  3.46it/s] 80%|████████  | 312/390 [02:25<00:22,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 00:15:38,648 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:15:38,649 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:15:38,649 >>   Batch size = 8
{'eval_loss': 1.0861748456954956, 'eval_runtime': 10.9003, 'eval_samples_per_second': 370.539, 'eval_steps_per_second': 46.329, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.79it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.42it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.63it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.99it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.41it/s][A
  7%|▋         | 33/505 [00:00<00:10, 47.07it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.91it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.56it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.61it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.58it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.56it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.59it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.57it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.60it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.47it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.48it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.44it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.38it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.47it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.52it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.52it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.58it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.51it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.43it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.46it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.43it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.46it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.41it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.45it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.53it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.55it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.49it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.48it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.49it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.38it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.39it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.43it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.50it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.58it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.55it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.56it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.43it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.54it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.52it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.42it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.44it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.47it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.54it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.41it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.44it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.44it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.43it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.40it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.46it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.48it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.49it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.48it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.40it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.47it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.49it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.52it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.38it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.38it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.47it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.48it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.48it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.42it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.47it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.48it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.49it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.44it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.46it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.42it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.24it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.50it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.50it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.42it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.46it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.47it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.44it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.48it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.42it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.47it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.37it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.47it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.43it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.41it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.45it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.47it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.48it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.40it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.45it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.45it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.44it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.50it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.43it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.42it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.49it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.48it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.47it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:36<00:22,  3.47it/s]
100%|██████████| 505/505 [00:10<00:00, 46.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:15:49,551 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:15:49,594 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:15:52,070 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:15:52,085 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:15:52,095 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:43<07:29,  5.83s/it] 81%|████████  | 314/390 [02:44<05:17,  4.17s/it] 81%|████████  | 315/390 [02:44<03:45,  3.01s/it] 81%|████████  | 316/390 [02:44<02:42,  2.19s/it] 81%|████████▏ | 317/390 [02:45<01:58,  1.62s/it] 82%|████████▏ | 318/390 [02:45<01:27,  1.22s/it] 82%|████████▏ | 319/390 [02:45<01:06,  1.06it/s] 82%|████████▏ | 320/390 [02:45<00:52,  1.34it/s] 82%|████████▏ | 321/390 [02:46<00:41,  1.65it/s] 83%|████████▎ | 322/390 [02:46<00:34,  1.95it/s] 83%|████████▎ | 323/390 [02:46<00:29,  2.25it/s] 83%|████████▎ | 324/390 [02:47<00:26,  2.51it/s] 83%|████████▎ | 325/390 [02:47<00:23,  2.73it/s] 84%|████████▎ | 326/390 [02:47<00:21,  2.92it/s] 84%|████████▍ | 327/390 [02:47<00:20,  3.07it/s] 84%|████████▍ | 328/390 [02:48<00:19,  3.18it/s] 84%|████████▍ | 329/390 [02:48<00:18,  3.26it/s] 85%|████████▍ | 330/390 [02:48<00:18,  3.32it/s] 85%|████████▍ | 331/390 [02:49<00:17,  3.36it/s] 85%|████████▌ | 332/390 [02:49<00:17,  3.39it/s] 85%|████████▌ | 333/390 [02:49<00:16,  3.41it/s] 86%|████████▌ | 334/390 [02:49<00:16,  3.43it/s] 86%|████████▌ | 335/390 [02:50<00:15,  3.44it/s] 86%|████████▌ | 336/390 [02:50<00:15,  3.44it/s] 86%|████████▋ | 337/390 [02:50<00:15,  3.45it/s] 87%|████████▋ | 338/390 [02:51<00:15,  3.44it/s] 87%|████████▋ | 339/390 [02:51<00:14,  3.45it/s] 87%|████████▋ | 340/390 [02:51<00:14,  3.45it/s] 87%|████████▋ | 341/390 [02:51<00:14,  3.45it/s] 88%|████████▊ | 342/390 [02:52<00:14,  3.40it/s] 88%|████████▊ | 343/390 [02:52<00:13,  3.42it/s] 88%|████████▊ | 344/390 [02:52<00:13,  3.43it/s] 88%|████████▊ | 345/390 [02:53<00:13,  3.44it/s] 89%|████████▊ | 346/390 [02:53<00:12,  3.45it/s] 89%|████████▉ | 347/390 [02:53<00:12,  3.44it/s] 89%|████████▉ | 348/390 [02:54<00:12,  3.45it/s] 89%|████████▉ | 349/390 [02:54<00:11,  3.45it/s] 90%|████████▉ | 350/390 [02:54<00:11,  3.46it/s] 90%|█████████ | 351/390 [02:54<00:11,  3.46it/s] 90%|█████████ | 352/390 [02:55<00:10,  3.46it/s] 91%|█████████ | 353/390 [02:55<00:10,  3.46it/s] 91%|█████████ | 354/390 [02:55<00:10,  3.46it/s] 91%|█████████ | 355/390 [02:56<00:10,  3.46it/s] 91%|█████████▏| 356/390 [02:56<00:09,  3.46it/s] 92%|█████████▏| 357/390 [02:56<00:09,  3.46it/s] 92%|█████████▏| 358/390 [02:56<00:09,  3.46it/s] 92%|█████████▏| 359/390 [02:57<00:08,  3.46it/s] 92%|█████████▏| 360/390 [02:57<00:08,  3.46it/s] 93%|█████████▎| 361/390 [02:57<00:08,  3.46it/s] 93%|█████████▎| 362/390 [02:58<00:08,  3.46it/s] 93%|█████████▎| 363/390 [02:58<00:07,  3.46it/s] 93%|█████████▎| 364/390 [02:58<00:07,  3.46it/s] 94%|█████████▎| 365/390 [02:58<00:07,  3.46it/s] 94%|█████████▍| 366/390 [02:59<00:06,  3.46it/s] 94%|█████████▍| 367/390 [02:59<00:06,  3.45it/s] 94%|█████████▍| 368/390 [02:59<00:06,  3.45it/s] 95%|█████████▍| 369/390 [03:00<00:06,  3.45it/s] 95%|█████████▍| 370/390 [03:00<00:05,  3.46it/s] 95%|█████████▌| 371/390 [03:00<00:05,  3.46it/s] 95%|█████████▌| 372/390 [03:00<00:05,  3.46it/s] 96%|█████████▌| 373/390 [03:01<00:04,  3.46it/s] 96%|█████████▌| 374/390 [03:01<00:04,  3.46it/s] 96%|█████████▌| 375/390 [03:01<00:04,  3.46it/s] 96%|█████████▋| 376/390 [03:02<00:04,  3.46it/s] 97%|█████████▋| 377/390 [03:02<00:03,  3.46it/s] 97%|█████████▋| 378/390 [03:02<00:03,  3.45it/s] 97%|█████████▋| 379/390 [03:02<00:03,  3.45it/s] 97%|█████████▋| 380/390 [03:03<00:02,  3.45it/s] 98%|█████████▊| 381/390 [03:03<00:02,  3.46it/s] 98%|█████████▊| 382/390 [03:03<00:02,  3.46it/s] 98%|█████████▊| 383/390 [03:04<00:02,  3.46it/s] 98%|█████████▊| 384/390 [03:04<00:01,  3.46it/s] 99%|█████████▊| 385/390 [03:04<00:01,  3.46it/s] 99%|█████████▉| 386/390 [03:05<00:01,  3.46it/s] 99%|█████████▉| 387/390 [03:05<00:00,  3.46it/s] 99%|█████████▉| 388/390 [03:05<00:00,  3.46it/s]100%|█████████▉| 389/390 [03:05<00:00,  3.45it/s]100%|██████████| 390/390 [03:06<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:16:19,649 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:16:19,649 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:16:19,649 >>   Batch size = 8
{'eval_loss': 1.0927734375, 'eval_runtime': 10.8704, 'eval_samples_per_second': 371.559, 'eval_steps_per_second': 46.456, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.26it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.48it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.69it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.00it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.55it/s][A
  7%|▋         | 33/505 [00:00<00:10, 47.17it/s][A
  8%|▊         | 38/505 [00:00<00:09, 46.76it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.55it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.43it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.41it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.46it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.49it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.54it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.63it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.66it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.49it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.41it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.37it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.39it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.28it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.43it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.53it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.59it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.58it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.53it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.55it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.45it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.47it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.41it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.40it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.48it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.52it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.50it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.54it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.47it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.42it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.43it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.38it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.34it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.43it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.50it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.49it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.49it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.53it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.51it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.43it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.45it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.36it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.51it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.39it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.45it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.52it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.46it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.51it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.49it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.41it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.35it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.38it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.45it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.49it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.44it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.51it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.49it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.44it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.47it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.48it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.35it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.39it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.45it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.49it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.43it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.40it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.42it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 46.47it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.42it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.40it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.36it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.45it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.41it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.44it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.43it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.42it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.44it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.42it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.38it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.38it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.40it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.42it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.45it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.42it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.40it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.43it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.43it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.42it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.44it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.40it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.43it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.41it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.48it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.47it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:17<00:00,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:16:30,545 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-29 00:16:30,563 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:16:32,907 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:16:32,925 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:16:32,935 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:16:37,684 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:16:37,687 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78 (score: 1.0567213296890259).
                                                 100%|██████████| 390/390 [03:26<00:00,  3.45it/s]100%|██████████| 390/390 [03:26<00:00,  1.89it/s]
[INFO|trainer.py:1894] 2023-08-29 00:16:39,556 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 00:16:39,573 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:16:42,148 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:16:42,161 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:16:42,176 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:16:42,347 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   train_loss               =     0.5641
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   train_runtime            = 0:03:26.07
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   train_samples_per_second =    121.315
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:42,347 >>   train_steps_per_second   =      1.893
{'eval_loss': 1.0989282131195068, 'eval_runtime': 10.8725, 'eval_samples_per_second': 371.486, 'eval_steps_per_second': 46.447, 'epoch': 4.99}
{'train_runtime': 206.0758, 'train_samples_per_second': 121.315, 'train_steps_per_second': 1.893, 'train_loss': 0.5640702467698318, 'epoch': 4.99}
08/29/2023 00:16:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:16:42,378 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:16:42,378 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 00:16:42,378 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 58.78it/s]  2%|▏         | 12/505 [00:00<00:09, 51.17it/s]  4%|▎         | 18/505 [00:00<00:09, 49.25it/s]  5%|▍         | 23/505 [00:00<00:09, 48.44it/s]  6%|▌         | 28/505 [00:00<00:09, 47.99it/s]  7%|▋         | 33/505 [00:00<00:09, 47.69it/s]  8%|▊         | 38/505 [00:00<00:09, 47.38it/s]  9%|▊         | 43/505 [00:00<00:09, 47.27it/s] 10%|▉         | 48/505 [00:00<00:09, 47.08it/s] 10%|█         | 53/505 [00:01<00:09, 46.98it/s] 11%|█▏        | 58/505 [00:01<00:09, 46.99it/s] 12%|█▏        | 63/505 [00:01<00:09, 46.95it/s] 13%|█▎        | 68/505 [00:01<00:09, 46.84it/s] 14%|█▍        | 73/505 [00:01<00:09, 46.88it/s] 15%|█▌        | 78/505 [00:01<00:09, 46.86it/s] 16%|█▋        | 83/505 [00:01<00:09, 46.85it/s] 17%|█▋        | 88/505 [00:01<00:08, 46.82it/s] 18%|█▊        | 93/505 [00:01<00:08, 46.82it/s] 19%|█▉        | 98/505 [00:02<00:08, 46.76it/s] 20%|██        | 103/505 [00:02<00:08, 46.80it/s] 21%|██▏       | 108/505 [00:02<00:08, 46.69it/s] 22%|██▏       | 113/505 [00:02<00:08, 46.70it/s] 23%|██▎       | 118/505 [00:02<00:08, 46.66it/s] 24%|██▍       | 123/505 [00:02<00:08, 46.79it/s] 25%|██▌       | 128/505 [00:02<00:08, 46.83it/s] 26%|██▋       | 133/505 [00:02<00:07, 46.89it/s] 27%|██▋       | 138/505 [00:02<00:07, 46.90it/s] 28%|██▊       | 143/505 [00:03<00:07, 46.82it/s] 29%|██▉       | 148/505 [00:03<00:07, 46.87it/s] 30%|███       | 153/505 [00:03<00:07, 46.90it/s] 31%|███▏      | 158/505 [00:03<00:07, 46.86it/s] 32%|███▏      | 163/505 [00:03<00:07, 46.80it/s] 33%|███▎      | 168/505 [00:03<00:07, 46.80it/s] 34%|███▍      | 173/505 [00:03<00:07, 46.81it/s] 35%|███▌      | 178/505 [00:03<00:06, 46.85it/s] 36%|███▌      | 183/505 [00:03<00:06, 46.90it/s] 37%|███▋      | 188/505 [00:03<00:06, 46.88it/s] 38%|███▊      | 193/505 [00:04<00:06, 46.86it/s] 39%|███▉      | 198/505 [00:04<00:06, 46.85it/s] 40%|████      | 203/505 [00:04<00:06, 46.82it/s] 41%|████      | 208/505 [00:04<00:06, 46.90it/s] 42%|████▏     | 213/505 [00:04<00:06, 46.78it/s] 43%|████▎     | 218/505 [00:04<00:06, 46.75it/s] 44%|████▍     | 223/505 [00:04<00:06, 46.78it/s] 45%|████▌     | 228/505 [00:04<00:05, 46.76it/s] 46%|████▌     | 233/505 [00:04<00:05, 46.75it/s] 47%|████▋     | 238/505 [00:05<00:05, 46.78it/s] 48%|████▊     | 243/505 [00:05<00:05, 46.79it/s] 49%|████▉     | 248/505 [00:05<00:05, 46.71it/s] 50%|█████     | 253/505 [00:05<00:05, 46.80it/s] 51%|█████     | 258/505 [00:05<00:05, 46.80it/s] 52%|█████▏    | 263/505 [00:05<00:05, 46.83it/s] 53%|█████▎    | 268/505 [00:05<00:05, 46.78it/s] 54%|█████▍    | 273/505 [00:05<00:04, 46.82it/s] 55%|█████▌    | 278/505 [00:05<00:04, 46.80it/s] 56%|█████▌    | 283/505 [00:06<00:04, 46.86it/s] 57%|█████▋    | 288/505 [00:06<00:04, 46.59it/s] 58%|█████▊    | 293/505 [00:06<00:04, 46.70it/s] 59%|█████▉    | 298/505 [00:06<00:04, 46.69it/s] 60%|██████    | 303/505 [00:06<00:04, 46.74it/s] 61%|██████    | 308/505 [00:06<00:04, 46.67it/s] 62%|██████▏   | 313/505 [00:06<00:04, 46.80it/s] 63%|██████▎   | 318/505 [00:06<00:04, 46.63it/s] 64%|██████▍   | 323/505 [00:06<00:03, 46.66it/s] 65%|██████▍   | 328/505 [00:06<00:03, 46.76it/s] 66%|██████▌   | 333/505 [00:07<00:03, 46.68it/s] 67%|██████▋   | 338/505 [00:07<00:03, 46.79it/s] 68%|██████▊   | 343/505 [00:07<00:03, 46.87it/s] 69%|██████▉   | 348/505 [00:07<00:03, 46.81it/s] 70%|██████▉   | 353/505 [00:07<00:03, 46.82it/s] 71%|███████   | 358/505 [00:07<00:03, 46.76it/s] 72%|███████▏  | 363/505 [00:07<00:03, 46.71it/s] 73%|███████▎  | 368/505 [00:07<00:02, 46.66it/s] 74%|███████▍  | 373/505 [00:07<00:02, 46.67it/s] 75%|███████▍  | 378/505 [00:08<00:02, 46.65it/s] 76%|███████▌  | 383/505 [00:08<00:02, 46.70it/s] 77%|███████▋  | 388/505 [00:08<00:02, 46.67it/s] 78%|███████▊  | 393/505 [00:08<00:02, 46.69it/s] 79%|███████▉  | 398/505 [00:08<00:02, 46.65it/s] 80%|███████▉  | 403/505 [00:08<00:02, 46.68it/s] 81%|████████  | 408/505 [00:08<00:02, 46.68it/s] 82%|████████▏ | 413/505 [00:08<00:01, 46.71it/s] 83%|████████▎ | 418/505 [00:08<00:01, 46.67it/s] 84%|████████▍ | 423/505 [00:09<00:01, 46.66it/s] 85%|████████▍ | 428/505 [00:09<00:01, 46.69it/s] 86%|████████▌ | 433/505 [00:09<00:01, 46.67it/s] 87%|████████▋ | 438/505 [00:09<00:01, 46.69it/s] 88%|████████▊ | 443/505 [00:09<00:01, 46.71it/s] 89%|████████▊ | 448/505 [00:09<00:01, 46.66it/s] 90%|████████▉ | 453/505 [00:09<00:01, 46.65it/s] 91%|█████████ | 458/505 [00:09<00:01, 46.61it/s] 92%|█████████▏| 463/505 [00:09<00:00, 46.68it/s] 93%|█████████▎| 468/505 [00:09<00:00, 46.67it/s] 94%|█████████▎| 473/505 [00:10<00:00, 46.65it/s] 95%|█████████▍| 478/505 [00:10<00:00, 46.68it/s] 96%|█████████▌| 483/505 [00:10<00:00, 46.72it/s] 97%|█████████▋| 488/505 [00:10<00:00, 46.67it/s] 98%|█████████▊| 493/505 [00:10<00:00, 46.69it/s] 99%|█████████▊| 498/505 [00:10<00:00, 46.60it/s]100%|█████████▉| 503/505 [00:10<00:00, 46.66it/s]100%|██████████| 505/505 [00:10<00:00, 46.84it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:16:53,182 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   eval_loss               =     1.0567
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   eval_runtime            = 0:00:10.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   eval_samples_per_second =    373.844
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   eval_steps_per_second   =     46.742
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:16:53,182 >>   perplexity              =     2.8769
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:16:59,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:16:59,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:16:59,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:16:59,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:16:59,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:17:00,546 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:17:00,547 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:17:01,129 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:17:02,159 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:17:02,159 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:04,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:04,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:04,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:04,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:04,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:17:05,630 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:17:05,631 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:17:06,232 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:17:06,381 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:17:06,381 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.56it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:32,  1.47it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.43it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.51it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:38,  1.48it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:45,  1.46it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:47,  1.46it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:48,  1.42it/s]Extractor Predicting: 73it [00:49,  1.42it/s]Extractor Predicting: 74it [00:49,  1.37it/s]Extractor Predicting: 75it [00:50,  1.40it/s]Extractor Predicting: 76it [00:51,  1.42it/s]Extractor Predicting: 77it [00:52,  1.42it/s]Extractor Predicting: 78it [00:52,  1.43it/s]Extractor Predicting: 79it [00:53,  1.44it/s]Extractor Predicting: 80it [00:54,  1.44it/s]Extractor Predicting: 81it [00:54,  1.45it/s]Extractor Predicting: 82it [00:55,  1.49it/s]Extractor Predicting: 83it [00:56,  1.46it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:58,  1.48it/s]Extractor Predicting: 87it [00:58,  1.51it/s]Extractor Predicting: 88it [00:59,  1.52it/s]Extractor Predicting: 89it [01:00,  1.50it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.53it/s]Extractor Predicting: 93it [01:02,  1.52it/s]Extractor Predicting: 94it [01:03,  1.50it/s]Extractor Predicting: 95it [01:04,  1.49it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:05,  1.47it/s]Extractor Predicting: 98it [01:06,  1.46it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.43it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:09,  1.45it/s]Extractor Predicting: 104it [01:10,  1.48it/s]Extractor Predicting: 105it [01:11,  1.48it/s]Extractor Predicting: 106it [01:11,  1.50it/s]Extractor Predicting: 107it [01:12,  1.50it/s]Extractor Predicting: 108it [01:12,  1.52it/s]Extractor Predicting: 109it [01:13,  1.52it/s]Extractor Predicting: 110it [01:14,  1.50it/s]Extractor Predicting: 111it [01:15,  1.46it/s]Extractor Predicting: 112it [01:15,  1.50it/s]Extractor Predicting: 113it [01:16,  1.49it/s]Extractor Predicting: 114it [01:17,  1.47it/s]Extractor Predicting: 115it [01:17,  1.52it/s]Extractor Predicting: 116it [01:18,  1.51it/s]Extractor Predicting: 117it [01:18,  1.50it/s]Extractor Predicting: 118it [01:19,  1.45it/s]Extractor Predicting: 119it [01:20,  1.49it/s]Extractor Predicting: 120it [01:21,  1.49it/s]Extractor Predicting: 121it [01:21,  1.47it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:23,  1.49it/s]Extractor Predicting: 125it [01:24,  1.26it/s]Extractor Predicting: 126it [01:25,  1.33it/s]Extractor Predicting: 127it [01:26,  1.31it/s]Extractor Predicting: 128it [01:26,  1.31it/s]Extractor Predicting: 129it [01:27,  1.33it/s]Extractor Predicting: 130it [01:28,  1.34it/s]Extractor Predicting: 131it [01:29,  1.32it/s]Extractor Predicting: 132it [01:29,  1.33it/s]Extractor Predicting: 133it [01:30,  1.34it/s]Extractor Predicting: 134it [01:31,  1.38it/s]Extractor Predicting: 135it [01:32,  1.39it/s]Extractor Predicting: 136it [01:32,  1.39it/s]Extractor Predicting: 137it [01:33,  1.41it/s]Extractor Predicting: 138it [01:34,  1.37it/s]Extractor Predicting: 139it [01:35,  1.38it/s]Extractor Predicting: 140it [01:35,  1.37it/s]Extractor Predicting: 141it [01:36,  1.38it/s]Extractor Predicting: 142it [01:37,  1.39it/s]Extractor Predicting: 143it [01:37,  1.38it/s]Extractor Predicting: 144it [01:38,  1.39it/s]Extractor Predicting: 145it [01:39,  1.38it/s]Extractor Predicting: 146it [01:40,  1.39it/s]Extractor Predicting: 147it [01:40,  1.40it/s]Extractor Predicting: 148it [01:41,  1.36it/s]Extractor Predicting: 149it [01:42,  1.40it/s]Extractor Predicting: 149it [01:42,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:56,324 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:56,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:56,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:56,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:56,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:18:56,932 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:18:56,933 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:18:57,498 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:18:58,500 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:18:58,500 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:01,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:01,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:01,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:01,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:01,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:19:02,238 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:19:02,239 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:19:02,833 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:19:02,982 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:19:02,982 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2599009900990099,
  "recall": 0.05199306759098787,
  "score": 0.08665153703321643,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.72it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.70it/s]Extractor Predicting: 26it [00:15,  1.65it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.70it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.64it/s]Extractor Predicting: 45it [00:27,  1.51it/s]Extractor Predicting: 46it [00:28,  1.49it/s]Extractor Predicting: 47it [00:28,  1.52it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:29,  1.57it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:31,  1.60it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:34,  1.55it/s]Extractor Predicting: 57it [00:34,  1.55it/s]Extractor Predicting: 58it [00:35,  1.58it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:36,  1.56it/s]Extractor Predicting: 61it [00:37,  1.57it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:38,  1.57it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:42,  1.58it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:43,  1.55it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:45,  1.56it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:46,  1.57it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.52it/s]Extractor Predicting: 79it [00:48,  1.55it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.56it/s]Extractor Predicting: 82it [00:50,  1.54it/s]Extractor Predicting: 83it [00:51,  1.53it/s]Extractor Predicting: 84it [00:52,  1.43it/s]Extractor Predicting: 85it [00:52,  1.46it/s]Extractor Predicting: 86it [00:53,  1.46it/s]Extractor Predicting: 87it [00:54,  1.51it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:55,  1.58it/s]Extractor Predicting: 90it [00:56,  1.58it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:57,  1.61it/s]Extractor Predicting: 94it [00:58,  1.62it/s]Extractor Predicting: 95it [00:59,  1.60it/s]Extractor Predicting: 96it [00:59,  1.63it/s]Extractor Predicting: 97it [01:00,  1.63it/s]Extractor Predicting: 98it [01:01,  1.62it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:02,  1.64it/s]Extractor Predicting: 101it [01:02,  1.62it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:04,  1.62it/s]Extractor Predicting: 105it [01:05,  1.57it/s]Extractor Predicting: 106it [01:06,  1.59it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:07,  1.57it/s]Extractor Predicting: 109it [01:08,  1.56it/s]Extractor Predicting: 110it [01:08,  1.57it/s]Extractor Predicting: 111it [01:09,  1.57it/s]Extractor Predicting: 112it [01:09,  1.57it/s]Extractor Predicting: 113it [01:10,  1.60it/s]Extractor Predicting: 114it [01:11,  1.59it/s]Extractor Predicting: 115it [01:11,  1.60it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:14,  1.69it/s]Extractor Predicting: 120it [01:14,  1.68it/s]Extractor Predicting: 121it [01:15,  1.67it/s]Extractor Predicting: 122it [01:15,  1.66it/s]Extractor Predicting: 123it [01:16,  1.64it/s]Extractor Predicting: 124it [01:17,  1.65it/s]Extractor Predicting: 125it [01:17,  1.65it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:20,  1.55it/s]Extractor Predicting: 130it [01:21,  1.53it/s]Extractor Predicting: 131it [01:21,  1.51it/s]Extractor Predicting: 132it [01:22,  1.52it/s]Extractor Predicting: 133it [01:23,  1.54it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:24,  1.51it/s]Extractor Predicting: 136it [01:24,  1.51it/s]Extractor Predicting: 137it [01:25,  1.50it/s]Extractor Predicting: 138it [01:26,  1.52it/s]Extractor Predicting: 139it [01:26,  1.52it/s]Extractor Predicting: 140it [01:27,  1.49it/s]Extractor Predicting: 141it [01:28,  1.49it/s]Extractor Predicting: 142it [01:29,  1.50it/s]Extractor Predicting: 143it [01:29,  1.52it/s]Extractor Predicting: 144it [01:30,  1.51it/s]Extractor Predicting: 145it [01:30,  1.50it/s]Extractor Predicting: 146it [01:31,  1.50it/s]Extractor Predicting: 147it [01:32,  1.51it/s]Extractor Predicting: 148it [01:32,  1.52it/s]Extractor Predicting: 149it [01:33,  1.55it/s]Extractor Predicting: 150it [01:34,  1.54it/s]Extractor Predicting: 151it [01:34,  1.52it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:36,  1.50it/s]Extractor Predicting: 155it [01:37,  1.52it/s]Extractor Predicting: 156it [01:38,  1.56it/s]Extractor Predicting: 157it [01:38,  1.62it/s]Extractor Predicting: 158it [01:39,  1.62it/s]Extractor Predicting: 159it [01:39,  1.62it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:41,  1.67it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:42,  1.61it/s]Extractor Predicting: 164it [01:43,  1.62it/s]Extractor Predicting: 165it [01:43,  1.66it/s]Extractor Predicting: 166it [01:44,  1.66it/s]Extractor Predicting: 167it [01:44,  1.71it/s]Extractor Predicting: 168it [01:45,  1.72it/s]Extractor Predicting: 169it [01:45,  1.75it/s]Extractor Predicting: 170it [01:46,  1.69it/s]Extractor Predicting: 171it [01:47,  1.62it/s]Extractor Predicting: 172it [01:47,  1.59it/s]Extractor Predicting: 173it [01:48,  1.63it/s]Extractor Predicting: 173it [01:48,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:20:58,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:20:58,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:20:58,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:20:58,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:20:58,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:20:58,668 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:20:58,669 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:20:58,971 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:20:59,985 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:20:59,985 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:21:02,224 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:21:02,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:21:02,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:21:02,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:21:02,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:21:02,620 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:21:02,621 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:21:02,886 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:21:03,043 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:21:03,043 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30906148867313915,
  "recall": 0.046068499758803665,
  "score": 0.08018471872376154,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.40it/s]Extractor Predicting: 10it [00:06,  1.41it/s]Extractor Predicting: 11it [00:07,  1.39it/s]Extractor Predicting: 12it [00:08,  1.36it/s]Extractor Predicting: 13it [00:09,  1.42it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.56it/s]Extractor Predicting: 16it [00:10,  1.65it/s]Extractor Predicting: 17it [00:11,  1.74it/s]Extractor Predicting: 18it [00:11,  1.78it/s]Extractor Predicting: 19it [00:12,  1.79it/s]Extractor Predicting: 20it [00:12,  1.81it/s]Extractor Predicting: 21it [00:13,  1.85it/s]Extractor Predicting: 22it [00:13,  1.81it/s]Extractor Predicting: 23it [00:14,  1.81it/s]Extractor Predicting: 24it [00:15,  1.80it/s]Extractor Predicting: 25it [00:15,  1.80it/s]Extractor Predicting: 26it [00:16,  1.84it/s]Extractor Predicting: 27it [00:16,  1.79it/s]Extractor Predicting: 28it [00:17,  1.80it/s]Extractor Predicting: 29it [00:17,  1.81it/s]Extractor Predicting: 30it [00:18,  1.85it/s]Extractor Predicting: 31it [00:18,  1.87it/s]Extractor Predicting: 32it [00:19,  1.85it/s]Extractor Predicting: 33it [00:19,  1.87it/s]Extractor Predicting: 34it [00:20,  1.86it/s]Extractor Predicting: 35it [00:21,  1.86it/s]Extractor Predicting: 36it [00:21,  1.81it/s]Extractor Predicting: 37it [00:22,  1.76it/s]Extractor Predicting: 38it [00:22,  1.82it/s]Extractor Predicting: 39it [00:23,  1.82it/s]Extractor Predicting: 40it [00:23,  1.83it/s]Extractor Predicting: 41it [00:24,  1.82it/s]Extractor Predicting: 42it [00:24,  1.85it/s]Extractor Predicting: 43it [00:25,  1.82it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:27,  1.50it/s]Extractor Predicting: 47it [00:28,  1.48it/s]Extractor Predicting: 48it [00:29,  1.47it/s]Extractor Predicting: 49it [00:29,  1.47it/s]Extractor Predicting: 50it [00:30,  1.45it/s]Extractor Predicting: 51it [00:31,  1.44it/s]Extractor Predicting: 52it [00:31,  1.43it/s]Extractor Predicting: 53it [00:32,  1.42it/s]Extractor Predicting: 54it [00:33,  1.33it/s]Extractor Predicting: 55it [00:34,  1.37it/s]Extractor Predicting: 56it [00:34,  1.37it/s]Extractor Predicting: 57it [00:35,  1.37it/s]Extractor Predicting: 58it [00:36,  1.39it/s]Extractor Predicting: 59it [00:36,  1.64it/s]Extractor Predicting: 59it [00:36,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:21:40,543 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:21:40,544 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:21:40,550 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:21:40,551 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:21:40,554 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:21:43,504 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:21:43,504 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:21:43,521 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:21:43,521 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:21:43,527 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:21:43,530 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8112391930835735,
  "recall": 0.18202392499191722,
  "score": 0.29733298125165036,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:21:43,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:44,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:45,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:45,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:46,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:47,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:48,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:48,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:49,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:50,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:50,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:51,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:52,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:52,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:54,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:54,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:55,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:56,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:56,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:04, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-29 00:21:57,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:58,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:59,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:00,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:00,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:01,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:02,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:03,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:03,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:04,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:05,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:05,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:06,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:07,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:08,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:08,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:09,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:10,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:10,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:11,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:12,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:12,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:13,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:03, 15.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:14,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:14,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:15,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:16,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:17,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:17,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:18,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:19,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:19,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:20,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:21,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:22,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:22,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:23,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:24,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:25,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:25,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:27,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:27,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:28,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:29,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:29,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:30,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:47<01:53, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:31,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:31,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:32,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:33,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:34,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:35,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:36,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:37,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:37,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:38,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:39,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:39,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:40,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:41,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:42,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:42,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:43,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:44,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:45,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:46,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:46,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:03<01:37, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:47,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:48,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:49,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:49,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:50,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:51,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:51,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:52,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:53,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:53,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:54,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:55,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:56,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:56,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:57,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:58,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:59,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:59,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:00,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:01,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:02,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:03,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:20<01:21, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-29 00:23:03,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:04,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:05,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:06,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:06,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:07,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:08,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:09,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:09,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:10,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:11,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:12,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:12,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:13,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:14,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:14,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:15,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:16,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:16,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:17,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:18,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:19,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:19,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:20,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:06, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-29 00:23:21,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:21,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:22,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:22,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:23,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:24,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:24,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:25,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:26,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:26,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:27,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:28,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:28,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:29,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:30,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:30,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:31,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:32,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:32,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:33,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:34,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:34,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:51<00:47, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-29 00:23:35,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:36,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:36,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:37,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:39,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:40,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:41,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:41,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:42,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:43,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:44,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:44,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:46,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:46,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:47,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:48,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:48,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:05<00:30, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-29 00:23:49,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:50,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:50,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:51,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:52,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:52,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:53,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:54,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:54,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:55,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:56,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:56,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:57,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:57,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:58,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:59,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:59,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:00,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:01,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:01,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:18<00:14, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-29 00:24:02,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:03,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:03,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:04,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:05,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:06,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:06,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:07,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:08,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:08,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:09,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:10,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:10,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:11,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:12,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:13,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:13,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:14,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:15,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:15,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:16,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:17,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:34<00:00, 14.82s/it]Generating: 100%|██████████| 10/10 [02:34<00:00, 15.42s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:22,674 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:22,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:22,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:22,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:22,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:24:22,988 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:24:22,990 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:23,250 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:24,331 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:24,331 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:26,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:26,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:26,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:26,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:26,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:24:27,218 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:24:27,219 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:27,494 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:27,654 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:27,654 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : given name .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.8072916666666666, 'errors': {'', "('White Sox', 'lowest point', '', 'On July 2010 , the White Sox traded Kenley Jansen , Ryan Kelly , Matt Holliday for Jason Vargas , and a 2016 $ 65 million cash value package for Trevor Bauer , Eric Vela and Tim Lincecum .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.79296875, 'errors': {'', "('PlayStation Music and Game Entertainment', 'genre', '', 'The album was released in Japan as the sixth studio album in the series as well as four singles to be released by Sony Corporation s PlayStation Music and Game Entertainment .')", 'not enough values to unpack (expected 2, got 1)', "('Miss USA', 'genre', '', 'The song was included on the soundtrack for the 2005 Disney Channel soap opera Miss USA based on the Miss USA , starring Jennifer Lopez , Lenny Wahlwood and Laura Dern .')", "('Lose Their Laughter', 'genre', '', 'The soundtrack to Führer , Godspeed You ! features songs by the band Iggy Pop , including a cover version of the song It was a Good Time , and the track Lose Their Laughter ( I Know You Only Live Twice ! ) .')", "('Madam Secretary', 'genre', '', 'In 2007 , she appeared on ABCs sitcom , Madam Secretary , where she played Rachel , a friend of Tom Cavanagh ’s played character , who suffers from bipolar disorder .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8551136363636364, 'errors': {'', "('country', 'is a list of', '', 'It is organized by the member state that owns the list and by the country that serves as its capital .')", 'not enough values to unpack (expected 2, got 1)', "('members of', 'is a list of', '', 'Some of the members of a particular group are not members of other lists , but members of all members of another list .')", "('members', 'is a list of', '', 'There is no list of members of the .')", "('country', 'is a list of', '', 'The list is organized by country .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8778409090909091, 'errors': {'', "('José Andrés Andrés Andrés Andrés Andrés', 'member of', '', 'She was the wife of the President of Bolivia , José Andrés Andrés Andrés Andrés Andrés , and mother to President Andrés I s daughter Anjou .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 8944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.37it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.41it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.55it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:08,  1.56it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:10,  1.49it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.50it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.60it/s]Extractor Estimating: 21it [00:13,  1.62it/s]Extractor Estimating: 22it [00:14,  1.60it/s]Extractor Estimating: 23it [00:15,  1.57it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:16,  1.57it/s]Extractor Estimating: 26it [00:16,  1.63it/s]Extractor Estimating: 27it [00:17,  1.66it/s]Extractor Estimating: 28it [00:18,  1.67it/s]Extractor Estimating: 29it [00:18,  1.68it/s]Extractor Estimating: 30it [00:19,  1.58it/s]Extractor Estimating: 31it [00:20,  1.60it/s]Extractor Estimating: 32it [00:20,  1.52it/s]Extractor Estimating: 33it [00:21,  1.61it/s]Extractor Estimating: 34it [00:21,  1.69it/s]Extractor Estimating: 35it [00:22,  1.73it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:23,  1.72it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:24,  1.72it/s]Extractor Estimating: 40it [00:25,  1.72it/s]Extractor Estimating: 41it [00:25,  1.67it/s]Extractor Estimating: 42it [00:26,  1.63it/s]Extractor Estimating: 43it [00:27,  1.59it/s]Extractor Estimating: 44it [00:27,  1.60it/s]Extractor Estimating: 45it [00:28,  1.60it/s]Extractor Estimating: 46it [00:29,  1.64it/s]Extractor Estimating: 47it [00:29,  1.63it/s]Extractor Estimating: 48it [00:30,  1.67it/s]Extractor Estimating: 49it [00:30,  1.67it/s]Extractor Estimating: 50it [00:31,  1.69it/s]Extractor Estimating: 51it [00:32,  1.60it/s]Extractor Estimating: 52it [00:32,  1.60it/s]Extractor Estimating: 53it [00:33,  1.64it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:34,  1.57it/s]Extractor Estimating: 56it [00:35,  1.53it/s]Extractor Estimating: 57it [00:35,  1.57it/s]Extractor Estimating: 58it [00:36,  1.60it/s]Extractor Estimating: 59it [00:37,  1.57it/s]Extractor Estimating: 60it [00:37,  1.56it/s]Extractor Estimating: 61it [00:38,  1.53it/s]Extractor Estimating: 62it [00:39,  1.53it/s]Extractor Estimating: 63it [00:39,  1.58it/s]Extractor Estimating: 64it [00:40,  1.60it/s]Extractor Estimating: 65it [00:41,  1.58it/s]Extractor Estimating: 66it [00:41,  1.61it/s]Extractor Estimating: 67it [00:42,  1.62it/s]Extractor Estimating: 68it [00:42,  1.62it/s]Extractor Estimating: 69it [00:43,  1.59it/s]Extractor Estimating: 70it [00:44,  1.60it/s]Extractor Estimating: 71it [00:44,  1.60it/s]Extractor Estimating: 72it [00:45,  1.51it/s]Extractor Estimating: 73it [00:46,  1.60it/s]Extractor Estimating: 74it [00:46,  1.62it/s]Extractor Estimating: 75it [00:47,  1.62it/s]Extractor Estimating: 76it [00:47,  1.65it/s]Extractor Estimating: 77it [00:48,  1.59it/s]Extractor Estimating: 78it [00:49,  1.59it/s]Extractor Estimating: 79it [00:49,  1.60it/s]Extractor Estimating: 80it [00:50,  1.62it/s]Extractor Estimating: 81it [00:51,  1.61it/s]Extractor Estimating: 82it [00:51,  1.62it/s]Extractor Estimating: 83it [00:52,  1.57it/s]Extractor Estimating: 84it [00:52,  1.59it/s]Extractor Estimating: 85it [00:53,  1.59it/s]Extractor Estimating: 86it [00:54,  1.59it/s]Extractor Estimating: 87it [00:54,  1.60it/s]Extractor Estimating: 88it [00:55,  1.58it/s]Extractor Estimating: 89it [00:56,  1.62it/s]Extractor Estimating: 90it [00:56,  1.63it/s]Extractor Estimating: 91it [00:57,  1.65it/s]Extractor Estimating: 92it [00:57,  1.63it/s]Extractor Estimating: 93it [00:58,  1.57it/s]Extractor Estimating: 94it [00:59,  1.55it/s]Extractor Estimating: 95it [00:59,  1.55it/s]Extractor Estimating: 96it [01:00,  1.57it/s]Extractor Estimating: 97it [01:01,  1.54it/s]Extractor Estimating: 98it [01:01,  1.55it/s]Extractor Estimating: 99it [01:02,  1.55it/s]Extractor Estimating: 100it [01:03,  1.62it/s]Extractor Estimating: 101it [01:03,  1.58it/s]Extractor Estimating: 102it [01:04,  1.58it/s]Extractor Estimating: 103it [01:04,  1.60it/s]Extractor Estimating: 104it [01:05,  1.63it/s]Extractor Estimating: 105it [01:06,  1.65it/s]Extractor Estimating: 106it [01:06,  1.65it/s]Extractor Estimating: 107it [01:07,  1.64it/s]Extractor Estimating: 108it [01:07,  1.65it/s]Extractor Estimating: 109it [01:08,  1.64it/s]Extractor Estimating: 110it [01:09,  1.67it/s]Extractor Estimating: 111it [01:09,  1.66it/s]Extractor Estimating: 112it [01:10,  1.64it/s]Extractor Estimating: 113it [01:10,  1.67it/s]Extractor Estimating: 114it [01:11,  1.59it/s]Extractor Estimating: 115it [01:12,  1.63it/s]Extractor Estimating: 116it [01:12,  1.63it/s]Extractor Estimating: 117it [01:13,  1.65it/s]Extractor Estimating: 118it [01:14,  1.61it/s]Extractor Estimating: 119it [01:14,  1.52it/s]Extractor Estimating: 120it [01:15,  1.56it/s]Extractor Estimating: 121it [01:16,  1.59it/s]Extractor Estimating: 122it [01:16,  1.58it/s]Extractor Estimating: 123it [01:17,  1.54it/s]Extractor Estimating: 124it [01:17,  1.60it/s]Extractor Estimating: 125it [01:18,  1.58it/s]Extractor Estimating: 126it [01:19,  1.58it/s]Extractor Estimating: 127it [01:19,  1.60it/s]Extractor Estimating: 128it [01:20,  1.61it/s]Extractor Estimating: 129it [01:21,  1.59it/s]Extractor Estimating: 130it [01:21,  1.58it/s]Extractor Estimating: 131it [01:22,  1.61it/s]Extractor Estimating: 132it [01:22,  1.60it/s]Extractor Estimating: 133it [01:23,  1.60it/s]Extractor Estimating: 134it [01:24,  1.51it/s]Extractor Estimating: 135it [01:24,  1.55it/s]Extractor Estimating: 136it [01:25,  1.56it/s]Extractor Estimating: 137it [01:26,  1.54it/s]Extractor Estimating: 138it [01:26,  1.51it/s]Extractor Estimating: 139it [01:27,  1.55it/s]Extractor Estimating: 140it [01:28,  1.56it/s]Extractor Estimating: 141it [01:28,  1.58it/s]Extractor Estimating: 142it [01:29,  1.56it/s]Extractor Estimating: 143it [01:30,  1.57it/s]Extractor Estimating: 144it [01:30,  1.59it/s]Extractor Estimating: 145it [01:31,  1.61it/s]Extractor Estimating: 146it [01:31,  1.56it/s]Extractor Estimating: 147it [01:32,  1.58it/s]Extractor Estimating: 148it [01:33,  1.57it/s]Extractor Estimating: 149it [01:33,  1.56it/s]Extractor Estimating: 150it [01:34,  1.57it/s]Extractor Estimating: 151it [01:35,  1.64it/s]Extractor Estimating: 152it [01:35,  1.74it/s]Extractor Estimating: 153it [01:36,  1.80it/s]Extractor Estimating: 154it [01:36,  1.76it/s]Extractor Estimating: 155it [01:37,  1.79it/s]Extractor Estimating: 156it [01:37,  1.82it/s]Extractor Estimating: 157it [01:38,  1.88it/s]Extractor Estimating: 158it [01:38,  1.85it/s]Extractor Estimating: 159it [01:39,  1.90it/s]Extractor Estimating: 160it [01:39,  1.89it/s]Extractor Estimating: 161it [01:40,  1.84it/s]Extractor Estimating: 162it [01:40,  1.90it/s]Extractor Estimating: 163it [01:41,  1.87it/s]Extractor Estimating: 164it [01:41,  1.86it/s]Extractor Estimating: 165it [01:42,  1.83it/s]Extractor Estimating: 166it [01:43,  1.83it/s]Extractor Estimating: 167it [01:43,  1.78it/s]Extractor Estimating: 168it [01:44,  1.82it/s]Extractor Estimating: 169it [01:44,  1.82it/s]Extractor Estimating: 170it [01:45,  1.88it/s]Extractor Estimating: 171it [01:45,  1.85it/s]Extractor Estimating: 172it [01:46,  1.86it/s]Extractor Estimating: 173it [01:46,  1.88it/s]Extractor Estimating: 174it [01:47,  1.87it/s]Extractor Estimating: 175it [01:47,  1.87it/s]Extractor Estimating: 176it [01:48,  1.86it/s]Extractor Estimating: 177it [01:49,  1.77it/s]Extractor Estimating: 178it [01:49,  1.69it/s]Extractor Estimating: 179it [01:50,  1.71it/s]Extractor Estimating: 180it [01:50,  1.69it/s]Extractor Estimating: 181it [01:51,  1.72it/s]Extractor Estimating: 182it [01:52,  1.72it/s]Extractor Estimating: 183it [01:52,  1.65it/s]Extractor Estimating: 184it [01:53,  1.70it/s]Extractor Estimating: 185it [01:53,  1.67it/s]Extractor Estimating: 186it [01:54,  1.61it/s]Extractor Estimating: 187it [01:55,  1.63it/s]Extractor Estimating: 188it [01:55,  1.62it/s]Extractor Estimating: 189it [01:56,  1.64it/s]Extractor Estimating: 190it [01:56,  1.61it/s]Extractor Estimating: 191it [01:57,  1.63it/s]Extractor Estimating: 192it [01:58,  1.67it/s]Extractor Estimating: 193it [01:58,  1.63it/s]Extractor Estimating: 194it [01:59,  1.63it/s]Extractor Estimating: 195it [02:00,  1.65it/s]Extractor Estimating: 196it [02:00,  1.63it/s]Extractor Estimating: 197it [02:01,  1.69it/s]Extractor Estimating: 198it [02:01,  1.66it/s]Extractor Estimating: 199it [02:02,  1.56it/s]Extractor Estimating: 200it [02:03,  1.55it/s]Extractor Estimating: 201it [02:03,  1.62it/s]Extractor Estimating: 202it [02:04,  1.69it/s]Extractor Estimating: 203it [02:04,  1.69it/s]Extractor Estimating: 204it [02:05,  1.65it/s]Extractor Estimating: 205it [02:06,  1.67it/s]Extractor Estimating: 206it [02:06,  1.75it/s]Extractor Estimating: 207it [02:07,  1.78it/s]Extractor Estimating: 208it [02:07,  1.81it/s]Extractor Estimating: 209it [02:08,  1.85it/s]Extractor Estimating: 210it [02:08,  1.86it/s]Extractor Estimating: 211it [02:09,  1.88it/s]Extractor Estimating: 212it [02:09,  1.77it/s]Extractor Estimating: 213it [02:10,  1.80it/s]Extractor Estimating: 214it [02:10,  1.79it/s]Extractor Estimating: 215it [02:11,  1.87it/s]Extractor Estimating: 216it [02:11,  1.87it/s]Extractor Estimating: 217it [02:12,  1.86it/s]Extractor Estimating: 218it [02:13,  1.84it/s]Extractor Estimating: 219it [02:13,  1.80it/s]Extractor Estimating: 220it [02:14,  1.81it/s]Extractor Estimating: 221it [02:14,  1.83it/s]Extractor Estimating: 222it [02:15,  1.84it/s]Extractor Estimating: 223it [02:15,  1.77it/s]Extractor Estimating: 224it [02:16,  1.75it/s]Extractor Estimating: 225it [02:17,  1.76it/s]Extractor Estimating: 226it [02:17,  1.68it/s]Extractor Estimating: 227it [02:18,  1.68it/s]Extractor Estimating: 228it [02:18,  1.68it/s]Extractor Estimating: 229it [02:19,  1.65it/s]Extractor Estimating: 230it [02:20,  1.63it/s]Extractor Estimating: 231it [02:20,  1.63it/s]Extractor Estimating: 232it [02:21,  1.62it/s]Extractor Estimating: 233it [02:21,  1.68it/s]Extractor Estimating: 234it [02:22,  1.68it/s]Extractor Estimating: 235it [02:23,  1.65it/s]Extractor Estimating: 236it [02:23,  1.63it/s]Extractor Estimating: 237it [02:24,  1.64it/s]Extractor Estimating: 238it [02:25,  1.62it/s]Extractor Estimating: 239it [02:25,  1.61it/s]Extractor Estimating: 240it [02:26,  1.61it/s]Extractor Estimating: 241it [02:26,  1.62it/s]Extractor Estimating: 242it [02:27,  1.56it/s]Extractor Estimating: 243it [02:28,  1.54it/s]Extractor Estimating: 244it [02:28,  1.62it/s]Extractor Estimating: 245it [02:29,  1.64it/s]Extractor Estimating: 246it [02:30,  1.61it/s]Extractor Estimating: 247it [02:30,  1.58it/s]Extractor Estimating: 248it [02:31,  1.58it/s]Extractor Estimating: 249it [02:31,  1.57it/s]Extractor Estimating: 250it [02:32,  1.87it/s]Extractor Estimating: 250it [02:32,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:17,677 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:17,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:17,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:17,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:17,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:27:18,287 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:27:18,288 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:18,865 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:19,876 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:19,876 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:23,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:23,135 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:23,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:23,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:23,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:27:23,760 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:27:23,764 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:24,320 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:24,474 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:24,475 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:00:07,230 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:00:07,262 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4982 mean pseudo reward: 0.957037465244496
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 18999
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19099, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19099, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.059, loss:419.5458
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.053, loss:412.3162
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.060, loss:371.7329
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.058, loss:383.8901
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.055, loss:344.7557
>> valid entity prec:0.5270, rec:0.4467, f1:0.4836
>> valid relation prec:0.2549, rec:0.0550, f1:0.0905
>> valid relation with NER prec:0.2549, rec:0.0550, f1:0.0905
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.507, loss:369.1580
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.060, loss:342.9918
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.058, loss:360.8740
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.062, loss:346.6539
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.057, loss:350.9588
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5310, rec:0.4175, f1:0.4675
>> valid relation prec:0.2486, rec:0.0426, f1:0.0728
>> valid relation with NER prec:0.2486, rec:0.0426, f1:0.0728
g_step 1100, step 60, avg_time 2.510, loss:356.6714
g_step 1200, step 160, avg_time 1.057, loss:349.1411
g_step 1300, step 52, avg_time 1.052, loss:338.0052
g_step 1400, step 152, avg_time 1.072, loss:325.0833
g_step 1500, step 44, avg_time 1.064, loss:330.5391
>> valid entity prec:0.5453, rec:0.4178, f1:0.4731
>> valid relation prec:0.2476, rec:0.0756, f1:0.1158
>> valid relation with NER prec:0.2476, rec:0.0756, f1:0.1158
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 144, avg_time 2.508, loss:321.4790
g_step 1700, step 36, avg_time 1.055, loss:321.9221
g_step 1800, step 136, avg_time 1.048, loss:304.0627
g_step 1900, step 28, avg_time 1.067, loss:292.6015
g_step 2000, step 128, avg_time 1.055, loss:298.2014
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5425, rec:0.4181, f1:0.4722
>> valid relation prec:0.2764, rec:0.0545, f1:0.0911
>> valid relation with NER prec:0.2764, rec:0.0545, f1:0.0911
g_step 2100, step 20, avg_time 2.506, loss:295.6181
g_step 2200, step 120, avg_time 1.068, loss:266.6313
g_step 2300, step 12, avg_time 1.040, loss:278.0664
g_step 2400, step 112, avg_time 1.059, loss:250.3299
g_step 2500, step 4, avg_time 1.061, loss:276.1686
>> valid entity prec:0.5265, rec:0.4601, f1:0.4911
>> valid relation prec:0.2022, rec:0.0687, f1:0.1025
>> valid relation with NER prec:0.2022, rec:0.0687, f1:0.1025
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.507, loss:245.4203
g_step 2700, step 204, avg_time 1.055, loss:265.3951
g_step 2800, step 96, avg_time 1.057, loss:246.9766
g_step 2900, step 196, avg_time 1.057, loss:245.1905
g_step 3000, step 88, avg_time 1.057, loss:226.6957
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5013, rec:0.5210, f1:0.5110
>> valid relation prec:0.2047, rec:0.0806, f1:0.1156
>> valid relation with NER prec:0.2047, rec:0.0806, f1:0.1156
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.515, loss:241.6477
g_step 3200, step 80, avg_time 1.056, loss:214.6671
g_step 3300, step 180, avg_time 1.063, loss:222.1966
g_step 3400, step 72, avg_time 1.059, loss:210.6547
g_step 3500, step 172, avg_time 1.053, loss:219.4123
>> valid entity prec:0.5125, rec:0.4014, f1:0.4502
>> valid relation prec:0.2392, rec:0.0523, f1:0.0858
>> valid relation with NER prec:0.2392, rec:0.0523, f1:0.0858
g_step 3600, step 64, avg_time 2.509, loss:198.9538
g_step 3700, step 164, avg_time 1.058, loss:209.8881
g_step 3800, step 56, avg_time 1.054, loss:194.5540
g_step 3900, step 156, avg_time 1.054, loss:200.3262
g_step 4000, step 48, avg_time 1.058, loss:206.3862
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4855, rec:0.4429, f1:0.4632
>> valid relation prec:0.2070, rec:0.0773, f1:0.1126
>> valid relation with NER prec:0.2070, rec:0.0773, f1:0.1126
g_step 4100, step 148, avg_time 2.507, loss:192.8421
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:00:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:00:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-00-07_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:00:08 - WARNING - datasets.builder -   Using custom data configuration default-f1b952dbda630f34
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f1b952dbda630f34/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:00:08,593 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:00:08,593 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:00:08,594 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:00:08,595 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:00:08,601 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,604 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,605 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,605 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,605 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,605 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:00:08,605 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:00:08,776 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:00:11,930 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:00:11,932 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f1b952dbda630f34/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.16ba/s] 40%|████      | 2/5 [00:00<00:00,  3.22ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.85ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.25ba/s]100%|██████████| 5/5 [00:01<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  3.94ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.94ba/s] 40%|████      | 2/5 [00:00<00:00,  4.20ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.29ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.34ba/s]100%|██████████| 5/5 [00:00<00:00,  5.27ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.64ba/s]100%|██████████| 5/5 [00:00<00:00,  9.99ba/s]100%|██████████| 5/5 [00:00<00:00,  9.81ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.06ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.52ba/s]100%|██████████| 5/5 [00:00<00:00, 12.90ba/s]100%|██████████| 5/5 [00:00<00:00, 11.79ba/s]
[INFO|trainer.py:414] 2023-08-29 02:00:15,460 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:00:15,473 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:00:15,473 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-29 02:00:15,473 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:00:15,473 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:00:15,473 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:00:15,473 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:00:15,473 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:00,  3.23it/s]  1%|          | 2/390 [00:00<01:54,  3.38it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:51,  3.46it/s]  1%|▏         | 5/390 [00:01<01:50,  3.47it/s]  2%|▏         | 6/390 [00:01<01:50,  3.48it/s]  2%|▏         | 7/390 [00:02<01:49,  3.48it/s]  2%|▏         | 8/390 [00:02<01:49,  3.49it/s]  2%|▏         | 9/390 [00:02<01:49,  3.49it/s]  3%|▎         | 10/390 [00:02<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:48,  3.48it/s]  3%|▎         | 12/390 [00:03<01:48,  3.48it/s]  3%|▎         | 13/390 [00:03<01:48,  3.49it/s]  4%|▎         | 14/390 [00:04<01:47,  3.49it/s]  4%|▍         | 15/390 [00:04<01:47,  3.49it/s]  4%|▍         | 16/390 [00:04<01:47,  3.48it/s]  4%|▍         | 17/390 [00:04<01:47,  3.48it/s]  5%|▍         | 18/390 [00:05<01:46,  3.48it/s]  5%|▍         | 19/390 [00:05<01:46,  3.49it/s]  5%|▌         | 20/390 [00:05<01:46,  3.49it/s]  5%|▌         | 21/390 [00:06<01:46,  3.47it/s]  6%|▌         | 22/390 [00:06<01:45,  3.48it/s]  6%|▌         | 23/390 [00:06<01:45,  3.48it/s]  6%|▌         | 24/390 [00:06<01:45,  3.48it/s]  6%|▋         | 25/390 [00:07<01:44,  3.48it/s]  7%|▋         | 26/390 [00:07<01:44,  3.49it/s]  7%|▋         | 27/390 [00:07<01:44,  3.49it/s]  7%|▋         | 28/390 [00:08<01:43,  3.49it/s]  7%|▋         | 29/390 [00:08<01:43,  3.49it/s]  8%|▊         | 30/390 [00:08<01:43,  3.49it/s]  8%|▊         | 31/390 [00:08<01:42,  3.49it/s]  8%|▊         | 32/390 [00:09<01:42,  3.48it/s]  8%|▊         | 33/390 [00:09<01:42,  3.48it/s]  9%|▊         | 34/390 [00:09<01:42,  3.48it/s]  9%|▉         | 35/390 [00:10<01:41,  3.49it/s]  9%|▉         | 36/390 [00:10<01:41,  3.48it/s]  9%|▉         | 37/390 [00:10<01:41,  3.49it/s] 10%|▉         | 38/390 [00:10<01:41,  3.48it/s] 10%|█         | 39/390 [00:11<01:40,  3.49it/s] 10%|█         | 40/390 [00:11<01:40,  3.49it/s] 11%|█         | 41/390 [00:11<01:39,  3.49it/s] 11%|█         | 42/390 [00:12<01:39,  3.49it/s] 11%|█         | 43/390 [00:12<01:40,  3.47it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:12<01:39,  3.48it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.48it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.48it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.48it/s] 13%|█▎        | 49/390 [00:14<01:37,  3.48it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.49it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.48it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.48it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.49it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.48it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.48it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.48it/s] 15%|█▌        | 59/390 [00:16<01:35,  3.48it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 63/390 [00:18<01:33,  3.49it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.48it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.47it/s] 17%|█▋        | 66/390 [00:18<01:33,  3.48it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.47it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.48it/s] 18%|█▊        | 69/390 [00:19<01:32,  3.48it/s] 18%|█▊        | 70/390 [00:20<01:31,  3.48it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.48it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.49it/s] 19%|█▊        | 73/390 [00:20<01:30,  3.48it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.47it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.47it/s] 20%|██        | 78/390 [00:22<01:29,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 02:00:37,936 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:00:37,937 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:00:37,937 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 59.36it/s][A
  2%|▏         | 12/505 [00:00<00:09, 51.61it/s][A
  4%|▎         | 18/505 [00:00<00:09, 49.53it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.64it/s][A
  6%|▌         | 28/505 [00:00<00:09, 48.14it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.87it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.56it/s][A
  9%|▊         | 43/505 [00:00<00:09, 47.07it/s][A
 10%|▉         | 48/505 [00:00<00:09, 47.10it/s][A
 10%|█         | 53/505 [00:01<00:09, 47.11it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 47.19it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 47.23it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 47.22it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 47.26it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 47.24it/s][A
 16%|█▋        | 83/505 [00:01<00:08, 47.03it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.96it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.92it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.84it/s][A
 20%|██        | 103/505 [00:02<00:08, 47.00it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 47.08it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 47.01it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 47.10it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 47.11it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 47.06it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 47.06it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.91it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.80it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.92it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.93it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.98it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 47.02it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 47.05it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.98it/s][A
 35%|███▌      | 178/505 [00:03<00:06, 46.97it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.98it/s][A
 37%|███▋      | 188/505 [00:03<00:06, 46.89it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.97it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.91it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.96it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.96it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 47.03it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 47.03it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.96it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.92it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.89it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.84it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.90it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.98it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.91it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.99it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 47.01it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.94it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.96it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.97it/s][A
 56%|█████▌    | 283/505 [00:05<00:04, 46.92it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.99it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 47.00it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.91it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.89it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.86it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.87it/s][A
 63%|██████▎   | 318/505 [00:06<00:03, 46.95it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.97it/s][A
 65%|██████▍   | 328/505 [00:06<00:03, 46.79it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.90it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.96it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.99it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.96it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.91it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.89it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.98it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.99it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.79it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.81it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.83it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.82it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.92it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.85it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.82it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.96it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 47.00it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.89it/s][A
 84%|████████▍ | 423/505 [00:08<00:01, 46.91it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.85it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.83it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.90it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.85it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.92it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.79it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.88it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.88it/s][A
 93%|█████████▎| 468/505 [00:09<00:00, 46.91it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.92it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.82it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.84it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.93it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.91it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.98it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.92it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:29,  3.48it/s]
100%|██████████| 505/505 [00:10<00:00, 46.92it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:00:48,707 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-29 02:00:48,726 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:00:51,076 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:00:51,093 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:00:51,101 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:40<29:23,  5.67s/it] 21%|██        | 80/390 [00:40<20:57,  4.06s/it] 21%|██        | 81/390 [00:41<15:04,  2.93s/it] 21%|██        | 82/390 [00:41<10:57,  2.13s/it] 21%|██▏       | 83/390 [00:41<08:05,  1.58s/it] 22%|██▏       | 84/390 [00:42<06:04,  1.19s/it] 22%|██▏       | 85/390 [00:42<04:40,  1.09it/s] 22%|██▏       | 86/390 [00:42<03:42,  1.37it/s] 22%|██▏       | 87/390 [00:42<03:01,  1.67it/s] 23%|██▎       | 88/390 [00:43<02:32,  1.98it/s] 23%|██▎       | 89/390 [00:43<02:12,  2.27it/s] 23%|██▎       | 90/390 [00:43<01:58,  2.54it/s] 23%|██▎       | 91/390 [00:44<01:48,  2.76it/s] 24%|██▎       | 92/390 [00:44<01:41,  2.94it/s] 24%|██▍       | 93/390 [00:44<01:36,  3.08it/s] 24%|██▍       | 94/390 [00:44<01:32,  3.19it/s] 24%|██▍       | 95/390 [00:45<01:30,  3.27it/s] 25%|██▍       | 96/390 [00:45<01:28,  3.33it/s] 25%|██▍       | 97/390 [00:45<01:26,  3.37it/s] 25%|██▌       | 98/390 [00:46<01:25,  3.40it/s] 25%|██▌       | 99/390 [00:46<01:25,  3.42it/s] 26%|██▌       | 100/390 [00:46<01:24,  3.44it/s] 26%|██▌       | 101/390 [00:46<01:23,  3.45it/s] 26%|██▌       | 102/390 [00:47<01:23,  3.44it/s] 26%|██▋       | 103/390 [00:47<01:23,  3.45it/s] 27%|██▋       | 104/390 [00:47<01:22,  3.46it/s] 27%|██▋       | 105/390 [00:48<01:22,  3.46it/s] 27%|██▋       | 106/390 [00:48<01:21,  3.47it/s] 27%|██▋       | 107/390 [00:48<01:21,  3.47it/s] 28%|██▊       | 108/390 [00:49<01:21,  3.47it/s] 28%|██▊       | 109/390 [00:49<01:21,  3.46it/s] 28%|██▊       | 110/390 [00:49<01:20,  3.47it/s] 28%|██▊       | 111/390 [00:49<01:20,  3.47it/s] 29%|██▊       | 112/390 [00:50<01:20,  3.47it/s] 29%|██▉       | 113/390 [00:50<01:20,  3.46it/s] 29%|██▉       | 114/390 [00:50<01:19,  3.46it/s] 29%|██▉       | 115/390 [00:51<01:19,  3.46it/s] 30%|██▉       | 116/390 [00:51<01:19,  3.46it/s] 30%|███       | 117/390 [00:51<01:18,  3.46it/s] 30%|███       | 118/390 [00:51<01:20,  3.36it/s] 31%|███       | 119/390 [00:52<01:20,  3.38it/s] 31%|███       | 120/390 [00:52<01:19,  3.41it/s] 31%|███       | 121/390 [00:52<01:18,  3.42it/s] 31%|███▏      | 122/390 [00:53<01:17,  3.44it/s] 32%|███▏      | 123/390 [00:53<01:17,  3.45it/s] 32%|███▏      | 124/390 [00:53<01:17,  3.45it/s] 32%|███▏      | 125/390 [00:53<01:16,  3.46it/s] 32%|███▏      | 126/390 [00:54<01:16,  3.46it/s] 33%|███▎      | 127/390 [00:54<01:15,  3.47it/s] 33%|███▎      | 128/390 [00:54<01:15,  3.46it/s] 33%|███▎      | 129/390 [00:55<01:15,  3.47it/s] 33%|███▎      | 130/390 [00:55<01:14,  3.47it/s] 34%|███▎      | 131/390 [00:55<01:14,  3.47it/s] 34%|███▍      | 132/390 [00:55<01:14,  3.46it/s] 34%|███▍      | 133/390 [00:56<01:14,  3.47it/s] 34%|███▍      | 134/390 [00:56<01:13,  3.46it/s] 35%|███▍      | 135/390 [00:56<01:13,  3.46it/s] 35%|███▍      | 136/390 [00:57<01:13,  3.46it/s] 35%|███▌      | 137/390 [00:57<01:13,  3.46it/s] 35%|███▌      | 138/390 [00:57<01:12,  3.47it/s] 36%|███▌      | 139/390 [00:57<01:12,  3.47it/s] 36%|███▌      | 140/390 [00:58<01:12,  3.47it/s] 36%|███▌      | 141/390 [00:58<01:11,  3.47it/s] 36%|███▋      | 142/390 [00:58<01:11,  3.47it/s] 37%|███▋      | 143/390 [00:59<01:11,  3.46it/s] 37%|███▋      | 144/390 [00:59<01:10,  3.47it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.45it/s] 37%|███▋      | 146/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:00<01:10,  3.46it/s] 38%|███▊      | 148/390 [01:00<01:09,  3.46it/s] 38%|███▊      | 149/390 [01:00<01:09,  3.46it/s] 38%|███▊      | 150/390 [01:01<01:09,  3.46it/s] 39%|███▊      | 151/390 [01:01<01:09,  3.46it/s] 39%|███▉      | 152/390 [01:01<01:08,  3.47it/s] 39%|███▉      | 153/390 [01:02<01:08,  3.46it/s] 39%|███▉      | 154/390 [01:02<01:08,  3.47it/s] 40%|███▉      | 155/390 [01:02<01:07,  3.46it/s] 40%|████      | 156/390 [01:02<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 02:01:18,413 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:01:18,413 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:01:18,413 >>   Batch size = 8
{'eval_loss': 1.0770916938781738, 'eval_runtime': 10.7546, 'eval_samples_per_second': 375.562, 'eval_steps_per_second': 46.957, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.47it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.67it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.95it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.30it/s][A
  6%|▌         | 28/505 [00:00<00:09, 47.85it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.54it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.10it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.76it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.76it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.69it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.84it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.84it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.89it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.96it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.93it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.79it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.60it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.58it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.60it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.74it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.69it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.79it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.89it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.86it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.76it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.64it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.58it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.66it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.71it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.73it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.84it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.73it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.77it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.73it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.67it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.61it/s][A
 37%|███▋      | 188/505 [00:03<00:06, 46.66it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.67it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.67it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.69it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.75it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.76it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.77it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.50it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.53it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.45it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.52it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.56it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.65it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.77it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.80it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.78it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.63it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.67it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.62it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.63it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.64it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.68it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.75it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.79it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.77it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.79it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.64it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.66it/s][A
 65%|██████▍   | 328/505 [00:06<00:03, 46.60it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.58it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.68it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.72it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.71it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.76it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.73it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.67it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.68it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.53it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.30it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.49it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.56it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.64it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.66it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.71it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.75it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.71it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.60it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.45it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.58it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.65it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.70it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.72it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.69it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.71it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.64it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.65it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.62it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.55it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.61it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.64it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.66it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.69it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.71it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.69it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:13<01:07,  3.45it/s]
100%|██████████| 505/505 [00:10<00:00, 46.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:01:29,255 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 02:01:29,287 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:01:31,560 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:01:31,577 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:01:31,592 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:21<22:07,  5.70s/it] 41%|████      | 158/390 [01:21<15:45,  4.08s/it] 41%|████      | 159/390 [01:21<11:18,  2.94s/it] 41%|████      | 160/390 [01:22<08:13,  2.14s/it] 41%|████▏     | 161/390 [01:22<06:03,  1.59s/it] 42%|████▏     | 162/390 [01:22<04:32,  1.20s/it] 42%|████▏     | 163/390 [01:22<03:29,  1.08it/s] 42%|████▏     | 164/390 [01:23<02:45,  1.36it/s] 42%|████▏     | 165/390 [01:23<02:14,  1.67it/s] 43%|████▎     | 166/390 [01:23<01:53,  1.98it/s] 43%|████▎     | 167/390 [01:24<01:38,  2.27it/s] 43%|████▎     | 168/390 [01:24<01:27,  2.53it/s] 43%|████▎     | 169/390 [01:24<01:20,  2.76it/s] 44%|████▎     | 170/390 [01:24<01:14,  2.94it/s] 44%|████▍     | 171/390 [01:25<01:11,  3.08it/s] 44%|████▍     | 172/390 [01:25<01:08,  3.19it/s] 44%|████▍     | 173/390 [01:25<01:06,  3.27it/s] 45%|████▍     | 174/390 [01:26<01:04,  3.33it/s] 45%|████▍     | 175/390 [01:26<01:03,  3.37it/s] 45%|████▌     | 176/390 [01:26<01:02,  3.40it/s] 45%|████▌     | 177/390 [01:26<01:02,  3.42it/s] 46%|████▌     | 178/390 [01:27<01:01,  3.43it/s] 46%|████▌     | 179/390 [01:27<01:01,  3.44it/s] 46%|████▌     | 180/390 [01:27<01:00,  3.45it/s] 46%|████▋     | 181/390 [01:28<01:00,  3.45it/s] 47%|████▋     | 182/390 [01:28<01:00,  3.46it/s] 47%|████▋     | 183/390 [01:28<00:59,  3.46it/s] 47%|████▋     | 184/390 [01:28<00:59,  3.47it/s] 47%|████▋     | 185/390 [01:29<00:59,  3.46it/s] 48%|████▊     | 186/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 187/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 188/390 [01:30<00:58,  3.47it/s] 48%|████▊     | 189/390 [01:30<00:58,  3.45it/s] 49%|████▊     | 190/390 [01:30<00:57,  3.46it/s] 49%|████▉     | 191/390 [01:31<00:57,  3.46it/s] 49%|████▉     | 192/390 [01:31<00:57,  3.47it/s] 49%|████▉     | 193/390 [01:31<00:56,  3.47it/s] 50%|████▉     | 194/390 [01:31<00:56,  3.47it/s] 50%|█████     | 195/390 [01:32<00:56,  3.47it/s] 50%|█████     | 196/390 [01:32<00:55,  3.47it/s] 51%|█████     | 197/390 [01:32<00:55,  3.47it/s] 51%|█████     | 198/390 [01:33<00:55,  3.47it/s] 51%|█████     | 199/390 [01:33<00:55,  3.47it/s] 51%|█████▏    | 200/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 201/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 202/390 [01:34<00:54,  3.46it/s] 52%|█████▏    | 203/390 [01:34<00:53,  3.47it/s] 52%|█████▏    | 204/390 [01:34<00:53,  3.46it/s] 53%|█████▎    | 205/390 [01:35<00:53,  3.47it/s] 53%|█████▎    | 206/390 [01:35<00:53,  3.47it/s] 53%|█████▎    | 207/390 [01:35<00:52,  3.47it/s] 53%|█████▎    | 208/390 [01:35<00:52,  3.47it/s] 54%|█████▎    | 209/390 [01:36<00:52,  3.47it/s] 54%|█████▍    | 210/390 [01:36<00:51,  3.47it/s] 54%|█████▍    | 211/390 [01:36<00:51,  3.46it/s] 54%|█████▍    | 212/390 [01:37<00:51,  3.46it/s] 55%|█████▍    | 213/390 [01:37<00:51,  3.46it/s] 55%|█████▍    | 214/390 [01:37<00:50,  3.46it/s] 55%|█████▌    | 215/390 [01:37<00:50,  3.46it/s] 55%|█████▌    | 216/390 [01:38<00:50,  3.46it/s] 56%|█████▌    | 217/390 [01:38<00:49,  3.47it/s] 56%|█████▌    | 218/390 [01:38<00:49,  3.46it/s] 56%|█████▌    | 219/390 [01:39<00:49,  3.47it/s] 56%|█████▋    | 220/390 [01:39<00:49,  3.46it/s] 57%|█████▋    | 221/390 [01:39<00:48,  3.47it/s] 57%|█████▋    | 222/390 [01:39<00:48,  3.46it/s] 57%|█████▋    | 223/390 [01:40<00:48,  3.46it/s] 57%|█████▋    | 224/390 [01:40<00:47,  3.46it/s] 58%|█████▊    | 225/390 [01:40<00:47,  3.46it/s] 58%|█████▊    | 226/390 [01:41<00:47,  3.46it/s] 58%|█████▊    | 227/390 [01:41<00:47,  3.46it/s] 58%|█████▊    | 228/390 [01:41<00:46,  3.46it/s] 59%|█████▊    | 229/390 [01:41<00:46,  3.46it/s] 59%|█████▉    | 230/390 [01:42<00:46,  3.47it/s] 59%|█████▉    | 231/390 [01:42<00:45,  3.47it/s] 59%|█████▉    | 232/390 [01:42<00:45,  3.47it/s] 60%|█████▉    | 233/390 [01:43<00:45,  3.46it/s] 60%|██████    | 234/390 [01:43<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 02:01:58,939 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:01:58,939 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:01:58,939 >>   Batch size = 8
{'eval_loss': 1.09579598903656, 'eval_runtime': 10.8178, 'eval_samples_per_second': 373.366, 'eval_steps_per_second': 46.682, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.87it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.54it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.91it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.27it/s][A
  6%|▌         | 28/505 [00:00<00:09, 47.73it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.42it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.01it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.67it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.68it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.68it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.77it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.83it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.89it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.91it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.80it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.56it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.50it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.43it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.49it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.57it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.74it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.82it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.83it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.72it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.63it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.49it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.31it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.63it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.68it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.77it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.88it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.78it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.71it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.65it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.51it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.59it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.54it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.60it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.73it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.82it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.86it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.75it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.76it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.58it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.66it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.62it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.57it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.66it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.71it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.79it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.73it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.59it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.57it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.56it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.59it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.58it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.49it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.63it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.69it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.70it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.67it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.60it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.53it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.55it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.50it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.50it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.50it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.52it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.60it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.26it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.62it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.65it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.65it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.67it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.66it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.51it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.62it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 45.93it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.51it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.53it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.47it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.61it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.59it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.61it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.61it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.53it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.56it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.58it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.54it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.49it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.58it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.54it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.58it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.59it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.55it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.48it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.54it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.54it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.53it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.54it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:54<00:45,  3.46it/s]
100%|██████████| 505/505 [00:10<00:00, 46.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:02:09,788 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 02:02:09,807 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:02:12,266 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:02:12,286 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:02:12,296 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:01<14:41,  5.69s/it] 61%|██████    | 236/390 [02:01<10:26,  4.07s/it] 61%|██████    | 237/390 [02:02<07:28,  2.93s/it] 61%|██████    | 238/390 [02:02<05:25,  2.14s/it] 61%|██████▏   | 239/390 [02:02<03:59,  1.58s/it] 62%|██████▏   | 240/390 [02:03<02:59,  1.20s/it] 62%|██████▏   | 241/390 [02:03<02:17,  1.08it/s] 62%|██████▏   | 242/390 [02:03<01:48,  1.36it/s] 62%|██████▏   | 243/390 [02:04<01:28,  1.67it/s] 63%|██████▎   | 244/390 [02:04<01:13,  1.98it/s] 63%|██████▎   | 245/390 [02:04<01:03,  2.27it/s] 63%|██████▎   | 246/390 [02:04<00:56,  2.53it/s] 63%|██████▎   | 247/390 [02:05<00:52,  2.75it/s] 64%|██████▎   | 248/390 [02:05<00:48,  2.93it/s] 64%|██████▍   | 249/390 [02:05<00:45,  3.08it/s] 64%|██████▍   | 250/390 [02:06<00:44,  3.18it/s] 64%|██████▍   | 251/390 [02:06<00:42,  3.26it/s] 65%|██████▍   | 252/390 [02:06<00:41,  3.32it/s] 65%|██████▍   | 253/390 [02:06<00:40,  3.37it/s] 65%|██████▌   | 254/390 [02:07<00:40,  3.40it/s] 65%|██████▌   | 255/390 [02:07<00:39,  3.42it/s] 66%|██████▌   | 256/390 [02:07<00:39,  3.43it/s] 66%|██████▌   | 257/390 [02:08<00:38,  3.44it/s] 66%|██████▌   | 258/390 [02:08<00:38,  3.44it/s] 66%|██████▋   | 259/390 [02:08<00:38,  3.44it/s] 67%|██████▋   | 260/390 [02:08<00:37,  3.45it/s] 67%|██████▋   | 261/390 [02:09<00:37,  3.46it/s] 67%|██████▋   | 262/390 [02:09<00:36,  3.46it/s] 67%|██████▋   | 263/390 [02:09<00:36,  3.46it/s] 68%|██████▊   | 264/390 [02:10<00:36,  3.47it/s] 68%|██████▊   | 265/390 [02:10<00:36,  3.46it/s] 68%|██████▊   | 266/390 [02:10<00:35,  3.46it/s] 68%|██████▊   | 267/390 [02:10<00:35,  3.46it/s] 69%|██████▊   | 268/390 [02:11<00:35,  3.47it/s] 69%|██████▉   | 269/390 [02:11<00:34,  3.46it/s] 69%|██████▉   | 270/390 [02:11<00:34,  3.46it/s] 69%|██████▉   | 271/390 [02:12<00:34,  3.46it/s] 70%|██████▉   | 272/390 [02:12<00:34,  3.46it/s] 70%|███████   | 273/390 [02:12<00:33,  3.46it/s] 70%|███████   | 274/390 [02:12<00:33,  3.47it/s] 71%|███████   | 275/390 [02:13<00:33,  3.47it/s] 71%|███████   | 276/390 [02:13<00:32,  3.46it/s] 71%|███████   | 277/390 [02:13<00:32,  3.46it/s] 71%|███████▏  | 278/390 [02:14<00:32,  3.46it/s] 72%|███████▏  | 279/390 [02:14<00:32,  3.47it/s] 72%|███████▏  | 280/390 [02:14<00:31,  3.45it/s] 72%|███████▏  | 281/390 [02:14<00:31,  3.46it/s] 72%|███████▏  | 282/390 [02:15<00:31,  3.46it/s] 73%|███████▎  | 283/390 [02:15<00:30,  3.46it/s] 73%|███████▎  | 284/390 [02:15<00:30,  3.46it/s] 73%|███████▎  | 285/390 [02:16<00:30,  3.46it/s] 73%|███████▎  | 286/390 [02:16<00:30,  3.46it/s] 74%|███████▎  | 287/390 [02:16<00:29,  3.46it/s] 74%|███████▍  | 288/390 [02:17<00:29,  3.46it/s] 74%|███████▍  | 289/390 [02:17<00:29,  3.47it/s] 74%|███████▍  | 290/390 [02:17<00:28,  3.46it/s] 75%|███████▍  | 291/390 [02:17<00:28,  3.45it/s] 75%|███████▍  | 292/390 [02:18<00:28,  3.46it/s] 75%|███████▌  | 293/390 [02:18<00:28,  3.46it/s] 75%|███████▌  | 294/390 [02:18<00:27,  3.47it/s] 76%|███████▌  | 295/390 [02:19<00:27,  3.46it/s] 76%|███████▌  | 296/390 [02:19<00:27,  3.47it/s] 76%|███████▌  | 297/390 [02:19<00:26,  3.46it/s] 76%|███████▋  | 298/390 [02:19<00:26,  3.47it/s] 77%|███████▋  | 299/390 [02:20<00:26,  3.46it/s] 77%|███████▋  | 300/390 [02:20<00:25,  3.46it/s] 77%|███████▋  | 301/390 [02:20<00:25,  3.47it/s] 77%|███████▋  | 302/390 [02:21<00:25,  3.46it/s] 78%|███████▊  | 303/390 [02:21<00:25,  3.46it/s] 78%|███████▊  | 304/390 [02:21<00:24,  3.46it/s] 78%|███████▊  | 305/390 [02:21<00:24,  3.47it/s] 78%|███████▊  | 306/390 [02:22<00:24,  3.46it/s] 79%|███████▊  | 307/390 [02:22<00:23,  3.46it/s] 79%|███████▉  | 308/390 [02:22<00:23,  3.46it/s] 79%|███████▉  | 309/390 [02:23<00:23,  3.46it/s] 79%|███████▉  | 310/390 [02:23<00:23,  3.46it/s] 80%|███████▉  | 311/390 [02:23<00:22,  3.46it/s] 80%|████████  | 312/390 [02:23<00:22,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 02:02:39,453 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:02:39,453 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:02:39,453 >>   Batch size = 8
{'eval_loss': 1.1103510856628418, 'eval_runtime': 10.8341, 'eval_samples_per_second': 372.806, 'eval_steps_per_second': 46.612, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.93it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.26it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.63it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.05it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.60it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.30it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.11it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.72it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.67it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.74it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.72it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.65it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.63it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.75it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.73it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.65it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.53it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.55it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.62it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.64it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.62it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.73it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.73it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.79it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.62it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.52it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.54it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.53it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.63it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.64it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.64it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.71it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.75it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.66it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.54it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.55it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.44it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.59it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.62it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.60it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.68it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.67it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.68it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.61it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.55it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.59it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.65it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.67it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.69it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.55it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.65it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.61it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.55it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.48it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.59it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.65it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.72it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.71it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.66it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.56it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.52it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.55it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.49it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.58it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.62it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.68it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.71it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.74it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.59it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.60it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.53it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.44it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.48it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.54it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.61it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.66it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.73it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.74it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.65it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.62it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.57it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.51it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.51it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.52it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.58it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.59it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.69it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.70it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.52it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.50it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.51it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.50it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.53it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.61it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.64it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.70it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.70it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.65it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.48it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:34<00:22,  3.46it/s]
100%|██████████| 505/505 [00:10<00:00, 46.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:02:50,314 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 02:02:50,356 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:02:52,455 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:02:52,476 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:02:52,488 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:41<07:10,  5.59s/it] 81%|████████  | 314/390 [02:42<05:03,  4.00s/it] 81%|████████  | 315/390 [02:42<03:36,  2.89s/it] 81%|████████  | 316/390 [02:42<02:35,  2.11s/it] 81%|████████▏ | 317/390 [02:43<01:53,  1.56s/it] 82%|████████▏ | 318/390 [02:43<01:24,  1.18s/it] 82%|████████▏ | 319/390 [02:43<01:04,  1.10it/s] 82%|████████▏ | 320/390 [02:43<00:50,  1.38it/s] 82%|████████▏ | 321/390 [02:44<00:40,  1.68it/s] 83%|████████▎ | 322/390 [02:44<00:34,  1.99it/s] 83%|████████▎ | 323/390 [02:44<00:29,  2.28it/s] 83%|████████▎ | 324/390 [02:45<00:25,  2.54it/s] 83%|████████▎ | 325/390 [02:45<00:23,  2.76it/s] 84%|████████▎ | 326/390 [02:45<00:21,  2.94it/s] 84%|████████▍ | 327/390 [02:45<00:20,  3.08it/s] 84%|████████▍ | 328/390 [02:46<00:19,  3.19it/s] 84%|████████▍ | 329/390 [02:46<00:18,  3.27it/s] 85%|████████▍ | 330/390 [02:46<00:18,  3.33it/s] 85%|████████▍ | 331/390 [02:47<00:17,  3.37it/s] 85%|████████▌ | 332/390 [02:47<00:17,  3.40it/s] 85%|████████▌ | 333/390 [02:47<00:16,  3.42it/s] 86%|████████▌ | 334/390 [02:47<00:16,  3.44it/s] 86%|████████▌ | 335/390 [02:48<00:15,  3.44it/s] 86%|████████▌ | 336/390 [02:48<00:15,  3.44it/s] 86%|████████▋ | 337/390 [02:48<00:15,  3.45it/s] 87%|████████▋ | 338/390 [02:49<00:15,  3.45it/s] 87%|████████▋ | 339/390 [02:49<00:14,  3.46it/s] 87%|████████▋ | 340/390 [02:49<00:14,  3.46it/s] 87%|████████▋ | 341/390 [02:49<00:14,  3.46it/s] 88%|████████▊ | 342/390 [02:50<00:13,  3.46it/s] 88%|████████▊ | 343/390 [02:50<00:13,  3.46it/s] 88%|████████▊ | 344/390 [02:50<00:13,  3.46it/s] 88%|████████▊ | 345/390 [02:51<00:13,  3.46it/s] 89%|████████▊ | 346/390 [02:51<00:12,  3.46it/s] 89%|████████▉ | 347/390 [02:51<00:12,  3.45it/s] 89%|████████▉ | 348/390 [02:52<00:12,  3.40it/s] 89%|████████▉ | 349/390 [02:52<00:12,  3.41it/s] 90%|████████▉ | 350/390 [02:52<00:11,  3.43it/s] 90%|█████████ | 351/390 [02:52<00:11,  3.44it/s] 90%|█████████ | 352/390 [02:53<00:11,  3.45it/s] 91%|█████████ | 353/390 [02:53<00:10,  3.45it/s] 91%|█████████ | 354/390 [02:53<00:10,  3.46it/s] 91%|█████████ | 355/390 [02:54<00:10,  3.46it/s] 91%|█████████▏| 356/390 [02:54<00:09,  3.46it/s] 92%|█████████▏| 357/390 [02:54<00:09,  3.46it/s] 92%|█████████▏| 358/390 [02:54<00:09,  3.44it/s] 92%|█████████▏| 359/390 [02:55<00:08,  3.45it/s] 92%|█████████▏| 360/390 [02:55<00:08,  3.46it/s] 93%|█████████▎| 361/390 [02:55<00:08,  3.46it/s] 93%|█████████▎| 362/390 [02:56<00:08,  3.46it/s] 93%|█████████▎| 363/390 [02:56<00:07,  3.46it/s] 93%|█████████▎| 364/390 [02:56<00:07,  3.46it/s] 94%|█████████▎| 365/390 [02:56<00:07,  3.46it/s] 94%|█████████▍| 366/390 [02:57<00:06,  3.46it/s] 94%|█████████▍| 367/390 [02:57<00:06,  3.46it/s] 94%|█████████▍| 368/390 [02:57<00:06,  3.46it/s] 95%|█████████▍| 369/390 [02:58<00:06,  3.46it/s] 95%|█████████▍| 370/390 [02:58<00:05,  3.46it/s] 95%|█████████▌| 371/390 [02:58<00:05,  3.46it/s] 95%|█████████▌| 372/390 [02:58<00:05,  3.47it/s] 96%|█████████▌| 373/390 [02:59<00:04,  3.46it/s] 96%|█████████▌| 374/390 [02:59<00:04,  3.45it/s] 96%|█████████▌| 375/390 [02:59<00:04,  3.45it/s] 96%|█████████▋| 376/390 [03:00<00:04,  3.46it/s] 97%|█████████▋| 377/390 [03:00<00:03,  3.46it/s] 97%|█████████▋| 378/390 [03:00<00:03,  3.46it/s] 97%|█████████▋| 379/390 [03:00<00:03,  3.46it/s] 97%|█████████▋| 380/390 [03:01<00:02,  3.46it/s] 98%|█████████▊| 381/390 [03:01<00:02,  3.47it/s] 98%|█████████▊| 382/390 [03:01<00:02,  3.46it/s] 98%|█████████▊| 383/390 [03:02<00:02,  3.46it/s] 98%|█████████▊| 384/390 [03:02<00:01,  3.46it/s] 99%|█████████▊| 385/390 [03:02<00:01,  3.46it/s] 99%|█████████▉| 386/390 [03:02<00:01,  3.46it/s] 99%|█████████▉| 387/390 [03:03<00:00,  3.46it/s] 99%|█████████▉| 388/390 [03:03<00:00,  3.46it/s]100%|█████████▉| 389/390 [03:03<00:00,  3.46it/s]100%|██████████| 390/390 [03:04<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 02:03:19,623 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:03:19,624 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:03:19,624 >>   Batch size = 8
{'eval_loss': 1.1206645965576172, 'eval_runtime': 10.8358, 'eval_samples_per_second': 372.748, 'eval_steps_per_second': 46.605, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.71it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.11it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.58it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.00it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.54it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.32it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.17it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.89it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.83it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.74it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.71it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.57it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.67it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.71it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.65it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.73it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.61it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.65it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.70it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.62it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.67it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.66it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.67it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.63it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.58it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.56it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.57it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.56it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.60it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.66it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.68it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.64it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.64it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.59it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.63it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.60it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.63it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.58it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.50it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.67it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.72it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.61it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.59it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.58it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.59it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.65it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.56it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.60it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.64it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.66it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.75it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.57it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.60it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.66it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.64it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.60it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.47it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.49it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.62it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.68it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.68it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.65it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.67it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.58it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.57it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.57it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.41it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.46it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.53it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.64it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.63it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.67it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.72it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.67it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.60it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.53it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.32it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.48it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.57it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.63it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.72it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.68it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.74it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.72it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.65it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.50it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.45it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.48it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.60it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.66it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.60it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.63it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.70it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.60it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.57it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.46it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.53it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.60it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.65it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.70it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:14<00:00,  3.47it/s]
100%|██████████| 505/505 [00:10<00:00, 46.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:03:30,472 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-29 02:03:30,486 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:03:32,837 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:03:32,851 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:03:32,877 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:03:37,376 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:03:37,380 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78 (score: 1.0770916938781738).
                                                 100%|██████████| 390/390 [03:23<00:00,  3.47it/s]100%|██████████| 390/390 [03:23<00:00,  1.91it/s]
[INFO|trainer.py:1894] 2023-08-29 02:03:39,135 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 02:03:39,152 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:03:41,445 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:03:41,462 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:03:41,476 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:03:41,661 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   train_loss               =     0.4693
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   train_runtime            = 0:03:23.65
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   train_samples_per_second =    122.731
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:41,661 >>   train_steps_per_second   =      1.915
{'eval_loss': 1.125462532043457, 'eval_runtime': 10.8339, 'eval_samples_per_second': 372.812, 'eval_steps_per_second': 46.613, 'epoch': 4.99}
{'train_runtime': 203.6567, 'train_samples_per_second': 122.731, 'train_steps_per_second': 1.915, 'train_loss': 0.4693316728640825, 'epoch': 4.99}
08/29/2023 02:03:41 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:03:41,704 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:03:41,704 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 02:03:41,704 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 58.63it/s]  2%|▏         | 12/505 [00:00<00:09, 51.27it/s]  4%|▎         | 18/505 [00:00<00:09, 49.42it/s]  5%|▍         | 23/505 [00:00<00:09, 48.66it/s]  6%|▌         | 28/505 [00:00<00:09, 48.18it/s]  7%|▋         | 33/505 [00:00<00:09, 47.80it/s]  8%|▊         | 38/505 [00:00<00:09, 47.60it/s]  9%|▊         | 43/505 [00:00<00:09, 47.43it/s] 10%|▉         | 48/505 [00:00<00:09, 47.26it/s] 10%|█         | 53/505 [00:01<00:09, 47.18it/s] 11%|█▏        | 58/505 [00:01<00:09, 47.13it/s] 12%|█▏        | 63/505 [00:01<00:09, 47.08it/s] 13%|█▎        | 68/505 [00:01<00:09, 47.15it/s] 14%|█▍        | 73/505 [00:01<00:09, 47.22it/s] 15%|█▌        | 78/505 [00:01<00:09, 47.16it/s] 16%|█▋        | 83/505 [00:01<00:08, 47.12it/s] 17%|█▋        | 88/505 [00:01<00:08, 47.04it/s] 18%|█▊        | 93/505 [00:01<00:08, 47.03it/s] 19%|█▉        | 98/505 [00:02<00:08, 47.01it/s] 20%|██        | 103/505 [00:02<00:08, 47.00it/s] 21%|██▏       | 108/505 [00:02<00:08, 46.96it/s] 22%|██▏       | 113/505 [00:02<00:08, 47.05it/s] 23%|██▎       | 118/505 [00:02<00:08, 47.05it/s] 24%|██▍       | 123/505 [00:02<00:08, 46.96it/s] 25%|██▌       | 128/505 [00:02<00:08, 47.02it/s] 26%|██▋       | 133/505 [00:02<00:07, 47.00it/s] 27%|██▋       | 138/505 [00:02<00:07, 46.88it/s] 28%|██▊       | 143/505 [00:03<00:07, 46.92it/s] 29%|██▉       | 148/505 [00:03<00:07, 46.83it/s] 30%|███       | 153/505 [00:03<00:07, 46.83it/s] 31%|███▏      | 158/505 [00:03<00:07, 46.82it/s] 32%|███▏      | 163/505 [00:03<00:07, 46.89it/s] 33%|███▎      | 168/505 [00:03<00:07, 46.82it/s] 34%|███▍      | 173/505 [00:03<00:07, 46.92it/s] 35%|███▌      | 178/505 [00:03<00:06, 46.91it/s] 36%|███▌      | 183/505 [00:03<00:06, 46.87it/s] 37%|███▋      | 188/505 [00:03<00:06, 46.89it/s] 38%|███▊      | 193/505 [00:04<00:06, 46.75it/s] 39%|███▉      | 198/505 [00:04<00:06, 46.76it/s] 40%|████      | 203/505 [00:04<00:06, 46.81it/s] 41%|████      | 208/505 [00:04<00:06, 46.81it/s] 42%|████▏     | 213/505 [00:04<00:06, 46.83it/s] 43%|████▎     | 218/505 [00:04<00:06, 46.89it/s] 44%|████▍     | 223/505 [00:04<00:06, 46.87it/s] 45%|████▌     | 228/505 [00:04<00:05, 46.84it/s] 46%|████▌     | 233/505 [00:04<00:05, 46.87it/s] 47%|████▋     | 238/505 [00:05<00:05, 46.78it/s] 48%|████▊     | 243/505 [00:05<00:05, 46.74it/s] 49%|████▉     | 248/505 [00:05<00:05, 46.72it/s] 50%|█████     | 253/505 [00:05<00:05, 46.72it/s] 51%|█████     | 258/505 [00:05<00:05, 46.81it/s] 52%|█████▏    | 263/505 [00:05<00:05, 46.90it/s] 53%|█████▎    | 268/505 [00:05<00:05, 46.93it/s] 54%|█████▍    | 273/505 [00:05<00:04, 46.86it/s] 55%|█████▌    | 278/505 [00:05<00:04, 46.88it/s] 56%|█████▌    | 283/505 [00:06<00:04, 46.84it/s] 57%|█████▋    | 288/505 [00:06<00:04, 46.70it/s] 58%|█████▊    | 293/505 [00:06<00:04, 46.72it/s] 59%|█████▉    | 298/505 [00:06<00:04, 46.47it/s] 60%|██████    | 303/505 [00:06<00:04, 46.69it/s] 61%|██████    | 308/505 [00:06<00:04, 46.72it/s] 62%|██████▏   | 313/505 [00:06<00:04, 46.66it/s] 63%|██████▎   | 318/505 [00:06<00:03, 46.75it/s] 64%|██████▍   | 323/505 [00:06<00:03, 46.80it/s] 65%|██████▍   | 328/505 [00:06<00:03, 46.74it/s] 66%|██████▌   | 333/505 [00:07<00:03, 46.74it/s] 67%|██████▋   | 338/505 [00:07<00:03, 46.61it/s] 68%|██████▊   | 343/505 [00:07<00:03, 46.64it/s] 69%|██████▉   | 348/505 [00:07<00:03, 46.69it/s] 70%|██████▉   | 353/505 [00:07<00:03, 46.74it/s] 71%|███████   | 358/505 [00:07<00:03, 46.73it/s] 72%|███████▏  | 363/505 [00:07<00:03, 46.79it/s] 73%|███████▎  | 368/505 [00:07<00:02, 46.78it/s] 74%|███████▍  | 373/505 [00:07<00:02, 46.80it/s] 75%|███████▍  | 378/505 [00:08<00:02, 46.81it/s] 76%|███████▌  | 383/505 [00:08<00:02, 46.81it/s] 77%|███████▋  | 388/505 [00:08<00:02, 46.73it/s] 78%|███████▊  | 393/505 [00:08<00:02, 46.69it/s] 79%|███████▉  | 398/505 [00:08<00:02, 46.72it/s] 80%|███████▉  | 403/505 [00:08<00:02, 46.80it/s] 81%|████████  | 408/505 [00:08<00:02, 46.81it/s] 82%|████████▏ | 413/505 [00:08<00:01, 46.70it/s] 83%|████████▎ | 418/505 [00:08<00:01, 46.74it/s] 84%|████████▍ | 423/505 [00:08<00:01, 46.82it/s] 85%|████████▍ | 428/505 [00:09<00:01, 46.78it/s] 86%|████████▌ | 433/505 [00:09<00:01, 46.83it/s] 87%|████████▋ | 438/505 [00:09<00:01, 46.76it/s] 88%|████████▊ | 443/505 [00:09<00:01, 46.77it/s] 89%|████████▊ | 448/505 [00:09<00:01, 46.82it/s] 90%|████████▉ | 453/505 [00:09<00:01, 46.81it/s] 91%|█████████ | 458/505 [00:09<00:01, 46.80it/s] 92%|█████████▏| 463/505 [00:09<00:00, 46.82it/s] 93%|█████████▎| 468/505 [00:09<00:00, 46.76it/s] 94%|█████████▎| 473/505 [00:10<00:00, 46.75it/s] 95%|█████████▍| 478/505 [00:10<00:00, 46.77it/s] 96%|█████████▌| 483/505 [00:10<00:00, 46.70it/s] 97%|█████████▋| 488/505 [00:10<00:00, 46.75it/s] 98%|█████████▊| 493/505 [00:10<00:00, 46.68it/s] 99%|█████████▊| 498/505 [00:10<00:00, 46.72it/s]100%|█████████▉| 503/505 [00:10<00:00, 46.76it/s]100%|██████████| 505/505 [00:10<00:00, 46.92it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:03:52,489 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   eval_loss               =     1.0771
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   eval_runtime            = 0:00:10.78
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   eval_samples_per_second =    374.506
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   eval_steps_per_second   =     46.825
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:03:52,489 >>   perplexity              =     2.9361
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:03:59,072 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:03:59,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:03:59,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:03:59,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:03:59,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:03:59,397 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:03:59,399 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:04:00,093 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:04:01,213 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:04:01,213 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:04:03,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:04:03,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:04:03,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:04:03,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:04:03,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:04:03,733 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:04:03,734 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:04:04,484 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:04:04,632 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:04:04,632 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:19,  1.48it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:21,  1.49it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.56it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:33,  1.48it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:34,  1.56it/s]Extractor Predicting: 53it [00:35,  1.55it/s]Extractor Predicting: 54it [00:36,  1.55it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.44it/s]Extractor Predicting: 57it [00:38,  1.47it/s]Extractor Predicting: 58it [00:39,  1.44it/s]Extractor Predicting: 59it [00:39,  1.47it/s]Extractor Predicting: 60it [00:40,  1.46it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.51it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.48it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:44,  1.49it/s]Extractor Predicting: 67it [00:45,  1.48it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:46,  1.42it/s]Extractor Predicting: 70it [00:47,  1.46it/s]Extractor Predicting: 71it [00:47,  1.45it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:49,  1.41it/s]Extractor Predicting: 74it [00:50,  1.36it/s]Extractor Predicting: 75it [00:50,  1.39it/s]Extractor Predicting: 76it [00:51,  1.42it/s]Extractor Predicting: 77it [00:52,  1.42it/s]Extractor Predicting: 78it [00:52,  1.43it/s]Extractor Predicting: 79it [00:53,  1.44it/s]Extractor Predicting: 80it [00:54,  1.44it/s]Extractor Predicting: 81it [00:54,  1.45it/s]Extractor Predicting: 82it [00:55,  1.49it/s]Extractor Predicting: 83it [00:56,  1.45it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:57,  1.45it/s]Extractor Predicting: 86it [00:58,  1.47it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:59,  1.51it/s]Extractor Predicting: 89it [01:00,  1.49it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.52it/s]Extractor Predicting: 93it [01:02,  1.51it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:04,  1.47it/s]Extractor Predicting: 96it [01:05,  1.45it/s]Extractor Predicting: 97it [01:05,  1.46it/s]Extractor Predicting: 98it [01:06,  1.45it/s]Extractor Predicting: 99it [01:07,  1.45it/s]Extractor Predicting: 100it [01:07,  1.45it/s]Extractor Predicting: 101it [01:08,  1.42it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:09,  1.44it/s]Extractor Predicting: 104it [01:10,  1.48it/s]Extractor Predicting: 105it [01:11,  1.47it/s]Extractor Predicting: 106it [01:11,  1.50it/s]Extractor Predicting: 107it [01:12,  1.50it/s]Extractor Predicting: 108it [01:13,  1.51it/s]Extractor Predicting: 109it [01:13,  1.52it/s]Extractor Predicting: 110it [01:14,  1.49it/s]Extractor Predicting: 111it [01:15,  1.46it/s]Extractor Predicting: 112it [01:15,  1.50it/s]Extractor Predicting: 113it [01:16,  1.49it/s]Extractor Predicting: 114it [01:17,  1.47it/s]Extractor Predicting: 115it [01:17,  1.52it/s]Extractor Predicting: 116it [01:18,  1.51it/s]Extractor Predicting: 117it [01:19,  1.50it/s]Extractor Predicting: 118it [01:19,  1.45it/s]Extractor Predicting: 119it [01:20,  1.49it/s]Extractor Predicting: 120it [01:21,  1.49it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:23,  1.49it/s]Extractor Predicting: 125it [01:24,  1.36it/s]Extractor Predicting: 126it [01:25,  1.41it/s]Extractor Predicting: 127it [01:26,  1.37it/s]Extractor Predicting: 128it [01:26,  1.36it/s]Extractor Predicting: 129it [01:27,  1.37it/s]Extractor Predicting: 130it [01:28,  1.37it/s]Extractor Predicting: 131it [01:29,  1.34it/s]Extractor Predicting: 132it [01:29,  1.34it/s]Extractor Predicting: 133it [01:30,  1.35it/s]Extractor Predicting: 134it [01:31,  1.39it/s]Extractor Predicting: 135it [01:32,  1.40it/s]Extractor Predicting: 136it [01:32,  1.40it/s]Extractor Predicting: 137it [01:33,  1.42it/s]Extractor Predicting: 138it [01:34,  1.38it/s]Extractor Predicting: 139it [01:34,  1.38it/s]Extractor Predicting: 140it [01:35,  1.37it/s]Extractor Predicting: 141it [01:36,  1.38it/s]Extractor Predicting: 142it [01:37,  1.39it/s]Extractor Predicting: 143it [01:37,  1.38it/s]Extractor Predicting: 144it [01:38,  1.39it/s]Extractor Predicting: 145it [01:39,  1.28it/s]Extractor Predicting: 146it [01:40,  1.31it/s]Extractor Predicting: 147it [01:40,  1.34it/s]Extractor Predicting: 148it [01:41,  1.32it/s]Extractor Predicting: 149it [01:42,  1.37it/s]Extractor Predicting: 149it [01:42,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:53,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:53,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:53,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:53,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:53,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:05:53,783 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:05:53,784 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:05:54,454 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:05:55,453 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:05:55,453 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:57,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:57,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:57,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:57,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:05:57,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:05:58,377 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:05:58,378 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:05:59,054 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:05:59,201 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:05:59,201 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2460545193687231,
  "recall": 0.08492201039861352,
  "score": 0.12626541505613845,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.71it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:09,  1.71it/s]Extractor Predicting: 16it [00:09,  1.71it/s]Extractor Predicting: 17it [00:10,  1.71it/s]Extractor Predicting: 18it [00:11,  1.67it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.68it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:17,  1.70it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:24,  1.59it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.64it/s]Extractor Predicting: 44it [00:26,  1.64it/s]Extractor Predicting: 45it [00:27,  1.49it/s]Extractor Predicting: 46it [00:28,  1.48it/s]Extractor Predicting: 47it [00:28,  1.51it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.57it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:31,  1.60it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.62it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:34,  1.55it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:35,  1.57it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:37,  1.56it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:42,  1.58it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.56it/s]Extractor Predicting: 74it [00:45,  1.57it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:47,  1.58it/s]Extractor Predicting: 78it [00:48,  1.53it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:49,  1.60it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.54it/s]Extractor Predicting: 83it [00:51,  1.54it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:52,  1.56it/s]Extractor Predicting: 86it [00:53,  1.41it/s]Extractor Predicting: 87it [00:54,  1.47it/s]Extractor Predicting: 88it [00:55,  1.51it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:56,  1.57it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:58,  1.61it/s]Extractor Predicting: 94it [00:58,  1.62it/s]Extractor Predicting: 95it [00:59,  1.60it/s]Extractor Predicting: 96it [00:59,  1.63it/s]Extractor Predicting: 97it [01:00,  1.65it/s]Extractor Predicting: 98it [01:01,  1.63it/s]Extractor Predicting: 99it [01:01,  1.61it/s]Extractor Predicting: 100it [01:02,  1.64it/s]Extractor Predicting: 101it [01:03,  1.62it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:04,  1.56it/s]Extractor Predicting: 104it [01:04,  1.61it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.59it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:07,  1.56it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:08,  1.58it/s]Extractor Predicting: 111it [01:09,  1.57it/s]Extractor Predicting: 112it [01:10,  1.57it/s]Extractor Predicting: 113it [01:10,  1.59it/s]Extractor Predicting: 114it [01:11,  1.58it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.61it/s]Extractor Predicting: 117it [01:13,  1.62it/s]Extractor Predicting: 118it [01:13,  1.66it/s]Extractor Predicting: 119it [01:14,  1.67it/s]Extractor Predicting: 120it [01:14,  1.67it/s]Extractor Predicting: 121it [01:15,  1.66it/s]Extractor Predicting: 122it [01:16,  1.65it/s]Extractor Predicting: 123it [01:16,  1.62it/s]Extractor Predicting: 124it [01:17,  1.63it/s]Extractor Predicting: 125it [01:18,  1.63it/s]Extractor Predicting: 126it [01:18,  1.61it/s]Extractor Predicting: 127it [01:19,  1.58it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:21,  1.52it/s]Extractor Predicting: 131it [01:22,  1.50it/s]Extractor Predicting: 132it [01:22,  1.52it/s]Extractor Predicting: 133it [01:23,  1.54it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:24,  1.51it/s]Extractor Predicting: 136it [01:25,  1.51it/s]Extractor Predicting: 137it [01:25,  1.50it/s]Extractor Predicting: 138it [01:26,  1.52it/s]Extractor Predicting: 139it [01:27,  1.52it/s]Extractor Predicting: 140it [01:27,  1.49it/s]Extractor Predicting: 141it [01:28,  1.50it/s]Extractor Predicting: 142it [01:29,  1.50it/s]Extractor Predicting: 143it [01:29,  1.52it/s]Extractor Predicting: 144it [01:30,  1.51it/s]Extractor Predicting: 145it [01:31,  1.50it/s]Extractor Predicting: 146it [01:31,  1.50it/s]Extractor Predicting: 147it [01:32,  1.51it/s]Extractor Predicting: 148it [01:33,  1.52it/s]Extractor Predicting: 149it [01:33,  1.55it/s]Extractor Predicting: 150it [01:34,  1.53it/s]Extractor Predicting: 151it [01:35,  1.53it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:37,  1.50it/s]Extractor Predicting: 155it [01:37,  1.51it/s]Extractor Predicting: 156it [01:38,  1.56it/s]Extractor Predicting: 157it [01:39,  1.61it/s]Extractor Predicting: 158it [01:39,  1.62it/s]Extractor Predicting: 159it [01:40,  1.62it/s]Extractor Predicting: 160it [01:40,  1.62it/s]Extractor Predicting: 161it [01:41,  1.67it/s]Extractor Predicting: 162it [01:42,  1.62it/s]Extractor Predicting: 163it [01:42,  1.60it/s]Extractor Predicting: 164it [01:43,  1.62it/s]Extractor Predicting: 165it [01:43,  1.66it/s]Extractor Predicting: 166it [01:44,  1.65it/s]Extractor Predicting: 167it [01:45,  1.71it/s]Extractor Predicting: 168it [01:45,  1.71it/s]Extractor Predicting: 169it [01:46,  1.74it/s]Extractor Predicting: 170it [01:46,  1.69it/s]Extractor Predicting: 171it [01:47,  1.61it/s]Extractor Predicting: 172it [01:48,  1.59it/s]Extractor Predicting: 173it [01:48,  1.63it/s]Extractor Predicting: 173it [01:48,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:54,775 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:54,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:54,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:54,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:54,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:07:55,112 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:07:55,113 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:07:55,787 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:07:56,788 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:07:56,788 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:58,924 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:58,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:58,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:58,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:07:58,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:07:59,666 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:07:59,667 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:07:59,926 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:08:00,082 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:08:00,082 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2129506008010681,
  "recall": 0.0769416304872166,
  "score": 0.11304039688164425,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:06,  1.40it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.55it/s]Extractor Predicting: 16it [00:10,  1.64it/s]Extractor Predicting: 17it [00:11,  1.73it/s]Extractor Predicting: 18it [00:11,  1.77it/s]Extractor Predicting: 19it [00:12,  1.78it/s]Extractor Predicting: 20it [00:12,  1.80it/s]Extractor Predicting: 21it [00:13,  1.84it/s]Extractor Predicting: 22it [00:14,  1.80it/s]Extractor Predicting: 23it [00:14,  1.80it/s]Extractor Predicting: 24it [00:15,  1.79it/s]Extractor Predicting: 25it [00:15,  1.79it/s]Extractor Predicting: 26it [00:16,  1.82it/s]Extractor Predicting: 27it [00:16,  1.77it/s]Extractor Predicting: 28it [00:17,  1.79it/s]Extractor Predicting: 29it [00:17,  1.80it/s]Extractor Predicting: 30it [00:18,  1.84it/s]Extractor Predicting: 31it [00:18,  1.86it/s]Extractor Predicting: 32it [00:19,  1.85it/s]Extractor Predicting: 33it [00:20,  1.86it/s]Extractor Predicting: 34it [00:20,  1.85it/s]Extractor Predicting: 35it [00:21,  1.85it/s]Extractor Predicting: 36it [00:21,  1.81it/s]Extractor Predicting: 37it [00:22,  1.76it/s]Extractor Predicting: 38it [00:22,  1.83it/s]Extractor Predicting: 39it [00:23,  1.82it/s]Extractor Predicting: 40it [00:23,  1.83it/s]Extractor Predicting: 41it [00:24,  1.82it/s]Extractor Predicting: 42it [00:24,  1.85it/s]Extractor Predicting: 43it [00:25,  1.82it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:27,  1.51it/s]Extractor Predicting: 47it [00:28,  1.48it/s]Extractor Predicting: 48it [00:29,  1.46it/s]Extractor Predicting: 49it [00:29,  1.37it/s]Extractor Predicting: 50it [00:30,  1.38it/s]Extractor Predicting: 51it [00:31,  1.39it/s]Extractor Predicting: 52it [00:32,  1.39it/s]Extractor Predicting: 53it [00:32,  1.39it/s]Extractor Predicting: 54it [00:33,  1.39it/s]Extractor Predicting: 55it [00:34,  1.41it/s]Extractor Predicting: 56it [00:34,  1.39it/s]Extractor Predicting: 57it [00:35,  1.39it/s]Extractor Predicting: 58it [00:36,  1.40it/s]Extractor Predicting: 59it [00:36,  1.64it/s]Extractor Predicting: 59it [00:36,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:08:37,752 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:08:37,753 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:08:37,758 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:08:37,759 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:08:37,762 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:08:40,852 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:08:40,854 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:08:40,883 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:08:40,884 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:08:40,889 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:08:40,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7815028901734105,
  "recall": 0.21855803427093437,
  "score": 0.34158665992925724,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:08:41,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:41,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:42,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:43,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:43,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:44,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:45,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:46,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:46,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:47,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:48,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:48,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:50,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:50,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:51,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:52,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:52,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:53,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:54,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:05, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-29 02:08:55,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:55,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:56,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:57,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:58,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:58,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:08:59,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:00,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:01,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:01,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:02,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:03,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:03,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:04,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:05,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:05,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:06,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:07,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:08,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:09,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:09,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:10,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:11,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:04, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-29 02:09:11,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:12,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:13,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:13,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:14,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:15,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:16,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:16,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:17,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:18,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:18,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:19,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:20,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:20,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:21,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:22,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:22,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:23,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:24,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:24,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:25,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:26,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:47, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-29 02:09:26,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:27,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:28,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:29,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:30,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:30,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:31,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:32,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:33,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:33,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:34,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:35,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:36,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:36,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:37,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:38,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:38,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:39,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:40,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:40,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:41,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:42,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:02<01:34, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:09:43,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:43,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:44,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:45,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:45,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:46,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:47,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:47,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:48,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:49,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:50,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:50,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:51,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:52,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:52,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:53,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:54,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:54,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:55,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:56,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:56,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:16<01:16, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-29 02:09:57,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:58,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:58,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:09:59,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:00,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:01,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:01,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:02,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:03,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:03,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:04,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:05,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:05,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:06,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:07,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:08,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:08,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:09,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:10,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:11,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:11,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:12,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:31<01:01, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-29 02:10:13,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:13,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:14,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:15,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:15,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:16,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:17,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:17,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:18,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:19,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:19,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:20,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:21,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:21,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:22,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:23,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:24,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:25,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:25,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:26,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:27,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:46<00:45, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-29 02:10:28,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:28,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:29,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:30,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:31,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:31,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:32,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:32,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:33,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:34,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:34,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:35,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:36,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:36,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:37,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:38,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:38,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:39,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:40,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:40,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:00<00:29, 14.71s/it][WARNING|generation_utils.py:914] 2023-08-29 02:10:41,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:42,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:42,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:43,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:44,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:44,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:45,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:46,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:46,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:47,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:48,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:49,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:49,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:50,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:50,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:51,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:52,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:52,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:53,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:54,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:14<00:14, 14.32s/it][WARNING|generation_utils.py:914] 2023-08-29 02:10:55,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:55,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:56,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:57,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:57,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:58,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:10:59,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:00,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:00,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:01,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:02,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:02,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:03,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:04,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:04,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:05,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:06,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:06,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:07,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:08,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:09,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:11:09,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:29<00:00, 14.66s/it]Generating: 100%|██████████| 10/10 [02:29<00:00, 14.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:15,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:15,440 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:15,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:15,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:15,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:11:15,861 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:11:15,862 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:11:16,158 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:11:17,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:11:17,203 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:18,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:18,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:18,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:18,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:11:18,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:11:18,856 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:11:18,858 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:11:19,116 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:11:19,276 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:11:19,276 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : given name .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.875, 'errors': {'', "('Juan Manuel Marquez', 'lowest point', '', 'From then on the season , he was traded along with several other highly regarded pitchers , including Juan Manuel Marquez and Don Mattingly .')"}}
['Relation : mother . Context : Later in life , he married his third wife , a young princess of the family at the time of the Napoleonic Wars . Head Entity : princess , Tail Entity : her third wife .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8920454545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1141 ) , he purchased from Charles IV of Denmark at the Cape of Good Hope the gold from a lake in the mouth of the river Thames . Head Entity : Charles IV of Denmark , Tail Entity : River Thames .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8664772727272727, 'errors': {'', "('Legend of the Five Rings', 'genre', '', 'The soundtrack features tracks such as The Last Airbender , The Last Airbender 2 , Ginyu Force , Legend of the Five Rings and many others .')", "('Amateria', 'genre', '', 'She is known for her role as the female lead on the ABC sitcom , Amateria , for six seasons , before moving to the role of a child actor in 2013 .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8835227272727273, 'errors': {'', "('Mayor', 'member of', '', 'In 1846 he became the Mayor of Boston , serving from 1846 until 1860 after which he was appointed to the office of Attorney General in 1855 .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 7951
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8051, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.38it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.49it/s]Extractor Estimating: 6it [00:04,  1.54it/s]Extractor Estimating: 7it [00:04,  1.51it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:09,  1.52it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.54it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:14,  1.50it/s]Extractor Estimating: 23it [00:15,  1.53it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.44it/s]Extractor Estimating: 26it [00:17,  1.46it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:19,  1.55it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:20,  1.64it/s]Extractor Estimating: 32it [00:20,  1.64it/s]Extractor Estimating: 33it [00:21,  1.68it/s]Extractor Estimating: 34it [00:22,  1.65it/s]Extractor Estimating: 35it [00:22,  1.74it/s]Extractor Estimating: 36it [00:23,  1.75it/s]Extractor Estimating: 37it [00:23,  1.80it/s]Extractor Estimating: 38it [00:24,  1.76it/s]Extractor Estimating: 39it [00:24,  1.74it/s]Extractor Estimating: 40it [00:25,  1.75it/s]Extractor Estimating: 41it [00:26,  1.74it/s]Extractor Estimating: 42it [00:27,  1.20it/s]Extractor Estimating: 43it [00:28,  1.30it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:29,  1.61it/s]Extractor Estimating: 47it [00:30,  1.65it/s]Extractor Estimating: 48it [00:30,  1.67it/s]Extractor Estimating: 49it [00:31,  1.69it/s]Extractor Estimating: 50it [00:31,  1.74it/s]Extractor Estimating: 51it [00:32,  1.73it/s]Extractor Estimating: 52it [00:33,  1.67it/s]Extractor Estimating: 53it [00:33,  1.65it/s]Extractor Estimating: 54it [00:34,  1.63it/s]Extractor Estimating: 55it [00:35,  1.65it/s]Extractor Estimating: 56it [00:35,  1.68it/s]Extractor Estimating: 57it [00:36,  1.68it/s]Extractor Estimating: 58it [00:36,  1.67it/s]Extractor Estimating: 59it [00:37,  1.67it/s]Extractor Estimating: 60it [00:38,  1.63it/s]Extractor Estimating: 61it [00:38,  1.65it/s]Extractor Estimating: 62it [00:39,  1.65it/s]Extractor Estimating: 63it [00:39,  1.67it/s]Extractor Estimating: 64it [00:40,  1.63it/s]Extractor Estimating: 65it [00:41,  1.64it/s]Extractor Estimating: 66it [00:41,  1.58it/s]Extractor Estimating: 67it [00:42,  1.63it/s]Extractor Estimating: 68it [00:42,  1.63it/s]Extractor Estimating: 69it [00:43,  1.64it/s]Extractor Estimating: 70it [00:44,  1.67it/s]Extractor Estimating: 71it [00:44,  1.66it/s]Extractor Estimating: 72it [00:45,  1.64it/s]Extractor Estimating: 73it [00:45,  1.66it/s]Extractor Estimating: 74it [00:46,  1.67it/s]Extractor Estimating: 75it [00:47,  1.67it/s]Extractor Estimating: 76it [00:47,  1.64it/s]Extractor Estimating: 77it [00:48,  1.61it/s]Extractor Estimating: 78it [00:49,  1.62it/s]Extractor Estimating: 79it [00:49,  1.58it/s]Extractor Estimating: 80it [00:50,  1.62it/s]Extractor Estimating: 81it [00:50,  1.61it/s]Extractor Estimating: 82it [00:51,  1.62it/s]Extractor Estimating: 83it [00:52,  1.64it/s]Extractor Estimating: 84it [00:52,  1.66it/s]Extractor Estimating: 85it [00:53,  1.52it/s]Extractor Estimating: 86it [00:54,  1.58it/s]Extractor Estimating: 87it [00:54,  1.58it/s]Extractor Estimating: 88it [00:55,  1.62it/s]Extractor Estimating: 89it [00:55,  1.64it/s]Extractor Estimating: 90it [00:56,  1.64it/s]Extractor Estimating: 91it [00:57,  1.66it/s]Extractor Estimating: 92it [00:57,  1.66it/s]Extractor Estimating: 93it [00:58,  1.68it/s]Extractor Estimating: 94it [00:58,  1.64it/s]Extractor Estimating: 95it [00:59,  1.67it/s]Extractor Estimating: 96it [01:00,  1.66it/s]Extractor Estimating: 97it [01:00,  1.66it/s]Extractor Estimating: 98it [01:01,  1.70it/s]Extractor Estimating: 99it [01:01,  1.70it/s]Extractor Estimating: 100it [01:02,  1.71it/s]Extractor Estimating: 101it [01:02,  1.75it/s]Extractor Estimating: 102it [01:03,  1.72it/s]Extractor Estimating: 103it [01:04,  1.73it/s]Extractor Estimating: 104it [01:04,  1.72it/s]Extractor Estimating: 105it [01:05,  1.72it/s]Extractor Estimating: 106it [01:05,  1.65it/s]Extractor Estimating: 107it [01:06,  1.64it/s]Extractor Estimating: 108it [01:07,  1.66it/s]Extractor Estimating: 109it [01:07,  1.69it/s]Extractor Estimating: 110it [01:08,  1.70it/s]Extractor Estimating: 111it [01:08,  1.73it/s]Extractor Estimating: 112it [01:09,  1.71it/s]Extractor Estimating: 113it [01:10,  1.70it/s]Extractor Estimating: 114it [01:10,  1.55it/s]Extractor Estimating: 115it [01:11,  1.56it/s]Extractor Estimating: 116it [01:12,  1.61it/s]Extractor Estimating: 117it [01:12,  1.65it/s]Extractor Estimating: 118it [01:13,  1.63it/s]Extractor Estimating: 119it [01:13,  1.63it/s]Extractor Estimating: 120it [01:14,  1.62it/s]Extractor Estimating: 121it [01:15,  1.60it/s]Extractor Estimating: 122it [01:15,  1.65it/s]Extractor Estimating: 123it [01:16,  1.65it/s]Extractor Estimating: 124it [01:16,  1.66it/s]Extractor Estimating: 125it [01:17,  1.67it/s]Extractor Estimating: 126it [01:18,  1.68it/s]Extractor Estimating: 127it [01:18,  1.70it/s]Extractor Estimating: 128it [01:19,  1.63it/s]Extractor Estimating: 129it [01:19,  1.57it/s]Extractor Estimating: 130it [01:20,  1.55it/s]Extractor Estimating: 131it [01:21,  1.57it/s]Extractor Estimating: 132it [01:21,  1.60it/s]Extractor Estimating: 133it [01:22,  1.59it/s]Extractor Estimating: 134it [01:23,  1.57it/s]Extractor Estimating: 135it [01:23,  1.56it/s]Extractor Estimating: 136it [01:24,  1.58it/s]Extractor Estimating: 137it [01:24,  1.63it/s]Extractor Estimating: 138it [01:25,  1.62it/s]Extractor Estimating: 139it [01:26,  1.62it/s]Extractor Estimating: 140it [01:26,  1.58it/s]Extractor Estimating: 141it [01:27,  1.59it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:28,  1.52it/s]Extractor Estimating: 144it [01:29,  1.54it/s]Extractor Estimating: 145it [01:30,  1.54it/s]Extractor Estimating: 146it [01:30,  1.54it/s]Extractor Estimating: 147it [01:31,  1.53it/s]Extractor Estimating: 148it [01:32,  1.55it/s]Extractor Estimating: 149it [01:32,  1.60it/s]Extractor Estimating: 150it [01:33,  1.65it/s]Extractor Estimating: 151it [01:33,  1.71it/s]Extractor Estimating: 152it [01:34,  1.73it/s]Extractor Estimating: 153it [01:34,  1.76it/s]Extractor Estimating: 154it [01:35,  1.81it/s]Extractor Estimating: 155it [01:35,  1.80it/s]Extractor Estimating: 156it [01:36,  1.83it/s]Extractor Estimating: 157it [01:36,  1.89it/s]Extractor Estimating: 158it [01:37,  1.93it/s]Extractor Estimating: 159it [01:37,  1.93it/s]Extractor Estimating: 160it [01:38,  1.92it/s]Extractor Estimating: 161it [01:38,  1.99it/s]Extractor Estimating: 162it [01:39,  1.95it/s]Extractor Estimating: 163it [01:40,  1.90it/s]Extractor Estimating: 164it [01:40,  1.82it/s]Extractor Estimating: 165it [01:41,  1.80it/s]Extractor Estimating: 166it [01:41,  1.80it/s]Extractor Estimating: 167it [01:42,  1.83it/s]Extractor Estimating: 168it [01:42,  1.82it/s]Extractor Estimating: 169it [01:44,  1.02it/s]Extractor Estimating: 170it [01:45,  1.18it/s]Extractor Estimating: 171it [01:45,  1.32it/s]Extractor Estimating: 172it [01:46,  1.45it/s]Extractor Estimating: 173it [01:47,  1.50it/s]Extractor Estimating: 174it [01:47,  1.61it/s]Extractor Estimating: 175it [01:48,  1.67it/s]Extractor Estimating: 176it [01:48,  1.62it/s]Extractor Estimating: 177it [01:49,  1.65it/s]Extractor Estimating: 178it [01:50,  1.58it/s]Extractor Estimating: 179it [01:50,  1.62it/s]Extractor Estimating: 180it [01:51,  1.63it/s]Extractor Estimating: 181it [01:51,  1.69it/s]Extractor Estimating: 182it [01:52,  1.69it/s]Extractor Estimating: 183it [01:53,  1.66it/s]Extractor Estimating: 184it [01:53,  1.68it/s]Extractor Estimating: 185it [01:54,  1.68it/s]Extractor Estimating: 186it [01:54,  1.67it/s]Extractor Estimating: 187it [01:55,  1.75it/s]Extractor Estimating: 188it [01:55,  1.75it/s]Extractor Estimating: 189it [01:56,  1.76it/s]Extractor Estimating: 190it [01:57,  1.73it/s]Extractor Estimating: 191it [01:57,  1.70it/s]Extractor Estimating: 192it [01:58,  1.67it/s]Extractor Estimating: 193it [01:58,  1.66it/s]Extractor Estimating: 194it [01:59,  1.68it/s]Extractor Estimating: 195it [02:00,  1.70it/s]Extractor Estimating: 196it [02:00,  1.69it/s]Extractor Estimating: 197it [02:01,  1.69it/s]Extractor Estimating: 198it [02:01,  1.72it/s]Extractor Estimating: 199it [02:02,  1.69it/s]Extractor Estimating: 200it [02:02,  1.72it/s]Extractor Estimating: 201it [02:03,  1.71it/s]Extractor Estimating: 202it [02:04,  1.73it/s]Extractor Estimating: 203it [02:04,  1.74it/s]Extractor Estimating: 204it [02:05,  1.75it/s]Extractor Estimating: 205it [02:05,  1.71it/s]Extractor Estimating: 206it [02:06,  1.68it/s]Extractor Estimating: 207it [02:07,  1.73it/s]Extractor Estimating: 208it [02:07,  1.78it/s]Extractor Estimating: 209it [02:08,  1.83it/s]Extractor Estimating: 210it [02:08,  1.82it/s]Extractor Estimating: 211it [02:09,  1.68it/s]Extractor Estimating: 212it [02:09,  1.76it/s]Extractor Estimating: 213it [02:10,  1.76it/s]Extractor Estimating: 214it [02:10,  1.80it/s]Extractor Estimating: 215it [02:11,  1.78it/s]Extractor Estimating: 216it [02:12,  1.82it/s]Extractor Estimating: 217it [02:12,  1.79it/s]Extractor Estimating: 218it [02:13,  1.82it/s]Extractor Estimating: 219it [02:13,  1.84it/s]Extractor Estimating: 220it [02:14,  1.86it/s]Extractor Estimating: 221it [02:14,  1.84it/s]Extractor Estimating: 222it [02:15,  1.94it/s]Extractor Estimating: 223it [02:15,  1.72it/s]Extractor Estimating: 224it [02:16,  1.74it/s]Extractor Estimating: 225it [02:17,  1.70it/s]Extractor Estimating: 226it [02:17,  1.68it/s]Extractor Estimating: 227it [02:18,  1.67it/s]Extractor Estimating: 228it [02:18,  1.65it/s]Extractor Estimating: 229it [02:19,  1.62it/s]Extractor Estimating: 230it [02:20,  1.60it/s]Extractor Estimating: 231it [02:20,  1.59it/s]Extractor Estimating: 232it [02:21,  1.67it/s]Extractor Estimating: 233it [02:22,  1.62it/s]Extractor Estimating: 234it [02:22,  1.68it/s]Extractor Estimating: 235it [02:23,  1.65it/s]Extractor Estimating: 236it [02:23,  1.61it/s]Extractor Estimating: 237it [02:24,  1.62it/s]Extractor Estimating: 238it [02:25,  1.62it/s]Extractor Estimating: 239it [02:25,  1.62it/s]Extractor Estimating: 240it [02:26,  1.61it/s]Extractor Estimating: 241it [02:26,  1.65it/s]Extractor Estimating: 242it [02:27,  1.63it/s]Extractor Estimating: 243it [02:28,  1.63it/s]Extractor Estimating: 244it [02:28,  1.61it/s]Extractor Estimating: 245it [02:29,  1.60it/s]Extractor Estimating: 246it [02:30,  1.58it/s]Extractor Estimating: 247it [02:30,  1.58it/s]Extractor Estimating: 248it [02:31,  1.61it/s]Extractor Estimating: 249it [02:31,  1.91it/s]Extractor Estimating: 249it [02:31,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:06,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:06,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:06,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:06,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:06,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:14:07,229 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:14:07,231 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:14:07,790 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:14:08,820 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:14:08,820 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:11,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:11,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:11,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:11,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:11,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:14:12,304 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:14:12,305 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:14:12,881 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:14:13,044 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:14:13,044 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:46:34,034 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:46:34,068 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4959 mean pseudo reward: 0.9573549171584483
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 16906
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17006, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17006, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.099, loss:456.7330
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.055, loss:411.9584
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 93, avg_time 1.054, loss:372.6387
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 193, avg_time 1.053, loss:373.5883
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 1.056, loss:352.3835
>> valid entity prec:0.5016, rec:0.4952, f1:0.4984
>> valid relation prec:0.2084, rec:0.0649, f1:0.0990
>> valid relation with NER prec:0.2084, rec:0.0649, f1:0.0990
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.523, loss:344.7286
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 79, avg_time 1.069, loss:319.1979
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 179, avg_time 1.060, loss:334.2163
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 1.051, loss:324.5828
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 1.047, loss:316.7730
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5044, rec:0.3705, f1:0.4272
>> valid relation prec:0.2270, rec:0.0513, f1:0.0837
>> valid relation with NER prec:0.2270, rec:0.0513, f1:0.0837
g_step 1100, step 65, avg_time 2.498, loss:308.0120
g_step 1200, step 165, avg_time 1.061, loss:321.2905
g_step 1300, step 58, avg_time 1.068, loss:295.8867
g_step 1400, step 158, avg_time 1.047, loss:298.1807
g_step 1500, step 51, avg_time 1.060, loss:281.6209
>> valid entity prec:0.5008, rec:0.4636, f1:0.4815
>> valid relation prec:0.2304, rec:0.0756, f1:0.1138
>> valid relation with NER prec:0.2304, rec:0.0756, f1:0.1138
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 151, avg_time 2.495, loss:281.5265
g_step 1700, step 44, avg_time 1.068, loss:266.4873
g_step 1800, step 144, avg_time 1.062, loss:268.2055
g_step 1900, step 37, avg_time 1.050, loss:266.9153
g_step 2000, step 137, avg_time 1.061, loss:253.4382
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5139, rec:0.4368, f1:0.4722
>> valid relation prec:0.2097, rec:0.0687, f1:0.1035
>> valid relation with NER prec:0.2097, rec:0.0687, f1:0.1035
g_step 2100, step 30, avg_time 2.496, loss:243.5700
g_step 2200, step 130, avg_time 1.065, loss:235.5433
g_step 2300, step 23, avg_time 1.049, loss:243.8525
g_step 2400, step 123, avg_time 1.055, loss:237.7681
g_step 2500, step 16, avg_time 1.076, loss:233.9308
>> valid entity prec:0.5114, rec:0.3715, f1:0.4304
>> valid relation prec:0.1992, rec:0.0602, f1:0.0925
>> valid relation with NER prec:0.1992, rec:0.0602, f1:0.0925
g_step 2600, step 116, avg_time 2.495, loss:221.6307
g_step 2700, step 9, avg_time 1.053, loss:234.3848
g_step 2800, step 109, avg_time 1.038, loss:220.4430
g_step 2900, step 2, avg_time 1.072, loss:215.9317
g_step 3000, step 102, avg_time 1.060, loss:199.6742
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5023, rec:0.4238, f1:0.4597
>> valid relation prec:0.2037, rec:0.0808, f1:0.1157
>> valid relation with NER prec:0.2037, rec:0.0808, f1:0.1157
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 202, avg_time 2.502, loss:222.1354
g_step 3200, step 95, avg_time 1.047, loss:204.3227
g_step 3300, step 195, avg_time 1.068, loss:199.0556
g_step 3400, step 88, avg_time 1.053, loss:195.6946
g_step 3500, step 188, avg_time 1.049, loss:194.9119
>> valid entity prec:0.4968, rec:0.4454, f1:0.4697
>> valid relation prec:0.2052, rec:0.0729, f1:0.1076
>> valid relation with NER prec:0.2052, rec:0.0729, f1:0.1076
g_step 3600, step 81, avg_time 2.499, loss:177.6675
g_step 3700, step 181, avg_time 1.070, loss:188.9156
g_step 3800, step 74, avg_time 1.050, loss:184.1369
g_step 3900, step 174, avg_time 1.060, loss:183.2997
g_step 4000, step 67, avg_time 1.063, loss:170.5066
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5097, rec:0.4247, f1:0.4633
>> valid relation prec:0.2500, rec:0.0892, f1:0.1315
>> valid relation with NER prec:0.2500, rec:0.0892, f1:0.1315
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 167, avg_time 2.500, loss:171.9145
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:46:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:46:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-46-34_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:46:35 - WARNING - datasets.builder -   Using custom data configuration default-4871fa75379ad269
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4871fa75379ad269/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:46:35,345 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:46:35,346 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:46:35,347 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:46:35,348 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:46:35,356 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,364 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,364 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,364 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,365 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,365 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:46:35,365 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:46:35,503 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:46:38,722 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:46:38,725 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4871fa75379ad269/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.26ba/s] 40%|████      | 2/5 [00:00<00:00,  4.12ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.50ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.71ba/s]100%|██████████| 5/5 [00:01<00:00,  4.85ba/s]100%|██████████| 5/5 [00:01<00:00,  4.57ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.08ba/s] 40%|████      | 2/5 [00:00<00:00,  4.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.38ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.42ba/s]100%|██████████| 5/5 [00:00<00:00,  5.39ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.63ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.83ba/s]100%|██████████| 5/5 [00:00<00:00, 10.15ba/s]100%|██████████| 5/5 [00:00<00:00,  9.99ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.13ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.58ba/s]100%|██████████| 5/5 [00:00<00:00, 12.93ba/s]100%|██████████| 5/5 [00:00<00:00, 11.83ba/s]
[INFO|trainer.py:414] 2023-08-29 03:46:42,034 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:46:42,058 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:46:42,058 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-29 03:46:42,058 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:46:42,058 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:46:42,058 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:46:42,058 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:46:42,058 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.33it/s]  1%|          | 2/390 [00:00<01:53,  3.43it/s]  1%|          | 3/390 [00:00<01:52,  3.45it/s]  1%|          | 4/390 [00:01<01:51,  3.47it/s]  1%|▏         | 5/390 [00:01<01:50,  3.48it/s]  2%|▏         | 6/390 [00:01<01:50,  3.48it/s]  2%|▏         | 7/390 [00:02<01:49,  3.49it/s]  2%|▏         | 8/390 [00:02<01:49,  3.49it/s]  2%|▏         | 9/390 [00:02<01:49,  3.49it/s]  3%|▎         | 10/390 [00:02<01:48,  3.49it/s]  3%|▎         | 11/390 [00:03<01:48,  3.49it/s]  3%|▎         | 12/390 [00:03<01:48,  3.49it/s]  3%|▎         | 13/390 [00:03<01:47,  3.49it/s]  4%|▎         | 14/390 [00:04<01:47,  3.49it/s]  4%|▍         | 15/390 [00:04<01:47,  3.49it/s]  4%|▍         | 16/390 [00:04<01:47,  3.49it/s]  4%|▍         | 17/390 [00:04<01:46,  3.49it/s]  5%|▍         | 18/390 [00:05<01:46,  3.49it/s]  5%|▍         | 19/390 [00:05<01:46,  3.48it/s]  5%|▌         | 20/390 [00:05<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:45,  3.48it/s]  6%|▌         | 22/390 [00:06<01:45,  3.49it/s]  6%|▌         | 23/390 [00:06<01:45,  3.49it/s]  6%|▌         | 24/390 [00:06<01:44,  3.49it/s]  6%|▋         | 25/390 [00:07<01:44,  3.49it/s]  7%|▋         | 26/390 [00:07<01:44,  3.49it/s]  7%|▋         | 27/390 [00:07<01:44,  3.49it/s]  7%|▋         | 28/390 [00:08<01:43,  3.49it/s]  7%|▋         | 29/390 [00:08<01:43,  3.49it/s]  8%|▊         | 30/390 [00:08<01:43,  3.49it/s]  8%|▊         | 31/390 [00:08<01:43,  3.48it/s]  8%|▊         | 32/390 [00:09<01:42,  3.49it/s]  8%|▊         | 33/390 [00:09<01:42,  3.48it/s]  9%|▊         | 34/390 [00:09<01:42,  3.49it/s]  9%|▉         | 35/390 [00:10<01:41,  3.48it/s]  9%|▉         | 36/390 [00:10<01:41,  3.48it/s]  9%|▉         | 37/390 [00:10<01:41,  3.47it/s] 10%|▉         | 38/390 [00:10<01:41,  3.48it/s] 10%|█         | 39/390 [00:11<01:40,  3.48it/s] 10%|█         | 40/390 [00:11<01:40,  3.48it/s] 11%|█         | 41/390 [00:11<01:40,  3.48it/s] 11%|█         | 42/390 [00:12<01:39,  3.48it/s] 11%|█         | 43/390 [00:12<01:39,  3.49it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.49it/s] 12%|█▏        | 45/390 [00:12<01:38,  3.49it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.49it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.49it/s] 12%|█▏        | 48/390 [00:13<01:38,  3.48it/s] 13%|█▎        | 49/390 [00:14<01:37,  3.49it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.49it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.49it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.49it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.48it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.48it/s] 14%|█▍        | 55/390 [00:15<01:36,  3.48it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.48it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.48it/s] 15%|█▍        | 58/390 [00:16<01:35,  3.48it/s] 15%|█▌        | 59/390 [00:16<01:34,  3.48it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 62/390 [00:17<01:34,  3.48it/s] 16%|█▌        | 63/390 [00:18<01:33,  3.48it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.48it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.48it/s] 17%|█▋        | 66/390 [00:18<01:32,  3.49it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.48it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.48it/s] 18%|█▊        | 69/390 [00:19<01:32,  3.48it/s] 18%|█▊        | 70/390 [00:20<01:31,  3.48it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.49it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.48it/s] 19%|█▊        | 73/390 [00:20<01:31,  3.48it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.48it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.48it/s] 20%|██        | 78/390 [00:22<01:29,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 03:47:04,494 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:47:04,494 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:47:04,494 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.68it/s][A
  2%|▏         | 12/505 [00:00<00:09, 51.06it/s][A
  4%|▎         | 18/505 [00:00<00:09, 49.11it/s][A
  5%|▍         | 23/505 [00:00<00:09, 48.48it/s][A
  6%|▌         | 28/505 [00:00<00:09, 48.00it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.51it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.41it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.95it/s][A
 10%|▉         | 48/505 [00:00<00:09, 47.00it/s][A
 10%|█         | 53/505 [00:01<00:09, 47.16it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 47.13it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 47.09it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 47.07it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 47.03it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.93it/s][A
 16%|█▋        | 83/505 [00:01<00:08, 46.96it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.70it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.79it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.93it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.95it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.93it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 47.06it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.98it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.92it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.87it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.75it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.66it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.94it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.96it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.96it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.99it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.99it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.91it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.98it/s][A
 35%|███▌      | 178/505 [00:03<00:06, 46.84it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.80it/s][A
 37%|███▋      | 188/505 [00:03<00:06, 46.88it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 47.00it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.91it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.96it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.93it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.86it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.80it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.79it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.74it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.88it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.78it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.87it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.86it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.94it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.85it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.91it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.85it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.77it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.80it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.89it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.82it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.96it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.87it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.79it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.84it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.82it/s][A
 63%|██████▎   | 318/505 [00:06<00:03, 46.76it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.85it/s][A
 65%|██████▍   | 328/505 [00:06<00:03, 46.84it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.87it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.83it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.90it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.74it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.86it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.80it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.76it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.72it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.71it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.79it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.87it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.86it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.71it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.72it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.69it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.73it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.73it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.68it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.77it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.75it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.76it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.83it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.75it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.64it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.68it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.70it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.70it/s][A
 93%|█████████▎| 468/505 [00:09<00:00, 46.78it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.75it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.81it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.78it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.71it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.75it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.76it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.73it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:29,  3.48it/s]
100%|██████████| 505/505 [00:10<00:00, 46.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:47:15,294 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-29 03:47:15,312 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:47:17,582 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:47:17,598 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:47:17,620 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:40<28:39,  5.53s/it] 21%|██        | 80/390 [00:40<20:26,  3.96s/it] 21%|██        | 81/390 [00:40<14:42,  2.86s/it] 21%|██        | 82/390 [00:41<10:42,  2.09s/it] 21%|██▏       | 83/390 [00:41<07:54,  1.55s/it] 22%|██▏       | 84/390 [00:41<05:57,  1.17s/it] 22%|██▏       | 85/390 [00:41<04:35,  1.11it/s] 22%|██▏       | 86/390 [00:42<03:38,  1.39it/s] 22%|██▏       | 87/390 [00:42<02:58,  1.70it/s] 23%|██▎       | 88/390 [00:42<02:30,  2.00it/s] 23%|██▎       | 89/390 [00:43<02:11,  2.30it/s] 23%|██▎       | 90/390 [00:43<01:57,  2.56it/s] 23%|██▎       | 91/390 [00:43<01:47,  2.77it/s] 24%|██▎       | 92/390 [00:43<01:41,  2.95it/s] 24%|██▍       | 93/390 [00:44<01:36,  3.09it/s] 24%|██▍       | 94/390 [00:44<01:32,  3.20it/s] 24%|██▍       | 95/390 [00:44<01:30,  3.28it/s] 25%|██▍       | 96/390 [00:45<01:28,  3.34it/s] 25%|██▍       | 97/390 [00:45<01:26,  3.37it/s] 25%|██▌       | 98/390 [00:45<01:25,  3.41it/s] 25%|██▌       | 99/390 [00:45<01:24,  3.43it/s] 26%|██▌       | 100/390 [00:46<01:24,  3.44it/s] 26%|██▌       | 101/390 [00:46<01:23,  3.45it/s] 26%|██▌       | 102/390 [00:46<01:23,  3.45it/s] 26%|██▋       | 103/390 [00:47<01:22,  3.46it/s] 27%|██▋       | 104/390 [00:47<01:22,  3.47it/s] 27%|██▋       | 105/390 [00:47<01:22,  3.47it/s] 27%|██▋       | 106/390 [00:47<01:21,  3.47it/s] 27%|██▋       | 107/390 [00:48<01:21,  3.47it/s] 28%|██▊       | 108/390 [00:48<01:21,  3.48it/s] 28%|██▊       | 109/390 [00:48<01:20,  3.48it/s] 28%|██▊       | 110/390 [00:49<01:20,  3.47it/s] 28%|██▊       | 111/390 [00:49<01:20,  3.47it/s] 29%|██▊       | 112/390 [00:49<01:20,  3.47it/s] 29%|██▉       | 113/390 [00:49<01:19,  3.46it/s] 29%|██▉       | 114/390 [00:50<01:19,  3.46it/s] 29%|██▉       | 115/390 [00:50<01:19,  3.47it/s] 30%|██▉       | 116/390 [00:50<01:18,  3.47it/s] 30%|███       | 117/390 [00:51<01:18,  3.47it/s] 30%|███       | 118/390 [00:51<01:18,  3.47it/s] 31%|███       | 119/390 [00:51<01:18,  3.47it/s] 31%|███       | 120/390 [00:51<01:17,  3.47it/s] 31%|███       | 121/390 [00:52<01:19,  3.39it/s] 31%|███▏      | 122/390 [00:52<01:18,  3.41it/s] 32%|███▏      | 123/390 [00:52<01:17,  3.43it/s] 32%|███▏      | 124/390 [00:53<01:17,  3.43it/s] 32%|███▏      | 125/390 [00:53<01:16,  3.44it/s] 32%|███▏      | 126/390 [00:53<01:16,  3.45it/s] 33%|███▎      | 127/390 [00:53<01:16,  3.45it/s] 33%|███▎      | 128/390 [00:54<01:15,  3.46it/s] 33%|███▎      | 129/390 [00:54<01:15,  3.46it/s] 33%|███▎      | 130/390 [00:54<01:15,  3.46it/s] 34%|███▎      | 131/390 [00:55<01:14,  3.47it/s] 34%|███▍      | 132/390 [00:55<01:14,  3.47it/s] 34%|███▍      | 133/390 [00:55<01:14,  3.47it/s] 34%|███▍      | 134/390 [00:56<01:13,  3.47it/s] 35%|███▍      | 135/390 [00:56<01:13,  3.46it/s] 35%|███▍      | 136/390 [00:56<01:13,  3.46it/s] 35%|███▌      | 137/390 [00:56<01:13,  3.46it/s] 35%|███▌      | 138/390 [00:57<01:12,  3.46it/s] 36%|███▌      | 139/390 [00:57<01:12,  3.47it/s] 36%|███▌      | 140/390 [00:57<01:12,  3.46it/s] 36%|███▌      | 141/390 [00:58<01:11,  3.47it/s] 36%|███▋      | 142/390 [00:58<01:11,  3.47it/s] 37%|███▋      | 143/390 [00:58<01:11,  3.46it/s] 37%|███▋      | 144/390 [00:58<01:10,  3.47it/s] 37%|███▋      | 145/390 [00:59<01:10,  3.47it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.42it/s] 38%|███▊      | 147/390 [00:59<01:10,  3.43it/s] 38%|███▊      | 148/390 [01:00<01:10,  3.44it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.44it/s] 38%|███▊      | 150/390 [01:00<01:09,  3.45it/s] 39%|███▊      | 151/390 [01:00<01:09,  3.45it/s] 39%|███▉      | 152/390 [01:01<01:08,  3.45it/s] 39%|███▉      | 153/390 [01:01<01:08,  3.46it/s] 39%|███▉      | 154/390 [01:01<01:08,  3.46it/s] 40%|███▉      | 155/390 [01:02<01:07,  3.46it/s] 40%|████      | 156/390 [01:02<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:47:44,486 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:47:44,486 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:47:44,486 >>   Batch size = 8
{'eval_loss': 1.1178969144821167, 'eval_runtime': 10.7815, 'eval_samples_per_second': 374.623, 'eval_steps_per_second': 46.839, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.33it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.47it/s][A
  4%|▎         | 18/505 [00:00<00:09, 48.86it/s][A
  5%|▍         | 23/505 [00:00<00:10, 48.14it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.62it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.34it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.07it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.70it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.56it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.69it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.72it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.66it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.72it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.82it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.72it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.71it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.54it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.46it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.54it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.59it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.54it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.71it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.64it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.72it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.74it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.65it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.54it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.63it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.59it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.66it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.66it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.71it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.62it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.71it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.59it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.54it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.48it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.59it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.60it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.69it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.67it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.66it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.65it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.69it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.47it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.54it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.55it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.63it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.66it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.71it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.68it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.77it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.68it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.46it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.52it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.51it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.42it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.60it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.57it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.71it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.70it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.71it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.46it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.56it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.50it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.62it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.61it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.66it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.55it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.61it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.62it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.58it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.50it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.55it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.57it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.65it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.62it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.63it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.59it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.67it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.60it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.46it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.46it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.58it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.61it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.66it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.58it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.63it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.64it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.57it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.42it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.47it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.48it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.51it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.58it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.67it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.64it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.65it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.57it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.41it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:10<00:00, 46.41it/s][A 40%|████      | 156/390 [01:13<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:47:55,347 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 03:47:55,366 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:47:57,730 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:47:57,744 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:47:57,753 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:20<22:02,  5.68s/it] 41%|████      | 158/390 [01:20<15:42,  4.06s/it] 41%|████      | 159/390 [01:21<11:16,  2.93s/it] 41%|████      | 160/390 [01:21<08:11,  2.14s/it] 41%|████▏     | 161/390 [01:21<06:02,  1.58s/it] 42%|████▏     | 162/390 [01:22<04:32,  1.19s/it] 42%|████▏     | 163/390 [01:22<03:29,  1.08it/s] 42%|████▏     | 164/390 [01:22<02:45,  1.37it/s] 42%|████▏     | 165/390 [01:22<02:14,  1.67it/s] 43%|████▎     | 166/390 [01:23<01:53,  1.98it/s] 43%|████▎     | 167/390 [01:23<01:38,  2.27it/s] 43%|████▎     | 168/390 [01:23<01:27,  2.53it/s] 43%|████▎     | 169/390 [01:24<01:20,  2.75it/s] 44%|████▎     | 170/390 [01:24<01:15,  2.93it/s] 44%|████▍     | 171/390 [01:24<01:11,  3.07it/s] 44%|████▍     | 172/390 [01:24<01:08,  3.18it/s] 44%|████▍     | 173/390 [01:25<01:06,  3.26it/s] 45%|████▍     | 174/390 [01:25<01:05,  3.32it/s] 45%|████▍     | 175/390 [01:25<01:03,  3.36it/s] 45%|████▌     | 176/390 [01:26<01:03,  3.39it/s] 45%|████▌     | 177/390 [01:26<01:02,  3.41it/s] 46%|████▌     | 178/390 [01:26<01:01,  3.43it/s] 46%|████▌     | 179/390 [01:26<01:01,  3.44it/s] 46%|████▌     | 180/390 [01:27<01:01,  3.44it/s] 46%|████▋     | 181/390 [01:27<01:00,  3.44it/s] 47%|████▋     | 182/390 [01:27<01:00,  3.45it/s] 47%|████▋     | 183/390 [01:28<00:59,  3.46it/s] 47%|████▋     | 184/390 [01:28<00:59,  3.46it/s] 47%|████▋     | 185/390 [01:28<00:59,  3.46it/s] 48%|████▊     | 186/390 [01:28<00:58,  3.46it/s] 48%|████▊     | 187/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 188/390 [01:29<00:58,  3.46it/s] 48%|████▊     | 189/390 [01:29<00:58,  3.46it/s] 49%|████▊     | 190/390 [01:30<00:57,  3.46it/s] 49%|████▉     | 191/390 [01:30<00:57,  3.45it/s] 49%|████▉     | 192/390 [01:30<00:57,  3.45it/s] 49%|████▉     | 193/390 [01:31<00:56,  3.46it/s] 50%|████▉     | 194/390 [01:31<00:56,  3.46it/s] 50%|█████     | 195/390 [01:31<00:56,  3.46it/s] 50%|█████     | 196/390 [01:31<00:56,  3.46it/s] 51%|█████     | 197/390 [01:32<00:55,  3.46it/s] 51%|█████     | 198/390 [01:32<00:55,  3.46it/s] 51%|█████     | 199/390 [01:32<00:55,  3.46it/s] 51%|█████▏    | 200/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 201/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 202/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 203/390 [01:33<00:54,  3.46it/s] 52%|█████▏    | 204/390 [01:34<00:53,  3.46it/s] 53%|█████▎    | 205/390 [01:34<00:53,  3.46it/s] 53%|█████▎    | 206/390 [01:34<00:53,  3.46it/s] 53%|█████▎    | 207/390 [01:35<00:52,  3.46it/s] 53%|█████▎    | 208/390 [01:35<00:52,  3.46it/s] 54%|█████▎    | 209/390 [01:35<00:52,  3.46it/s] 54%|█████▍    | 210/390 [01:35<00:52,  3.46it/s] 54%|█████▍    | 211/390 [01:36<00:51,  3.46it/s] 54%|█████▍    | 212/390 [01:36<00:51,  3.46it/s] 55%|█████▍    | 213/390 [01:36<00:51,  3.46it/s] 55%|█████▍    | 214/390 [01:37<00:50,  3.46it/s] 55%|█████▌    | 215/390 [01:37<00:50,  3.46it/s] 55%|█████▌    | 216/390 [01:37<00:50,  3.45it/s] 56%|█████▌    | 217/390 [01:37<00:50,  3.45it/s] 56%|█████▌    | 218/390 [01:38<00:49,  3.45it/s] 56%|█████▌    | 219/390 [01:38<00:49,  3.46it/s] 56%|█████▋    | 220/390 [01:38<00:49,  3.46it/s] 57%|█████▋    | 221/390 [01:39<00:48,  3.46it/s] 57%|█████▋    | 222/390 [01:39<00:48,  3.46it/s] 57%|█████▋    | 223/390 [01:39<00:48,  3.46it/s] 57%|█████▋    | 224/390 [01:39<00:47,  3.46it/s] 58%|█████▊    | 225/390 [01:40<00:47,  3.46it/s] 58%|█████▊    | 226/390 [01:40<00:47,  3.46it/s] 58%|█████▊    | 227/390 [01:40<00:47,  3.45it/s] 58%|█████▊    | 228/390 [01:41<00:46,  3.46it/s] 59%|█████▊    | 229/390 [01:41<00:46,  3.45it/s] 59%|█████▉    | 230/390 [01:41<00:46,  3.46it/s] 59%|█████▉    | 231/390 [01:42<00:46,  3.46it/s] 59%|█████▉    | 232/390 [01:42<00:45,  3.46it/s] 60%|█████▉    | 233/390 [01:42<00:45,  3.46it/s] 60%|██████    | 234/390 [01:42<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:48:24,975 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:48:24,975 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:48:24,975 >>   Batch size = 8
{'eval_loss': 1.142250895500183, 'eval_runtime': 10.8393, 'eval_samples_per_second': 372.625, 'eval_steps_per_second': 46.59, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.97it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.39it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.61it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.99it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.45it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.21it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.00it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.70it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.65it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.70it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.68it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.67it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.66it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.65it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.56it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.69it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.53it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.48it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.55it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.61it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.58it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.61it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.53it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.41it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.50it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 46.44it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.32it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.42it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.48it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.56it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.51it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.57it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.62it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.65it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.48it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.45it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.44it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.53it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.45it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.58it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.48it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.51it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.56it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.53it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.50it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.41it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.38it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.54it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.57it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.57it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.58it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.66it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.49it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.51it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.34it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.46it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.55it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.59it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.57it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.63it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.55it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.51it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.46it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.56it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.28it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.41it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.50it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.56it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.62it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.61it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.51it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.48it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.46it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.46it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.50it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.56it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.52it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.56it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.48it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.50it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.46it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.51it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.46it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.45it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.42it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.50it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 45.42it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 45.85it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.07it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.11it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.19it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.30it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.35it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.45it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.37it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.31it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.44it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.53it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.55it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.62it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:10<00:00, 46.62it/s][A 60%|██████    | 234/390 [01:53<00:45,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:48:35,859 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 03:48:35,884 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:48:38,426 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:48:38,448 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:48:38,464 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:01<14:51,  5.75s/it] 61%|██████    | 236/390 [02:01<10:33,  4.11s/it] 61%|██████    | 237/390 [02:01<07:33,  2.97s/it] 61%|██████    | 238/390 [02:02<05:28,  2.16s/it] 61%|██████▏   | 239/390 [02:02<04:01,  1.60s/it] 62%|██████▏   | 240/390 [02:02<03:00,  1.21s/it] 62%|██████▏   | 241/390 [02:03<02:18,  1.07it/s] 62%|██████▏   | 242/390 [02:03<01:49,  1.35it/s] 62%|██████▏   | 243/390 [02:03<01:28,  1.66it/s] 63%|██████▎   | 244/390 [02:03<01:14,  1.97it/s] 63%|██████▎   | 245/390 [02:04<01:04,  2.26it/s] 63%|██████▎   | 246/390 [02:04<00:57,  2.52it/s] 63%|██████▎   | 247/390 [02:04<00:52,  2.74it/s] 64%|██████▎   | 248/390 [02:05<00:48,  2.93it/s] 64%|██████▍   | 249/390 [02:05<00:45,  3.07it/s] 64%|██████▍   | 250/390 [02:05<00:44,  3.18it/s] 64%|██████▍   | 251/390 [02:05<00:42,  3.26it/s] 65%|██████▍   | 252/390 [02:06<00:41,  3.32it/s] 65%|██████▍   | 253/390 [02:06<00:40,  3.37it/s] 65%|██████▌   | 254/390 [02:06<00:40,  3.39it/s] 65%|██████▌   | 255/390 [02:07<00:39,  3.42it/s] 66%|██████▌   | 256/390 [02:07<00:39,  3.43it/s] 66%|██████▌   | 257/390 [02:07<00:38,  3.44it/s] 66%|██████▌   | 258/390 [02:07<00:38,  3.45it/s] 66%|██████▋   | 259/390 [02:08<00:38,  3.43it/s] 67%|██████▋   | 260/390 [02:08<00:37,  3.44it/s] 67%|██████▋   | 261/390 [02:08<00:37,  3.45it/s] 67%|██████▋   | 262/390 [02:09<00:37,  3.46it/s] 67%|██████▋   | 263/390 [02:09<00:36,  3.46it/s] 68%|██████▊   | 264/390 [02:09<00:36,  3.46it/s] 68%|██████▊   | 265/390 [02:10<00:36,  3.46it/s] 68%|██████▊   | 266/390 [02:10<00:35,  3.46it/s] 68%|██████▊   | 267/390 [02:10<00:35,  3.46it/s] 69%|██████▊   | 268/390 [02:10<00:35,  3.46it/s] 69%|██████▉   | 269/390 [02:11<00:34,  3.46it/s] 69%|██████▉   | 270/390 [02:11<00:34,  3.45it/s] 69%|██████▉   | 271/390 [02:11<00:34,  3.45it/s] 70%|██████▉   | 272/390 [02:12<00:34,  3.46it/s] 70%|███████   | 273/390 [02:12<00:33,  3.46it/s] 70%|███████   | 274/390 [02:12<00:33,  3.46it/s] 71%|███████   | 275/390 [02:12<00:33,  3.46it/s] 71%|███████   | 276/390 [02:13<00:32,  3.46it/s] 71%|███████   | 277/390 [02:13<00:32,  3.46it/s] 71%|███████▏  | 278/390 [02:13<00:32,  3.46it/s] 72%|███████▏  | 279/390 [02:14<00:32,  3.46it/s] 72%|███████▏  | 280/390 [02:14<00:31,  3.46it/s] 72%|███████▏  | 281/390 [02:14<00:31,  3.46it/s] 72%|███████▏  | 282/390 [02:14<00:31,  3.46it/s] 73%|███████▎  | 283/390 [02:15<00:30,  3.46it/s] 73%|███████▎  | 284/390 [02:15<00:30,  3.46it/s] 73%|███████▎  | 285/390 [02:15<00:30,  3.46it/s] 73%|███████▎  | 286/390 [02:16<00:30,  3.46it/s] 74%|███████▎  | 287/390 [02:16<00:29,  3.46it/s] 74%|███████▍  | 288/390 [02:16<00:29,  3.46it/s] 74%|███████▍  | 289/390 [02:16<00:29,  3.46it/s] 74%|███████▍  | 290/390 [02:17<00:28,  3.46it/s] 75%|███████▍  | 291/390 [02:17<00:28,  3.46it/s] 75%|███████▍  | 292/390 [02:17<00:28,  3.46it/s] 75%|███████▌  | 293/390 [02:18<00:28,  3.46it/s] 75%|███████▌  | 294/390 [02:18<00:27,  3.46it/s] 76%|███████▌  | 295/390 [02:18<00:27,  3.46it/s] 76%|███████▌  | 296/390 [02:18<00:27,  3.46it/s] 76%|███████▌  | 297/390 [02:19<00:26,  3.46it/s] 76%|███████▋  | 298/390 [02:19<00:26,  3.46it/s] 77%|███████▋  | 299/390 [02:19<00:26,  3.46it/s] 77%|███████▋  | 300/390 [02:20<00:25,  3.46it/s] 77%|███████▋  | 301/390 [02:20<00:25,  3.46it/s] 77%|███████▋  | 302/390 [02:20<00:25,  3.46it/s] 78%|███████▊  | 303/390 [02:21<00:25,  3.45it/s] 78%|███████▊  | 304/390 [02:21<00:24,  3.46it/s] 78%|███████▊  | 305/390 [02:21<00:24,  3.46it/s] 78%|███████▊  | 306/390 [02:21<00:24,  3.46it/s] 79%|███████▊  | 307/390 [02:22<00:24,  3.46it/s] 79%|███████▉  | 308/390 [02:22<00:23,  3.46it/s] 79%|███████▉  | 309/390 [02:22<00:23,  3.46it/s] 79%|███████▉  | 310/390 [02:23<00:23,  3.46it/s] 80%|███████▉  | 311/390 [02:23<00:22,  3.46it/s] 80%|████████  | 312/390 [02:23<00:22,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:49:05,710 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:49:05,710 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:49:05,710 >>   Batch size = 8
{'eval_loss': 1.1600391864776611, 'eval_runtime': 10.8662, 'eval_samples_per_second': 371.704, 'eval_steps_per_second': 46.475, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.86it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.38it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.63it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.99it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.58it/s][A
  7%|▋         | 33/505 [00:00<00:09, 47.28it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.07it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.58it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.46it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.67it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.64it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.64it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.67it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.68it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.72it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.69it/s][A
 17%|█▋        | 88/505 [00:01<00:09, 46.05it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.55it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.52it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.54it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.50it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.63it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.65it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.68it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.63it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.59it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.45it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.37it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.35it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.42it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.55it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.58it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.62it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.62it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.55it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.52it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.47it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.51it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.44it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.58it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.55it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.58it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.64it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.66it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.55it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.46it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.44it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.47it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.51it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.53it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.61it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.62it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.58it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.52it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.54it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.47it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.26it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.53it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.50it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.57it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.59it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.62it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.56it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.56it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.46it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.44it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.45it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.56it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.59it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.58it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.54it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.51it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.43it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.47it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.41it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.49it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.43it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.46it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.47it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.57it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.56it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.60it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.44it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.44it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.42it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.46it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.51it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.57it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.47it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.52it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.46it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.54it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.39it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.46it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.43it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.46it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.43it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.46it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.52it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.67it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:10<00:00, 46.67it/s][A 80%|████████  | 312/390 [02:34<00:22,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:49:16,583 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 03:49:16,611 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:49:18,901 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:49:18,920 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:49:18,932 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:42<07:40,  5.98s/it] 81%|████████  | 314/390 [02:43<05:24,  4.28s/it] 81%|████████  | 315/390 [02:43<03:50,  3.08s/it] 81%|████████  | 316/390 [02:43<02:45,  2.24s/it] 81%|████████▏ | 317/390 [02:44<02:00,  1.66s/it] 82%|████████▏ | 318/390 [02:44<01:29,  1.24s/it] 82%|████████▏ | 319/390 [02:44<01:08,  1.04it/s] 82%|████████▏ | 320/390 [02:44<00:52,  1.32it/s] 82%|████████▏ | 321/390 [02:45<00:42,  1.62it/s] 83%|████████▎ | 322/390 [02:45<00:35,  1.93it/s] 83%|████████▎ | 323/390 [02:45<00:30,  2.23it/s] 83%|████████▎ | 324/390 [02:46<00:26,  2.49it/s] 83%|████████▎ | 325/390 [02:46<00:23,  2.72it/s] 84%|████████▎ | 326/390 [02:46<00:22,  2.91it/s] 84%|████████▍ | 327/390 [02:46<00:20,  3.06it/s] 84%|████████▍ | 328/390 [02:47<00:19,  3.17it/s] 84%|████████▍ | 329/390 [02:47<00:18,  3.26it/s] 85%|████████▍ | 330/390 [02:47<00:18,  3.32it/s] 85%|████████▍ | 331/390 [02:48<00:17,  3.36it/s] 85%|████████▌ | 332/390 [02:48<00:17,  3.39it/s] 85%|████████▌ | 333/390 [02:48<00:16,  3.41it/s] 86%|████████▌ | 334/390 [02:48<00:16,  3.43it/s] 86%|████████▌ | 335/390 [02:49<00:15,  3.44it/s] 86%|████████▌ | 336/390 [02:49<00:15,  3.43it/s] 86%|████████▋ | 337/390 [02:49<00:15,  3.44it/s] 87%|████████▋ | 338/390 [02:50<00:15,  3.45it/s] 87%|████████▋ | 339/390 [02:50<00:14,  3.45it/s] 87%|████████▋ | 340/390 [02:50<00:14,  3.46it/s] 87%|████████▋ | 341/390 [02:50<00:14,  3.46it/s] 88%|████████▊ | 342/390 [02:51<00:13,  3.46it/s] 88%|████████▊ | 343/390 [02:51<00:13,  3.46it/s] 88%|████████▊ | 344/390 [02:51<00:13,  3.47it/s] 88%|████████▊ | 345/390 [02:52<00:12,  3.46it/s] 89%|████████▊ | 346/390 [02:52<00:13,  3.38it/s] 89%|████████▉ | 347/390 [02:52<00:12,  3.38it/s] 89%|████████▉ | 348/390 [02:53<00:12,  3.40it/s] 89%|████████▉ | 349/390 [02:53<00:11,  3.42it/s] 90%|████████▉ | 350/390 [02:53<00:11,  3.43it/s] 90%|█████████ | 351/390 [02:53<00:11,  3.45it/s] 90%|█████████ | 352/390 [02:54<00:11,  3.45it/s] 91%|█████████ | 353/390 [02:54<00:10,  3.45it/s] 91%|█████████ | 354/390 [02:54<00:10,  3.46it/s] 91%|█████████ | 355/390 [02:55<00:10,  3.46it/s] 91%|█████████▏| 356/390 [02:55<00:09,  3.46it/s] 92%|█████████▏| 357/390 [02:55<00:09,  3.46it/s] 92%|█████████▏| 358/390 [02:55<00:09,  3.45it/s] 92%|█████████▏| 359/390 [02:56<00:08,  3.46it/s] 92%|█████████▏| 360/390 [02:56<00:08,  3.46it/s] 93%|█████████▎| 361/390 [02:56<00:08,  3.46it/s] 93%|█████████▎| 362/390 [02:57<00:08,  3.46it/s] 93%|█████████▎| 363/390 [02:57<00:07,  3.46it/s] 93%|█████████▎| 364/390 [02:57<00:07,  3.47it/s] 94%|█████████▎| 365/390 [02:57<00:07,  3.47it/s] 94%|█████████▍| 366/390 [02:58<00:06,  3.47it/s] 94%|█████████▍| 367/390 [02:58<00:06,  3.47it/s] 94%|█████████▍| 368/390 [02:58<00:06,  3.47it/s] 95%|█████████▍| 369/390 [02:59<00:06,  3.45it/s] 95%|█████████▍| 370/390 [02:59<00:05,  3.45it/s] 95%|█████████▌| 371/390 [02:59<00:05,  3.45it/s] 95%|█████████▌| 372/390 [02:59<00:05,  3.45it/s] 96%|█████████▌| 373/390 [03:00<00:04,  3.46it/s] 96%|█████████▌| 374/390 [03:00<00:04,  3.46it/s] 96%|█████████▌| 375/390 [03:00<00:04,  3.46it/s] 96%|█████████▋| 376/390 [03:01<00:04,  3.46it/s] 97%|█████████▋| 377/390 [03:01<00:03,  3.47it/s] 97%|█████████▋| 378/390 [03:01<00:03,  3.46it/s] 97%|█████████▋| 379/390 [03:01<00:03,  3.46it/s] 97%|█████████▋| 380/390 [03:02<00:02,  3.46it/s] 98%|█████████▊| 381/390 [03:02<00:02,  3.46it/s] 98%|█████████▊| 382/390 [03:02<00:02,  3.46it/s] 98%|█████████▊| 383/390 [03:03<00:02,  3.46it/s] 98%|█████████▊| 384/390 [03:03<00:01,  3.46it/s] 99%|█████████▊| 385/390 [03:03<00:01,  3.46it/s] 99%|█████████▉| 386/390 [03:03<00:01,  3.46it/s] 99%|█████████▉| 387/390 [03:04<00:00,  3.46it/s] 99%|█████████▉| 388/390 [03:04<00:00,  3.46it/s]100%|█████████▉| 389/390 [03:04<00:00,  3.46it/s]100%|██████████| 390/390 [03:05<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:49:47,213 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:49:47,214 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:49:47,214 >>   Batch size = 8
{'eval_loss': 1.1700291633605957, 'eval_runtime': 10.8568, 'eval_samples_per_second': 372.024, 'eval_steps_per_second': 46.514, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.31it/s][A
  2%|▏         | 12/505 [00:00<00:09, 50.36it/s][A
  4%|▎         | 18/505 [00:00<00:10, 48.68it/s][A
  5%|▍         | 23/505 [00:00<00:10, 47.97it/s][A
  6%|▌         | 28/505 [00:00<00:10, 47.45it/s][A
  7%|▋         | 33/505 [00:00<00:10, 47.16it/s][A
  8%|▊         | 38/505 [00:00<00:09, 47.03it/s][A
  9%|▊         | 43/505 [00:00<00:09, 46.77it/s][A
 10%|▉         | 48/505 [00:01<00:09, 46.62it/s][A
 10%|█         | 53/505 [00:01<00:09, 46.59it/s][A
 11%|█▏        | 58/505 [00:01<00:09, 46.63it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 46.65it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 46.71it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 46.73it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 46.64it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 46.64it/s][A
 17%|█▋        | 88/505 [00:01<00:08, 46.55it/s][A
 18%|█▊        | 93/505 [00:01<00:08, 46.31it/s][A
 19%|█▉        | 98/505 [00:02<00:08, 46.40it/s][A
 20%|██        | 103/505 [00:02<00:08, 46.41it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 46.46it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 46.57it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 46.61it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 46.70it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 46.62it/s][A
 26%|██▋       | 133/505 [00:02<00:07, 46.51it/s][A
 27%|██▋       | 138/505 [00:02<00:07, 46.49it/s][A
 28%|██▊       | 143/505 [00:03<00:07, 46.39it/s][A
 29%|██▉       | 148/505 [00:03<00:07, 46.46it/s][A
 30%|███       | 153/505 [00:03<00:07, 46.47it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 46.55it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 46.63it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 46.68it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 46.65it/s][A
 35%|███▌      | 178/505 [00:03<00:07, 46.60it/s][A
 36%|███▌      | 183/505 [00:03<00:06, 46.56it/s][A
 37%|███▋      | 188/505 [00:04<00:06, 46.42it/s][A
 38%|███▊      | 193/505 [00:04<00:06, 46.51it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 46.49it/s][A
 40%|████      | 203/505 [00:04<00:06, 46.59it/s][A
 41%|████      | 208/505 [00:04<00:06, 46.63it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 46.53it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 46.61it/s][A
 44%|████▍     | 223/505 [00:04<00:06, 46.64it/s][A
 45%|████▌     | 228/505 [00:04<00:05, 46.54it/s][A
 46%|████▌     | 233/505 [00:04<00:05, 46.57it/s][A
 47%|████▋     | 238/505 [00:05<00:05, 46.48it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 46.45it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 46.54it/s][A
 50%|█████     | 253/505 [00:05<00:05, 46.60it/s][A
 51%|█████     | 258/505 [00:05<00:05, 46.60it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 46.68it/s][A
 53%|█████▎    | 268/505 [00:05<00:05, 46.56it/s][A
 54%|█████▍    | 273/505 [00:05<00:04, 46.49it/s][A
 55%|█████▌    | 278/505 [00:05<00:04, 46.44it/s][A
 56%|█████▌    | 283/505 [00:06<00:04, 46.44it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 46.44it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 46.45it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 46.54it/s][A
 60%|██████    | 303/505 [00:06<00:04, 46.59it/s][A
 61%|██████    | 308/505 [00:06<00:04, 46.60it/s][A
 62%|██████▏   | 313/505 [00:06<00:04, 46.59it/s][A
 63%|██████▎   | 318/505 [00:06<00:04, 46.52it/s][A
 64%|██████▍   | 323/505 [00:06<00:03, 46.60it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 46.45it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 46.38it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 46.48it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 46.52it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 46.52it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 46.61it/s][A
 71%|███████   | 358/505 [00:07<00:03, 46.53it/s][A
 72%|███████▏  | 363/505 [00:07<00:03, 46.58it/s][A
 73%|███████▎  | 368/505 [00:07<00:02, 46.50it/s][A
 74%|███████▍  | 373/505 [00:07<00:02, 46.38it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 46.37it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 46.37it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 46.46it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 46.58it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 46.46it/s][A
 80%|███████▉  | 403/505 [00:08<00:02, 46.45it/s][A
 81%|████████  | 408/505 [00:08<00:02, 46.44it/s][A
 82%|████████▏ | 413/505 [00:08<00:01, 46.42it/s][A
 83%|████████▎ | 418/505 [00:08<00:01, 46.39it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 46.39it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 46.36it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 46.41it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 46.38it/s][A
 88%|████████▊ | 443/505 [00:09<00:01, 46.55it/s][A
 89%|████████▊ | 448/505 [00:09<00:01, 46.50it/s][A
 90%|████████▉ | 453/505 [00:09<00:01, 46.61it/s][A
 91%|█████████ | 458/505 [00:09<00:01, 46.58it/s][A
 92%|█████████▏| 463/505 [00:09<00:00, 46.53it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 46.49it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 46.43it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 46.41it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 46.50it/s][A
 97%|█████████▋| 488/505 [00:10<00:00, 46.58it/s][A
 98%|█████████▊| 493/505 [00:10<00:00, 46.50it/s][A
 99%|█████████▊| 498/505 [00:10<00:00, 46.54it/s][A
100%|█████████▉| 503/505 [00:10<00:00, 46.61it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:10<00:00, 46.61it/s][A100%|██████████| 390/390 [03:16<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:49:58,090 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-29 03:49:58,118 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:50:00,902 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:50:00,923 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:50:00,932 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:50:05,309 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:50:05,312 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78 (score: 1.1178969144821167).
                                                 100%|██████████| 390/390 [03:25<00:00,  3.46it/s]100%|██████████| 390/390 [03:25<00:00,  1.90it/s]
[INFO|trainer.py:1894] 2023-08-29 03:50:07,147 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 03:50:07,186 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:50:09,395 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:50:09,414 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:50:09,423 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:50:09,618 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   train_loss               =     0.3765
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   train_runtime            = 0:03:25.08
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   train_samples_per_second =    121.901
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:09,618 >>   train_steps_per_second   =      1.902
{'eval_loss': 1.1773966550827026, 'eval_runtime': 10.8547, 'eval_samples_per_second': 372.096, 'eval_steps_per_second': 46.524, 'epoch': 4.99}
{'train_runtime': 205.0846, 'train_samples_per_second': 121.901, 'train_steps_per_second': 1.902, 'train_loss': 0.37645338009565305, 'epoch': 4.99}
08/29/2023 03:50:09 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:50:09,660 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:50:09,660 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 03:50:09,660 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 58.73it/s]  2%|▏         | 12/505 [00:00<00:09, 51.14it/s]  4%|▎         | 18/505 [00:00<00:09, 49.20it/s]  5%|▍         | 23/505 [00:00<00:09, 48.52it/s]  6%|▌         | 28/505 [00:00<00:09, 48.08it/s]  7%|▋         | 33/505 [00:00<00:09, 47.77it/s]  8%|▊         | 38/505 [00:00<00:09, 47.54it/s]  9%|▊         | 43/505 [00:00<00:09, 47.37it/s] 10%|▉         | 48/505 [00:00<00:09, 47.18it/s] 10%|█         | 53/505 [00:01<00:09, 47.11it/s] 11%|█▏        | 58/505 [00:01<00:09, 47.05it/s] 12%|█▏        | 63/505 [00:01<00:09, 46.89it/s] 13%|█▎        | 68/505 [00:01<00:09, 47.01it/s] 14%|█▍        | 73/505 [00:01<00:09, 46.97it/s] 15%|█▌        | 78/505 [00:01<00:09, 46.87it/s] 16%|█▋        | 83/505 [00:01<00:08, 46.94it/s] 17%|█▋        | 88/505 [00:01<00:08, 46.86it/s] 18%|█▊        | 93/505 [00:01<00:08, 46.84it/s] 19%|█▉        | 98/505 [00:02<00:08, 46.89it/s] 20%|██        | 103/505 [00:02<00:08, 46.79it/s] 21%|██▏       | 108/505 [00:02<00:08, 46.90it/s] 22%|██▏       | 113/505 [00:02<00:08, 46.81it/s] 23%|██▎       | 118/505 [00:02<00:08, 46.90it/s] 24%|██▍       | 123/505 [00:02<00:08, 46.88it/s] 25%|██▌       | 128/505 [00:02<00:08, 46.97it/s] 26%|██▋       | 133/505 [00:02<00:07, 46.76it/s] 27%|██▋       | 138/505 [00:02<00:07, 46.87it/s] 28%|██▊       | 143/505 [00:03<00:07, 46.78it/s] 29%|██▉       | 148/505 [00:03<00:07, 46.74it/s] 30%|███       | 153/505 [00:03<00:07, 46.85it/s] 31%|███▏      | 158/505 [00:03<00:07, 46.95it/s] 32%|███▏      | 163/505 [00:03<00:07, 46.81it/s] 33%|███▎      | 168/505 [00:03<00:07, 46.88it/s] 34%|███▍      | 173/505 [00:03<00:07, 46.91it/s] 35%|███▌      | 178/505 [00:03<00:06, 46.86it/s] 36%|███▌      | 183/505 [00:03<00:06, 46.81it/s] 37%|███▋      | 188/505 [00:03<00:06, 46.84it/s] 38%|███▊      | 193/505 [00:04<00:06, 46.68it/s] 39%|███▉      | 198/505 [00:04<00:06, 46.74it/s] 40%|████      | 203/505 [00:04<00:06, 46.68it/s] 41%|████      | 208/505 [00:04<00:06, 46.86it/s] 42%|████▏     | 213/505 [00:04<00:06, 46.78it/s] 43%|████▎     | 218/505 [00:04<00:06, 46.97it/s] 44%|████▍     | 223/505 [00:04<00:06, 46.87it/s] 45%|████▌     | 228/505 [00:04<00:05, 46.95it/s] 46%|████▌     | 233/505 [00:04<00:05, 46.93it/s] 47%|████▋     | 238/505 [00:05<00:05, 46.86it/s] 48%|████▊     | 243/505 [00:05<00:05, 46.72it/s] 49%|████▉     | 248/505 [00:05<00:05, 46.79it/s] 50%|█████     | 253/505 [00:05<00:05, 46.82it/s] 51%|█████     | 258/505 [00:05<00:05, 46.88it/s] 52%|█████▏    | 263/505 [00:05<00:05, 46.83it/s] 53%|█████▎    | 268/505 [00:05<00:05, 46.84it/s] 54%|█████▍    | 273/505 [00:05<00:04, 46.76it/s] 55%|█████▌    | 278/505 [00:05<00:04, 46.85it/s] 56%|█████▌    | 283/505 [00:06<00:04, 46.77it/s] 57%|█████▋    | 288/505 [00:06<00:04, 46.72it/s] 58%|█████▊    | 293/505 [00:06<00:04, 46.71it/s] 59%|█████▉    | 298/505 [00:06<00:04, 46.77it/s] 60%|██████    | 303/505 [00:06<00:04, 46.89it/s] 61%|██████    | 308/505 [00:06<00:04, 46.94it/s] 62%|██████▏   | 313/505 [00:06<00:04, 46.76it/s] 63%|██████▎   | 318/505 [00:06<00:03, 46.84it/s] 64%|██████▍   | 323/505 [00:06<00:03, 46.79it/s] 65%|██████▍   | 328/505 [00:06<00:03, 46.78it/s] 66%|██████▌   | 333/505 [00:07<00:03, 46.74it/s] 67%|██████▋   | 338/505 [00:07<00:03, 46.59it/s] 68%|██████▊   | 343/505 [00:07<00:03, 46.74it/s] 69%|██████▉   | 348/505 [00:07<00:03, 46.76it/s] 70%|██████▉   | 353/505 [00:07<00:03, 46.75it/s] 71%|███████   | 358/505 [00:07<00:03, 46.80it/s] 72%|███████▏  | 363/505 [00:07<00:03, 46.75it/s] 73%|███████▎  | 368/505 [00:07<00:02, 46.73it/s] 74%|███████▍  | 373/505 [00:07<00:02, 46.75it/s] 75%|███████▍  | 378/505 [00:08<00:02, 46.68it/s] 76%|███████▌  | 383/505 [00:08<00:02, 46.71it/s] 77%|███████▋  | 388/505 [00:08<00:02, 46.83it/s] 78%|███████▊  | 393/505 [00:08<00:02, 46.80it/s] 79%|███████▉  | 398/505 [00:08<00:02, 46.78it/s] 80%|███████▉  | 403/505 [00:08<00:02, 46.76it/s] 81%|████████  | 408/505 [00:08<00:02, 46.71it/s] 82%|████████▏ | 413/505 [00:08<00:01, 46.68it/s] 83%|████████▎ | 418/505 [00:08<00:01, 46.64it/s] 84%|████████▍ | 423/505 [00:09<00:01, 46.62it/s] 85%|████████▍ | 428/505 [00:09<00:01, 46.67it/s] 86%|████████▌ | 433/505 [00:09<00:01, 46.67it/s] 87%|████████▋ | 438/505 [00:09<00:01, 46.69it/s] 88%|████████▊ | 443/505 [00:09<00:01, 46.76it/s] 89%|████████▊ | 448/505 [00:09<00:01, 46.73it/s] 90%|████████▉ | 453/505 [00:09<00:01, 46.69it/s] 91%|█████████ | 458/505 [00:09<00:01, 46.77it/s] 92%|█████████▏| 463/505 [00:09<00:00, 46.74it/s] 93%|█████████▎| 468/505 [00:09<00:00, 46.53it/s] 94%|█████████▎| 473/505 [00:10<00:00, 46.53it/s] 95%|█████████▍| 478/505 [00:10<00:00, 46.57it/s] 96%|█████████▌| 483/505 [00:10<00:00, 46.70it/s] 97%|█████████▋| 488/505 [00:10<00:00, 46.80it/s] 98%|█████████▊| 493/505 [00:10<00:00, 46.72it/s] 99%|█████████▊| 498/505 [00:10<00:00, 46.75it/s]100%|█████████▉| 503/505 [00:10<00:00, 46.68it/s]100%|██████████| 505/505 [00:10<00:00, 46.88it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:50:20,455 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   eval_loss               =     1.1179
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   eval_runtime            = 0:00:10.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   eval_samples_per_second =    374.139
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   eval_steps_per_second   =     46.779
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:50:20,456 >>   perplexity              =     3.0584
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:50:27,824 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:50:27,825 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:28,516 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:29,516 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:29,516 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:32,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:32,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:32,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:32,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:32,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:50:32,987 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:50:32,990 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:33,547 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:33,697 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:33,697 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.56it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:31,  1.42it/s]Extractor Predicting: 47it [00:31,  1.43it/s]Extractor Predicting: 48it [00:32,  1.39it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:34,  1.50it/s]Extractor Predicting: 52it [00:35,  1.54it/s]Extractor Predicting: 53it [00:35,  1.54it/s]Extractor Predicting: 54it [00:36,  1.54it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:39,  1.47it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:45,  1.49it/s]Extractor Predicting: 68it [00:45,  1.46it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:47,  1.46it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:48,  1.42it/s]Extractor Predicting: 73it [00:49,  1.42it/s]Extractor Predicting: 74it [00:50,  1.38it/s]Extractor Predicting: 75it [00:50,  1.41it/s]Extractor Predicting: 76it [00:51,  1.43it/s]Extractor Predicting: 77it [00:52,  1.43it/s]Extractor Predicting: 78it [00:52,  1.44it/s]Extractor Predicting: 79it [00:53,  1.45it/s]Extractor Predicting: 80it [00:54,  1.45it/s]Extractor Predicting: 81it [00:54,  1.46it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:56,  1.46it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:57,  1.47it/s]Extractor Predicting: 86it [00:58,  1.48it/s]Extractor Predicting: 87it [00:58,  1.51it/s]Extractor Predicting: 88it [00:59,  1.52it/s]Extractor Predicting: 89it [01:00,  1.50it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.52it/s]Extractor Predicting: 92it [01:02,  1.53it/s]Extractor Predicting: 93it [01:02,  1.52it/s]Extractor Predicting: 94it [01:03,  1.50it/s]Extractor Predicting: 95it [01:04,  1.48it/s]Extractor Predicting: 96it [01:04,  1.46it/s]Extractor Predicting: 97it [01:05,  1.46it/s]Extractor Predicting: 98it [01:06,  1.46it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.43it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:09,  1.44it/s]Extractor Predicting: 104it [01:10,  1.47it/s]Extractor Predicting: 105it [01:11,  1.47it/s]Extractor Predicting: 106it [01:11,  1.49it/s]Extractor Predicting: 107it [01:12,  1.50it/s]Extractor Predicting: 108it [01:12,  1.52it/s]Extractor Predicting: 109it [01:13,  1.52it/s]Extractor Predicting: 110it [01:14,  1.50it/s]Extractor Predicting: 111it [01:15,  1.47it/s]Extractor Predicting: 112it [01:15,  1.50it/s]Extractor Predicting: 113it [01:16,  1.50it/s]Extractor Predicting: 114it [01:17,  1.36it/s]Extractor Predicting: 115it [01:17,  1.43it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.45it/s]Extractor Predicting: 118it [01:19,  1.41it/s]Extractor Predicting: 119it [01:20,  1.45it/s]Extractor Predicting: 120it [01:21,  1.46it/s]Extractor Predicting: 121it [01:21,  1.44it/s]Extractor Predicting: 122it [01:22,  1.46it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.37it/s]Extractor Predicting: 126it [01:25,  1.41it/s]Extractor Predicting: 127it [01:26,  1.37it/s]Extractor Predicting: 128it [01:27,  1.36it/s]Extractor Predicting: 129it [01:27,  1.36it/s]Extractor Predicting: 130it [01:28,  1.36it/s]Extractor Predicting: 131it [01:29,  1.34it/s]Extractor Predicting: 132it [01:30,  1.34it/s]Extractor Predicting: 133it [01:30,  1.35it/s]Extractor Predicting: 134it [01:31,  1.39it/s]Extractor Predicting: 135it [01:32,  1.40it/s]Extractor Predicting: 136it [01:32,  1.40it/s]Extractor Predicting: 137it [01:33,  1.43it/s]Extractor Predicting: 138it [01:34,  1.38it/s]Extractor Predicting: 139it [01:34,  1.38it/s]Extractor Predicting: 140it [01:35,  1.38it/s]Extractor Predicting: 141it [01:36,  1.38it/s]Extractor Predicting: 142it [01:37,  1.39it/s]Extractor Predicting: 143it [01:37,  1.39it/s]Extractor Predicting: 144it [01:38,  1.40it/s]Extractor Predicting: 145it [01:39,  1.38it/s]Extractor Predicting: 146it [01:40,  1.39it/s]Extractor Predicting: 147it [01:40,  1.40it/s]Extractor Predicting: 148it [01:41,  1.36it/s]Extractor Predicting: 149it [01:42,  1.40it/s]Extractor Predicting: 149it [01:42,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:22,556 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:22,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:22,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:22,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:22,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:52:22,869 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:52:22,870 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:52:23,541 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:52:24,549 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:52:24,549 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:26,728 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:26,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:26,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:26,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:52:26,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:52:27,061 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:52:27,062 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:52:27,737 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:52:27,884 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:52:27,884 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26305220883534136,
  "recall": 0.06486754147066105,
  "score": 0.10407149950347566,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:10,  1.72it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:15,  1.65it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:22,  1.62it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.51it/s]Extractor Predicting: 46it [00:28,  1.49it/s]Extractor Predicting: 47it [00:28,  1.52it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:29,  1.58it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:31,  1.60it/s]Extractor Predicting: 53it [00:32,  1.50it/s]Extractor Predicting: 54it [00:33,  1.53it/s]Extractor Predicting: 55it [00:33,  1.54it/s]Extractor Predicting: 56it [00:34,  1.51it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:35,  1.55it/s]Extractor Predicting: 59it [00:36,  1.53it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:37,  1.55it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:38,  1.56it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.61it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.60it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:42,  1.57it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:44,  1.56it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:45,  1.56it/s]Extractor Predicting: 75it [00:46,  1.58it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.52it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.56it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:51,  1.53it/s]Extractor Predicting: 84it [00:52,  1.56it/s]Extractor Predicting: 85it [00:53,  1.55it/s]Extractor Predicting: 86it [00:53,  1.53it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:54,  1.57it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:56,  1.61it/s]Extractor Predicting: 91it [00:56,  1.60it/s]Extractor Predicting: 92it [00:57,  1.60it/s]Extractor Predicting: 93it [00:57,  1.63it/s]Extractor Predicting: 94it [00:58,  1.63it/s]Extractor Predicting: 95it [00:59,  1.61it/s]Extractor Predicting: 96it [00:59,  1.65it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:01,  1.64it/s]Extractor Predicting: 99it [01:01,  1.62it/s]Extractor Predicting: 100it [01:02,  1.65it/s]Extractor Predicting: 101it [01:02,  1.64it/s]Extractor Predicting: 102it [01:03,  1.59it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:04,  1.61it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:07,  1.57it/s]Extractor Predicting: 109it [01:07,  1.56it/s]Extractor Predicting: 110it [01:08,  1.57it/s]Extractor Predicting: 111it [01:09,  1.57it/s]Extractor Predicting: 112it [01:09,  1.57it/s]Extractor Predicting: 113it [01:10,  1.60it/s]Extractor Predicting: 114it [01:11,  1.59it/s]Extractor Predicting: 115it [01:11,  1.60it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:14,  1.68it/s]Extractor Predicting: 120it [01:14,  1.67it/s]Extractor Predicting: 121it [01:15,  1.67it/s]Extractor Predicting: 122it [01:15,  1.66it/s]Extractor Predicting: 123it [01:16,  1.63it/s]Extractor Predicting: 124it [01:17,  1.65it/s]Extractor Predicting: 125it [01:17,  1.64it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.58it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:21,  1.53it/s]Extractor Predicting: 131it [01:21,  1.51it/s]Extractor Predicting: 132it [01:22,  1.53it/s]Extractor Predicting: 133it [01:23,  1.54it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:24,  1.52it/s]Extractor Predicting: 136it [01:25,  1.39it/s]Extractor Predicting: 137it [01:25,  1.41it/s]Extractor Predicting: 138it [01:26,  1.45it/s]Extractor Predicting: 139it [01:27,  1.47it/s]Extractor Predicting: 140it [01:27,  1.45it/s]Extractor Predicting: 141it [01:28,  1.46it/s]Extractor Predicting: 142it [01:29,  1.47it/s]Extractor Predicting: 143it [01:29,  1.50it/s]Extractor Predicting: 144it [01:30,  1.49it/s]Extractor Predicting: 145it [01:31,  1.48it/s]Extractor Predicting: 146it [01:31,  1.48it/s]Extractor Predicting: 147it [01:32,  1.50it/s]Extractor Predicting: 148it [01:33,  1.50it/s]Extractor Predicting: 149it [01:33,  1.53it/s]Extractor Predicting: 150it [01:34,  1.52it/s]Extractor Predicting: 151it [01:35,  1.51it/s]Extractor Predicting: 152it [01:35,  1.51it/s]Extractor Predicting: 153it [01:36,  1.49it/s]Extractor Predicting: 154it [01:37,  1.49it/s]Extractor Predicting: 155it [01:37,  1.50it/s]Extractor Predicting: 156it [01:38,  1.55it/s]Extractor Predicting: 157it [01:39,  1.61it/s]Extractor Predicting: 158it [01:39,  1.61it/s]Extractor Predicting: 159it [01:40,  1.61it/s]Extractor Predicting: 160it [01:40,  1.62it/s]Extractor Predicting: 161it [01:41,  1.66it/s]Extractor Predicting: 162it [01:42,  1.61it/s]Extractor Predicting: 163it [01:42,  1.60it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:43,  1.66it/s]Extractor Predicting: 166it [01:44,  1.65it/s]Extractor Predicting: 167it [01:45,  1.70it/s]Extractor Predicting: 168it [01:45,  1.71it/s]Extractor Predicting: 169it [01:46,  1.74it/s]Extractor Predicting: 170it [01:46,  1.69it/s]Extractor Predicting: 171it [01:47,  1.62it/s]Extractor Predicting: 172it [01:48,  1.59it/s]Extractor Predicting: 173it [01:48,  1.62it/s]Extractor Predicting: 173it [01:48,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:24,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:24,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:24,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:24,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:24,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:54:24,964 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:54:24,965 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:54:25,548 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:54:26,534 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:54:26,535 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:29,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:29,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:29,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:29,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:29,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:54:30,163 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:54:30,164 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:54:30,736 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:54:30,893 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:54:30,893 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21110529072812992,
  "recall": 0.09720212252773758,
  "score": 0.13311312964492156,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.40it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.33it/s]Extractor Predicting: 12it [00:08,  1.31it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.55it/s]Extractor Predicting: 16it [00:10,  1.63it/s]Extractor Predicting: 17it [00:11,  1.71it/s]Extractor Predicting: 18it [00:11,  1.76it/s]Extractor Predicting: 19it [00:12,  1.77it/s]Extractor Predicting: 20it [00:12,  1.80it/s]Extractor Predicting: 21it [00:13,  1.84it/s]Extractor Predicting: 22it [00:14,  1.80it/s]Extractor Predicting: 23it [00:14,  1.79it/s]Extractor Predicting: 24it [00:15,  1.78it/s]Extractor Predicting: 25it [00:15,  1.78it/s]Extractor Predicting: 26it [00:16,  1.82it/s]Extractor Predicting: 27it [00:16,  1.78it/s]Extractor Predicting: 28it [00:17,  1.80it/s]Extractor Predicting: 29it [00:17,  1.80it/s]Extractor Predicting: 30it [00:18,  1.84it/s]Extractor Predicting: 31it [00:19,  1.85it/s]Extractor Predicting: 32it [00:19,  1.84it/s]Extractor Predicting: 33it [00:20,  1.85it/s]Extractor Predicting: 34it [00:20,  1.84it/s]Extractor Predicting: 35it [00:21,  1.84it/s]Extractor Predicting: 36it [00:21,  1.79it/s]Extractor Predicting: 37it [00:22,  1.75it/s]Extractor Predicting: 38it [00:22,  1.81it/s]Extractor Predicting: 39it [00:23,  1.80it/s]Extractor Predicting: 40it [00:24,  1.81it/s]Extractor Predicting: 41it [00:24,  1.80it/s]Extractor Predicting: 42it [00:25,  1.84it/s]Extractor Predicting: 43it [00:25,  1.80it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.57it/s]Extractor Predicting: 46it [00:27,  1.50it/s]Extractor Predicting: 47it [00:28,  1.47it/s]Extractor Predicting: 48it [00:29,  1.45it/s]Extractor Predicting: 49it [00:29,  1.45it/s]Extractor Predicting: 50it [00:30,  1.44it/s]Extractor Predicting: 51it [00:31,  1.43it/s]Extractor Predicting: 52it [00:32,  1.42it/s]Extractor Predicting: 53it [00:32,  1.41it/s]Extractor Predicting: 54it [00:33,  1.41it/s]Extractor Predicting: 55it [00:34,  1.43it/s]Extractor Predicting: 56it [00:34,  1.41it/s]Extractor Predicting: 57it [00:35,  1.40it/s]Extractor Predicting: 58it [00:36,  1.41it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 59it [00:36,  1.61it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7712952158693116,
  "recall": 0.21370837374717103,
  "score": 0.3346835443037975,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
