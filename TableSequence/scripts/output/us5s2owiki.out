Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:41, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:02, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:49<01:55, 16.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:07<01:43, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:21<01:19, 15.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:35<01:01, 15.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:50<00:45, 15.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:03<00:29, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:19<00:14, 14.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:39<00:00, 16.62s/it]Generating: 100%|██████████| 10/10 [02:39<00:00, 15.97s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 222, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 305, 'raw': 480}
{'target': 600, 'success': 326, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 370, 'raw': 576}
{'target': 600, 'success': 392, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 439, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 505, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 564, 'raw': 864}
{'target': 600, 'success': 585, 'raw': 896}
{'target': 600, 'success': 607, 'raw': 928}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.6540948275862069, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 519, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : official language .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Nettles', 'lyrics by', '', 'Nettles is also an album which is set to be released in March 2015 by Columbia Records . #')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1790 88 ) , a fleet of warships , the Royal Navy , raided the town of Old Wharf in the East Riding , Cornwall . Head Entity : royal navy , Tail Entity : the Royal Navy .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8973214285714286, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.775, 'errors': {'', "('the t.', 'use', '', 'He gained the nickname D. M. C. , since he had used the same name , and became the first American surgeon to use the penultimate vowel , the t.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 121, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 161, 'raw': 288}
{'target': 600, 'success': 177, 'raw': 320}
{'target': 600, 'success': 192, 'raw': 352}
{'target': 600, 'success': 213, 'raw': 384}
{'target': 600, 'success': 227, 'raw': 416}
{'target': 600, 'success': 243, 'raw': 448}
{'target': 600, 'success': 266, 'raw': 480}
{'target': 600, 'success': 285, 'raw': 512}
{'target': 600, 'success': 301, 'raw': 544}
{'target': 600, 'success': 319, 'raw': 576}
{'target': 600, 'success': 339, 'raw': 608}
{'target': 600, 'success': 358, 'raw': 640}
{'target': 600, 'success': 382, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 422, 'raw': 736}
{'target': 600, 'success': 438, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 489, 'raw': 864}
{'target': 600, 'success': 505, 'raw': 896}
{'target': 600, 'success': 525, 'raw': 928}
{'target': 600, 'success': 541, 'raw': 960}
{'target': 600, 'success': 559, 'raw': 992}
{'target': 600, 'success': 580, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5681818181818182, 'errors': {'', "('The Tempest', 'voice type', '', 'He has contributed to the BBC science fiction drama series The Tempest , which became an adaptation of the story of The Tempest first published in 1981 .')", "('Hans Euler', 'voice type', '', 'He is a member of the Danish House of Lords as a member of the Prime Minister ( Hans Euler ) of Sweden .')", "('Jürgen Habermas', 'voice type', '', 'The soundtrack was composed by Theodor Reichert , based on the novel of the same name by Jürgen Habermas .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 12825
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12925, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.56s/it]Extractor Estimating: 2it [00:16,  7.15s/it]Extractor Estimating: 3it [00:17,  4.15s/it]Extractor Estimating: 4it [00:18,  2.96s/it]Extractor Estimating: 5it [00:24,  3.93s/it]Extractor Estimating: 6it [00:24,  2.80s/it]Extractor Estimating: 7it [00:25,  2.08s/it]Extractor Estimating: 8it [00:25,  1.61s/it]Extractor Estimating: 9it [00:26,  1.28s/it]Extractor Estimating: 10it [00:27,  1.08s/it]Extractor Estimating: 11it [00:27,  1.03it/s]Extractor Estimating: 12it [00:28,  1.16it/s]Extractor Estimating: 13it [00:29,  1.28it/s]Extractor Estimating: 14it [00:29,  1.37it/s]Extractor Estimating: 15it [00:30,  1.44it/s]Extractor Estimating: 16it [00:30,  1.48it/s]Extractor Estimating: 17it [00:31,  1.53it/s]Extractor Estimating: 18it [00:32,  1.55it/s]Extractor Estimating: 19it [00:32,  1.54it/s]Extractor Estimating: 20it [00:33,  1.54it/s]Extractor Estimating: 21it [00:34,  1.58it/s]Extractor Estimating: 22it [00:34,  1.58it/s]Extractor Estimating: 23it [00:35,  1.56it/s]Extractor Estimating: 24it [00:36,  1.53it/s]Extractor Estimating: 25it [00:36,  1.57it/s]Extractor Estimating: 26it [00:37,  1.61it/s]Extractor Estimating: 27it [00:37,  1.59it/s]Extractor Estimating: 28it [00:38,  1.62it/s]Extractor Estimating: 29it [00:39,  1.67it/s]Extractor Estimating: 30it [00:39,  1.65it/s]Extractor Estimating: 31it [00:40,  1.65it/s]Extractor Estimating: 32it [00:40,  1.57it/s]Extractor Estimating: 33it [00:41,  1.61it/s]Extractor Estimating: 34it [00:42,  1.64it/s]Extractor Estimating: 35it [00:42,  1.57it/s]Extractor Estimating: 36it [00:43,  1.64it/s]Extractor Estimating: 37it [00:44,  1.61it/s]Extractor Estimating: 38it [00:44,  1.59it/s]Extractor Estimating: 39it [00:45,  1.63it/s]Extractor Estimating: 40it [00:45,  1.60it/s]Extractor Estimating: 41it [00:46,  1.56it/s]Extractor Estimating: 42it [00:47,  1.60it/s]Extractor Estimating: 43it [00:47,  1.64it/s]Extractor Estimating: 44it [00:48,  1.68it/s]Extractor Estimating: 45it [00:48,  1.68it/s]Extractor Estimating: 46it [00:49,  1.68it/s]Extractor Estimating: 47it [00:50,  1.66it/s]Extractor Estimating: 48it [00:50,  1.68it/s]Extractor Estimating: 49it [00:51,  1.65it/s]Extractor Estimating: 50it [00:51,  1.67it/s]Extractor Estimating: 51it [00:52,  1.65it/s]Extractor Estimating: 52it [00:53,  1.60it/s]Extractor Estimating: 53it [00:53,  1.62it/s]Extractor Estimating: 54it [00:54,  1.61it/s]Extractor Estimating: 55it [00:55,  1.63it/s]Extractor Estimating: 56it [00:55,  1.58it/s]Extractor Estimating: 57it [00:56,  1.55it/s]Extractor Estimating: 58it [00:56,  1.63it/s]Extractor Estimating: 59it [00:57,  1.65it/s]Extractor Estimating: 60it [00:58,  1.62it/s]Extractor Estimating: 61it [00:58,  1.59it/s]Extractor Estimating: 62it [00:59,  1.62it/s]Extractor Estimating: 63it [01:00,  1.63it/s]Extractor Estimating: 64it [01:00,  1.62it/s]Extractor Estimating: 65it [01:01,  1.61it/s]Extractor Estimating: 66it [01:01,  1.60it/s]Extractor Estimating: 67it [01:02,  1.59it/s]Extractor Estimating: 68it [01:03,  1.60it/s]Extractor Estimating: 69it [01:03,  1.62it/s]Extractor Estimating: 70it [01:04,  1.60it/s]Extractor Estimating: 71it [01:05,  1.59it/s]Extractor Estimating: 72it [01:05,  1.59it/s]Extractor Estimating: 73it [01:06,  1.60it/s]Extractor Estimating: 74it [01:06,  1.62it/s]Extractor Estimating: 75it [01:07,  1.64it/s]Extractor Estimating: 76it [01:08,  1.69it/s]Extractor Estimating: 77it [01:08,  1.58it/s]Extractor Estimating: 78it [01:09,  1.58it/s]Extractor Estimating: 79it [01:09,  1.65it/s]Extractor Estimating: 80it [01:10,  1.67it/s]Extractor Estimating: 81it [01:11,  1.66it/s]Extractor Estimating: 82it [01:11,  1.62it/s]Extractor Estimating: 83it [01:12,  1.60it/s]Extractor Estimating: 84it [01:13,  1.55it/s]Extractor Estimating: 85it [01:13,  1.53it/s]Extractor Estimating: 86it [01:14,  1.55it/s]Extractor Estimating: 87it [01:15,  1.52it/s]Extractor Estimating: 88it [01:15,  1.58it/s]Extractor Estimating: 89it [01:16,  1.58it/s]Extractor Estimating: 90it [01:16,  1.58it/s]Extractor Estimating: 91it [01:17,  1.57it/s]Extractor Estimating: 92it [01:18,  1.59it/s]Extractor Estimating: 93it [01:18,  1.55it/s]Extractor Estimating: 94it [01:19,  1.54it/s]Extractor Estimating: 95it [01:20,  1.55it/s]Extractor Estimating: 96it [01:20,  1.54it/s]Extractor Estimating: 97it [01:21,  1.42it/s]Extractor Estimating: 98it [01:22,  1.46it/s]Extractor Estimating: 99it [01:22,  1.49it/s]Extractor Estimating: 100it [01:23,  1.53it/s]Extractor Estimating: 101it [01:24,  1.55it/s]Extractor Estimating: 102it [01:24,  1.54it/s]Extractor Estimating: 103it [01:25,  1.57it/s]Extractor Estimating: 104it [01:26,  1.60it/s]Extractor Estimating: 105it [01:26,  1.63it/s]Extractor Estimating: 106it [01:27,  1.61it/s]Extractor Estimating: 107it [01:27,  1.60it/s]Extractor Estimating: 108it [01:28,  1.63it/s]Extractor Estimating: 109it [01:29,  1.60it/s]Extractor Estimating: 110it [01:29,  1.62it/s]Extractor Estimating: 111it [01:30,  1.60it/s]Extractor Estimating: 112it [01:31,  1.61it/s]Extractor Estimating: 113it [01:31,  1.58it/s]Extractor Estimating: 114it [01:32,  1.62it/s]Extractor Estimating: 115it [01:32,  1.63it/s]Extractor Estimating: 116it [01:33,  1.65it/s]Extractor Estimating: 117it [01:34,  1.63it/s]Extractor Estimating: 118it [01:34,  1.66it/s]Extractor Estimating: 119it [01:35,  1.66it/s]Extractor Estimating: 120it [01:35,  1.63it/s]Extractor Estimating: 121it [01:36,  1.63it/s]Extractor Estimating: 122it [01:37,  1.65it/s]Extractor Estimating: 123it [01:37,  1.63it/s]Extractor Estimating: 124it [01:38,  1.62it/s]Extractor Estimating: 125it [01:38,  1.62it/s]Extractor Estimating: 126it [01:39,  1.63it/s]Extractor Estimating: 127it [01:40,  1.58it/s]Extractor Estimating: 128it [01:40,  1.64it/s]Extractor Estimating: 129it [01:41,  1.59it/s]Extractor Estimating: 130it [01:42,  1.55it/s]Extractor Estimating: 131it [01:42,  1.52it/s]Extractor Estimating: 132it [01:43,  1.57it/s]Extractor Estimating: 133it [01:44,  1.51it/s]Extractor Estimating: 134it [01:44,  1.52it/s]Extractor Estimating: 135it [01:45,  1.56it/s]Extractor Estimating: 136it [01:46,  1.59it/s]Extractor Estimating: 137it [01:46,  1.55it/s]Extractor Estimating: 138it [01:47,  1.55it/s]Extractor Estimating: 139it [01:48,  1.51it/s]Extractor Estimating: 140it [01:48,  1.56it/s]Extractor Estimating: 141it [01:49,  1.60it/s]Extractor Estimating: 142it [01:49,  1.62it/s]Extractor Estimating: 143it [01:50,  1.61it/s]Extractor Estimating: 144it [01:51,  1.58it/s]Extractor Estimating: 145it [01:51,  1.57it/s]Extractor Estimating: 146it [01:52,  1.53it/s]Extractor Estimating: 147it [01:53,  1.55it/s]Extractor Estimating: 148it [01:53,  1.57it/s]Extractor Estimating: 149it [01:54,  1.59it/s]Extractor Estimating: 150it [01:54,  1.59it/s]Extractor Estimating: 151it [01:55,  1.63it/s]Extractor Estimating: 152it [01:56,  1.63it/s]Extractor Estimating: 153it [01:56,  1.64it/s]Extractor Estimating: 154it [01:57,  1.63it/s]Extractor Estimating: 155it [01:57,  1.63it/s]Extractor Estimating: 156it [01:58,  1.63it/s]Extractor Estimating: 157it [01:59,  1.61it/s]Extractor Estimating: 158it [01:59,  1.59it/s]Extractor Estimating: 159it [02:00,  1.61it/s]Extractor Estimating: 160it [02:01,  1.60it/s]Extractor Estimating: 161it [02:01,  1.59it/s]Extractor Estimating: 162it [02:02,  1.61it/s]Extractor Estimating: 163it [02:02,  1.64it/s]Extractor Estimating: 164it [02:03,  1.60it/s]Extractor Estimating: 165it [02:04,  1.58it/s]Extractor Estimating: 166it [02:04,  1.58it/s]Extractor Estimating: 167it [02:05,  1.58it/s]Extractor Estimating: 168it [02:06,  1.47it/s]Extractor Estimating: 169it [02:06,  1.51it/s]Extractor Estimating: 170it [02:07,  1.51it/s]Extractor Estimating: 171it [02:08,  1.58it/s]Extractor Estimating: 172it [02:08,  1.57it/s]Extractor Estimating: 173it [02:09,  1.63it/s]Extractor Estimating: 174it [02:09,  1.62it/s]Extractor Estimating: 175it [02:10,  1.61it/s]Extractor Estimating: 176it [02:11,  1.59it/s]Extractor Estimating: 177it [02:11,  1.60it/s]Extractor Estimating: 178it [02:12,  1.58it/s]Extractor Estimating: 179it [02:13,  1.58it/s]Extractor Estimating: 180it [02:13,  1.56it/s]Extractor Estimating: 181it [02:14,  1.61it/s]Extractor Estimating: 182it [02:15,  1.61it/s]Extractor Estimating: 183it [02:15,  1.57it/s]Extractor Estimating: 184it [02:16,  1.59it/s]Extractor Estimating: 185it [02:16,  1.62it/s]Extractor Estimating: 186it [02:17,  1.66it/s]Extractor Estimating: 187it [02:18,  1.65it/s]Extractor Estimating: 188it [02:18,  1.66it/s]Extractor Estimating: 189it [02:19,  1.64it/s]Extractor Estimating: 190it [02:20,  1.53it/s]Extractor Estimating: 191it [02:20,  1.60it/s]Extractor Estimating: 192it [02:21,  1.57it/s]Extractor Estimating: 193it [02:21,  1.58it/s]Extractor Estimating: 194it [02:22,  1.62it/s]Extractor Estimating: 195it [02:23,  1.63it/s]Extractor Estimating: 196it [02:23,  1.65it/s]Extractor Estimating: 197it [02:24,  1.66it/s]Extractor Estimating: 198it [02:24,  1.63it/s]Extractor Estimating: 199it [02:25,  1.51it/s]Extractor Estimating: 200it [02:26,  1.56it/s]Extractor Estimating: 201it [02:26,  1.57it/s]Extractor Estimating: 202it [02:27,  1.59it/s]Extractor Estimating: 203it [02:28,  1.62it/s]Extractor Estimating: 204it [02:28,  1.62it/s]Extractor Estimating: 205it [02:29,  1.63it/s]Extractor Estimating: 206it [02:29,  1.66it/s]Extractor Estimating: 207it [02:30,  1.60it/s]Extractor Estimating: 208it [02:31,  1.62it/s]Extractor Estimating: 209it [02:31,  1.61it/s]Extractor Estimating: 210it [02:32,  1.63it/s]Extractor Estimating: 211it [02:33,  1.61it/s]Extractor Estimating: 212it [02:33,  1.61it/s]Extractor Estimating: 213it [02:34,  1.59it/s]Extractor Estimating: 214it [02:34,  1.62it/s]Extractor Estimating: 215it [02:35,  1.57it/s]Extractor Estimating: 216it [02:36,  1.57it/s]Extractor Estimating: 217it [02:36,  1.59it/s]Extractor Estimating: 218it [02:37,  1.62it/s]Extractor Estimating: 219it [02:37,  1.65it/s]Extractor Estimating: 220it [02:38,  1.65it/s]Extractor Estimating: 221it [02:39,  1.62it/s]Extractor Estimating: 222it [02:39,  1.64it/s]Extractor Estimating: 223it [02:40,  1.64it/s]Extractor Estimating: 224it [02:41,  1.63it/s]Extractor Estimating: 225it [02:41,  1.61it/s]Extractor Estimating: 226it [02:42,  1.60it/s]Extractor Estimating: 227it [02:42,  1.62it/s]Extractor Estimating: 228it [02:43,  1.56it/s]Extractor Estimating: 229it [02:44,  1.60it/s]Extractor Estimating: 230it [02:44,  1.58it/s]Extractor Estimating: 231it [02:45,  1.52it/s]Extractor Estimating: 232it [02:46,  1.57it/s]Extractor Estimating: 233it [02:46,  1.56it/s]Extractor Estimating: 234it [02:47,  1.47it/s]Extractor Estimating: 235it [02:48,  1.52it/s]Extractor Estimating: 236it [02:48,  1.55it/s]Extractor Estimating: 237it [02:49,  1.57it/s]Extractor Estimating: 238it [02:50,  1.58it/s]Extractor Estimating: 239it [02:50,  1.51it/s]Extractor Estimating: 240it [02:51,  1.50it/s]Extractor Estimating: 241it [02:52,  1.51it/s]Extractor Estimating: 242it [02:52,  1.60it/s]Extractor Estimating: 243it [02:53,  1.60it/s]Extractor Estimating: 244it [02:53,  1.65it/s]Extractor Estimating: 245it [02:54,  1.69it/s]Extractor Estimating: 246it [02:55,  1.68it/s]Extractor Estimating: 247it [02:55,  1.67it/s]Extractor Estimating: 248it [02:56,  1.62it/s]Extractor Estimating: 249it [02:56,  1.65it/s]Extractor Estimating: 250it [02:57,  1.56it/s]Extractor Estimating: 250it [02:57,  1.41it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 1042 mean pseudo reward: 0.9658052023780772
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 14890
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14990, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14990, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 12, avg_time 1.277, loss:239.4295
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 24, avg_time 1.002, loss:159.5276
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 36, avg_time 1.001, loss:135.7372
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 4, avg_time 0.997, loss:110.8516
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 16, avg_time 0.995, loss:96.6139
>> valid entity prec:0.4419, rec:0.4568, f1:0.4492
>> valid relation prec:0.2535, rec:0.1114, f1:0.1548
>> valid relation with NER prec:0.2535, rec:0.1114, f1:0.1548
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 28, avg_time 2.485, loss:76.3501
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 1.006, loss:88.6538
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 8, avg_time 0.997, loss:80.7308
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:18:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:18:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-18-29_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:18:30 - WARNING - datasets.builder -   Using custom data configuration default-072bcfb589e866a4
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-072bcfb589e866a4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:18:32,907 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:18:32,979 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:18:32,980 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:18:32,981 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:18:33,186 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:18:33,300 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:18:33,768 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:18:37,091 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:18:37,141 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-072bcfb589e866a4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 08:18:37 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1459709c8ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.78ba/s]100%|██████████| 2/2 [00:00<00:00,  3.46ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.27ba/s] 40%|████      | 2/5 [00:00<00:00,  4.01ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.30ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.45ba/s]100%|██████████| 5/5 [00:01<00:00,  5.23ba/s]100%|██████████| 5/5 [00:01<00:00,  4.66ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  6.44ba/s]100%|██████████| 2/2 [00:00<00:00, 12.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.13ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.03ba/s]100%|██████████| 5/5 [00:00<00:00, 10.79ba/s]100%|██████████| 5/5 [00:00<00:00,  9.63ba/s]
[INFO|trainer.py:414] 2023-08-29 08:18:40,595 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:18:40,603 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:18:40,604 >>   Num examples = 1042
[INFO|trainer.py:1149] 2023-08-29 08:18:40,604 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:18:40,604 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:18:40,604 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:18:40,604 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:18:40,604 >>   Total optimization steps = 80
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<00:23,  3.37it/s]  2%|▎         | 2/80 [00:00<00:22,  3.43it/s]  4%|▍         | 3/80 [00:00<00:22,  3.45it/s]  5%|▌         | 4/80 [00:01<00:21,  3.46it/s]  6%|▋         | 5/80 [00:01<00:21,  3.46it/s]  8%|▊         | 6/80 [00:01<00:21,  3.47it/s]  9%|▉         | 7/80 [00:02<00:21,  3.47it/s] 10%|█         | 8/80 [00:02<00:20,  3.47it/s] 11%|█▏        | 9/80 [00:02<00:20,  3.47it/s] 12%|█▎        | 10/80 [00:02<00:20,  3.47it/s] 14%|█▍        | 11/80 [00:03<00:20,  3.42it/s] 15%|█▌        | 12/80 [00:03<00:19,  3.44it/s] 16%|█▋        | 13/80 [00:03<00:19,  3.45it/s] 18%|█▊        | 14/80 [00:04<00:19,  3.46it/s] 19%|█▉        | 15/80 [00:04<00:18,  3.46it/s] 20%|██        | 16/80 [00:04<00:18,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 08:18:45,295 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:18:45,295 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:18:45,295 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.26it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.21it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.44it/s][A
  4%|▍         | 22/577 [00:00<00:12, 46.21it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.77it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.61it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.55it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.40it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.26it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.46it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.53it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.59it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.85it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.83it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.00it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 45.00it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.08it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.09it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.34it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.22it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.26it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.25it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.23it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.14it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.14it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.18it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.26it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.37it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.43it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.48it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.35it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.35it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.34it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.41it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.38it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.44it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.46it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.37it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.38it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.60it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.83it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.99it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.12it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.24it/s][A
 39%|███▉      | 227/577 [00:04<00:07, 45.29it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.37it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.41it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.26it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.33it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.34it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.31it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.30it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.38it/s][A
 47%|████▋     | 272/577 [00:05<00:06, 45.39it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.46it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.37it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.36it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.41it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.39it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.40it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.33it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.40it/s][A
 55%|█████▍    | 317/577 [00:06<00:05, 45.34it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.32it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.40it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.46it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.43it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.94it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.09it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.18it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.25it/s][A
 63%|██████▎   | 362/577 [00:07<00:04, 45.26it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.24it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.26it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.33it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.30it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.32it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.35it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.45it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.27it/s][A
 71%|███████   | 407/577 [00:08<00:03, 45.43it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.30it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.26it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.14it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.27it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.27it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.36it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.39it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.29it/s][A
 78%|███████▊  | 452/577 [00:09<00:02, 45.37it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.34it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.28it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.23it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.27it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.32it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.37it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.38it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.34it/s][A
 86%|████████▌ | 497/577 [00:10<00:01, 45.29it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.29it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.29it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.24it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.23it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.36it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.44it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.45it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.37it/s][A
 94%|█████████▍| 542/577 [00:11<00:00, 45.22it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.20it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.07it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.02it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.00it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.30it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.29it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.45it/s][A                                               
                                                 [A 20%|██        | 16/80 [00:17<00:18,  3.47it/s]
100%|██████████| 577/577 [00:12<00:00, 45.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:18:58,107 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16
[INFO|configuration_utils.py:351] 2023-08-29 08:18:58,290 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:19:01,348 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:19:01,573 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:19:01,621 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16/special_tokens_map.json
 21%|██▏       | 17/80 [00:29<08:00,  7.62s/it] 22%|██▎       | 18/80 [00:29<05:36,  5.43s/it] 24%|██▍       | 19/80 [00:29<03:57,  3.89s/it] 25%|██▌       | 20/80 [00:30<02:48,  2.81s/it] 26%|██▋       | 21/80 [00:30<02:01,  2.05s/it] 28%|██▊       | 22/80 [00:30<01:28,  1.52s/it] 29%|██▉       | 23/80 [00:31<01:05,  1.15s/it] 30%|███       | 24/80 [00:31<00:50,  1.12it/s] 31%|███▏      | 25/80 [00:31<00:39,  1.40it/s] 32%|███▎      | 26/80 [00:31<00:31,  1.70it/s] 34%|███▍      | 27/80 [00:32<00:26,  2.00it/s] 35%|███▌      | 28/80 [00:32<00:22,  2.28it/s] 36%|███▋      | 29/80 [00:32<00:20,  2.47it/s] 38%|███▊      | 30/80 [00:33<00:18,  2.69it/s] 39%|███▉      | 31/80 [00:33<00:17,  2.88it/s] 40%|████      | 32/80 [00:33<00:15,  3.02it/s][INFO|trainer.py:2140] 2023-08-29 08:19:14,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:19:14,411 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:19:14,411 >>   Batch size = 8
{'eval_loss': 1.024859070777893, 'eval_runtime': 12.7528, 'eval_samples_per_second': 361.412, 'eval_steps_per_second': 45.245, 'epoch': 0.97}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.24it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.26it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.79it/s][A
  4%|▍         | 22/577 [00:00<00:11, 47.03it/s][A
  5%|▍         | 27/577 [00:00<00:11, 46.20it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.84it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.58it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.29it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.33it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.52it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.53it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.72it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.62it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.58it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.36it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.17it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 42.27it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.34it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.11it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.57it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.98it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.10it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.16it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.17it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.77it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 44.79it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.97it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.19it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.40it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.60it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.62it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.58it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.37it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.13it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.02it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.15it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.24it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.39it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.49it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.51it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.49it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.50it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.43it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.29it/s][A
 39%|███▉      | 227/577 [00:05<00:08, 42.35it/s][A
 40%|████      | 232/577 [00:05<00:07, 43.41it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.15it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.57it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.84it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.04it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.08it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.09it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 44.49it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.88it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.10it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.21it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.36it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.41it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.55it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.54it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.44it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.25it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.30it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.22it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.28it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.23it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.42it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.46it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.46it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.39it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.24it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.71it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.97it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.11it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.09it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.18it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.32it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.37it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.36it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.17it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.07it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.03it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.18it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.16it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.29it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.33it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.28it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.11it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 44.95it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.86it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.92it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.03it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.11it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.30it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.42it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.33it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.28it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.15it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.17it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 42.46it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.47it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.15it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.69it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.98it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.01it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.00it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.06it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.82it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.87it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.99it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.09it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.51it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.89it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.11it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.17it/s][A                                               
                                                 [A 40%|████      | 32/80 [00:46<00:15,  3.02it/s]
100%|██████████| 577/577 [00:12<00:00, 45.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:19:27,292 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32
[INFO|configuration_utils.py:351] 2023-08-29 08:19:27,453 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:19:31,078 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:19:31,334 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:19:31,419 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32/special_tokens_map.json
 41%|████▏     | 33/80 [00:58<05:56,  7.58s/it] 42%|████▎     | 34/80 [00:58<04:08,  5.39s/it] 44%|████▍     | 35/80 [00:58<02:53,  3.86s/it] 45%|████▌     | 36/80 [00:59<02:02,  2.79s/it] 46%|████▋     | 37/80 [00:59<01:27,  2.04s/it] 48%|████▊     | 38/80 [00:59<01:03,  1.52s/it] 49%|████▉     | 39/80 [01:00<00:47,  1.15s/it] 50%|█████     | 40/80 [01:00<00:35,  1.12it/s] 51%|█████▏    | 41/80 [01:00<00:27,  1.40it/s] 52%|█████▎    | 42/80 [01:00<00:22,  1.71it/s] 54%|█████▍    | 43/80 [01:01<00:18,  2.01it/s] 55%|█████▌    | 44/80 [01:01<00:15,  2.29it/s] 56%|█████▋    | 45/80 [01:01<00:13,  2.50it/s] 57%|█████▊    | 46/80 [01:02<00:12,  2.72it/s] 59%|█████▉    | 47/80 [01:02<00:11,  2.90it/s] 60%|██████    | 48/80 [01:02<00:10,  3.04it/s][INFO|trainer.py:2140] 2023-08-29 08:19:43,313 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:19:43,313 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:19:43,313 >>   Batch size = 8
{'eval_loss': 1.0135517120361328, 'eval_runtime': 12.8453, 'eval_samples_per_second': 358.809, 'eval_steps_per_second': 44.919, 'epoch': 1.97}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.31it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.13it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.61it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.62it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.99it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.72it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.46it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.35it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.41it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.45it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.49it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.67it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.69it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.41it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.33it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.29it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.44it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.10it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.50it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.81it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.19it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.32it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.13it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.19it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.04it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 44.99it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.19it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.26it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.50it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.61it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.63it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.37it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.33it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.15it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.22it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.26it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.27it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.38it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.58it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.60it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.45it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.25it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.09it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.19it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.59it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.93it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.13it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.28it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.38it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.27it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.19it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.06it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.05it/s][A
 47%|████▋     | 272/577 [00:05<00:06, 45.11it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.38it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.48it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.58it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.58it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.45it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.32it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.16it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.05it/s][A
 55%|█████▍    | 317/577 [00:06<00:05, 45.12it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.21it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.43it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.54it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.55it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.53it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.25it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.07it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 44.98it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.97it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.00it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.53it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.90it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.20it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.22it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.22it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.06it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.02it/s][A
 71%|███████   | 407/577 [00:08<00:03, 44.85it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.03it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.21it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.38it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.53it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.51it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.45it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.20it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.17it/s][A
 78%|███████▊  | 452/577 [00:09<00:02, 45.11it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.08it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.14it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.38it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.43it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.49it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.32it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.25it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.04it/s][A
 86%|████████▌ | 497/577 [00:10<00:01, 45.01it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.00it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 43.39it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.15it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.60it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.82it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.95it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.97it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 44.90it/s][A
 94%|█████████▍| 542/577 [00:11<00:00, 44.79it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.54it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.83it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.04it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.27it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.30it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.41it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.50it/s][A                                               
                                                 [A 60%|██████    | 48/80 [01:15<00:10,  3.04it/s]
100%|██████████| 577/577 [00:12<00:00, 45.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:19:56,166 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-29 08:19:56,473 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:20:00,448 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:20:00,575 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:20:00,629 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48/special_tokens_map.json
 61%|██████▏   | 49/80 [01:26<03:48,  7.37s/it] 62%|██████▎   | 50/80 [01:26<02:37,  5.25s/it] 64%|██████▍   | 51/80 [01:27<01:49,  3.76s/it] 65%|██████▌   | 52/80 [01:27<01:16,  2.72s/it] 66%|██████▋   | 53/80 [01:27<00:53,  1.99s/it] 68%|██████▊   | 54/80 [01:27<00:38,  1.48s/it] 69%|██████▉   | 55/80 [01:28<00:28,  1.13s/it] 70%|███████   | 56/80 [01:28<00:21,  1.14it/s] 71%|███████▏  | 57/80 [01:28<00:16,  1.42it/s] 72%|███████▎  | 58/80 [01:29<00:12,  1.72it/s] 74%|███████▍  | 59/80 [01:29<00:10,  2.02it/s] 75%|███████▌  | 60/80 [01:29<00:08,  2.30it/s] 76%|███████▋  | 61/80 [01:29<00:07,  2.55it/s] 78%|███████▊  | 62/80 [01:30<00:06,  2.76it/s] 79%|███████▉  | 63/80 [01:30<00:05,  2.93it/s] 80%|████████  | 64/80 [01:30<00:05,  3.07it/s][INFO|trainer.py:2140] 2023-08-29 08:20:11,515 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:20:11,515 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:20:11,515 >>   Batch size = 8
{'eval_loss': 1.021507978439331, 'eval_runtime': 12.8063, 'eval_samples_per_second': 359.901, 'eval_steps_per_second': 45.056, 'epoch': 2.97}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.06it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.01it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.31it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.54it/s][A
  5%|▍         | 27/577 [00:00<00:11, 46.04it/s][A
  6%|▌         | 32/577 [00:00<00:12, 42.29it/s][A
  6%|▋         | 37/577 [00:00<00:12, 43.39it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.08it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.66it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.98it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.23it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.21it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.16it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.95it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.88it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.07it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.26it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.48it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.57it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.59it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.55it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.28it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.11it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.00it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.13it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.29it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.54it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.54it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.67it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.53it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.29it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.06it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 41.95it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.10it/s][A
 31%|███       | 177/577 [00:03<00:09, 43.88it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.49it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.86it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.16it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.17it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.01it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.76it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.79it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.12it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.16it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.43it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.53it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.67it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.52it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.23it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.96it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.98it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.13it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.15it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.39it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.47it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.54it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.47it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.24it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.08it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 42.69it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 43.65it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 44.17it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.63it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.97it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.16it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.19it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.08it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.91it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.84it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.04it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.16it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.33it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.53it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.51it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.46it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.27it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.01it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.12it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.04it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.23it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.25it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.48it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.64it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.53it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.27it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.12it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 44.43it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.71it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 44.81it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.07it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.22it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.40it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.32it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.24it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.08it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.01it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.11it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.08it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.18it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.33it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.40it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.44it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.42it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.31it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.30it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.23it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.17it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.15it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.29it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.41it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.37it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.41it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.27it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.23it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.20it/s][A                                               
                                                 [A 80%|████████  | 64/80 [01:43<00:05,  3.07it/s]
100%|██████████| 577/577 [00:12<00:00, 45.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:20:24,370 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64
[INFO|configuration_utils.py:351] 2023-08-29 08:20:24,488 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:20:26,952 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:20:27,049 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:20:27,115 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64/special_tokens_map.json
 81%|████████▏ | 65/80 [01:52<01:40,  6.72s/it] 82%|████████▎ | 66/80 [01:52<01:07,  4.79s/it] 84%|████████▍ | 67/80 [01:53<00:44,  3.44s/it] 85%|████████▌ | 68/80 [01:53<00:29,  2.50s/it] 86%|████████▋ | 69/80 [01:53<00:20,  1.84s/it] 88%|████████▊ | 70/80 [01:53<00:13,  1.37s/it] 89%|████████▉ | 71/80 [01:54<00:09,  1.05s/it] 90%|█████████ | 72/80 [01:54<00:06,  1.22it/s] 91%|█████████▏| 73/80 [01:54<00:04,  1.51it/s] 92%|█████████▎| 74/80 [01:55<00:03,  1.81it/s] 94%|█████████▍| 75/80 [01:55<00:02,  2.11it/s] 95%|█████████▌| 76/80 [01:55<00:01,  2.38it/s] 96%|█████████▋| 77/80 [01:56<00:01,  2.59it/s] 98%|█████████▊| 78/80 [01:56<00:00,  2.67it/s] 99%|█████████▉| 79/80 [01:56<00:00,  2.86it/s]100%|██████████| 80/80 [01:56<00:00,  3.00it/s][INFO|trainer.py:2140] 2023-08-29 08:20:37,572 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:20:37,572 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:20:37,572 >>   Batch size = 8
{'eval_loss': 1.0243897438049316, 'eval_runtime': 12.8393, 'eval_samples_per_second': 358.976, 'eval_steps_per_second': 44.94, 'epoch': 3.97}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.74it/s][A
  2%|▏         | 12/577 [00:00<00:18, 30.99it/s][A
  3%|▎         | 17/577 [00:00<00:15, 36.31it/s][A
  4%|▍         | 22/577 [00:00<00:14, 39.29it/s][A
  5%|▍         | 27/577 [00:00<00:13, 41.29it/s][A
  6%|▌         | 32/577 [00:00<00:12, 42.80it/s][A
  6%|▋         | 37/577 [00:00<00:12, 43.57it/s][A
  7%|▋         | 42/577 [00:01<00:12, 44.24it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.64it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.94it/s][A
 10%|▉         | 57/577 [00:01<00:11, 44.66it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.82it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.99it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.25it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.43it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.47it/s][A
 15%|█▌        | 87/577 [00:02<00:10, 45.41it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.50it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.00it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.85it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.86it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.96it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.36it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.47it/s][A
 22%|██▏       | 127/577 [00:02<00:11, 39.57it/s][A
 23%|██▎       | 132/577 [00:03<00:10, 41.30it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 42.63it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 43.55it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.23it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.62it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.93it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.15it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.75it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.59it/s][A
 31%|███       | 177/577 [00:04<00:08, 44.62it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.93it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.13it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.32it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.36it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.44it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.44it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.22it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.92it/s][A
 38%|███▊      | 222/577 [00:05<00:07, 44.88it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.16it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.23it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.34it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.38it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.56it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.55it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.42it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.08it/s][A
 46%|████▋     | 267/577 [00:06<00:06, 45.08it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.13it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.38it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.38it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.38it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.39it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.52it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.40it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.17it/s][A
 54%|█████▍    | 312/577 [00:07<00:05, 45.00it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.10it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.27it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.34it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.47it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.43it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.48it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.38it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.17it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 45.05it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.06it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.20it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.30it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.39it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.42it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.44it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.31it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.18it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 45.16it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.01it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.10it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.31it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.38it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.51it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.48it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.25it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.17it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 45.16it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.14it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.21it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.34it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.41it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.47it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.32it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.25it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 45.19it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.12it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.07it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.24it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.28it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.43it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.32it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.31it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.22it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.13it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.17it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.12it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.26it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.34it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.32it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.29it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.26it/s][A                                               
                                                 [A100%|██████████| 80/80 [02:09<00:00,  3.00it/s]
100%|██████████| 577/577 [00:12<00:00, 45.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:20:50,472 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-29 08:20:50,499 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:20:52,124 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:20:52,141 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:20:52,156 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:20:55,110 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:20:55,110 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32 (score: 1.0135517120361328).
                                               100%|██████████| 80/80 [02:15<00:00,  3.00it/s]100%|██████████| 80/80 [02:15<00:00,  1.70s/it]
[INFO|trainer.py:1894] 2023-08-29 08:20:56,602 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 08:20:56,619 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:20:58,138 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:20:58,154 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:20:58,166 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:20:58,330 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   epoch                    =       4.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   train_loss               =     0.6006
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   train_runtime            = 0:02:15.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   train_samples            =       1042
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   train_samples_per_second =     38.311
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:58,331 >>   train_steps_per_second   =      0.588
{'eval_loss': 1.0238821506500244, 'eval_runtime': 12.8937, 'eval_samples_per_second': 357.461, 'eval_steps_per_second': 44.751, 'epoch': 4.97}
{'train_runtime': 135.9931, 'train_samples_per_second': 38.311, 'train_steps_per_second': 0.588, 'train_loss': 0.6006222248077393, 'epoch': 4.97}
08/29/2023 08:20:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:20:58,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:20:58,372 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 08:20:58,372 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 56.09it/s]  2%|▏         | 12/577 [00:00<00:11, 49.32it/s]  3%|▎         | 17/577 [00:00<00:11, 47.87it/s]  4%|▍         | 22/577 [00:00<00:11, 47.18it/s]  5%|▍         | 27/577 [00:00<00:11, 46.73it/s]  6%|▌         | 32/577 [00:00<00:11, 46.45it/s]  6%|▋         | 37/577 [00:00<00:11, 46.20it/s]  7%|▋         | 42/577 [00:00<00:11, 45.77it/s]  8%|▊         | 47/577 [00:01<00:11, 45.23it/s]  9%|▉         | 52/577 [00:01<00:11, 44.89it/s] 10%|▉         | 57/577 [00:01<00:11, 44.84it/s] 11%|█         | 62/577 [00:01<00:11, 44.89it/s] 12%|█▏        | 67/577 [00:01<00:11, 45.06it/s] 12%|█▏        | 72/577 [00:01<00:11, 45.25it/s] 13%|█▎        | 77/577 [00:01<00:11, 45.41it/s] 14%|█▍        | 82/577 [00:01<00:10, 45.60it/s] 15%|█▌        | 87/577 [00:01<00:10, 45.65it/s] 16%|█▌        | 92/577 [00:02<00:10, 45.35it/s] 17%|█▋        | 97/577 [00:02<00:10, 44.99it/s] 18%|█▊        | 102/577 [00:02<00:10, 44.95it/s] 19%|█▊        | 107/577 [00:02<00:10, 45.02it/s] 19%|█▉        | 112/577 [00:02<00:10, 45.16it/s] 20%|██        | 117/577 [00:02<00:10, 45.35it/s] 21%|██        | 122/577 [00:02<00:09, 45.54it/s] 22%|██▏       | 127/577 [00:02<00:09, 45.68it/s] 23%|██▎       | 132/577 [00:02<00:09, 45.64it/s] 24%|██▎       | 137/577 [00:02<00:09, 45.40it/s] 25%|██▍       | 142/577 [00:03<00:09, 45.14it/s] 25%|██▌       | 147/577 [00:03<00:09, 45.11it/s] 26%|██▋       | 152/577 [00:03<00:09, 45.01it/s] 27%|██▋       | 157/577 [00:03<00:09, 45.04it/s] 28%|██▊       | 162/577 [00:03<00:09, 45.15it/s] 29%|██▉       | 167/577 [00:03<00:09, 45.46it/s] 30%|██▉       | 172/577 [00:03<00:08, 45.51it/s] 31%|███       | 177/577 [00:03<00:08, 45.64it/s] 32%|███▏      | 182/577 [00:03<00:08, 45.40it/s] 32%|███▏      | 187/577 [00:04<00:08, 45.19it/s] 33%|███▎      | 192/577 [00:04<00:08, 45.06it/s] 34%|███▍      | 197/577 [00:04<00:08, 45.03it/s] 35%|███▌      | 202/577 [00:04<00:08, 44.53it/s] 36%|███▌      | 207/577 [00:04<00:08, 44.91it/s] 37%|███▋      | 212/577 [00:04<00:08, 45.19it/s] 38%|███▊      | 217/577 [00:04<00:07, 45.24it/s] 38%|███▊      | 222/577 [00:04<00:07, 45.47it/s] 39%|███▉      | 227/577 [00:04<00:07, 45.36it/s] 40%|████      | 232/577 [00:05<00:07, 45.24it/s] 41%|████      | 237/577 [00:05<00:07, 45.14it/s] 42%|████▏     | 242/577 [00:05<00:07, 45.05it/s] 43%|████▎     | 247/577 [00:05<00:07, 45.03it/s] 44%|████▎     | 252/577 [00:05<00:07, 45.09it/s] 45%|████▍     | 257/577 [00:05<00:07, 45.34it/s] 45%|████▌     | 262/577 [00:05<00:06, 45.53it/s] 46%|████▋     | 267/577 [00:05<00:06, 45.50it/s] 47%|████▋     | 272/577 [00:05<00:06, 45.51it/s] 48%|████▊     | 277/577 [00:06<00:06, 45.28it/s] 49%|████▉     | 282/577 [00:06<00:06, 45.24it/s] 50%|████▉     | 287/577 [00:06<00:06, 45.14it/s] 51%|█████     | 292/577 [00:06<00:06, 45.15it/s] 51%|█████▏    | 297/577 [00:06<00:06, 45.13it/s] 52%|█████▏    | 302/577 [00:06<00:06, 45.25it/s] 53%|█████▎    | 307/577 [00:06<00:05, 45.38it/s] 54%|█████▍    | 312/577 [00:06<00:05, 45.54it/s] 55%|█████▍    | 317/577 [00:06<00:05, 45.42it/s] 56%|█████▌    | 322/577 [00:07<00:05, 45.30it/s] 57%|█████▋    | 327/577 [00:07<00:05, 45.28it/s] 58%|█████▊    | 332/577 [00:07<00:05, 45.05it/s] 58%|█████▊    | 337/577 [00:07<00:05, 45.08it/s] 59%|█████▉    | 342/577 [00:07<00:05, 45.13it/s] 60%|██████    | 347/577 [00:07<00:05, 45.31it/s] 61%|██████    | 352/577 [00:07<00:04, 45.38it/s] 62%|██████▏   | 357/577 [00:07<00:04, 45.48it/s] 63%|██████▎   | 362/577 [00:07<00:04, 45.42it/s] 64%|██████▎   | 367/577 [00:08<00:04, 45.38it/s] 64%|██████▍   | 372/577 [00:08<00:04, 45.24it/s] 65%|██████▌   | 377/577 [00:08<00:04, 44.93it/s] 66%|██████▌   | 382/577 [00:08<00:04, 45.09it/s] 67%|██████▋   | 387/577 [00:08<00:04, 45.08it/s] 68%|██████▊   | 392/577 [00:08<00:04, 45.28it/s] 69%|██████▉   | 397/577 [00:08<00:03, 45.34it/s] 70%|██████▉   | 402/577 [00:08<00:03, 45.52it/s] 71%|███████   | 407/577 [00:08<00:03, 45.49it/s] 71%|███████▏  | 412/577 [00:09<00:03, 45.44it/s] 72%|███████▏  | 417/577 [00:09<00:03, 45.19it/s] 73%|███████▎  | 422/577 [00:09<00:03, 45.14it/s] 74%|███████▍  | 427/577 [00:09<00:03, 45.06it/s] 75%|███████▍  | 432/577 [00:09<00:03, 45.18it/s] 76%|███████▌  | 437/577 [00:09<00:03, 45.24it/s] 77%|███████▋  | 442/577 [00:09<00:02, 45.35it/s] 77%|███████▋  | 447/577 [00:09<00:02, 45.38it/s] 78%|███████▊  | 452/577 [00:09<00:02, 45.47it/s] 79%|███████▉  | 457/577 [00:10<00:02, 45.42it/s] 80%|████████  | 462/577 [00:10<00:02, 45.16it/s] 81%|████████  | 467/577 [00:10<00:02, 45.04it/s] 82%|████████▏ | 472/577 [00:10<00:02, 45.08it/s] 83%|████████▎ | 477/577 [00:10<00:02, 45.12it/s] 84%|████████▎ | 482/577 [00:10<00:02, 45.28it/s] 84%|████████▍ | 487/577 [00:10<00:01, 45.28it/s] 85%|████████▌ | 492/577 [00:10<00:01, 45.35it/s] 86%|████████▌ | 497/577 [00:10<00:01, 45.39it/s] 87%|████████▋ | 502/577 [00:11<00:01, 45.27it/s] 88%|████████▊ | 507/577 [00:11<00:01, 45.20it/s] 89%|████████▊ | 512/577 [00:11<00:01, 45.15it/s] 90%|████████▉ | 517/577 [00:11<00:01, 45.12it/s] 90%|█████████ | 522/577 [00:11<00:01, 45.04it/s] 91%|█████████▏| 527/577 [00:11<00:01, 45.23it/s] 92%|█████████▏| 532/577 [00:11<00:00, 45.17it/s] 93%|█████████▎| 537/577 [00:11<00:00, 45.40it/s] 94%|█████████▍| 542/577 [00:11<00:00, 45.28it/s] 95%|█████████▍| 547/577 [00:12<00:00, 45.03it/s] 96%|█████████▌| 552/577 [00:12<00:00, 45.12it/s] 97%|█████████▋| 557/577 [00:12<00:00, 45.04it/s] 97%|█████████▋| 562/577 [00:12<00:00, 45.05it/s] 98%|█████████▊| 567/577 [00:12<00:00, 45.02it/s] 99%|█████████▉| 572/577 [00:12<00:00, 45.05it/s]100%|██████████| 577/577 [00:12<00:00, 45.10it/s]100%|██████████| 577/577 [00:12<00:00, 45.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:21:11,117 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   epoch                   =       4.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   eval_loss               =     1.0136
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   eval_runtime            = 0:00:12.74
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   eval_samples_per_second =    361.634
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   eval_steps_per_second   =     45.273
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:21:11,117 >>   perplexity              =     2.7554
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:22,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:22,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:22,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:22,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:22,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:21:23,343 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:21:23,344 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:21:23,909 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:21:24,927 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:21:24,927 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:27,881 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:27,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:27,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:27,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:21:27,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:21:28,521 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:21:28,522 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:21:29,099 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:21:29,254 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:21:29,254 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-80
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-64
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-16
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-32
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/checkpoint-48
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:27,  1.52it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:29,  1.52it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.53it/s]Extractor Predicting: 64it [00:40,  1.51it/s]Extractor Predicting: 65it [00:41,  1.52it/s]Extractor Predicting: 66it [00:42,  1.57it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:43,  1.63it/s]Extractor Predicting: 70it [00:44,  1.61it/s]Extractor Predicting: 71it [00:45,  1.64it/s]Extractor Predicting: 72it [00:45,  1.68it/s]Extractor Predicting: 73it [00:46,  1.62it/s]Extractor Predicting: 74it [00:46,  1.65it/s]Extractor Predicting: 75it [00:47,  1.66it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:49,  1.64it/s]Extractor Predicting: 79it [00:49,  1.64it/s]Extractor Predicting: 80it [00:50,  1.64it/s]Extractor Predicting: 81it [00:51,  1.58it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:52,  1.58it/s]Extractor Predicting: 84it [00:53,  1.63it/s]Extractor Predicting: 85it [00:53,  1.69it/s]Extractor Predicting: 86it [00:54,  1.68it/s]Extractor Predicting: 87it [00:54,  1.71it/s]Extractor Predicting: 88it [00:55,  1.63it/s]Extractor Predicting: 89it [00:56,  1.62it/s]Extractor Predicting: 90it [00:56,  1.61it/s]Extractor Predicting: 91it [00:57,  1.60it/s]Extractor Predicting: 92it [00:57,  1.61it/s]Extractor Predicting: 93it [00:58,  1.63it/s]Extractor Predicting: 94it [00:59,  1.63it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [01:00,  1.62it/s]Extractor Predicting: 97it [01:01,  1.63it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:02,  1.47it/s]Extractor Predicting: 100it [01:03,  1.47it/s]Extractor Predicting: 101it [01:03,  1.49it/s]Extractor Predicting: 102it [01:04,  1.53it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:05,  1.58it/s]Extractor Predicting: 105it [01:06,  1.61it/s]Extractor Predicting: 106it [01:06,  1.54it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.62it/s]Extractor Predicting: 109it [01:08,  1.60it/s]Extractor Predicting: 110it [01:09,  1.59it/s]Extractor Predicting: 111it [01:10,  1.57it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:12,  1.54it/s]Extractor Predicting: 115it [01:12,  1.55it/s]Extractor Predicting: 116it [01:13,  1.57it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:15,  1.66it/s]Extractor Predicting: 120it [01:15,  1.64it/s]Extractor Predicting: 121it [01:16,  1.64it/s]Extractor Predicting: 122it [01:16,  1.62it/s]Extractor Predicting: 123it [01:17,  1.61it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:18,  1.58it/s]Extractor Predicting: 126it [01:19,  1.63it/s]Extractor Predicting: 127it [01:19,  1.65it/s]Extractor Predicting: 128it [01:20,  1.63it/s]Extractor Predicting: 129it [01:21,  1.61it/s]Extractor Predicting: 130it [01:21,  1.58it/s]Extractor Predicting: 131it [01:22,  1.57it/s]Extractor Predicting: 132it [01:23,  1.60it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:24,  1.58it/s]Extractor Predicting: 135it [01:25,  1.58it/s]Extractor Predicting: 136it [01:25,  1.58it/s]Extractor Predicting: 137it [01:26,  1.60it/s]Extractor Predicting: 138it [01:26,  1.60it/s]Extractor Predicting: 139it [01:27,  1.56it/s]Extractor Predicting: 140it [01:28,  1.55it/s]Extractor Predicting: 141it [01:28,  1.54it/s]Extractor Predicting: 142it [01:29,  1.53it/s]Extractor Predicting: 143it [01:30,  1.55it/s]Extractor Predicting: 144it [01:30,  1.53it/s]Extractor Predicting: 145it [01:31,  1.53it/s]Extractor Predicting: 146it [01:32,  1.50it/s]Extractor Predicting: 147it [01:32,  1.51it/s]Extractor Predicting: 148it [01:33,  1.54it/s]Extractor Predicting: 149it [01:34,  1.55it/s]Extractor Predicting: 150it [01:34,  1.54it/s]Extractor Predicting: 151it [01:35,  1.58it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:36,  1.59it/s]Extractor Predicting: 154it [01:37,  1.57it/s]Extractor Predicting: 155it [01:37,  1.56it/s]Extractor Predicting: 156it [01:38,  1.54it/s]Extractor Predicting: 157it [01:39,  1.57it/s]Extractor Predicting: 158it [01:39,  1.58it/s]Extractor Predicting: 159it [01:40,  1.57it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:41,  1.60it/s]Extractor Predicting: 162it [01:42,  1.58it/s]Extractor Predicting: 163it [01:43,  1.54it/s]Extractor Predicting: 164it [01:43,  1.54it/s]Extractor Predicting: 165it [01:44,  1.55it/s]Extractor Predicting: 166it [01:45,  1.53it/s]Extractor Predicting: 167it [01:45,  1.52it/s]Extractor Predicting: 168it [01:46,  1.54it/s]Extractor Predicting: 169it [01:46,  1.55it/s]Extractor Predicting: 170it [01:47,  1.55it/s]Extractor Predicting: 171it [01:47,  1.84it/s]Extractor Predicting: 171it [01:47,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:28,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:28,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:28,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:28,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:28,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:23:28,998 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:23:28,999 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:23:29,591 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:23:30,632 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:23:30,632 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:33,735 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:33,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:33,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:33,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:23:33,753 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:23:34,412 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:23:34,413 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:23:35,047 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:23:35,216 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:23:35,216 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2651790347690711,
  "recall": 0.11087003688435669,
  "score": 0.15636474908200734,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.67it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:22,  1.60it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:27,  1.64it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:32,  1.63it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:36,  1.63it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:37,  1.59it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.61it/s]Extractor Predicting: 64it [00:39,  1.48it/s]Extractor Predicting: 65it [00:40,  1.53it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:41,  1.58it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:42,  1.63it/s]Extractor Predicting: 70it [00:43,  1.58it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:44,  1.58it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:47,  1.63it/s]Extractor Predicting: 77it [00:47,  1.67it/s]Extractor Predicting: 78it [00:48,  1.64it/s]Extractor Predicting: 79it [00:49,  1.63it/s]Extractor Predicting: 80it [00:49,  1.65it/s]Extractor Predicting: 81it [00:50,  1.65it/s]Extractor Predicting: 82it [00:50,  1.65it/s]Extractor Predicting: 83it [00:51,  1.65it/s]Extractor Predicting: 84it [00:52,  1.63it/s]Extractor Predicting: 85it [00:52,  1.64it/s]Extractor Predicting: 86it [00:53,  1.63it/s]Extractor Predicting: 87it [00:54,  1.60it/s]Extractor Predicting: 88it [00:54,  1.62it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:57,  1.59it/s]Extractor Predicting: 93it [00:57,  1.59it/s]Extractor Predicting: 94it [00:58,  1.58it/s]Extractor Predicting: 95it [00:59,  1.57it/s]Extractor Predicting: 96it [00:59,  1.52it/s]Extractor Predicting: 97it [01:00,  1.57it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.68it/s]Extractor Predicting: 100it [01:02,  1.67it/s]Extractor Predicting: 101it [01:02,  1.63it/s]Extractor Predicting: 102it [01:03,  1.65it/s]Extractor Predicting: 103it [01:03,  1.65it/s]Extractor Predicting: 104it [01:04,  1.66it/s]Extractor Predicting: 105it [01:05,  1.65it/s]Extractor Predicting: 106it [01:05,  1.65it/s]Extractor Predicting: 107it [01:06,  1.68it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:09,  1.50it/s]Extractor Predicting: 112it [01:09,  1.51it/s]Extractor Predicting: 113it [01:10,  1.57it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.59it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:12,  1.58it/s]Extractor Predicting: 118it [01:13,  1.55it/s]Extractor Predicting: 119it [01:14,  1.57it/s]Extractor Predicting: 120it [01:14,  1.54it/s]Extractor Predicting: 121it [01:15,  1.52it/s]Extractor Predicting: 122it [01:16,  1.55it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:17,  1.56it/s]Extractor Predicting: 125it [01:17,  1.94it/s]Extractor Predicting: 125it [01:17,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:01,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:01,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:01,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:01,311 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:01,311 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:25:01,919 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:25:01,920 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:25:02,507 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:25:03,511 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:25:03,511 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:06,589 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:06,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:06,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:06,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:25:06,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:25:07,585 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:25:07,587 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:25:08,146 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:25:08,317 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:25:08,317 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45652173913043476,
  "recall": 0.28197381671701915,
  "score": 0.34862004565262505,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  2.02it/s]Extractor Predicting: 6it [00:03,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:25:12,974 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:12,975 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:25:13,018 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:13,019 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:25:13,036 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:25:25,511 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:25:25,562 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:25:25,943 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:25,943 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:25:26,072 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:26,160 >> loading file outputs/wrapper/wiki/unseen_5_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5975609756097561,
  "recall": 0.19291338582677164,
  "score": 0.29166666666666663,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:25:26,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:27,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:27,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:28,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:29,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:29,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:30,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:31,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:31,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:32,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:32,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:34,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:34,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:35,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:36,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:36,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:37,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:37,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:38,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:39,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:39,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:40,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:09, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-29 08:25:41,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:41,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:42,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:42,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:43,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:44,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:44,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:45,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:45,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:46,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:46,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:47,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:48,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:48,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:49,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:49,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:50,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:50,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:51,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:51,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:52,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:44, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-29 08:25:53,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:53,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:54,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:55,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:55,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:56,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:56,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:57,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:58,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:58,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:59,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:59,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:00,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:00,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:01,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:02,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:02,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:03,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:04,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:04,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:05,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:05,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:06,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:40<01:33, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:07,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:07,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:08,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:08,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:09,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:10,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:11,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:11,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:12,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:13,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:13,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:14,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:14,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:15,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:16,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:16,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:17,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:18,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:19,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:19,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:20,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:20,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:21,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:56<01:26, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:22,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:23,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:24,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:24,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:25,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:26,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:26,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:27,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:28,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:28,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:29,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:30,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:30,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:31,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:31,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:32,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:32,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:33,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:34,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:34,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:35,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:09<01:09, 13.87s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:35,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:36,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:37,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:37,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:38,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:38,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:39,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:40,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:41,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:42,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:42,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:43,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:43,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:44,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:45,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:45,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:46,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:46,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:47,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:48,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:21<00:54, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:48,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:49,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:49,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:50,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:51,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:51,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:52,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:52,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:53,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:54,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:54,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:55,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:56,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:57,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:57,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:58,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:58,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:59,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:00,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:00,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:01,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:02,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:36<00:41, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:02,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:03,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:03,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:04,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:05,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:05,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:06,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:06,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:07,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:07,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:08,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:09,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:09,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:10,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:10,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:11,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:12,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:12,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:13,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:14,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:48<00:26, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:14,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:15,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:16,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:16,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:17,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:17,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:18,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:19,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:19,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:20,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:20,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:21,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:22,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:22,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:23,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:23,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:24,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:25,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:25,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:26,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:26,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:27,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:01<00:13, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:27,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:28,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:29,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:29,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:30,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:30,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:31,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:31,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:32,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:33,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:33,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:34,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:34,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:35,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:35,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:36,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:37,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:37,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:38,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:39,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:40,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:40,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:41,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:42,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:42,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:43,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:44,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:44,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:45,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:45,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:46,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:47,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:47,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:48,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:49,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:23<00:00, 15.81s/it]Generating: 100%|██████████| 10/10 [02:23<00:00, 14.30s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:56,311 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:56,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:56,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:56,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:56,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:27:57,026 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:27:57,027 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:27:57,630 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:27:58,721 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:27:58,721 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:28:01,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:28:01,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:28:01,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:28:01,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:28:01,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:28:02,336 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:28:02,338 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:28:02,912 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:28:03,087 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:28:03,087 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : made from material .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9285714285714286, 'errors': {'', "('The TSR', 'manufacturer', '', 'The TSR is a compact compact disc s , designed for use with the sports car and is powered by a single lithium ion battery .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : official language .', 'success_rate': 0.7903645833333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year , the couple built a home for their son , who was at the time working as a cookbook writer for the BBC , and a guest house for her husband Robert Lloyd . Head Entity : Robert Lloyd , Tail Entity : his wife .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8693181818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 70, 'raw': 128}
{'target': 600, 'success': 84, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 125, 'raw': 224}
{'target': 600, 'success': 139, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 175, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 206, 'raw': 384}
{'target': 600, 'success': 221, 'raw': 416}
{'target': 600, 'success': 244, 'raw': 448}
{'target': 600, 'success': 261, 'raw': 480}
{'target': 600, 'success': 276, 'raw': 512}
{'target': 600, 'success': 291, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 321, 'raw': 608}
{'target': 600, 'success': 342, 'raw': 640}
{'target': 600, 'success': 356, 'raw': 672}
{'target': 600, 'success': 373, 'raw': 704}
{'target': 600, 'success': 388, 'raw': 736}
{'target': 600, 'success': 403, 'raw': 768}
{'target': 600, 'success': 420, 'raw': 800}
{'target': 600, 'success': 443, 'raw': 832}
{'target': 600, 'success': 458, 'raw': 864}
{'target': 600, 'success': 474, 'raw': 896}
{'target': 600, 'success': 489, 'raw': 928}
{'target': 600, 'success': 511, 'raw': 960}
{'target': 600, 'success': 529, 'raw': 992}
{'target': 600, 'success': 545, 'raw': 1024}
{'target': 600, 'success': 567, 'raw': 1056}
{'target': 600, 'success': 583, 'raw': 1088}
{'target': 600, 'success': 596, 'raw': 1120}
{'target': 600, 'success': 618, 'raw': 1152}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5364583333333334, 'errors': {'', "('SoundCloud', 'voice type', '', 'The album was released in 1978 on the American recording label SoundCloud .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 9529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.53it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.65it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.62it/s]Extractor Estimating: 17it [00:10,  1.55it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:11,  1.61it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:13,  1.61it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.63it/s]Extractor Estimating: 27it [00:16,  1.64it/s]Extractor Estimating: 28it [00:17,  1.68it/s]Extractor Estimating: 29it [00:18,  1.66it/s]Extractor Estimating: 30it [00:18,  1.65it/s]Extractor Estimating: 31it [00:19,  1.71it/s]Extractor Estimating: 32it [00:19,  1.72it/s]Extractor Estimating: 33it [00:20,  1.76it/s]Extractor Estimating: 34it [00:20,  1.72it/s]Extractor Estimating: 35it [00:21,  1.75it/s]Extractor Estimating: 36it [00:22,  1.75it/s]Extractor Estimating: 37it [00:22,  1.77it/s]Extractor Estimating: 38it [00:23,  1.80it/s]Extractor Estimating: 39it [00:23,  1.73it/s]Extractor Estimating: 40it [00:24,  1.74it/s]Extractor Estimating: 41it [00:24,  1.76it/s]Extractor Estimating: 42it [00:25,  1.75it/s]Extractor Estimating: 43it [00:26,  1.71it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:27,  1.68it/s]Extractor Estimating: 46it [00:27,  1.69it/s]Extractor Estimating: 47it [00:28,  1.78it/s]Extractor Estimating: 48it [00:29,  1.75it/s]Extractor Estimating: 49it [00:29,  1.80it/s]Extractor Estimating: 50it [00:30,  1.77it/s]Extractor Estimating: 51it [00:30,  1.77it/s]Extractor Estimating: 52it [00:31,  1.77it/s]Extractor Estimating: 53it [00:31,  1.72it/s]Extractor Estimating: 54it [00:32,  1.73it/s]Extractor Estimating: 55it [00:32,  1.78it/s]Extractor Estimating: 56it [00:33,  1.73it/s]Extractor Estimating: 57it [00:34,  1.78it/s]Extractor Estimating: 58it [00:34,  1.73it/s]Extractor Estimating: 59it [00:35,  1.73it/s]Extractor Estimating: 60it [00:35,  1.71it/s]Extractor Estimating: 61it [00:36,  1.72it/s]Extractor Estimating: 62it [00:37,  1.69it/s]Extractor Estimating: 63it [00:37,  1.64it/s]Extractor Estimating: 64it [00:38,  1.67it/s]Extractor Estimating: 65it [00:38,  1.72it/s]Extractor Estimating: 66it [00:39,  1.72it/s]Extractor Estimating: 67it [00:39,  1.74it/s]Extractor Estimating: 68it [00:40,  1.76it/s]Extractor Estimating: 69it [00:41,  1.76it/s]Extractor Estimating: 70it [00:41,  1.74it/s]Extractor Estimating: 71it [00:42,  1.70it/s]Extractor Estimating: 72it [00:42,  1.71it/s]Extractor Estimating: 73it [00:43,  1.73it/s]Extractor Estimating: 74it [00:44,  1.77it/s]Extractor Estimating: 75it [00:44,  1.77it/s]Extractor Estimating: 76it [00:45,  1.78it/s]Extractor Estimating: 77it [00:45,  1.73it/s]Extractor Estimating: 78it [00:46,  1.74it/s]Extractor Estimating: 79it [00:46,  1.79it/s]Extractor Estimating: 80it [00:47,  1.75it/s]Extractor Estimating: 81it [00:48,  1.52it/s]Extractor Estimating: 82it [00:48,  1.56it/s]Extractor Estimating: 83it [00:49,  1.55it/s]Extractor Estimating: 84it [00:50,  1.59it/s]Extractor Estimating: 85it [00:50,  1.63it/s]Extractor Estimating: 86it [00:51,  1.61it/s]Extractor Estimating: 87it [00:51,  1.62it/s]Extractor Estimating: 88it [00:52,  1.64it/s]Extractor Estimating: 89it [00:53,  1.61it/s]Extractor Estimating: 90it [00:53,  1.66it/s]Extractor Estimating: 91it [00:54,  1.62it/s]Extractor Estimating: 92it [00:54,  1.69it/s]Extractor Estimating: 93it [00:55,  1.65it/s]Extractor Estimating: 94it [00:56,  1.61it/s]Extractor Estimating: 95it [00:56,  1.59it/s]Extractor Estimating: 96it [00:57,  1.56it/s]Extractor Estimating: 97it [00:58,  1.59it/s]Extractor Estimating: 98it [00:58,  1.60it/s]Extractor Estimating: 99it [00:59,  1.65it/s]Extractor Estimating: 100it [00:59,  1.64it/s]Extractor Estimating: 101it [01:00,  1.65it/s]Extractor Estimating: 102it [01:01,  1.49it/s]Extractor Estimating: 103it [01:01,  1.54it/s]Extractor Estimating: 104it [01:02,  1.57it/s]Extractor Estimating: 105it [01:03,  1.59it/s]Extractor Estimating: 106it [01:03,  1.57it/s]Extractor Estimating: 107it [01:04,  1.61it/s]Extractor Estimating: 108it [01:05,  1.64it/s]Extractor Estimating: 109it [01:05,  1.65it/s]Extractor Estimating: 110it [01:06,  1.61it/s]Extractor Estimating: 111it [01:06,  1.62it/s]Extractor Estimating: 112it [01:07,  1.61it/s]Extractor Estimating: 113it [01:08,  1.61it/s]Extractor Estimating: 114it [01:08,  1.58it/s]Extractor Estimating: 115it [01:09,  1.58it/s]Extractor Estimating: 116it [01:10,  1.59it/s]Extractor Estimating: 117it [01:10,  1.63it/s]Extractor Estimating: 118it [01:11,  1.62it/s]Extractor Estimating: 119it [01:11,  1.67it/s]Extractor Estimating: 120it [01:12,  1.69it/s]Extractor Estimating: 121it [01:13,  1.63it/s]Extractor Estimating: 122it [01:13,  1.65it/s]Extractor Estimating: 123it [01:14,  1.66it/s]Extractor Estimating: 124it [01:14,  1.65it/s]Extractor Estimating: 125it [01:15,  1.65it/s]Extractor Estimating: 126it [01:16,  1.62it/s]Extractor Estimating: 127it [01:16,  1.62it/s]Extractor Estimating: 128it [01:17,  1.59it/s]Extractor Estimating: 129it [01:18,  1.56it/s]Extractor Estimating: 130it [01:18,  1.58it/s]Extractor Estimating: 131it [01:19,  1.58it/s]Extractor Estimating: 132it [01:19,  1.53it/s]Extractor Estimating: 133it [01:20,  1.55it/s]Extractor Estimating: 134it [01:21,  1.54it/s]Extractor Estimating: 135it [01:21,  1.48it/s]Extractor Estimating: 136it [01:22,  1.47it/s]Extractor Estimating: 137it [01:23,  1.48it/s]Extractor Estimating: 138it [01:23,  1.52it/s]Extractor Estimating: 139it [01:24,  1.53it/s]Extractor Estimating: 140it [01:25,  1.57it/s]Extractor Estimating: 141it [01:25,  1.57it/s]Extractor Estimating: 142it [01:26,  1.55it/s]Extractor Estimating: 143it [01:27,  1.59it/s]Extractor Estimating: 144it [01:27,  1.57it/s]Extractor Estimating: 145it [01:28,  1.54it/s]Extractor Estimating: 146it [01:29,  1.58it/s]Extractor Estimating: 147it [01:29,  1.60it/s]Extractor Estimating: 148it [01:30,  1.59it/s]Extractor Estimating: 149it [01:30,  1.57it/s]Extractor Estimating: 150it [01:31,  1.53it/s]Extractor Estimating: 151it [01:32,  1.59it/s]Extractor Estimating: 152it [01:32,  1.61it/s]Extractor Estimating: 153it [01:33,  1.60it/s]Extractor Estimating: 154it [01:34,  1.57it/s]Extractor Estimating: 155it [01:34,  1.59it/s]Extractor Estimating: 156it [01:35,  1.59it/s]Extractor Estimating: 157it [01:35,  1.58it/s]Extractor Estimating: 158it [01:36,  1.63it/s]Extractor Estimating: 159it [01:37,  1.63it/s]Extractor Estimating: 160it [01:37,  1.61it/s]Extractor Estimating: 161it [01:38,  1.59it/s]Extractor Estimating: 162it [01:39,  1.62it/s]Extractor Estimating: 163it [01:39,  1.62it/s]Extractor Estimating: 164it [01:40,  1.65it/s]Extractor Estimating: 165it [01:41,  1.49it/s]Extractor Estimating: 166it [01:41,  1.52it/s]Extractor Estimating: 167it [01:42,  1.57it/s]Extractor Estimating: 168it [01:42,  1.62it/s]Extractor Estimating: 169it [01:43,  1.60it/s]Extractor Estimating: 170it [01:44,  1.57it/s]Extractor Estimating: 171it [01:44,  1.59it/s]Extractor Estimating: 172it [01:45,  1.55it/s]Extractor Estimating: 173it [01:46,  1.57it/s]Extractor Estimating: 174it [01:46,  1.57it/s]Extractor Estimating: 175it [01:47,  1.59it/s]Extractor Estimating: 176it [01:47,  1.61it/s]Extractor Estimating: 177it [01:48,  1.62it/s]Extractor Estimating: 178it [01:49,  1.67it/s]Extractor Estimating: 179it [01:49,  1.61it/s]Extractor Estimating: 180it [01:50,  1.62it/s]Extractor Estimating: 181it [01:50,  1.63it/s]Extractor Estimating: 182it [01:51,  1.63it/s]Extractor Estimating: 183it [01:52,  1.64it/s]Extractor Estimating: 184it [01:52,  1.69it/s]Extractor Estimating: 185it [01:53,  1.66it/s]Extractor Estimating: 186it [01:53,  1.64it/s]Extractor Estimating: 187it [01:54,  1.66it/s]Extractor Estimating: 188it [01:55,  1.63it/s]Extractor Estimating: 189it [01:55,  1.64it/s]Extractor Estimating: 190it [01:56,  1.63it/s]Extractor Estimating: 191it [01:57,  1.64it/s]Extractor Estimating: 192it [01:57,  1.67it/s]Extractor Estimating: 193it [01:58,  1.65it/s]Extractor Estimating: 194it [01:58,  1.63it/s]Extractor Estimating: 195it [01:59,  1.63it/s]Extractor Estimating: 196it [02:00,  1.68it/s]Extractor Estimating: 197it [02:00,  1.66it/s]Extractor Estimating: 198it [02:01,  1.64it/s]Extractor Estimating: 199it [02:01,  1.68it/s]Extractor Estimating: 200it [02:02,  1.66it/s]Extractor Estimating: 201it [02:03,  1.65it/s]Extractor Estimating: 202it [02:03,  1.63it/s]Extractor Estimating: 203it [02:04,  1.59it/s]Extractor Estimating: 204it [02:04,  1.62it/s]Extractor Estimating: 205it [02:05,  1.60it/s]Extractor Estimating: 206it [02:06,  1.67it/s]Extractor Estimating: 207it [02:06,  1.71it/s]Extractor Estimating: 208it [02:07,  1.67it/s]Extractor Estimating: 209it [02:07,  1.68it/s]Extractor Estimating: 210it [02:08,  1.69it/s]Extractor Estimating: 211it [02:09,  1.71it/s]Extractor Estimating: 212it [02:09,  1.69it/s]Extractor Estimating: 213it [02:10,  1.67it/s]Extractor Estimating: 214it [02:10,  1.67it/s]Extractor Estimating: 215it [02:11,  1.68it/s]Extractor Estimating: 216it [02:12,  1.63it/s]Extractor Estimating: 217it [02:12,  1.58it/s]Extractor Estimating: 218it [02:13,  1.65it/s]Extractor Estimating: 219it [02:13,  1.70it/s]Extractor Estimating: 220it [02:14,  1.66it/s]Extractor Estimating: 221it [02:15,  1.64it/s]Extractor Estimating: 222it [02:15,  1.66it/s]Extractor Estimating: 223it [02:16,  1.69it/s]Extractor Estimating: 224it [02:16,  1.66it/s]Extractor Estimating: 225it [02:17,  1.71it/s]Extractor Estimating: 226it [02:18,  1.68it/s]Extractor Estimating: 227it [02:18,  1.66it/s]Extractor Estimating: 228it [02:19,  1.69it/s]Extractor Estimating: 229it [02:19,  1.65it/s]Extractor Estimating: 230it [02:20,  1.67it/s]Extractor Estimating: 231it [02:21,  1.62it/s]Extractor Estimating: 232it [02:21,  1.65it/s]Extractor Estimating: 233it [02:22,  1.67it/s]Extractor Estimating: 234it [02:22,  1.65it/s]Extractor Estimating: 235it [02:23,  1.65it/s]Extractor Estimating: 236it [02:24,  1.65it/s]Extractor Estimating: 237it [02:24,  1.59it/s]Extractor Estimating: 238it [02:25,  1.45it/s]Extractor Estimating: 239it [02:26,  1.42it/s]Extractor Estimating: 240it [02:27,  1.47it/s]Extractor Estimating: 241it [02:27,  1.54it/s]Extractor Estimating: 242it [02:28,  1.49it/s]Extractor Estimating: 243it [02:29,  1.46it/s]Extractor Estimating: 244it [02:29,  1.51it/s]Extractor Estimating: 245it [02:30,  1.58it/s]Extractor Estimating: 246it [02:30,  1.58it/s]Extractor Estimating: 247it [02:31,  1.57it/s]Extractor Estimating: 248it [02:32,  1.60it/s]Extractor Estimating: 249it [02:32,  1.53it/s]Extractor Estimating: 250it [02:33,  1.55it/s]Extractor Estimating: 250it [02:33,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:56,965 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:56,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:56,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:56,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:56,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:30:57,740 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:30:57,741 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:30:58,331 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:30:59,409 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:30:59,409 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:31:02,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:31:02,328 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:31:02,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:31:02,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:31:02,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:31:02,992 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:31:02,993 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:31:03,560 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:31:03,723 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:31:03,724 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:06:10,976 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:06:10,998 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 2008 mean pseudo reward: 0.9790579085001689
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 15389
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15489, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15489, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 16, avg_time 1.035, loss:404.8136
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 32, avg_time 0.971, loss:310.8601
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 48, avg_time 0.997, loss:273.1747
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 64, avg_time 0.983, loss:224.7274
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.986, loss:223.1994
>> valid entity prec:0.4806, rec:0.3995, f1:0.4363
>> valid relation prec:0.2674, rec:0.0878, f1:0.1322
>> valid relation with NER prec:0.2674, rec:0.0878, f1:0.1322
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 12, avg_time 2.459, loss:192.4429
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 28, avg_time 1.004, loss:169.1362
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 44, avg_time 0.985, loss:164.2673
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.983, loss:163.7578
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 76, avg_time 0.984, loss:163.8797
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4768, rec:0.3790, f1:0.4223
>> valid relation prec:0.2803, rec:0.0948, f1:0.1417
>> valid relation with NER prec:0.2803, rec:0.0948, f1:0.1417
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 8, avg_time 2.452, loss:158.7651
g_step 1200, step 24, avg_time 0.987, loss:129.3266
g_step 1300, step 40, avg_time 0.992, loss:126.1896
g_step 1400, step 56, avg_time 0.992, loss:131.4989
g_step 1500, step 72, avg_time 0.987, loss:114.0156
>> valid entity prec:0.4730, rec:0.4006, f1:0.4338
>> valid relation prec:0.1918, rec:0.1022, f1:0.1334
>> valid relation with NER prec:0.1918, rec:0.1022, f1:0.1334
g_step 1600, step 4, avg_time 2.452, loss:110.9362
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:06:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:06:11 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-06-10_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:06:12 - WARNING - datasets.builder -   Using custom data configuration default-6538d5a3387514e6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6538d5a3387514e6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:06:13,654 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:06:13,655 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:06:13,655 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:06:13,656 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:06:13,702 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:06:13,735 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:06:14,012 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:06:17,344 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:06:17,379 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6538d5a3387514e6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.58ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  2.90ba/s]100%|██████████| 3/3 [00:00<00:00,  4.23ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.97ba/s] 40%|████      | 2/5 [00:00<00:00,  3.79ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.15ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.35ba/s]100%|██████████| 5/5 [00:01<00:00,  5.11ba/s]100%|██████████| 5/5 [00:01<00:00,  4.50ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  5.39ba/s]100%|██████████| 3/3 [00:00<00:00, 10.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.59ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.02ba/s]100%|██████████| 5/5 [00:00<00:00, 10.14ba/s]100%|██████████| 5/5 [00:00<00:00,  9.09ba/s]
[INFO|trainer.py:414] 2023-08-29 09:06:21,520 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:06:21,622 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:06:21,622 >>   Num examples = 2008
[INFO|trainer.py:1149] 2023-08-29 09:06:21,622 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:06:21,622 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:06:21,622 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:06:21,622 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:06:21,622 >>   Total optimization steps = 155
  0%|          | 0/155 [00:00<?, ?it/s]  1%|          | 1/155 [00:00<00:46,  3.30it/s]  1%|▏         | 2/155 [00:00<00:44,  3.40it/s]  2%|▏         | 3/155 [00:00<00:44,  3.44it/s]  3%|▎         | 4/155 [00:01<00:43,  3.45it/s]  3%|▎         | 5/155 [00:01<00:43,  3.46it/s]  4%|▍         | 6/155 [00:01<00:42,  3.47it/s]  5%|▍         | 7/155 [00:02<00:42,  3.47it/s]  5%|▌         | 8/155 [00:02<00:43,  3.37it/s]  6%|▌         | 9/155 [00:02<00:43,  3.39it/s]  6%|▋         | 10/155 [00:02<00:42,  3.40it/s]  7%|▋         | 11/155 [00:03<00:42,  3.41it/s]  8%|▊         | 12/155 [00:03<00:41,  3.42it/s]  8%|▊         | 13/155 [00:03<00:41,  3.42it/s]  9%|▉         | 14/155 [00:04<00:41,  3.42it/s] 10%|▉         | 15/155 [00:04<00:40,  3.43it/s] 10%|█         | 16/155 [00:04<00:40,  3.43it/s] 11%|█         | 17/155 [00:04<00:40,  3.43it/s] 12%|█▏        | 18/155 [00:05<00:39,  3.43it/s] 12%|█▏        | 19/155 [00:05<00:40,  3.37it/s] 13%|█▎        | 20/155 [00:05<00:39,  3.39it/s] 14%|█▎        | 21/155 [00:06<00:39,  3.40it/s] 14%|█▍        | 22/155 [00:06<00:41,  3.20it/s] 15%|█▍        | 23/155 [00:06<00:40,  3.27it/s] 15%|█▌        | 24/155 [00:07<00:39,  3.31it/s] 16%|█▌        | 25/155 [00:07<00:38,  3.35it/s] 17%|█▋        | 26/155 [00:07<00:38,  3.37it/s] 17%|█▋        | 27/155 [00:07<00:37,  3.39it/s] 18%|█▊        | 28/155 [00:08<00:37,  3.40it/s] 19%|█▊        | 29/155 [00:08<00:36,  3.41it/s] 19%|█▉        | 30/155 [00:08<00:37,  3.36it/s] 20%|██        | 31/155 [00:09<00:36,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 09:06:30,815 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:06:30,815 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:06:30,815 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.52it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.16it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.58it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.56it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.81it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.53it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.33it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.28it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.33it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.53it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.61it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.67it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.45it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.34it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.18it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.21it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.22it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.19it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.36it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.48it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.50it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 42.97it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.80it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.28it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.48it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 44.72it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.79it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.00it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.20it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.04it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.19it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.33it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.25it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.28it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.13it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.14it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.22it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.22it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.15it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.12it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.14it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.20it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.10it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.02it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.01it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.14it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.22it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.29it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 42.61it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 43.59it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.19it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.62it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 44.54it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.93it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.92it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.19it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.05it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.07it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.22it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.37it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.35it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.32it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.18it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.09it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.23it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.20it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.27it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.27it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.45it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.44it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.37it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.25it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.17it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.09it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.20it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 42.53it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.49it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.11it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.64it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 44.88it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.89it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.80it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.98it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.77it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.89it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.02it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.22it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.43it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.49it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.40it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.34it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.16it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.05it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.96it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.12it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.21it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.30it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.41it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.46it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.54it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.38it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.29it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 43.59it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.25it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.57it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.90it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 44.98it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.17it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.11it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.21it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.93it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.93it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.08it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.30it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.36it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:12<00:00, 45.36it/s][A 20%|██        | 31/155 [00:22<00:36,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:06:43,741 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31
[INFO|configuration_utils.py:351] 2023-08-29 09:06:43,894 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:06:46,781 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:06:46,858 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:06:46,902 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31/special_tokens_map.json
 21%|██        | 32/155 [00:31<14:10,  6.92s/it] 21%|██▏       | 33/155 [00:31<10:02,  4.94s/it] 22%|██▏       | 34/155 [00:32<07:09,  3.55s/it] 23%|██▎       | 35/155 [00:32<05:08,  2.57s/it] 23%|██▎       | 36/155 [00:32<03:44,  1.89s/it] 24%|██▍       | 37/155 [00:33<02:46,  1.41s/it] 25%|██▍       | 38/155 [00:33<02:05,  1.07s/it] 25%|██▌       | 39/155 [00:33<01:37,  1.19it/s] 26%|██▌       | 40/155 [00:33<01:17,  1.48it/s] 26%|██▋       | 41/155 [00:34<01:03,  1.79it/s] 27%|██▋       | 42/155 [00:34<00:54,  2.09it/s] 28%|██▊       | 43/155 [00:34<00:47,  2.37it/s] 28%|██▊       | 44/155 [00:35<00:43,  2.58it/s] 29%|██▉       | 45/155 [00:35<00:39,  2.79it/s] 30%|██▉       | 46/155 [00:35<00:36,  2.95it/s] 30%|███       | 47/155 [00:35<00:35,  3.08it/s] 31%|███       | 48/155 [00:36<00:33,  3.17it/s] 32%|███▏      | 49/155 [00:36<00:32,  3.24it/s] 32%|███▏      | 50/155 [00:36<00:31,  3.29it/s] 33%|███▎      | 51/155 [00:37<00:31,  3.33it/s] 34%|███▎      | 52/155 [00:37<00:30,  3.36it/s] 34%|███▍      | 53/155 [00:37<00:30,  3.38it/s] 35%|███▍      | 54/155 [00:37<00:29,  3.39it/s] 35%|███▌      | 55/155 [00:38<00:29,  3.34it/s] 36%|███▌      | 56/155 [00:38<00:29,  3.37it/s] 37%|███▋      | 57/155 [00:38<00:28,  3.38it/s] 37%|███▋      | 58/155 [00:39<00:28,  3.39it/s] 38%|███▊      | 59/155 [00:39<00:28,  3.40it/s] 39%|███▊      | 60/155 [00:39<00:27,  3.41it/s] 39%|███▉      | 61/155 [00:40<00:27,  3.41it/s] 40%|████      | 62/155 [00:40<00:27,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:07:02,010 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:07:02,010 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:07:02,010 >>   Batch size = 8
{'eval_loss': 1.0420902967453003, 'eval_runtime': 12.8677, 'eval_samples_per_second': 358.185, 'eval_steps_per_second': 44.841, 'epoch': 0.98}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.07it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.86it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.28it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.53it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.89it/s][A
  6%|▌         | 32/577 [00:00<00:12, 42.99it/s][A
  6%|▋         | 37/577 [00:00<00:12, 43.76it/s][A
  7%|▋         | 42/577 [00:00<00:12, 44.21it/s][A
  8%|▊         | 47/577 [00:01<00:11, 44.61it/s][A
  9%|▉         | 52/577 [00:01<00:11, 44.90it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.15it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.32it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.34it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.97it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.06it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.06it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.18it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.19it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.36it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.41it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.53it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.36it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.20it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.10it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.05it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.08it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.22it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.31it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.34it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.43it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.30it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.22it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.18it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.55it/s][A
 31%|███       | 177/577 [00:03<00:08, 44.62it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.78it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.91it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.05it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.12it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.25it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.02it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.10it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.13it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.14it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.14it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.16it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.27it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.29it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.31it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.25it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.29it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.21it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.26it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.22it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.32it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.35it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.42it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.39it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.33it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 41.44it/s][A
 54%|█████▍    | 312/577 [00:06<00:06, 42.71it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 43.46it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.95it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.41it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.67it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.90it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.05it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.73it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.85it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.08it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.09it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.26it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.30it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.45it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.49it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.44it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.11it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.04it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.14it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.19it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.33it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.30it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.44it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.41it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.29it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.03it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 42.93it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 43.62it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.09it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.60it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.80it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.10it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.22it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.13it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.89it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.86it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 44.89it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.14it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.33it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.35it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.43it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.40it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.21it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.96it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.03it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.12it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.25it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.30it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.36it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.32it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.21it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.95it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.89it/s][A
100%|██████████| 577/577 [00:12<00:00, 43.82it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:12<00:00, 43.82it/s][A 40%|████      | 62/155 [00:53<00:27,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:07:15,010 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62
[INFO|configuration_utils.py:351] 2023-08-29 09:07:15,159 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:07:18,217 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:07:18,400 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:07:18,532 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62/special_tokens_map.json
 41%|████      | 63/155 [01:06<12:30,  8.16s/it] 41%|████▏     | 64/155 [01:07<08:49,  5.82s/it] 42%|████▏     | 65/155 [01:07<06:14,  4.16s/it] 43%|████▎     | 66/155 [01:07<04:26,  3.00s/it] 43%|████▎     | 67/155 [01:08<03:12,  2.19s/it] 44%|████▍     | 68/155 [01:08<02:20,  1.62s/it] 45%|████▍     | 69/155 [01:08<01:44,  1.22s/it] 45%|████▌     | 70/155 [01:08<01:20,  1.06it/s] 46%|████▌     | 71/155 [01:09<01:02,  1.34it/s] 46%|████▋     | 72/155 [01:09<00:50,  1.64it/s] 47%|████▋     | 73/155 [01:09<00:42,  1.94it/s] 48%|████▊     | 74/155 [01:10<00:36,  2.23it/s] 48%|████▊     | 75/155 [01:10<00:32,  2.47it/s] 49%|████▉     | 76/155 [01:10<00:29,  2.69it/s] 50%|████▉     | 77/155 [01:11<00:27,  2.88it/s] 50%|█████     | 78/155 [01:11<00:25,  3.03it/s] 51%|█████     | 79/155 [01:11<00:24,  3.14it/s] 52%|█████▏    | 80/155 [01:11<00:23,  3.22it/s] 52%|█████▏    | 81/155 [01:12<00:22,  3.28it/s] 53%|█████▎    | 82/155 [01:12<00:21,  3.32it/s] 54%|█████▎    | 83/155 [01:12<00:21,  3.35it/s] 54%|█████▍    | 84/155 [01:13<00:21,  3.37it/s] 55%|█████▍    | 85/155 [01:13<00:20,  3.39it/s] 55%|█████▌    | 86/155 [01:13<00:20,  3.33it/s] 56%|█████▌    | 87/155 [01:13<00:20,  3.35it/s] 57%|█████▋    | 88/155 [01:14<00:19,  3.37it/s] 57%|█████▋    | 89/155 [01:14<00:19,  3.39it/s] 58%|█████▊    | 90/155 [01:14<00:19,  3.40it/s] 59%|█████▊    | 91/155 [01:15<00:18,  3.41it/s] 59%|█████▉    | 92/155 [01:15<00:18,  3.41it/s] 60%|██████    | 93/155 [01:15<00:18,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:07:37,366 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:07:37,366 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:07:37,366 >>   Batch size = 8
{'eval_loss': 1.0348443984985352, 'eval_runtime': 12.8897, 'eval_samples_per_second': 357.573, 'eval_steps_per_second': 44.765, 'epoch': 1.98}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.43it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.40it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.35it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.58it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.87it/s][A
  6%|▌         | 32/577 [00:00<00:12, 45.10it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.26it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.11it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.28it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.47it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.58it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.34it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.16it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.01it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.03it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.08it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.15it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.32it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.44it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.58it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.54it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.28it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.22it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.17it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.20it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.24it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.12it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.52it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.55it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.51it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.36it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.26it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.12it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.07it/s][A
 31%|███       | 177/577 [00:03<00:08, 44.57it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.78it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.99it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.21it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.30it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.21it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.12it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.91it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.98it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.07it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.19it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.27it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.43it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.47it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.47it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.36it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.18it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.23it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.21it/s][A
 47%|████▋     | 272/577 [00:05<00:06, 45.29it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.23it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.41it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.32it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.38it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.22it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.21it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.20it/s][A
 54%|█████▍    | 312/577 [00:06<00:06, 40.50it/s][A
 55%|█████▍    | 317/577 [00:07<00:06, 41.95it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 43.06it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 43.84it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.46it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.83it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.07it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.03it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.78it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 44.66it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.76it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.02it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.11it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.43it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.59it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.55it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.39it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.08it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 44.90it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.93it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.15it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.24it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.35it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.48it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.54it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.41it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.14it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 43.82it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.21it/s][A
 79%|███████▉  | 457/577 [00:10<00:03, 39.37it/s][A
 80%|████████  | 462/577 [00:10<00:02, 41.13it/s][A
 81%|████████  | 467/577 [00:10<00:02, 42.44it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.39it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.04it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.53it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.88it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.10it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.66it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.63it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.64it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.04it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.19it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.38it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.52it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.61it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.33it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.08it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.92it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.88it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.11it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.23it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.39it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.47it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.60it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:12<00:00, 45.60it/s][A 60%|██████    | 93/155 [01:28<00:18,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:07:50,364 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93
[INFO|configuration_utils.py:351] 2023-08-29 09:07:50,493 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:07:53,869 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:07:54,024 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:07:54,109 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93/special_tokens_map.json
 61%|██████    | 94/155 [01:39<07:31,  7.40s/it] 61%|██████▏   | 95/155 [01:40<05:16,  5.28s/it] 62%|██████▏   | 96/155 [01:40<03:43,  3.78s/it] 63%|██████▎   | 97/155 [01:40<02:38,  2.73s/it] 63%|██████▎   | 98/155 [01:40<01:54,  2.00s/it] 64%|██████▍   | 99/155 [01:41<01:23,  1.49s/it] 65%|██████▍   | 100/155 [01:41<01:02,  1.13s/it] 65%|██████▌   | 101/155 [01:41<00:47,  1.14it/s] 66%|██████▌   | 102/155 [01:42<00:37,  1.42it/s] 66%|██████▋   | 103/155 [01:42<00:30,  1.73it/s] 67%|██████▋   | 104/155 [01:42<00:25,  2.03it/s] 68%|██████▊   | 105/155 [01:42<00:21,  2.31it/s] 68%|██████▊   | 106/155 [01:43<00:19,  2.51it/s] 69%|██████▉   | 107/155 [01:43<00:17,  2.73it/s] 70%|██████▉   | 108/155 [01:43<00:16,  2.91it/s] 70%|███████   | 109/155 [01:44<00:15,  3.05it/s] 71%|███████   | 110/155 [01:44<00:14,  3.15it/s] 72%|███████▏  | 111/155 [01:44<00:13,  3.23it/s] 72%|███████▏  | 112/155 [01:44<00:13,  3.28it/s] 73%|███████▎  | 113/155 [01:45<00:12,  3.32it/s] 74%|███████▎  | 114/155 [01:45<00:12,  3.35it/s] 74%|███████▍  | 115/155 [01:45<00:11,  3.38it/s] 75%|███████▍  | 116/155 [01:46<00:11,  3.39it/s] 75%|███████▌  | 117/155 [01:46<00:11,  3.33it/s] 76%|███████▌  | 118/155 [01:46<00:11,  3.36it/s] 77%|███████▋  | 119/155 [01:47<00:10,  3.37it/s] 77%|███████▋  | 120/155 [01:47<00:10,  3.39it/s] 78%|███████▊  | 121/155 [01:47<00:10,  3.39it/s] 79%|███████▊  | 122/155 [01:47<00:09,  3.40it/s] 79%|███████▉  | 123/155 [01:48<00:09,  3.41it/s] 80%|████████  | 124/155 [01:48<00:09,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 09:08:10,190 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:08:10,190 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:08:10,190 >>   Batch size = 8
{'eval_loss': 1.0570433139801025, 'eval_runtime': 12.8962, 'eval_samples_per_second': 357.393, 'eval_steps_per_second': 44.742, 'epoch': 2.98}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.05it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.13it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.54it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.63it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.89it/s][A
  6%|▌         | 32/577 [00:00<00:12, 45.06it/s][A
  6%|▋         | 37/577 [00:00<00:12, 44.99it/s][A
  7%|▋         | 42/577 [00:00<00:11, 44.84it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.09it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.23it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.32it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.38it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.44it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.38it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.33it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.25it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.14it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.15it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.34it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.37it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.40it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.41it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.38it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.26it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.23it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.12it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.14it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.23it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.38it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.39it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.42it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.31it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.24it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 43.71it/s][A
 31%|███       | 177/577 [00:03<00:09, 44.25it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.54it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.79it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.95it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.15it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.30it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.27it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.07it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.01it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.10it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.16it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.21it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.33it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.37it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.40it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.29it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.12it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.09it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.15it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.25it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.35it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.27it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.38it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.33it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.30it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.15it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.13it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 44.62it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.89it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.11it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.18it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.29it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.17it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.24it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.11it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.02it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.02it/s][A
 63%|██████▎   | 362/577 [00:07<00:04, 45.10it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.25it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.40it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.39it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.28it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.20it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.13it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.08it/s][A
 71%|███████   | 407/577 [00:08<00:03, 45.13it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.29it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.39it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.42it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.32it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.23it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.20it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.10it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.03it/s][A
 78%|███████▊  | 452/577 [00:09<00:02, 45.08it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.17it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.32it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.36it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.31it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.24it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.22it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.16it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.03it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.14it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 41.66it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 42.87it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 43.58it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.28it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.63it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.80it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.92it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 44.97it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 44.63it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 44.73it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.95it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.21it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.30it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.41it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.35it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.38it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:12<00:00, 45.38it/s][A 80%|████████  | 124/155 [02:01<00:09,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:08:23,194 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-29 09:08:23,514 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:08:27,280 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:08:27,466 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:08:27,566 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124/special_tokens_map.json
 81%|████████  | 125/155 [02:13<03:51,  7.73s/it] 81%|████████▏ | 126/155 [02:13<02:39,  5.50s/it] 82%|████████▏ | 127/155 [02:14<01:50,  3.94s/it] 83%|████████▎ | 128/155 [02:14<01:16,  2.85s/it] 83%|████████▎ | 129/155 [02:14<00:54,  2.08s/it] 84%|████████▍ | 130/155 [02:15<00:38,  1.54s/it] 85%|████████▍ | 131/155 [02:15<00:28,  1.17s/it] 85%|████████▌ | 132/155 [02:15<00:20,  1.10it/s] 86%|████████▌ | 133/155 [02:15<00:15,  1.39it/s] 86%|████████▋ | 134/155 [02:16<00:12,  1.69it/s] 87%|████████▋ | 135/155 [02:16<00:10,  1.99it/s] 88%|████████▊ | 136/155 [02:16<00:08,  2.28it/s] 88%|████████▊ | 137/155 [02:17<00:07,  2.50it/s] 89%|████████▉ | 138/155 [02:17<00:06,  2.72it/s] 90%|████████▉ | 139/155 [02:17<00:05,  2.90it/s] 90%|█████████ | 140/155 [02:18<00:04,  3.04it/s] 91%|█████████ | 141/155 [02:18<00:04,  3.15it/s] 92%|█████████▏| 142/155 [02:18<00:04,  3.23it/s] 92%|█████████▏| 143/155 [02:18<00:03,  3.28it/s] 93%|█████████▎| 144/155 [02:19<00:03,  3.32it/s] 94%|█████████▎| 145/155 [02:19<00:02,  3.35it/s] 94%|█████████▍| 146/155 [02:19<00:02,  3.37it/s] 95%|█████████▍| 147/155 [02:20<00:02,  3.38it/s] 95%|█████████▌| 148/155 [02:20<00:02,  3.28it/s] 96%|█████████▌| 149/155 [02:20<00:01,  3.32it/s] 97%|█████████▋| 150/155 [02:20<00:01,  3.35it/s] 97%|█████████▋| 151/155 [02:21<00:01,  3.37it/s] 98%|█████████▊| 152/155 [02:21<00:00,  3.38it/s] 99%|█████████▊| 153/155 [02:21<00:00,  3.40it/s] 99%|█████████▉| 154/155 [02:22<00:00,  3.40it/s]100%|██████████| 155/155 [02:22<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 09:08:44,065 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:08:44,066 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:08:44,066 >>   Batch size = 8
{'eval_loss': 1.0621416568756104, 'eval_runtime': 12.8454, 'eval_samples_per_second': 358.805, 'eval_steps_per_second': 44.919, 'epoch': 3.98}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.49it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.62it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.59it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.58it/s][A
  5%|▍         | 28/577 [00:00<00:11, 46.07it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.47it/s][A
  7%|▋         | 38/577 [00:00<00:12, 43.99it/s][A
  7%|▋         | 43/577 [00:00<00:12, 44.44it/s][A
  8%|▊         | 48/577 [00:01<00:11, 44.68it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.12it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.21it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.32it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.34it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.14it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 44.91it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 44.92it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 45.16it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.24it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.45it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.43it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.50it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.39it/s][A
 20%|██        | 118/577 [00:02<00:10, 45.30it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 45.06it/s][A
 22%|██▏       | 128/577 [00:02<00:09, 45.00it/s][A
 23%|██▎       | 133/577 [00:02<00:09, 45.11it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 45.31it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 45.36it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 45.50it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.51it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.37it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.04it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 44.98it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 44.97it/s][A
 31%|███       | 178/577 [00:03<00:09, 43.12it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 43.84it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 44.41it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 44.72it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.03it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.15it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.13it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.09it/s][A
 38%|███▊      | 218/577 [00:04<00:08, 44.79it/s][A
 39%|███▊      | 223/577 [00:04<00:08, 43.59it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 44.24it/s][A
 40%|████      | 233/577 [00:05<00:07, 44.68it/s][A
 41%|████      | 238/577 [00:05<00:07, 44.99it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.15it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.20it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 45.18it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 45.12it/s][A
 46%|████▌     | 263/577 [00:05<00:07, 44.75it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 44.81it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 45.04it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 45.19it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 45.35it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 45.36it/s][A
 51%|█████     | 293/577 [00:06<00:06, 45.49it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.40it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.08it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 44.93it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.03it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 45.07it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 45.29it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.34it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.45it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.42it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.38it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.17it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.02it/s][A
 62%|██████▏   | 358/577 [00:07<00:05, 43.11it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 43.87it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 44.35it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 44.73it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.03it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.16it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.19it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.01it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 44.77it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 44.84it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.06it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.28it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 45.31it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 45.49it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 45.47it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.40it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.19it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 45.02it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 44.96it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 45.01it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 44.99it/s][A
 80%|████████  | 463/577 [00:10<00:02, 45.11it/s][A
 81%|████████  | 468/577 [00:10<00:02, 45.38it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.39it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.46it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.29it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.09it/s][A
 85%|████████▌ | 493/577 [00:10<00:02, 40.58it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 42.01it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 43.02it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 43.86it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 44.34it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 44.74it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 44.93it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 44.98it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 44.75it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 44.64it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 44.73it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 44.95it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 45.16it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 45.35it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 45.39it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 45.49it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 45.37it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:12<00:00, 45.37it/s][A100%|██████████| 155/155 [02:35<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:08:57,146 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155
[INFO|configuration_utils.py:351] 2023-08-29 09:08:57,349 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:09:00,673 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:09:00,803 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:09:00,870 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:09:07,375 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:09:07,387 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62 (score: 1.0348443984985352).
                                                 100%|██████████| 155/155 [02:53<00:00,  3.41it/s]100%|██████████| 155/155 [02:53<00:00,  1.12s/it]
[INFO|trainer.py:1894] 2023-08-29 09:09:15,071 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 09:09:15,201 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:09:17,953 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:09:18,062 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:09:18,145 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:09:18,556 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   epoch                    =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   train_loss               =     0.4755
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   train_runtime            = 0:02:53.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   train_samples            =       2008
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   train_samples_per_second =     57.899
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:18,556 >>   train_steps_per_second   =      0.894
{'eval_loss': 1.0673787593841553, 'eval_runtime': 12.8348, 'eval_samples_per_second': 359.102, 'eval_steps_per_second': 44.956, 'epoch': 4.98}
{'train_runtime': 173.4052, 'train_samples_per_second': 57.899, 'train_steps_per_second': 0.894, 'train_loss': 0.47552849554246474, 'epoch': 4.98}
08/29/2023 09:09:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:09:18,728 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:09:18,728 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 09:09:18,728 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.93it/s]  2%|▏         | 12/577 [00:00<00:11, 49.55it/s]  3%|▎         | 18/577 [00:00<00:11, 47.81it/s]  4%|▍         | 23/577 [00:00<00:11, 47.24it/s]  5%|▍         | 28/577 [00:00<00:11, 46.69it/s]  6%|▌         | 33/577 [00:00<00:11, 46.47it/s]  7%|▋         | 38/577 [00:00<00:11, 46.33it/s]  7%|▋         | 43/577 [00:00<00:11, 45.86it/s]  8%|▊         | 48/577 [00:01<00:11, 45.22it/s]  9%|▉         | 53/577 [00:01<00:11, 45.02it/s] 10%|█         | 58/577 [00:01<00:11, 45.18it/s] 11%|█         | 63/577 [00:01<00:11, 45.20it/s] 12%|█▏        | 68/577 [00:01<00:11, 45.38it/s] 13%|█▎        | 73/577 [00:01<00:11, 45.54it/s] 14%|█▎        | 78/577 [00:01<00:10, 45.70it/s] 14%|█▍        | 83/577 [00:01<00:10, 45.66it/s] 15%|█▌        | 88/577 [00:01<00:10, 45.42it/s] 16%|█▌        | 93/577 [00:02<00:10, 44.93it/s] 17%|█▋        | 98/577 [00:02<00:10, 44.86it/s] 18%|█▊        | 103/577 [00:02<00:10, 44.88it/s] 19%|█▊        | 108/577 [00:02<00:10, 45.18it/s] 20%|█▉        | 113/577 [00:02<00:10, 45.26it/s] 20%|██        | 118/577 [00:02<00:10, 45.41it/s] 21%|██▏       | 123/577 [00:02<00:09, 45.56it/s] 22%|██▏       | 128/577 [00:02<00:09, 45.63it/s] 23%|██▎       | 133/577 [00:02<00:09, 45.48it/s] 24%|██▍       | 138/577 [00:03<00:09, 45.19it/s] 25%|██▍       | 143/577 [00:03<00:09, 44.96it/s] 26%|██▌       | 148/577 [00:03<00:09, 45.00it/s] 27%|██▋       | 153/577 [00:03<00:09, 45.05it/s] 27%|██▋       | 158/577 [00:03<00:09, 45.19it/s] 28%|██▊       | 163/577 [00:03<00:09, 45.35it/s] 29%|██▉       | 168/577 [00:03<00:09, 45.42it/s] 30%|██▉       | 173/577 [00:03<00:08, 45.55it/s] 31%|███       | 178/577 [00:03<00:08, 45.50it/s] 32%|███▏      | 183/577 [00:04<00:09, 39.64it/s] 33%|███▎      | 188/577 [00:04<00:09, 41.40it/s] 33%|███▎      | 193/577 [00:04<00:09, 42.60it/s] 34%|███▍      | 198/577 [00:04<00:08, 43.60it/s] 35%|███▌      | 203/577 [00:04<00:08, 44.23it/s] 36%|███▌      | 208/577 [00:04<00:08, 44.65it/s] 37%|███▋      | 213/577 [00:04<00:08, 45.00it/s] 38%|███▊      | 218/577 [00:04<00:07, 45.01it/s] 39%|███▊      | 223/577 [00:04<00:07, 44.74it/s] 40%|███▉      | 228/577 [00:05<00:07, 44.63it/s] 40%|████      | 233/577 [00:05<00:07, 44.67it/s] 41%|████      | 238/577 [00:05<00:07, 44.96it/s] 42%|████▏     | 243/577 [00:05<00:07, 45.03it/s] 43%|████▎     | 248/577 [00:05<00:07, 45.08it/s] 44%|████▍     | 253/577 [00:05<00:07, 45.36it/s] 45%|████▍     | 258/577 [00:05<00:07, 45.37it/s] 46%|████▌     | 263/577 [00:05<00:06, 45.38it/s] 46%|████▋     | 268/577 [00:05<00:06, 45.31it/s] 47%|████▋     | 273/577 [00:06<00:06, 45.23it/s] 48%|████▊     | 278/577 [00:06<00:06, 45.14it/s] 49%|████▉     | 283/577 [00:06<00:06, 45.05it/s] 50%|████▉     | 288/577 [00:06<00:06, 45.03it/s] 51%|█████     | 293/577 [00:06<00:06, 45.29it/s] 52%|█████▏    | 298/577 [00:06<00:06, 45.43it/s] 53%|█████▎    | 303/577 [00:06<00:06, 45.47it/s] 53%|█████▎    | 308/577 [00:06<00:05, 45.36it/s] 54%|█████▍    | 313/577 [00:06<00:05, 45.29it/s] 55%|█████▌    | 318/577 [00:07<00:06, 41.39it/s] 56%|█████▌    | 323/577 [00:07<00:05, 42.56it/s] 57%|█████▋    | 328/577 [00:07<00:05, 43.40it/s] 58%|█████▊    | 333/577 [00:07<00:05, 44.00it/s] 59%|█████▊    | 338/577 [00:07<00:05, 44.47it/s] 59%|█████▉    | 343/577 [00:07<00:05, 44.76it/s] 60%|██████    | 348/577 [00:07<00:05, 45.02it/s] 61%|██████    | 353/577 [00:07<00:04, 45.13it/s] 62%|██████▏   | 358/577 [00:07<00:04, 44.87it/s] 63%|██████▎   | 363/577 [00:08<00:04, 44.94it/s] 64%|██████▍   | 368/577 [00:08<00:04, 44.91it/s] 65%|██████▍   | 373/577 [00:08<00:04, 45.08it/s] 66%|██████▌   | 378/577 [00:08<00:04, 45.11it/s] 66%|██████▋   | 383/577 [00:08<00:04, 45.28it/s] 67%|██████▋   | 388/577 [00:08<00:04, 40.81it/s] 68%|██████▊   | 393/577 [00:08<00:04, 42.32it/s] 69%|██████▉   | 398/577 [00:08<00:04, 43.34it/s] 70%|██████▉   | 403/577 [00:08<00:03, 44.04it/s] 71%|███████   | 408/577 [00:09<00:03, 44.35it/s] 72%|███████▏  | 413/577 [00:09<00:03, 44.55it/s] 72%|███████▏  | 418/577 [00:09<00:04, 39.45it/s] 73%|███████▎  | 423/577 [00:09<00:08, 18.82it/s] 74%|███████▍  | 427/577 [00:10<00:07, 21.32it/s] 75%|███████▍  | 432/577 [00:10<00:05, 25.63it/s] 76%|███████▌  | 437/577 [00:10<00:04, 29.67it/s] 77%|███████▋  | 442/577 [00:10<00:04, 33.25it/s] 77%|███████▋  | 447/577 [00:10<00:03, 36.24it/s] 78%|███████▊  | 452/577 [00:10<00:03, 38.68it/s] 79%|███████▉  | 457/577 [00:10<00:02, 40.57it/s] 80%|████████  | 462/577 [00:10<00:02, 41.92it/s] 81%|████████  | 467/577 [00:10<00:02, 42.58it/s] 82%|████████▏ | 472/577 [00:11<00:02, 43.03it/s] 83%|████████▎ | 477/577 [00:11<00:02, 43.46it/s] 84%|████████▎ | 482/577 [00:11<00:02, 44.07it/s] 84%|████████▍ | 487/577 [00:11<00:02, 44.50it/s] 85%|████████▌ | 492/577 [00:11<00:01, 44.82it/s] 86%|████████▌ | 497/577 [00:11<00:01, 45.16it/s] 87%|████████▋ | 502/577 [00:11<00:01, 45.23it/s] 88%|████████▊ | 507/577 [00:11<00:01, 45.32it/s] 89%|████████▊ | 512/577 [00:11<00:01, 45.00it/s] 90%|████████▉ | 517/577 [00:12<00:01, 44.81it/s] 90%|█████████ | 522/577 [00:12<00:01, 44.83it/s] 91%|█████████▏| 527/577 [00:12<00:01, 44.89it/s] 92%|█████████▏| 532/577 [00:12<00:00, 45.09it/s] 93%|█████████▎| 537/577 [00:12<00:00, 45.26it/s] 94%|█████████▍| 542/577 [00:12<00:00, 45.36it/s] 95%|█████████▍| 547/577 [00:12<00:00, 45.50it/s] 96%|█████████▌| 552/577 [00:12<00:00, 45.38it/s] 97%|█████████▋| 557/577 [00:12<00:00, 45.20it/s] 97%|█████████▋| 562/577 [00:13<00:00, 44.98it/s] 98%|█████████▊| 567/577 [00:13<00:00, 44.16it/s] 99%|█████████▉| 572/577 [00:13<00:00, 44.44it/s]100%|██████████| 577/577 [00:13<00:00, 44.87it/s]100%|██████████| 577/577 [00:13<00:00, 43.08it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:09:32,140 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,140 >>   epoch                   =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,140 >>   eval_loss               =     1.0348
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,140 >>   eval_runtime            = 0:00:13.41
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,141 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,141 >>   eval_samples_per_second =    343.633
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,141 >>   eval_steps_per_second   =     43.019
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:09:32,141 >>   perplexity              =     2.8147
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:40,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:40,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:40,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:40,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:40,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:09:41,399 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:09:41,400 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:41,760 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:42,877 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:42,877 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:44,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:44,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:44,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:44,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:44,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:09:44,948 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:09:44,949 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:45,263 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:45,547 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:45,547 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-31
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-155
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-62
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/checkpoint-93
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:24,  1.58it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:27,  1.53it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.52it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:35,  1.56it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:39,  1.52it/s]Extractor Predicting: 63it [00:39,  1.52it/s]Extractor Predicting: 64it [00:40,  1.51it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:42,  1.56it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:44,  1.61it/s]Extractor Predicting: 71it [00:44,  1.62it/s]Extractor Predicting: 72it [00:45,  1.67it/s]Extractor Predicting: 73it [00:45,  1.65it/s]Extractor Predicting: 74it [00:46,  1.67it/s]Extractor Predicting: 75it [00:47,  1.67it/s]Extractor Predicting: 76it [00:47,  1.63it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:49,  1.63it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.63it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:52,  1.46it/s]Extractor Predicting: 84it [00:52,  1.54it/s]Extractor Predicting: 85it [00:53,  1.63it/s]Extractor Predicting: 86it [00:54,  1.65it/s]Extractor Predicting: 87it [00:54,  1.68it/s]Extractor Predicting: 88it [00:55,  1.61it/s]Extractor Predicting: 89it [00:55,  1.61it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:57,  1.61it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:59,  1.63it/s]Extractor Predicting: 95it [00:59,  1.63it/s]Extractor Predicting: 96it [01:00,  1.63it/s]Extractor Predicting: 97it [01:00,  1.62it/s]Extractor Predicting: 98it [01:01,  1.58it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:02,  1.57it/s]Extractor Predicting: 101it [01:03,  1.56it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.60it/s]Extractor Predicting: 105it [01:05,  1.63it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:07,  1.62it/s]Extractor Predicting: 108it [01:07,  1.64it/s]Extractor Predicting: 109it [01:08,  1.62it/s]Extractor Predicting: 110it [01:09,  1.61it/s]Extractor Predicting: 111it [01:09,  1.58it/s]Extractor Predicting: 112it [01:10,  1.58it/s]Extractor Predicting: 113it [01:10,  1.59it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.63it/s]Extractor Predicting: 118it [01:14,  1.63it/s]Extractor Predicting: 119it [01:14,  1.69it/s]Extractor Predicting: 120it [01:15,  1.66it/s]Extractor Predicting: 121it [01:15,  1.65it/s]Extractor Predicting: 122it [01:16,  1.63it/s]Extractor Predicting: 123it [01:17,  1.62it/s]Extractor Predicting: 124it [01:17,  1.64it/s]Extractor Predicting: 125it [01:18,  1.58it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:19,  1.65it/s]Extractor Predicting: 128it [01:20,  1.63it/s]Extractor Predicting: 129it [01:20,  1.62it/s]Extractor Predicting: 130it [01:21,  1.57it/s]Extractor Predicting: 131it [01:22,  1.57it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:24,  1.59it/s]Extractor Predicting: 136it [01:25,  1.59it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:26,  1.61it/s]Extractor Predicting: 139it [01:27,  1.58it/s]Extractor Predicting: 140it [01:27,  1.55it/s]Extractor Predicting: 141it [01:28,  1.54it/s]Extractor Predicting: 142it [01:29,  1.53it/s]Extractor Predicting: 143it [01:29,  1.55it/s]Extractor Predicting: 144it [01:30,  1.54it/s]Extractor Predicting: 145it [01:31,  1.53it/s]Extractor Predicting: 146it [01:31,  1.50it/s]Extractor Predicting: 147it [01:32,  1.51it/s]Extractor Predicting: 148it [01:32,  1.54it/s]Extractor Predicting: 149it [01:33,  1.56it/s]Extractor Predicting: 150it [01:34,  1.56it/s]Extractor Predicting: 151it [01:34,  1.59it/s]Extractor Predicting: 152it [01:35,  1.57it/s]Extractor Predicting: 153it [01:36,  1.60it/s]Extractor Predicting: 154it [01:36,  1.57it/s]Extractor Predicting: 155it [01:37,  1.57it/s]Extractor Predicting: 156it [01:38,  1.55it/s]Extractor Predicting: 157it [01:38,  1.58it/s]Extractor Predicting: 158it [01:39,  1.58it/s]Extractor Predicting: 159it [01:39,  1.54it/s]Extractor Predicting: 160it [01:40,  1.56it/s]Extractor Predicting: 161it [01:41,  1.60it/s]Extractor Predicting: 162it [01:41,  1.59it/s]Extractor Predicting: 163it [01:42,  1.55it/s]Extractor Predicting: 164it [01:43,  1.52it/s]Extractor Predicting: 165it [01:43,  1.54it/s]Extractor Predicting: 166it [01:44,  1.53it/s]Extractor Predicting: 167it [01:45,  1.55it/s]Extractor Predicting: 168it [01:45,  1.56it/s]Extractor Predicting: 169it [01:46,  1.55it/s]Extractor Predicting: 170it [01:47,  1.42it/s]Extractor Predicting: 171it [01:47,  1.71it/s]Extractor Predicting: 171it [01:47,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,357 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:11:46,098 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:11:46,099 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:46,686 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:47,810 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:47,810 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,836 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,836 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,836 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,836 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:11:50,329 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:11:50,330 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:50,657 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:50,904 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:50,904 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2849162011173184,
  "recall": 0.08852245606422217,
  "score": 0.13507697401092533,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:22,  1.61it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:30,  1.63it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:32,  1.64it/s]Extractor Predicting: 53it [00:32,  1.64it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:35,  1.61it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:38,  1.61it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.63it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:42,  1.65it/s]Extractor Predicting: 70it [00:43,  1.62it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:44,  1.59it/s]Extractor Predicting: 73it [00:45,  1.59it/s]Extractor Predicting: 74it [00:45,  1.48it/s]Extractor Predicting: 75it [00:46,  1.52it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:47,  1.61it/s]Extractor Predicting: 78it [00:48,  1.63it/s]Extractor Predicting: 79it [00:48,  1.62it/s]Extractor Predicting: 80it [00:49,  1.64it/s]Extractor Predicting: 81it [00:50,  1.64it/s]Extractor Predicting: 82it [00:50,  1.64it/s]Extractor Predicting: 83it [00:51,  1.64it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:52,  1.62it/s]Extractor Predicting: 86it [00:53,  1.63it/s]Extractor Predicting: 87it [00:53,  1.60it/s]Extractor Predicting: 88it [00:54,  1.61it/s]Extractor Predicting: 89it [00:55,  1.61it/s]Extractor Predicting: 90it [00:55,  1.58it/s]Extractor Predicting: 91it [00:56,  1.59it/s]Extractor Predicting: 92it [00:57,  1.59it/s]Extractor Predicting: 93it [00:57,  1.60it/s]Extractor Predicting: 94it [00:58,  1.59it/s]Extractor Predicting: 95it [00:58,  1.58it/s]Extractor Predicting: 96it [00:59,  1.55it/s]Extractor Predicting: 97it [01:00,  1.57it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.67it/s]Extractor Predicting: 100it [01:01,  1.67it/s]Extractor Predicting: 101it [01:02,  1.64it/s]Extractor Predicting: 102it [01:03,  1.66it/s]Extractor Predicting: 103it [01:03,  1.65it/s]Extractor Predicting: 104it [01:04,  1.66it/s]Extractor Predicting: 105it [01:05,  1.64it/s]Extractor Predicting: 106it [01:05,  1.65it/s]Extractor Predicting: 107it [01:06,  1.68it/s]Extractor Predicting: 108it [01:06,  1.66it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:08,  1.51it/s]Extractor Predicting: 112it [01:09,  1.53it/s]Extractor Predicting: 113it [01:10,  1.58it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:12,  1.58it/s]Extractor Predicting: 118it [01:13,  1.55it/s]Extractor Predicting: 119it [01:13,  1.57it/s]Extractor Predicting: 120it [01:14,  1.54it/s]Extractor Predicting: 121it [01:15,  1.53it/s]Extractor Predicting: 122it [01:15,  1.55it/s]Extractor Predicting: 123it [01:16,  1.57it/s]Extractor Predicting: 124it [01:17,  1.57it/s]Extractor Predicting: 125it [01:17,  1.93it/s]Extractor Predicting: 125it [01:17,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:16,183 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:16,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:16,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:16,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:16,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:13:16,486 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:13:16,487 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:13:16,764 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:13:17,788 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:13:17,788 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:19,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:19,197 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:19,197 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:19,197 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:19,197 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:13:19,534 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:13:19,535 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:13:19,821 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:13:19,950 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:13:19,950 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4863768115942029,
  "recall": 0.28163813360187984,
  "score": 0.3567176870748299,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  2.02it/s]Extractor Predicting: 6it [00:03,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-29 09:13:24,631 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:13:24,632 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:13:24,673 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:13:24,675 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 09:13:24,694 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:13:39,235 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 09:13:39,249 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 09:13:39,338 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:13:39,339 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:13:39,381 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:13:39,400 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6578947368421053,
  "recall": 0.1968503937007874,
  "score": 0.3030303030303031,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 09:13:39,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:40,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:40,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:41,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:42,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:42,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:43,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:44,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:45,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:45,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:46,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:46,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:47,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:48,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:48,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:49,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:49,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:50,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:51,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:51,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:52,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:01, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 09:13:53,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:53,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:54,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:55,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:55,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:56,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:56,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:57,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:58,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:59,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:00,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:00,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:01,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:02,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:02,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:03,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:04,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:04,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:05,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:06,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:27<01:48, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:06,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:07,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:07,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:08,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:09,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:09,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:10,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:10,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:11,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:11,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:12,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:13,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:13,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:14,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:14,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:15,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:16,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:16,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:17,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:17,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:38<01:29, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:18,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:19,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:19,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:20,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:21,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:21,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:22,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:22,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:23,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:24,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:24,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:25,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:26,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:26,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:27,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:28,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:28,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:29,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:29,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:30,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:30,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:31,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:32,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:53<01:19, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:32,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:33,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:34,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:34,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:35,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:36,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:36,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:37,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:37,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:38,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:39,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:39,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:40,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:41,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:41,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:42,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:42,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:43,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:44,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:44,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:05<01:05, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:45,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:45,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:46,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:47,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:48,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:48,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:49,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:50,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:50,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:51,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:51,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:53,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:53,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:54,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:55,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:56,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:56,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:57,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:18<00:52, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:58,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:59,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:00,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:00,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:01,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:02,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:03,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:03,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:04,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:05,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:05,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:06,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:06,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:07,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:08,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:08,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:09,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:10,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:10,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:31<00:39, 13.03s/it][WARNING|generation_utils.py:914] 2023-08-29 09:15:11,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:11,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:12,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:13,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:13,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:14,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:14,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:15,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:16,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:16,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:17,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:18,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:18,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:19,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:19,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:20,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:21,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:22,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:22,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:43<00:25, 12.81s/it][WARNING|generation_utils.py:914] 2023-08-29 09:15:23,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:24,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:24,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:25,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:26,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:26,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:27,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:27,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:28,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:29,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:29,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:31,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:31,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:32,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:32,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:33,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:33,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:34,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:35,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:35,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:36,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:57<00:12, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-29 09:15:37,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:37,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:38,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:38,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:39,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:39,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:40,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:41,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:42,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:42,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:43,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:43,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:44,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:44,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:45,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:46,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:47,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:47,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:48,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:48,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:49,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:50,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:50,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:51,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:51,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:52,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:53,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:53,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:54,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:54,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:55,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:56,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:57,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:57,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:58,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:58,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:59,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:15:59,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:16:00,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:16:00,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:16:01,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:22<00:00, 16.70s/it]Generating: 100%|██████████| 10/10 [02:22<00:00, 14.24s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:07,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:07,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:07,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:07,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:07,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:16:07,828 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:16:07,829 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:16:08,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:16:09,187 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:16:09,187 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:12,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:12,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:12,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:12,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:12,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:16:12,867 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:16:12,868 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:16:13,450 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:16:13,623 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:16:13,623 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : official language .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.9211309523809523, 'errors': {'', "('use', 'use', '', 'The most famous example is the use of the word .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 21, 'raw': 64}
{'target': 600, 'success': 35, 'raw': 96}
{'target': 600, 'success': 49, 'raw': 128}
{'target': 600, 'success': 60, 'raw': 160}
{'target': 600, 'success': 72, 'raw': 192}
{'target': 600, 'success': 85, 'raw': 224}
{'target': 600, 'success': 98, 'raw': 256}
{'target': 600, 'success': 118, 'raw': 288}
{'target': 600, 'success': 134, 'raw': 320}
{'target': 600, 'success': 149, 'raw': 352}
{'target': 600, 'success': 162, 'raw': 384}
{'target': 600, 'success': 178, 'raw': 416}
{'target': 600, 'success': 195, 'raw': 448}
{'target': 600, 'success': 205, 'raw': 480}
{'target': 600, 'success': 219, 'raw': 512}
{'target': 600, 'success': 232, 'raw': 544}
{'target': 600, 'success': 248, 'raw': 576}
{'target': 600, 'success': 263, 'raw': 608}
{'target': 600, 'success': 280, 'raw': 640}
{'target': 600, 'success': 294, 'raw': 672}
{'target': 600, 'success': 311, 'raw': 704}
{'target': 600, 'success': 327, 'raw': 736}
{'target': 600, 'success': 342, 'raw': 768}
{'target': 600, 'success': 359, 'raw': 800}
{'target': 600, 'success': 375, 'raw': 832}
{'target': 600, 'success': 389, 'raw': 864}
{'target': 600, 'success': 402, 'raw': 896}
{'target': 600, 'success': 421, 'raw': 928}
{'target': 600, 'success': 436, 'raw': 960}
{'target': 600, 'success': 452, 'raw': 992}
{'target': 600, 'success': 466, 'raw': 1024}
{'target': 600, 'success': 481, 'raw': 1056}
{'target': 600, 'success': 497, 'raw': 1088}
{'target': 600, 'success': 509, 'raw': 1120}
{'target': 600, 'success': 522, 'raw': 1152}
{'target': 600, 'success': 538, 'raw': 1184}
{'target': 600, 'success': 551, 'raw': 1216}
{'target': 600, 'success': 564, 'raw': 1248}
{'target': 600, 'success': 580, 'raw': 1280}
{'target': 600, 'success': 591, 'raw': 1312}
{'target': 600, 'success': 607, 'raw': 1344}
{'prompt': 'Relation : voice type .', 'success_rate': 0.45163690476190477, 'errors': {'', "('Waiting For Godot', 'voice type', '', 'In 2002 , he sang at the London Opera s premiere of the play , Waiting For Godot , which was inspired by his appearance on the television episode of his opera A Christmas Carol .')", "('James Martin', 'voice type', '', 'The song was recorded by the Canadian recording artist James Martin , for the American version of his album , No Way Out .')", "('The People v. O. J. Simpson', 'voice type', '', 'He sang for the BBC s series The People v. O. J. Simpson , and directed the film version of The Man with the Golden Gun .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 7958
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8058, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.37it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.44it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:13,  1.59it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.63it/s]Extractor Estimating: 25it [00:15,  1.57it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.62it/s]Extractor Estimating: 28it [00:17,  1.65it/s]Extractor Estimating: 29it [00:18,  1.63it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:19,  1.69it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:21,  1.68it/s]Extractor Estimating: 35it [00:21,  1.66it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:22,  1.65it/s]Extractor Estimating: 38it [00:23,  1.62it/s]Extractor Estimating: 39it [00:24,  1.64it/s]Extractor Estimating: 40it [00:24,  1.63it/s]Extractor Estimating: 41it [00:25,  1.63it/s]Extractor Estimating: 42it [00:26,  1.62it/s]Extractor Estimating: 43it [00:26,  1.61it/s]Extractor Estimating: 44it [00:27,  1.67it/s]Extractor Estimating: 45it [00:27,  1.67it/s]Extractor Estimating: 46it [00:28,  1.64it/s]Extractor Estimating: 47it [00:29,  1.62it/s]Extractor Estimating: 48it [00:29,  1.68it/s]Extractor Estimating: 49it [00:30,  1.71it/s]Extractor Estimating: 50it [00:30,  1.68it/s]Extractor Estimating: 51it [00:31,  1.73it/s]Extractor Estimating: 52it [00:32,  1.69it/s]Extractor Estimating: 53it [00:32,  1.71it/s]Extractor Estimating: 54it [00:33,  1.72it/s]Extractor Estimating: 55it [00:33,  1.71it/s]Extractor Estimating: 56it [00:34,  1.68it/s]Extractor Estimating: 57it [00:34,  1.70it/s]Extractor Estimating: 58it [00:35,  1.70it/s]Extractor Estimating: 59it [00:36,  1.70it/s]Extractor Estimating: 60it [00:36,  1.73it/s]Extractor Estimating: 61it [00:37,  1.75it/s]Extractor Estimating: 62it [00:37,  1.78it/s]Extractor Estimating: 63it [00:38,  1.82it/s]Extractor Estimating: 64it [00:38,  1.82it/s]Extractor Estimating: 65it [00:39,  1.78it/s]Extractor Estimating: 66it [00:40,  1.75it/s]Extractor Estimating: 67it [00:40,  1.71it/s]Extractor Estimating: 68it [00:41,  1.77it/s]Extractor Estimating: 69it [00:41,  1.73it/s]Extractor Estimating: 70it [00:42,  1.79it/s]Extractor Estimating: 71it [00:42,  1.79it/s]Extractor Estimating: 72it [00:43,  1.76it/s]Extractor Estimating: 73it [00:44,  1.71it/s]Extractor Estimating: 74it [00:44,  1.74it/s]Extractor Estimating: 75it [00:45,  1.78it/s]Extractor Estimating: 76it [00:45,  1.80it/s]Extractor Estimating: 77it [00:46,  1.61it/s]Extractor Estimating: 78it [00:47,  1.65it/s]Extractor Estimating: 79it [00:47,  1.64it/s]Extractor Estimating: 80it [00:48,  1.63it/s]Extractor Estimating: 81it [00:48,  1.63it/s]Extractor Estimating: 82it [00:49,  1.64it/s]Extractor Estimating: 83it [00:49,  1.71it/s]Extractor Estimating: 84it [00:50,  1.73it/s]Extractor Estimating: 85it [00:51,  1.69it/s]Extractor Estimating: 86it [00:51,  1.68it/s]Extractor Estimating: 87it [00:52,  1.63it/s]Extractor Estimating: 88it [00:53,  1.60it/s]Extractor Estimating: 89it [00:53,  1.68it/s]Extractor Estimating: 90it [00:54,  1.68it/s]Extractor Estimating: 91it [00:54,  1.68it/s]Extractor Estimating: 92it [00:55,  1.71it/s]Extractor Estimating: 93it [00:55,  1.76it/s]Extractor Estimating: 94it [00:56,  1.73it/s]Extractor Estimating: 95it [00:57,  1.71it/s]Extractor Estimating: 96it [00:57,  1.67it/s]Extractor Estimating: 97it [00:58,  1.69it/s]Extractor Estimating: 98it [00:58,  1.69it/s]Extractor Estimating: 99it [00:59,  1.72it/s]Extractor Estimating: 100it [01:00,  1.73it/s]Extractor Estimating: 101it [01:00,  1.72it/s]Extractor Estimating: 102it [01:01,  1.63it/s]Extractor Estimating: 103it [01:01,  1.64it/s]Extractor Estimating: 104it [01:02,  1.67it/s]Extractor Estimating: 105it [01:03,  1.65it/s]Extractor Estimating: 106it [01:03,  1.66it/s]Extractor Estimating: 107it [01:04,  1.62it/s]Extractor Estimating: 108it [01:04,  1.63it/s]Extractor Estimating: 109it [01:05,  1.66it/s]Extractor Estimating: 110it [01:06,  1.64it/s]Extractor Estimating: 111it [01:06,  1.60it/s]Extractor Estimating: 112it [01:07,  1.62it/s]Extractor Estimating: 113it [01:07,  1.64it/s]Extractor Estimating: 114it [01:08,  1.62it/s]Extractor Estimating: 115it [01:09,  1.60it/s]Extractor Estimating: 116it [01:09,  1.60it/s]Extractor Estimating: 117it [01:10,  1.63it/s]Extractor Estimating: 118it [01:11,  1.65it/s]Extractor Estimating: 119it [01:11,  1.68it/s]Extractor Estimating: 120it [01:12,  1.67it/s]Extractor Estimating: 121it [01:12,  1.67it/s]Extractor Estimating: 122it [01:13,  1.61it/s]Extractor Estimating: 123it [01:14,  1.64it/s]Extractor Estimating: 124it [01:14,  1.62it/s]Extractor Estimating: 125it [01:15,  1.62it/s]Extractor Estimating: 126it [01:16,  1.58it/s]Extractor Estimating: 127it [01:16,  1.58it/s]Extractor Estimating: 128it [01:17,  1.58it/s]Extractor Estimating: 129it [01:17,  1.55it/s]Extractor Estimating: 130it [01:18,  1.56it/s]Extractor Estimating: 131it [01:19,  1.53it/s]Extractor Estimating: 132it [01:19,  1.55it/s]Extractor Estimating: 133it [01:20,  1.52it/s]Extractor Estimating: 134it [01:21,  1.52it/s]Extractor Estimating: 135it [01:21,  1.53it/s]Extractor Estimating: 136it [01:22,  1.55it/s]Extractor Estimating: 137it [01:23,  1.54it/s]Extractor Estimating: 138it [01:23,  1.57it/s]Extractor Estimating: 139it [01:24,  1.55it/s]Extractor Estimating: 140it [01:25,  1.56it/s]Extractor Estimating: 141it [01:25,  1.56it/s]Extractor Estimating: 142it [01:26,  1.56it/s]Extractor Estimating: 143it [01:27,  1.56it/s]Extractor Estimating: 144it [01:27,  1.49it/s]Extractor Estimating: 145it [01:28,  1.49it/s]Extractor Estimating: 146it [01:29,  1.51it/s]Extractor Estimating: 147it [01:29,  1.47it/s]Extractor Estimating: 148it [01:30,  1.52it/s]Extractor Estimating: 149it [01:31,  1.47it/s]Extractor Estimating: 150it [01:31,  1.50it/s]Extractor Estimating: 151it [01:32,  1.42it/s]Extractor Estimating: 152it [01:33,  1.45it/s]Extractor Estimating: 153it [01:33,  1.49it/s]Extractor Estimating: 154it [01:34,  1.53it/s]Extractor Estimating: 155it [01:35,  1.55it/s]Extractor Estimating: 156it [01:35,  1.55it/s]Extractor Estimating: 157it [01:36,  1.53it/s]Extractor Estimating: 158it [01:36,  1.58it/s]Extractor Estimating: 159it [01:37,  1.58it/s]Extractor Estimating: 160it [01:38,  1.60it/s]Extractor Estimating: 161it [01:38,  1.58it/s]Extractor Estimating: 162it [01:39,  1.59it/s]Extractor Estimating: 163it [01:40,  1.60it/s]Extractor Estimating: 164it [01:40,  1.59it/s]Extractor Estimating: 165it [01:41,  1.62it/s]Extractor Estimating: 166it [01:41,  1.63it/s]Extractor Estimating: 167it [01:42,  1.64it/s]Extractor Estimating: 168it [01:43,  1.61it/s]Extractor Estimating: 169it [01:43,  1.56it/s]Extractor Estimating: 170it [01:44,  1.60it/s]Extractor Estimating: 171it [01:45,  1.62it/s]Extractor Estimating: 172it [01:45,  1.61it/s]Extractor Estimating: 173it [01:46,  1.60it/s]Extractor Estimating: 174it [01:46,  1.56it/s]Extractor Estimating: 175it [01:47,  1.54it/s]Extractor Estimating: 176it [01:48,  1.57it/s]Extractor Estimating: 177it [01:48,  1.61it/s]Extractor Estimating: 178it [01:49,  1.64it/s]Extractor Estimating: 179it [01:50,  1.60it/s]Extractor Estimating: 180it [01:50,  1.63it/s]Extractor Estimating: 181it [01:51,  1.60it/s]Extractor Estimating: 182it [01:51,  1.62it/s]Extractor Estimating: 183it [01:52,  1.54it/s]Extractor Estimating: 184it [01:53,  1.53it/s]Extractor Estimating: 185it [01:53,  1.56it/s]Extractor Estimating: 186it [01:54,  1.54it/s]Extractor Estimating: 187it [01:55,  1.58it/s]Extractor Estimating: 188it [01:55,  1.57it/s]Extractor Estimating: 189it [01:56,  1.61it/s]Extractor Estimating: 190it [01:57,  1.63it/s]Extractor Estimating: 191it [01:57,  1.60it/s]Extractor Estimating: 192it [01:58,  1.64it/s]Extractor Estimating: 193it [01:58,  1.66it/s]Extractor Estimating: 194it [01:59,  1.64it/s]Extractor Estimating: 195it [02:00,  1.61it/s]Extractor Estimating: 196it [02:00,  1.51it/s]Extractor Estimating: 197it [02:01,  1.55it/s]Extractor Estimating: 198it [02:02,  1.60it/s]Extractor Estimating: 199it [02:02,  1.68it/s]Extractor Estimating: 200it [02:03,  1.65it/s]Extractor Estimating: 201it [02:03,  1.63it/s]Extractor Estimating: 202it [02:04,  1.66it/s]Extractor Estimating: 203it [02:05,  1.66it/s]Extractor Estimating: 204it [02:05,  1.67it/s]Extractor Estimating: 205it [02:06,  1.67it/s]Extractor Estimating: 206it [02:06,  1.65it/s]Extractor Estimating: 207it [02:07,  1.65it/s]Extractor Estimating: 208it [02:08,  1.67it/s]Extractor Estimating: 209it [02:08,  1.67it/s]Extractor Estimating: 210it [02:09,  1.65it/s]Extractor Estimating: 211it [02:09,  1.69it/s]Extractor Estimating: 212it [02:10,  1.67it/s]Extractor Estimating: 213it [02:11,  1.67it/s]Extractor Estimating: 214it [02:11,  1.73it/s]Extractor Estimating: 215it [02:12,  1.77it/s]Extractor Estimating: 216it [02:12,  1.75it/s]Extractor Estimating: 217it [02:13,  1.74it/s]Extractor Estimating: 218it [02:13,  1.72it/s]Extractor Estimating: 219it [02:14,  1.77it/s]Extractor Estimating: 220it [02:14,  1.78it/s]Extractor Estimating: 221it [02:15,  1.75it/s]Extractor Estimating: 222it [02:16,  1.67it/s]Extractor Estimating: 223it [02:16,  1.69it/s]Extractor Estimating: 224it [02:17,  1.66it/s]Extractor Estimating: 225it [02:18,  1.64it/s]Extractor Estimating: 226it [02:18,  1.61it/s]Extractor Estimating: 227it [02:19,  1.61it/s]Extractor Estimating: 228it [02:19,  1.61it/s]Extractor Estimating: 229it [02:20,  1.47it/s]Extractor Estimating: 230it [02:21,  1.50it/s]Extractor Estimating: 231it [02:22,  1.50it/s]Extractor Estimating: 232it [02:22,  1.53it/s]Extractor Estimating: 233it [02:23,  1.54it/s]Extractor Estimating: 234it [02:23,  1.57it/s]Extractor Estimating: 235it [02:24,  1.57it/s]Extractor Estimating: 236it [02:25,  1.58it/s]Extractor Estimating: 237it [02:25,  1.55it/s]Extractor Estimating: 238it [02:26,  1.55it/s]Extractor Estimating: 239it [02:27,  1.56it/s]Extractor Estimating: 240it [02:27,  1.58it/s]Extractor Estimating: 241it [02:28,  1.60it/s]Extractor Estimating: 242it [02:28,  1.67it/s]Extractor Estimating: 243it [02:29,  1.66it/s]Extractor Estimating: 244it [02:30,  1.60it/s]Extractor Estimating: 245it [02:30,  1.60it/s]Extractor Estimating: 246it [02:31,  1.58it/s]Extractor Estimating: 247it [02:32,  1.59it/s]Extractor Estimating: 248it [02:32,  1.59it/s]Extractor Estimating: 249it [02:33,  1.59it/s]Extractor Estimating: 250it [02:33,  1.59it/s]Extractor Estimating: 250it [02:33,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:06,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:06,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:06,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:06,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:06,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:19:07,142 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:19:07,143 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:19:07,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:19:08,794 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:19:08,795 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:11,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:11,743 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:11,743 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:11,743 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:11,743 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:19:12,408 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:19:12,409 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:19:12,993 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:19:13,161 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:19:13,161 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:13:47,226 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:13:47,245 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 2999 mean pseudo reward: 0.9735768160511564
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 15651
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15751, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15751, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.998, loss:350.5818
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 1.052, loss:307.7219
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.007, loss:284.4234
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 1.005, loss:266.5618
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.021, loss:242.6951
>> valid entity prec:0.4863, rec:0.4104, f1:0.4451
>> valid relation prec:0.2671, rec:0.0990, f1:0.1444
>> valid relation with NER prec:0.2671, rec:0.0990, f1:0.1444
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.016, loss:231.9362
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 1.006, loss:227.0562
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.015, loss:218.6682
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 1.007, loss:209.2339
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 1.021, loss:210.7532
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4734, rec:0.3961, f1:0.4314
>> valid relation prec:0.2517, rec:0.0959, f1:0.1389
>> valid relation with NER prec:0.2517, rec:0.0959, f1:0.1389
g_step 1100, step 100, avg_time 1.014, loss:201.1820
g_step 1200, step 75, avg_time 1.008, loss:197.3116
g_step 1300, step 50, avg_time 1.021, loss:178.8701
g_step 1400, step 25, avg_time 1.013, loss:187.1187
g_step 1500, step 125, avg_time 1.010, loss:168.9147
>> valid entity prec:0.4563, rec:0.3848, f1:0.4175
>> valid relation prec:0.2867, rec:0.0889, f1:0.1358
>> valid relation with NER prec:0.2867, rec:0.0889, f1:0.1358
g_step 1600, step 100, avg_time 1.013, loss:155.3930
g_step 1700, step 75, avg_time 1.024, loss:164.2416
g_step 1800, step 50, avg_time 1.005, loss:152.8651
g_step 1900, step 25, avg_time 1.024, loss:140.2403
g_step 2000, step 125, avg_time 1.005, loss:140.1444
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4864, rec:0.3787, f1:0.4258
>> valid relation prec:0.2233, rec:0.1040, f1:0.1419
>> valid relation with NER prec:0.2233, rec:0.1040, f1:0.1419
g_step 2100, step 100, avg_time 1.011, loss:126.2749
g_step 2200, step 75, avg_time 1.013, loss:119.4752
g_step 2300, step 50, avg_time 1.011, loss:133.5585
g_step 2400, step 25, avg_time 1.023, loss:116.9816
g_step 2500, step 125, avg_time 1.019, loss:114.6684
>> valid entity prec:0.4699, rec:0.4196, f1:0.4433
>> valid relation prec:0.2020, rec:0.1042, f1:0.1375
>> valid relation with NER prec:0.2020, rec:0.1042, f1:0.1375
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:13:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:13:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-13-47_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:13:48 - WARNING - datasets.builder -   Using custom data configuration default-041e8c19e740d395
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-041e8c19e740d395/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:13:49,640 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:13:49,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:13:49,641 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:13:49,642 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:13:49,727 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,784 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,784 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,785 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,785 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,785 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:13:49,785 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:13:50,233 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:13:53,596 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:13:53,639 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-041e8c19e740d395/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.44ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.45ba/s]100%|██████████| 3/3 [00:00<00:00,  3.99ba/s]100%|██████████| 3/3 [00:00<00:00,  3.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.69ba/s] 40%|████      | 2/5 [00:00<00:00,  3.59ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.99ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.24ba/s]100%|██████████| 5/5 [00:01<00:00,  5.02ba/s]100%|██████████| 5/5 [00:01<00:00,  4.35ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  3.81ba/s]100%|██████████| 3/3 [00:00<00:00,  7.38ba/s]100%|██████████| 3/3 [00:00<00:00,  6.74ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.89ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.93ba/s]100%|██████████| 5/5 [00:00<00:00,  7.35ba/s]100%|██████████| 5/5 [00:00<00:00,  5.87ba/s]
[INFO|trainer.py:414] 2023-08-29 10:14:00,966 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:14:01,133 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:14:01,133 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-29 10:14:01,133 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:14:01,133 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:14:01,133 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:14:01,133 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:14:01,133 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:01<03:56,  1.01s/it]  1%|          | 2/235 [00:01<02:24,  1.62it/s]  1%|▏         | 3/235 [00:01<01:58,  1.96it/s]  2%|▏         | 4/235 [00:02<01:40,  2.29it/s]  2%|▏         | 5/235 [00:02<01:28,  2.60it/s]  3%|▎         | 6/235 [00:02<01:20,  2.84it/s]  3%|▎         | 7/235 [00:02<01:17,  2.94it/s]  3%|▎         | 8/235 [00:03<01:13,  3.08it/s]  4%|▍         | 9/235 [00:03<01:11,  3.18it/s]  4%|▍         | 10/235 [00:03<01:09,  3.25it/s]  5%|▍         | 11/235 [00:04<01:07,  3.31it/s]  5%|▌         | 12/235 [00:04<01:06,  3.34it/s]  6%|▌         | 13/235 [00:04<01:05,  3.36it/s]  6%|▌         | 14/235 [00:04<01:05,  3.38it/s]  6%|▋         | 15/235 [00:05<01:04,  3.40it/s]  7%|▋         | 16/235 [00:05<01:04,  3.41it/s]  7%|▋         | 17/235 [00:05<01:03,  3.41it/s]  8%|▊         | 18/235 [00:06<01:03,  3.42it/s]  8%|▊         | 19/235 [00:06<01:03,  3.42it/s]  9%|▊         | 20/235 [00:06<01:02,  3.42it/s]  9%|▉         | 21/235 [00:07<01:02,  3.42it/s]  9%|▉         | 22/235 [00:07<01:02,  3.42it/s] 10%|▉         | 23/235 [00:07<01:03,  3.36it/s] 10%|█         | 24/235 [00:07<01:02,  3.38it/s] 11%|█         | 25/235 [00:08<01:01,  3.39it/s] 11%|█         | 26/235 [00:08<01:01,  3.40it/s] 11%|█▏        | 27/235 [00:08<01:01,  3.40it/s] 12%|█▏        | 28/235 [00:09<01:00,  3.41it/s] 12%|█▏        | 29/235 [00:09<01:00,  3.42it/s] 13%|█▎        | 30/235 [00:09<01:00,  3.41it/s] 13%|█▎        | 31/235 [00:09<00:59,  3.42it/s] 14%|█▎        | 32/235 [00:10<00:59,  3.42it/s] 14%|█▍        | 33/235 [00:10<00:59,  3.42it/s] 14%|█▍        | 34/235 [00:10<00:58,  3.42it/s] 15%|█▍        | 35/235 [00:11<00:58,  3.42it/s] 15%|█▌        | 36/235 [00:11<00:58,  3.42it/s] 16%|█▌        | 37/235 [00:11<00:57,  3.42it/s] 16%|█▌        | 38/235 [00:12<00:57,  3.42it/s] 17%|█▋        | 39/235 [00:12<00:57,  3.42it/s] 17%|█▋        | 40/235 [00:12<00:59,  3.27it/s] 17%|█▋        | 41/235 [00:12<00:58,  3.31it/s] 18%|█▊        | 42/235 [00:13<00:57,  3.34it/s] 18%|█▊        | 43/235 [00:13<00:57,  3.36it/s] 19%|█▊        | 44/235 [00:13<00:56,  3.38it/s] 19%|█▉        | 45/235 [00:14<00:56,  3.39it/s] 20%|█▉        | 46/235 [00:14<00:55,  3.40it/s] 20%|██        | 47/235 [00:14<00:53,  3.53it/s][INFO|trainer.py:2140] 2023-08-29 10:14:15,809 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:14:15,809 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:14:15,810 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.60it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.15it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.50it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.57it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.84it/s][A
  6%|▌         | 32/577 [00:00<00:12, 45.40it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.29it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.24it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.18it/s][A
  9%|▉         | 52/577 [00:01<00:12, 42.89it/s][A
 10%|▉         | 57/577 [00:01<00:11, 43.72it/s][A
 11%|█         | 62/577 [00:01<00:11, 44.34it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 44.72it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.99it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.83it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.93it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 44.85it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.85it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.83it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.90it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.14it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.36it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.41it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.34it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 42.54it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.34it/s][A
 24%|██▎       | 137/577 [00:03<00:10, 43.84it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.15it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.40it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.86it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.04it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.13it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.82it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.04it/s][A
 31%|███       | 177/577 [00:03<00:09, 41.97it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 43.09it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.71it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.24it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.63it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.93it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.92it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.97it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 43.22it/s][A
 38%|███▊      | 222/577 [00:04<00:08, 43.87it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.32it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.62it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.81it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.94it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.07it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.15it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.94it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.75it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 44.83it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 44.97it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.17it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.23it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.31it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.25it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.19it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 43.77it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 44.22it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 44.43it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 44.86it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.00it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.03it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.17it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.20it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.94it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.81it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.86it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 44.29it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.63it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.92it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.08it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.17it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.95it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 44.92it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.74it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.68it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 44.96it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.03it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.21it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.07it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.50it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 41.80it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 42.81it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 43.55it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.00it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.37it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.63it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.78it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.87it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.76it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.72it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.88it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.07it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.04it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 45.05it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.07it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.16it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.20it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.99it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 41.23it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 29.08it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 33.01it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 35.97it/s][A
 93%|█████████▎| 537/577 [00:12<00:01, 38.33it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 40.23it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 41.71it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 42.79it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 43.48it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 43.92it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 43.82it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 42.73it/s][A
100%|██████████| 577/577 [00:13<00:00, 43.67it/s][A                                                
                                                 [A 20%|██        | 47/235 [00:27<00:53,  3.53it/s]
100%|██████████| 577/577 [00:13<00:00, 43.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:14:28,968 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-29 10:14:29,102 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:14:32,035 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:14:32,312 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:14:32,405 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:37<22:19,  7.17s/it] 21%|██        | 49/235 [00:38<15:50,  5.11s/it] 21%|██▏       | 50/235 [00:38<11:17,  3.66s/it] 22%|██▏       | 51/235 [00:38<08:08,  2.65s/it] 22%|██▏       | 52/235 [00:39<05:55,  1.94s/it] 23%|██▎       | 53/235 [00:39<04:23,  1.45s/it] 23%|██▎       | 54/235 [00:39<03:19,  1.10s/it] 23%|██▎       | 55/235 [00:39<02:34,  1.16it/s] 24%|██▍       | 56/235 [00:40<02:03,  1.45it/s] 24%|██▍       | 57/235 [00:40<01:41,  1.75it/s] 25%|██▍       | 58/235 [00:40<01:26,  2.05it/s] 25%|██▌       | 59/235 [00:41<01:15,  2.33it/s] 26%|██▌       | 60/235 [00:41<01:09,  2.52it/s] 26%|██▌       | 61/235 [00:41<01:03,  2.73it/s] 26%|██▋       | 62/235 [00:42<00:59,  2.91it/s] 27%|██▋       | 63/235 [00:42<00:56,  3.05it/s] 27%|██▋       | 64/235 [00:42<00:54,  3.15it/s] 28%|██▊       | 65/235 [00:42<00:52,  3.22it/s] 28%|██▊       | 66/235 [00:43<00:51,  3.28it/s] 29%|██▊       | 67/235 [00:43<00:50,  3.32it/s] 29%|██▉       | 68/235 [00:43<00:49,  3.35it/s] 29%|██▉       | 69/235 [00:44<00:49,  3.37it/s] 30%|██▉       | 70/235 [00:44<00:48,  3.38it/s] 30%|███       | 71/235 [00:44<00:49,  3.31it/s] 31%|███       | 72/235 [00:44<00:48,  3.35it/s] 31%|███       | 73/235 [00:45<00:48,  3.37it/s] 31%|███▏      | 74/235 [00:45<00:47,  3.38it/s] 32%|███▏      | 75/235 [00:45<00:47,  3.40it/s] 32%|███▏      | 76/235 [00:46<00:46,  3.40it/s] 33%|███▎      | 77/235 [00:46<00:46,  3.41it/s] 33%|███▎      | 78/235 [00:46<00:46,  3.41it/s] 34%|███▎      | 79/235 [00:47<00:45,  3.41it/s] 34%|███▍      | 80/235 [00:47<00:45,  3.41it/s] 34%|███▍      | 81/235 [00:47<00:45,  3.41it/s] 35%|███▍      | 82/235 [00:47<00:45,  3.36it/s] 35%|███▌      | 83/235 [00:48<00:45,  3.37it/s] 36%|███▌      | 84/235 [00:48<00:44,  3.39it/s] 36%|███▌      | 85/235 [00:48<00:44,  3.40it/s] 37%|███▋      | 86/235 [00:49<00:43,  3.40it/s] 37%|███▋      | 87/235 [00:49<00:43,  3.41it/s] 37%|███▋      | 88/235 [00:49<00:43,  3.41it/s] 38%|███▊      | 89/235 [00:49<00:42,  3.41it/s] 38%|███▊      | 90/235 [00:50<00:42,  3.41it/s] 39%|███▊      | 91/235 [00:50<00:42,  3.41it/s] 39%|███▉      | 92/235 [00:50<00:41,  3.41it/s] 40%|███▉      | 93/235 [00:51<00:43,  3.30it/s] 40%|████      | 94/235 [00:51<00:40,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 10:14:52,582 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:14:52,583 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:14:52,583 >>   Batch size = 8
{'eval_loss': 1.0683397054672241, 'eval_runtime': 13.1207, 'eval_samples_per_second': 351.278, 'eval_steps_per_second': 43.976, 'epoch': 1.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.15it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.36it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.76it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.65it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.93it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.47it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.35it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.30it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.38it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.50it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.50it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.53it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.45it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.23it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.05it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.02it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 44.96it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.13it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.36it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.43it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.48it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.41it/s][A
 20%|██        | 117/577 [00:02<00:11, 39.95it/s][A
 21%|██        | 122/577 [00:02<00:10, 41.62it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 42.69it/s][A
 23%|██▎       | 132/577 [00:02<00:10, 43.66it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.15it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.65it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.97it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.05it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.69it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.44it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.69it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.88it/s][A
 31%|███       | 177/577 [00:03<00:09, 41.38it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 42.68it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 43.52it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.08it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.52it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.86it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.94it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.77it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.63it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 44.68it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.00it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.25it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.23it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.47it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.43it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.42it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.03it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.89it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.00it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.01it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.22it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.34it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.38it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.46it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.41it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.09it/s][A
 53%|█████▎    | 307/577 [00:07<00:05, 45.12it/s][A
 54%|█████▍    | 312/577 [00:07<00:08, 31.39it/s][A
 55%|█████▍    | 317/577 [00:07<00:07, 34.65it/s][A
 56%|█████▌    | 322/577 [00:07<00:06, 37.37it/s][A
 57%|█████▋    | 327/577 [00:07<00:06, 39.56it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 41.15it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 42.46it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 43.32it/s][A
 60%|██████    | 347/577 [00:07<00:05, 43.93it/s][A
 61%|██████    | 352/577 [00:07<00:05, 43.88it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.17it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.57it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.80it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.06it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.30it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.29it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.39it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.21it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.97it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.89it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.92it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.08it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.19it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.34it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.41it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.38it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.22it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 41.85it/s][A
 77%|███████▋  | 447/577 [00:10<00:03, 42.93it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 43.71it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.24it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.64it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.87it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.18it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.16it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.84it/s][A
 84%|████████▍ | 487/577 [00:11<00:02, 44.87it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 45.02it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.09it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.24it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.32it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.43it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.48it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.22it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.95it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 45.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.99it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.12it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.24it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.34it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.48it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.48it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.28it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.89it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.04it/s][A                                                
                                                 [A 40%|████      | 94/235 [01:04<00:40,  3.45it/s]
100%|██████████| 577/577 [00:12<00:00, 45.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:15:05,731 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 10:15:05,891 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:15:08,694 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:15:08,840 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:15:08,905 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:14<16:51,  7.22s/it] 41%|████      | 96/235 [01:15<11:56,  5.15s/it] 41%|████▏     | 97/235 [01:15<08:29,  3.69s/it] 42%|████▏     | 98/235 [01:15<06:06,  2.67s/it] 42%|████▏     | 99/235 [01:16<04:26,  1.96s/it] 43%|████▎     | 100/235 [01:16<03:16,  1.46s/it] 43%|████▎     | 101/235 [01:16<02:28,  1.11s/it] 43%|████▎     | 102/235 [01:16<01:54,  1.16it/s] 44%|████▍     | 103/235 [01:17<01:31,  1.44it/s] 44%|████▍     | 104/235 [01:17<01:15,  1.75it/s] 45%|████▍     | 105/235 [01:17<01:03,  2.05it/s] 45%|████▌     | 106/235 [01:18<00:55,  2.33it/s] 46%|████▌     | 107/235 [01:18<00:50,  2.53it/s] 46%|████▌     | 108/235 [01:18<00:46,  2.75it/s] 46%|████▋     | 109/235 [01:18<00:43,  2.92it/s] 47%|████▋     | 110/235 [01:19<00:40,  3.05it/s] 47%|████▋     | 111/235 [01:19<00:39,  3.15it/s] 48%|████▊     | 112/235 [01:19<00:38,  3.23it/s] 48%|████▊     | 113/235 [01:20<00:37,  3.28it/s] 49%|████▊     | 114/235 [01:20<00:36,  3.32it/s] 49%|████▉     | 115/235 [01:20<00:35,  3.35it/s] 49%|████▉     | 116/235 [01:21<00:35,  3.37it/s] 50%|████▉     | 117/235 [01:21<00:34,  3.38it/s] 50%|█████     | 118/235 [01:21<00:35,  3.28it/s] 51%|█████     | 119/235 [01:21<00:34,  3.32it/s] 51%|█████     | 120/235 [01:22<00:34,  3.35it/s] 51%|█████▏    | 121/235 [01:22<00:33,  3.37it/s] 52%|█████▏    | 122/235 [01:22<00:33,  3.39it/s] 52%|█████▏    | 123/235 [01:23<00:32,  3.39it/s] 53%|█████▎    | 124/235 [01:23<00:32,  3.40it/s] 53%|█████▎    | 125/235 [01:23<00:32,  3.40it/s] 54%|█████▎    | 126/235 [01:24<00:32,  3.34it/s] 54%|█████▍    | 127/235 [01:24<00:32,  3.36it/s] 54%|█████▍    | 128/235 [01:24<00:31,  3.38it/s] 55%|█████▍    | 129/235 [01:24<00:32,  3.30it/s] 55%|█████▌    | 130/235 [01:25<00:31,  3.33it/s] 56%|█████▌    | 131/235 [01:25<00:30,  3.36it/s] 56%|█████▌    | 132/235 [01:25<00:30,  3.38it/s] 57%|█████▋    | 133/235 [01:26<00:30,  3.39it/s] 57%|█████▋    | 134/235 [01:26<00:37,  2.66it/s] 57%|█████▋    | 135/235 [01:26<00:35,  2.85it/s] 58%|█████▊    | 136/235 [01:27<00:33,  3.00it/s] 58%|█████▊    | 137/235 [01:27<00:31,  3.11it/s] 59%|█████▊    | 138/235 [01:27<00:30,  3.20it/s] 59%|█████▉    | 139/235 [01:28<00:29,  3.28it/s] 60%|█████▉    | 140/235 [01:28<00:28,  3.33it/s] 60%|██████    | 141/235 [01:28<00:26,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 10:15:29,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:15:29,797 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:15:29,797 >>   Batch size = 8
{'eval_loss': 1.0839077234268188, 'eval_runtime': 13.0104, 'eval_samples_per_second': 354.256, 'eval_steps_per_second': 44.349, 'epoch': 2.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.22it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.37it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.25it/s][A
  4%|▍         | 22/577 [00:00<00:12, 45.73it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.53it/s][A
  6%|▌         | 32/577 [00:00<00:12, 45.33it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.15it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.21it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.24it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.35it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.36it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.34it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.36it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.29it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.17it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.09it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.03it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.20it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.25it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.24it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.29it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.36it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.27it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.19it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.15it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.05it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.06it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.16it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.24it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.30it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.34it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 44.39it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 44.60it/s][A
 31%|███       | 177/577 [00:03<00:08, 44.70it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.82it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.85it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.19it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.02it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.18it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.22it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.22it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.14it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 44.98it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.15it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.07it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.33it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.18it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.17it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.23it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.34it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.21it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.03it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.09it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.07it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.31it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.30it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.26it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.27it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 44.80it/s][A
 53%|█████▎    | 307/577 [00:06<00:06, 45.00it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 44.99it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.01it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.88it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.19it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.21it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.37it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.25it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.29it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.19it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.09it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.11it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.09it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.13it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.18it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.14it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.31it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.24it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.24it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.14it/s][A
 71%|███████   | 407/577 [00:08<00:03, 45.08it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.11it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.10it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.16it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.18it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.25it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.23it/s][A
 77%|███████▋  | 442/577 [00:09<00:03, 44.21it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 44.62it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.71it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.80it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.89it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.07it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.95it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.13it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.00it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.07it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.10it/s][A
 86%|████████▌ | 497/577 [00:10<00:01, 45.09it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.18it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.12it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.25it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 45.19it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.19it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.12it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.10it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.18it/s][A
 94%|█████████▍| 542/577 [00:11<00:00, 45.25it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.14it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.10it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.13it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.10it/s][A
100%|██████████| 577/577 [00:12<00:00, 43.15it/s][A                                                 
                                                 [A 60%|██████    | 141/235 [01:41<00:26,  3.48it/s]
100%|██████████| 577/577 [00:12<00:00, 43.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:15:42,680 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-29 10:15:42,826 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:15:46,169 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:15:46,284 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:15:46,332 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [01:51<11:09,  7.20s/it] 61%|██████    | 143/235 [01:52<07:52,  5.13s/it] 61%|██████▏   | 144/235 [01:52<05:34,  3.68s/it] 62%|██████▏   | 145/235 [01:52<03:59,  2.66s/it] 62%|██████▏   | 146/235 [01:53<02:53,  1.95s/it] 63%|██████▎   | 147/235 [01:53<02:07,  1.45s/it] 63%|██████▎   | 148/235 [01:53<01:36,  1.10s/it] 63%|██████▎   | 149/235 [01:54<01:13,  1.16it/s] 64%|██████▍   | 150/235 [01:54<00:58,  1.45it/s] 64%|██████▍   | 151/235 [01:54<00:47,  1.76it/s] 65%|██████▍   | 152/235 [01:54<00:40,  2.07it/s] 65%|██████▌   | 153/235 [01:55<00:34,  2.35it/s] 66%|██████▌   | 154/235 [01:55<00:32,  2.51it/s] 66%|██████▌   | 155/235 [01:55<00:29,  2.74it/s] 66%|██████▋   | 156/235 [01:56<00:27,  2.92it/s] 67%|██████▋   | 157/235 [01:56<00:25,  3.07it/s] 67%|██████▋   | 158/235 [01:56<00:24,  3.18it/s] 68%|██████▊   | 159/235 [01:56<00:23,  3.26it/s] 68%|██████▊   | 160/235 [01:57<00:22,  3.32it/s] 69%|██████▊   | 161/235 [01:57<00:21,  3.36it/s] 69%|██████▉   | 162/235 [01:57<00:21,  3.40it/s] 69%|██████▉   | 163/235 [01:58<00:21,  3.42it/s] 70%|██████▉   | 164/235 [01:58<00:20,  3.43it/s] 70%|███████   | 165/235 [01:58<00:21,  3.28it/s] 71%|███████   | 166/235 [01:59<00:20,  3.33it/s] 71%|███████   | 167/235 [01:59<00:20,  3.37it/s] 71%|███████▏  | 168/235 [01:59<00:19,  3.40it/s] 72%|███████▏  | 169/235 [01:59<00:19,  3.42it/s] 72%|███████▏  | 170/235 [02:00<00:18,  3.43it/s] 73%|███████▎  | 171/235 [02:00<00:18,  3.44it/s] 73%|███████▎  | 172/235 [02:00<00:18,  3.45it/s] 74%|███████▎  | 173/235 [02:01<00:17,  3.45it/s] 74%|███████▍  | 174/235 [02:01<00:17,  3.46it/s] 74%|███████▍  | 175/235 [02:01<00:17,  3.46it/s] 75%|███████▍  | 176/235 [02:01<00:17,  3.29it/s] 75%|███████▌  | 177/235 [02:02<00:17,  3.34it/s] 76%|███████▌  | 178/235 [02:02<00:16,  3.38it/s] 76%|███████▌  | 179/235 [02:02<00:16,  3.40it/s] 77%|███████▋  | 180/235 [02:03<00:16,  3.42it/s] 77%|███████▋  | 181/235 [02:03<00:15,  3.44it/s] 77%|███████▋  | 182/235 [02:03<00:15,  3.44it/s] 78%|███████▊  | 183/235 [02:03<00:15,  3.45it/s] 78%|███████▊  | 184/235 [02:04<00:14,  3.45it/s] 79%|███████▊  | 185/235 [02:04<00:14,  3.46it/s] 79%|███████▉  | 186/235 [02:04<00:14,  3.46it/s] 80%|███████▉  | 187/235 [02:05<00:14,  3.36it/s] 80%|████████  | 188/235 [02:05<00:13,  3.51it/s][INFO|trainer.py:2140] 2023-08-29 10:16:06,567 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:16:06,567 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:16:06,567 >>   Batch size = 8
{'eval_loss': 1.101976752281189, 'eval_runtime': 12.8013, 'eval_samples_per_second': 360.043, 'eval_steps_per_second': 45.074, 'epoch': 3.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.61it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.72it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.46it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.29it/s][A
  5%|▍         | 28/577 [00:00<00:12, 45.68it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.33it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.29it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.42it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.34it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.45it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.29it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.28it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.08it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 44.94it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 44.89it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 44.97it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 45.04it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.29it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.31it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.37it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.39it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.27it/s][A
 20%|██        | 118/577 [00:02<00:10, 42.89it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 43.50it/s][A
 22%|██▏       | 128/577 [00:02<00:10, 43.88it/s][A
 23%|██▎       | 133/577 [00:02<00:10, 44.25it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 44.69it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 45.03it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 45.22it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.32it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.00it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 44.98it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 45.11it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.02it/s][A
 31%|███       | 178/577 [00:03<00:08, 45.00it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.12it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.31it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.38it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.37it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.15it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.12it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.11it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.07it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 45.16it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.13it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.17it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.28it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.37it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.27it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 44.08it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 44.47it/s][A
 46%|████▌     | 263/577 [00:05<00:07, 44.63it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 44.78it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 44.95it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 45.09it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 45.09it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 45.24it/s][A
 51%|█████     | 293/577 [00:06<00:06, 44.99it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.08it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.12it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 45.19it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.24it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 45.17it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 45.24it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.24it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.30it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.16it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.11it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.17it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.21it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 45.27it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 45.23it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 45.23it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 45.28it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.19it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.20it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.07it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.16it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.12it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.27it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.28it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.14it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 45.31it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 45.16it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 45.21it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.17it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.08it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 44.75it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 45.00it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 45.15it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 45.13it/s][A
 80%|████████  | 463/577 [00:10<00:02, 45.26it/s][A
 81%|████████  | 468/577 [00:10<00:02, 45.27it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.35it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.08it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.05it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.11it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 45.24it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 45.25it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 45.30it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 45.36it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 45.39it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 45.16it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.09it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.11it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.05it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 43.84it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 44.46it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 44.74it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 44.98it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 45.02it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 44.96it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 44.94it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 44.99it/s][A                                                 
                                                 [A 80%|████████  | 188/235 [02:18<00:13,  3.51it/s]
100%|██████████| 577/577 [00:12<00:00, 44.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:16:19,559 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 10:16:19,798 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:16:23,672 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:16:23,841 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:16:23,979 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [02:31<06:07,  7.99s/it] 81%|████████  | 190/235 [02:31<04:15,  5.68s/it] 81%|████████▏ | 191/235 [02:31<02:58,  4.07s/it] 82%|████████▏ | 192/235 [02:32<02:06,  2.93s/it] 82%|████████▏ | 193/235 [02:32<01:29,  2.14s/it] 83%|████████▎ | 194/235 [02:32<01:05,  1.59s/it] 83%|████████▎ | 195/235 [02:33<00:47,  1.20s/it] 83%|████████▎ | 196/235 [02:33<00:36,  1.08it/s] 84%|████████▍ | 197/235 [02:33<00:27,  1.36it/s] 84%|████████▍ | 198/235 [02:34<00:22,  1.66it/s] 85%|████████▍ | 199/235 [02:34<00:18,  1.96it/s] 85%|████████▌ | 200/235 [02:34<00:15,  2.25it/s] 86%|████████▌ | 201/235 [02:34<00:13,  2.45it/s] 86%|████████▌ | 202/235 [02:35<00:12,  2.68it/s] 86%|████████▋ | 203/235 [02:35<00:11,  2.86it/s] 87%|████████▋ | 204/235 [02:35<00:10,  3.01it/s] 87%|████████▋ | 205/235 [02:36<00:09,  3.12it/s] 88%|████████▊ | 206/235 [02:36<00:09,  3.20it/s] 88%|████████▊ | 207/235 [02:36<00:08,  3.26it/s] 89%|████████▊ | 208/235 [02:36<00:08,  3.31it/s] 89%|████████▉ | 209/235 [02:37<00:07,  3.34it/s] 89%|████████▉ | 210/235 [02:37<00:07,  3.36it/s] 90%|████████▉ | 211/235 [02:37<00:07,  3.38it/s] 90%|█████████ | 212/235 [02:38<00:06,  3.31it/s] 91%|█████████ | 213/235 [02:38<00:06,  3.34it/s] 91%|█████████ | 214/235 [02:38<00:06,  3.36it/s] 91%|█████████▏| 215/235 [02:39<00:05,  3.38it/s] 92%|█████████▏| 216/235 [02:39<00:05,  3.39it/s] 92%|█████████▏| 217/235 [02:39<00:05,  3.40it/s] 93%|█████████▎| 218/235 [02:39<00:04,  3.40it/s] 93%|█████████▎| 219/235 [02:40<00:04,  3.41it/s] 94%|█████████▎| 220/235 [02:40<00:04,  3.41it/s] 94%|█████████▍| 221/235 [02:40<00:04,  3.41it/s] 94%|█████████▍| 222/235 [02:41<00:03,  3.41it/s] 95%|█████████▍| 223/235 [02:41<00:03,  3.33it/s] 95%|█████████▌| 224/235 [02:41<00:03,  3.36it/s] 96%|█████████▌| 225/235 [02:42<00:02,  3.38it/s] 96%|█████████▌| 226/235 [02:42<00:02,  3.39it/s] 97%|█████████▋| 227/235 [02:42<00:02,  3.39it/s] 97%|█████████▋| 228/235 [02:42<00:02,  3.40it/s] 97%|█████████▋| 229/235 [02:43<00:01,  3.41it/s] 98%|█████████▊| 230/235 [02:43<00:01,  3.41it/s] 98%|█████████▊| 231/235 [02:43<00:01,  3.41it/s] 99%|█████████▊| 232/235 [02:44<00:00,  3.41it/s] 99%|█████████▉| 233/235 [02:44<00:00,  3.41it/s]100%|█████████▉| 234/235 [02:44<00:00,  3.34it/s]100%|██████████| 235/235 [02:44<00:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 10:16:46,074 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:16:46,074 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:16:46,074 >>   Batch size = 8
{'eval_loss': 1.114698052406311, 'eval_runtime': 12.797, 'eval_samples_per_second': 360.164, 'eval_steps_per_second': 45.089, 'epoch': 4.0}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.44it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.61it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.44it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.17it/s][A
  5%|▍         | 28/577 [00:00<00:11, 45.83it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.49it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.40it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.25it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.34it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.39it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.52it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.45it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.29it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.22it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 45.11it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 45.14it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 44.97it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.30it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.37it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.52it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.24it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.30it/s][A
 20%|██        | 118/577 [00:02<00:10, 44.73it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 44.77it/s][A
 22%|██▏       | 128/577 [00:02<00:10, 44.84it/s][A
 23%|██▎       | 133/577 [00:02<00:09, 44.80it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 44.93it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 45.26it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 45.31it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.40it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.28it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.19it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 45.05it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.06it/s][A
 31%|███       | 178/577 [00:03<00:08, 44.99it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.17it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.34it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.34it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.31it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.16it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 44.97it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.04it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.08it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 45.03it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.16it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.25it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.37it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.42it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.28it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 44.96it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 44.98it/s][A
 46%|████▌     | 263/577 [00:05<00:06, 45.16it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 44.97it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 45.19it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 43.73it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 44.36it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 44.77it/s][A
 51%|█████     | 293/577 [00:06<00:06, 44.96it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 44.88it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 44.95it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 44.93it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 44.96it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 44.84it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 44.94it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.25it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.31it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.40it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.32it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.24it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.08it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 44.96it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 44.97it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 45.04it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 45.12it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.37it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.38it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.31it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.14it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.09it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.12it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.08it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 41.34it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 42.50it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 43.40it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 44.12it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 44.60it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 44.87it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 44.82it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 45.00it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 44.62it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 44.64it/s][A
 80%|████████  | 463/577 [00:10<00:02, 44.89it/s][A
 81%|████████  | 468/577 [00:10<00:02, 44.96it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.13it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.28it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.44it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.43it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 45.23it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 44.85it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 44.80it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 44.95it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 45.12it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 45.22it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.34it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.53it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.47it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 45.20it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 44.88it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 38.87it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 40.75it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 42.07it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 43.09it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 43.78it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 44.39it/s][A                                                 
                                                 [A100%|██████████| 235/235 [02:57<00:00,  3.48it/s]
100%|██████████| 577/577 [00:12<00:00, 44.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:16:59,115 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-29 10:16:59,444 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:17:03,743 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:17:03,969 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:17:04,081 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:17:12,379 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:17:12,401 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47 (score: 1.0683397054672241).
                                                 100%|██████████| 235/235 [03:20<00:00,  3.48it/s]100%|██████████| 235/235 [03:20<00:00,  1.17it/s]
[INFO|trainer.py:1894] 2023-08-29 10:17:22,146 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 10:17:22,366 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:17:25,776 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:17:25,931 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:17:25,992 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:17:26,439 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   train_loss               =     0.4237
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   train_runtime            = 0:03:20.89
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   train_samples_per_second =     74.667
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:26,439 >>   train_steps_per_second   =       1.17
{'eval_loss': 1.1194119453430176, 'eval_runtime': 12.8579, 'eval_samples_per_second': 358.457, 'eval_steps_per_second': 44.875, 'epoch': 5.0}
{'train_runtime': 200.8911, 'train_samples_per_second': 74.667, 'train_steps_per_second': 1.17, 'train_loss': 0.4236564149247839, 'epoch': 5.0}
08/29/2023 10:17:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:17:26,663 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:17:26,663 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 10:17:26,663 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 55.91it/s]  2%|▏         | 12/577 [00:00<00:11, 49.80it/s]  3%|▎         | 18/577 [00:00<00:11, 48.10it/s]  4%|▍         | 23/577 [00:00<00:11, 47.19it/s]  5%|▍         | 28/577 [00:00<00:11, 46.81it/s]  6%|▌         | 33/577 [00:00<00:11, 46.55it/s]  7%|▋         | 38/577 [00:00<00:11, 46.26it/s]  7%|▋         | 43/577 [00:00<00:11, 45.92it/s]  8%|▊         | 48/577 [00:01<00:11, 45.36it/s]  9%|▉         | 53/577 [00:01<00:11, 45.03it/s] 10%|█         | 58/577 [00:01<00:11, 45.17it/s] 11%|█         | 63/577 [00:01<00:11, 45.31it/s] 12%|█▏        | 68/577 [00:01<00:11, 45.56it/s] 13%|█▎        | 73/577 [00:01<00:11, 45.49it/s] 14%|█▎        | 78/577 [00:01<00:10, 45.66it/s] 14%|█▍        | 83/577 [00:01<00:10, 45.73it/s] 15%|█▌        | 88/577 [00:01<00:10, 45.59it/s] 16%|█▌        | 93/577 [00:02<00:10, 45.22it/s] 17%|█▋        | 98/577 [00:02<00:10, 45.03it/s] 18%|█▊        | 103/577 [00:02<00:10, 43.13it/s] 19%|█▊        | 108/577 [00:02<00:10, 44.06it/s] 20%|█▉        | 113/577 [00:02<00:10, 44.54it/s] 20%|██        | 118/577 [00:02<00:10, 45.03it/s] 21%|██▏       | 123/577 [00:02<00:10, 45.27it/s] 22%|██▏       | 128/577 [00:02<00:09, 45.36it/s] 23%|██▎       | 133/577 [00:02<00:09, 45.37it/s] 24%|██▍       | 138/577 [00:03<00:09, 44.30it/s] 25%|██▍       | 143/577 [00:03<00:09, 44.29it/s] 26%|██▌       | 148/577 [00:03<00:09, 44.53it/s] 27%|██▋       | 153/577 [00:03<00:09, 44.80it/s] 27%|██▋       | 158/577 [00:03<00:09, 45.10it/s] 28%|██▊       | 163/577 [00:03<00:09, 45.28it/s] 29%|██▉       | 168/577 [00:03<00:08, 45.48it/s] 30%|██▉       | 173/577 [00:03<00:08, 45.58it/s] 31%|███       | 178/577 [00:03<00:08, 45.60it/s] 32%|███▏      | 183/577 [00:04<00:08, 45.28it/s] 33%|███▎      | 188/577 [00:04<00:08, 44.98it/s] 33%|███▎      | 193/577 [00:04<00:08, 44.91it/s] 34%|███▍      | 198/577 [00:04<00:08, 45.02it/s] 35%|███▌      | 203/577 [00:04<00:08, 45.27it/s] 36%|███▌      | 208/577 [00:04<00:08, 45.40it/s] 37%|███▋      | 213/577 [00:04<00:08, 45.43it/s] 38%|███▊      | 218/577 [00:04<00:07, 45.63it/s] 39%|███▊      | 223/577 [00:04<00:08, 43.06it/s] 40%|███▉      | 228/577 [00:05<00:07, 44.29it/s] 40%|████      | 233/577 [00:05<00:14, 23.13it/s] 41%|████      | 237/577 [00:05<00:13, 25.59it/s] 42%|████▏     | 242/577 [00:05<00:11, 29.70it/s] 43%|████▎     | 247/577 [00:05<00:09, 33.35it/s] 44%|████▎     | 252/577 [00:05<00:08, 36.42it/s] 45%|████▍     | 257/577 [00:06<00:08, 38.82it/s] 45%|████▌     | 262/577 [00:06<00:07, 40.73it/s] 46%|████▋     | 267/577 [00:06<00:07, 42.19it/s] 47%|████▋     | 272/577 [00:06<00:07, 43.02it/s] 48%|████▊     | 277/577 [00:06<00:06, 43.39it/s] 49%|████▉     | 282/577 [00:06<00:06, 43.68it/s] 50%|████▉     | 287/577 [00:06<00:06, 44.03it/s] 51%|█████     | 292/577 [00:06<00:06, 44.49it/s] 51%|█████▏    | 297/577 [00:06<00:06, 44.78it/s] 52%|█████▏    | 302/577 [00:07<00:06, 45.19it/s] 53%|█████▎    | 307/577 [00:07<00:05, 45.38it/s] 54%|█████▍    | 312/577 [00:07<00:05, 45.50it/s] 55%|█████▍    | 317/577 [00:07<00:05, 45.34it/s] 56%|█████▌    | 322/577 [00:07<00:05, 45.07it/s] 57%|█████▋    | 327/577 [00:07<00:05, 44.89it/s] 58%|█████▊    | 332/577 [00:07<00:05, 44.82it/s] 58%|█████▊    | 337/577 [00:07<00:05, 45.08it/s] 59%|█████▉    | 342/577 [00:07<00:05, 45.16it/s] 60%|██████    | 347/577 [00:08<00:05, 45.39it/s] 61%|██████    | 352/577 [00:08<00:04, 45.56it/s] 62%|██████▏   | 357/577 [00:08<00:04, 45.54it/s] 63%|██████▎   | 362/577 [00:08<00:04, 45.44it/s] 64%|██████▎   | 367/577 [00:08<00:04, 45.15it/s] 64%|██████▍   | 372/577 [00:08<00:04, 44.96it/s] 65%|██████▌   | 377/577 [00:08<00:04, 43.57it/s] 66%|██████▌   | 382/577 [00:08<00:04, 44.18it/s] 67%|██████▋   | 387/577 [00:08<00:04, 44.58it/s] 68%|██████▊   | 392/577 [00:09<00:04, 44.80it/s] 69%|██████▉   | 397/577 [00:09<00:03, 45.01it/s] 70%|██████▉   | 402/577 [00:09<00:03, 45.23it/s] 71%|███████   | 407/577 [00:09<00:03, 45.32it/s] 71%|███████▏  | 412/577 [00:09<00:03, 45.26it/s] 72%|███████▏  | 417/577 [00:09<00:03, 44.90it/s] 73%|███████▎  | 422/577 [00:09<00:03, 44.99it/s] 74%|███████▍  | 427/577 [00:09<00:03, 45.18it/s] 75%|███████▍  | 432/577 [00:09<00:03, 45.32it/s] 76%|███████▌  | 437/577 [00:10<00:03, 45.36it/s] 77%|███████▋  | 442/577 [00:10<00:02, 45.29it/s] 77%|███████▋  | 447/577 [00:10<00:02, 45.38it/s] 78%|███████▊  | 452/577 [00:10<00:02, 45.44it/s] 79%|███████▉  | 457/577 [00:10<00:02, 45.22it/s] 80%|████████  | 462/577 [00:10<00:02, 45.15it/s] 81%|████████  | 467/577 [00:10<00:02, 44.99it/s] 82%|████████▏ | 472/577 [00:10<00:02, 45.07it/s] 83%|████████▎ | 477/577 [00:10<00:02, 45.34it/s] 84%|████████▎ | 482/577 [00:11<00:02, 45.32it/s] 84%|████████▍ | 487/577 [00:11<00:01, 45.36it/s] 85%|████████▌ | 492/577 [00:11<00:01, 45.31it/s] 86%|████████▌ | 497/577 [00:11<00:01, 45.28it/s] 87%|████████▋ | 502/577 [00:11<00:01, 45.11it/s] 88%|████████▊ | 507/577 [00:11<00:01, 45.00it/s] 89%|████████▊ | 512/577 [00:11<00:01, 44.94it/s] 90%|████████▉ | 517/577 [00:11<00:01, 44.36it/s] 90%|█████████ | 522/577 [00:11<00:01, 44.86it/s] 91%|█████████▏| 527/577 [00:12<00:01, 44.99it/s] 92%|█████████▏| 532/577 [00:12<00:00, 45.14it/s] 93%|█████████▎| 537/577 [00:12<00:00, 45.17it/s] 94%|█████████▍| 542/577 [00:12<00:00, 45.29it/s] 95%|█████████▍| 547/577 [00:12<00:00, 45.09it/s] 96%|█████████▌| 552/577 [00:12<00:00, 45.06it/s] 97%|█████████▋| 557/577 [00:12<00:00, 44.95it/s] 97%|█████████▋| 562/577 [00:12<00:00, 44.87it/s] 98%|█████████▊| 567/577 [00:12<00:00, 45.26it/s] 99%|█████████▉| 572/577 [00:13<00:00, 45.34it/s]100%|██████████| 577/577 [00:13<00:00, 45.48it/s]100%|██████████| 577/577 [00:13<00:00, 43.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:17:39,803 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   eval_loss               =     1.0683
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   eval_runtime            = 0:00:13.13
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   eval_samples_per_second =    350.773
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   eval_steps_per_second   =     43.913
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:17:39,803 >>   perplexity              =     2.9105
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:49,876 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:49,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:49,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:49,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:49,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:17:50,517 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:17:50,518 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:17:51,129 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:17:52,190 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:17:52,191 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:55,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:55,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:55,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:55,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:55,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:17:55,839 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:17:55,840 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:17:56,440 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:17:56,627 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:17:56,627 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-235
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-94
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-47
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/checkpoint-141
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.53it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:44,  1.60it/s]Extractor Predicting: 71it [00:45,  1.62it/s]Extractor Predicting: 72it [00:45,  1.67it/s]Extractor Predicting: 73it [00:46,  1.64it/s]Extractor Predicting: 74it [00:47,  1.66it/s]Extractor Predicting: 75it [00:47,  1.67it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:49,  1.63it/s]Extractor Predicting: 79it [00:50,  1.63it/s]Extractor Predicting: 80it [00:50,  1.63it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:51,  1.61it/s]Extractor Predicting: 83it [00:52,  1.60it/s]Extractor Predicting: 84it [00:53,  1.64it/s]Extractor Predicting: 85it [00:53,  1.70it/s]Extractor Predicting: 86it [00:54,  1.69it/s]Extractor Predicting: 87it [00:54,  1.72it/s]Extractor Predicting: 88it [00:55,  1.64it/s]Extractor Predicting: 89it [00:56,  1.63it/s]Extractor Predicting: 90it [00:56,  1.61it/s]Extractor Predicting: 91it [00:57,  1.60it/s]Extractor Predicting: 92it [00:58,  1.61it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:59,  1.63it/s]Extractor Predicting: 95it [00:59,  1.63it/s]Extractor Predicting: 96it [01:00,  1.61it/s]Extractor Predicting: 97it [01:01,  1.63it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:02,  1.60it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:03,  1.54it/s]Extractor Predicting: 102it [01:04,  1.57it/s]Extractor Predicting: 103it [01:04,  1.56it/s]Extractor Predicting: 104it [01:05,  1.60it/s]Extractor Predicting: 105it [01:06,  1.63it/s]Extractor Predicting: 106it [01:06,  1.55it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:08,  1.64it/s]Extractor Predicting: 109it [01:08,  1.61it/s]Extractor Predicting: 110it [01:09,  1.62it/s]Extractor Predicting: 111it [01:09,  1.58it/s]Extractor Predicting: 112it [01:10,  1.58it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:12,  1.47it/s]Extractor Predicting: 115it [01:12,  1.50it/s]Extractor Predicting: 116it [01:13,  1.51it/s]Extractor Predicting: 117it [01:13,  1.57it/s]Extractor Predicting: 118it [01:14,  1.58it/s]Extractor Predicting: 119it [01:15,  1.64it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:16,  1.60it/s]Extractor Predicting: 122it [01:16,  1.59it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:18,  1.60it/s]Extractor Predicting: 125it [01:18,  1.57it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:20,  1.63it/s]Extractor Predicting: 128it [01:20,  1.60it/s]Extractor Predicting: 129it [01:21,  1.59it/s]Extractor Predicting: 130it [01:22,  1.56it/s]Extractor Predicting: 131it [01:22,  1.56it/s]Extractor Predicting: 132it [01:23,  1.59it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:24,  1.55it/s]Extractor Predicting: 135it [01:25,  1.56it/s]Extractor Predicting: 136it [01:25,  1.57it/s]Extractor Predicting: 137it [01:26,  1.59it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:27,  1.55it/s]Extractor Predicting: 140it [01:28,  1.54it/s]Extractor Predicting: 141it [01:29,  1.54it/s]Extractor Predicting: 142it [01:29,  1.52it/s]Extractor Predicting: 143it [01:30,  1.55it/s]Extractor Predicting: 144it [01:31,  1.53it/s]Extractor Predicting: 145it [01:31,  1.53it/s]Extractor Predicting: 146it [01:32,  1.50it/s]Extractor Predicting: 147it [01:33,  1.51it/s]Extractor Predicting: 148it [01:33,  1.54it/s]Extractor Predicting: 149it [01:34,  1.54it/s]Extractor Predicting: 150it [01:34,  1.54it/s]Extractor Predicting: 151it [01:35,  1.58it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:36,  1.60it/s]Extractor Predicting: 154it [01:37,  1.57it/s]Extractor Predicting: 155it [01:38,  1.57it/s]Extractor Predicting: 156it [01:38,  1.55it/s]Extractor Predicting: 157it [01:39,  1.58it/s]Extractor Predicting: 158it [01:40,  1.59it/s]Extractor Predicting: 159it [01:40,  1.56it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:41,  1.61it/s]Extractor Predicting: 162it [01:42,  1.60it/s]Extractor Predicting: 163it [01:43,  1.56it/s]Extractor Predicting: 164it [01:43,  1.55it/s]Extractor Predicting: 165it [01:44,  1.56it/s]Extractor Predicting: 166it [01:45,  1.53it/s]Extractor Predicting: 167it [01:45,  1.55it/s]Extractor Predicting: 168it [01:46,  1.56it/s]Extractor Predicting: 169it [01:47,  1.56it/s]Extractor Predicting: 170it [01:47,  1.57it/s]Extractor Predicting: 171it [01:48,  1.86it/s]Extractor Predicting: 171it [01:48,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:57,729 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:57,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:57,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:57,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:57,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:19:58,385 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:19:58,386 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:19:58,978 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:20:00,012 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:20:00,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:03,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:03,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:03,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:03,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:03,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:20:03,524 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:20:03,525 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:20:04,249 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:20:04,420 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:20:04,420 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.28219346888478125,
  "recall": 0.09937079626817097,
  "score": 0.14698331193838254,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:12,  1.63it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:28,  1.65it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:32,  1.64it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:33,  1.60it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.61it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:40,  1.61it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.64it/s]Extractor Predicting: 69it [00:43,  1.65it/s]Extractor Predicting: 70it [00:43,  1.62it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:47,  1.64it/s]Extractor Predicting: 77it [00:48,  1.68it/s]Extractor Predicting: 78it [00:48,  1.68it/s]Extractor Predicting: 79it [00:49,  1.65it/s]Extractor Predicting: 80it [00:49,  1.67it/s]Extractor Predicting: 81it [00:50,  1.64it/s]Extractor Predicting: 82it [00:51,  1.51it/s]Extractor Predicting: 83it [00:51,  1.55it/s]Extractor Predicting: 84it [00:52,  1.55it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.59it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.54it/s]Extractor Predicting: 91it [00:57,  1.56it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:58,  1.58it/s]Extractor Predicting: 94it [00:58,  1.58it/s]Extractor Predicting: 95it [00:59,  1.57it/s]Extractor Predicting: 96it [01:00,  1.54it/s]Extractor Predicting: 97it [01:00,  1.55it/s]Extractor Predicting: 98it [01:01,  1.57it/s]Extractor Predicting: 99it [01:02,  1.66it/s]Extractor Predicting: 100it [01:02,  1.66it/s]Extractor Predicting: 101it [01:03,  1.63it/s]Extractor Predicting: 102it [01:03,  1.64it/s]Extractor Predicting: 103it [01:04,  1.64it/s]Extractor Predicting: 104it [01:05,  1.65it/s]Extractor Predicting: 105it [01:05,  1.62it/s]Extractor Predicting: 106it [01:06,  1.64it/s]Extractor Predicting: 107it [01:06,  1.66it/s]Extractor Predicting: 108it [01:07,  1.65it/s]Extractor Predicting: 109it [01:08,  1.59it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:09,  1.52it/s]Extractor Predicting: 112it [01:10,  1.53it/s]Extractor Predicting: 113it [01:10,  1.57it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.58it/s]Extractor Predicting: 118it [01:14,  1.55it/s]Extractor Predicting: 119it [01:14,  1.57it/s]Extractor Predicting: 120it [01:15,  1.53it/s]Extractor Predicting: 121it [01:16,  1.47it/s]Extractor Predicting: 122it [01:16,  1.51it/s]Extractor Predicting: 123it [01:17,  1.54it/s]Extractor Predicting: 124it [01:17,  1.53it/s]Extractor Predicting: 125it [01:18,  1.90it/s]Extractor Predicting: 125it [01:18,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:31,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:31,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:31,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:31,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:31,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:21:31,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:21:31,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:21:32,055 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:21:33,097 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:21:33,097 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:34,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:34,936 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:34,936 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:34,936 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:21:34,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:21:35,278 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:21:35,279 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:21:35,562 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:21:35,704 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:21:35,704 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4725274725274725,
  "recall": 0.27425310506881506,
  "score": 0.3470688190314359,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:03,  1.90it/s]Extractor Predicting: 6it [00:03,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-29 10:21:40,799 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:21:40,800 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:21:40,851 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:21:40,852 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 10:21:40,883 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:21:53,065 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 10:21:53,107 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 10:21:53,282 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:21:53,283 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:21:53,399 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:21:53,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5949367088607594,
  "recall": 0.18503937007874016,
  "score": 0.2822822822822823,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 10:21:53,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:54,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:55,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:56,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:57,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:57,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:58,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:59,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:21:59,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:00,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:01,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:01,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:02,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:02,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:03,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:04,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:04,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:05,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:06,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:56, 12.90s/it][WARNING|generation_utils.py:914] 2023-08-29 10:22:06,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:07,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:08,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:08,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:09,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:10,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:10,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:11,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:12,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:12,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:13,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:13,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:14,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:15,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:15,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:16,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:16,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:17,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:17,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:18,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:25<01:40, 12.57s/it][WARNING|generation_utils.py:914] 2023-08-29 10:22:19,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:20,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:20,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:21,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:21,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:22,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:23,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:23,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:24,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:24,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:25,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:25,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:26,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:27,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:27,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:28,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:28,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:29,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:29,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:36<01:24, 12.03s/it][WARNING|generation_utils.py:914] 2023-08-29 10:22:30,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:31,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:31,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:32,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:33,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:33,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:34,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:34,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:35,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:36,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:36,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:37,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:38,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:39,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:39,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:40,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:41,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:41,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:42,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:43,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:43,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:50<01:16, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-29 10:22:44,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:45,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:45,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:46,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:46,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:47,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:48,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:48,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:49,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:50,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:50,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:51,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:51,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:52,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:53,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:53,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:54,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:55,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:55,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:56,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:03<01:03, 12.73s/it][WARNING|generation_utils.py:914] 2023-08-29 10:22:57,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:57,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:58,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:58,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:22:59,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:00,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:00,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:01,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:02,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:02,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:03,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:03,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:04,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:05,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:05,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:06,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:06,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:07,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:08,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:09,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:15<00:50, 12.74s/it][WARNING|generation_utils.py:914] 2023-08-29 10:23:09,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:10,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:11,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:11,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:12,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:13,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:13,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:14,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:14,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:15,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:16,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:16,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:17,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:18,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:18,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:19,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:20,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:20,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:21,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:21,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:22,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:29<00:38, 12.92s/it][WARNING|generation_utils.py:914] 2023-08-29 10:23:23,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:23,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:24,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:25,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:25,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:26,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:26,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:27,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:28,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:28,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:29,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:30,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:30,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:31,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:32,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:33,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:33,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:34,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:34,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:41<00:25, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-29 10:23:35,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:35,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:36,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:37,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:37,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:38,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:38,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:39,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:39,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:40,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:41,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:42,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:42,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:43,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:43,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:44,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:45,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:45,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:46,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:46,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:53<00:12, 12.43s/it][WARNING|generation_utils.py:914] 2023-08-29 10:23:47,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:47,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:48,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:48,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:49,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:50,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:50,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:51,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:51,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:52,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:52,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:53,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:54,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:54,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:55,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:55,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:56,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:56,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:57,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:58,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:58,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:23:59,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:00,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:00,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:01,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:01,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:02,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:03,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:03,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:04,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:04,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:05,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:24:05,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:12<00:00, 14.56s/it]Generating: 100%|██████████| 10/10 [02:12<00:00, 13.26s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:15,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:15,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:15,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:15,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:15,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:24:16,658 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:24:16,660 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:24:17,282 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:24:18,371 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:24:18,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:19,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:20,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:20,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:20,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:24:20,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:24:20,437 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:24:20,438 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:24:20,774 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:24:20,922 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:24:20,923 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : made from material . Context : Later in the year ( 1541 ) , he made his debut in the play at the Royal Academy Academy of Music in London , performing in the first performance of the play by John Robert Smith . Head Entity : John Robert Smith , Tail Entity : Royal Academy of Music .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : official language .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9703125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : use .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : voice type . Context : Later in the year , she became a regular on a variety of pop music videos at different times , including one in which she sang the theme song to a song by the Beatles , titled Never Say Never . Head Entity : McCartney , Tail Entity : pop music .\n']
{'target': 600, 'success': 13, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 68, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 108, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 165, 'raw': 288}
{'target': 600, 'success': 187, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 242, 'raw': 416}
{'target': 600, 'success': 264, 'raw': 448}
{'target': 600, 'success': 280, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 314, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 356, 'raw': 608}
{'target': 600, 'success': 373, 'raw': 640}
{'target': 600, 'success': 389, 'raw': 672}
{'target': 600, 'success': 403, 'raw': 704}
{'target': 600, 'success': 420, 'raw': 736}
{'target': 600, 'success': 434, 'raw': 768}
{'target': 600, 'success': 454, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 493, 'raw': 864}
{'target': 600, 'success': 516, 'raw': 896}
{'target': 600, 'success': 535, 'raw': 928}
{'target': 600, 'success': 558, 'raw': 960}
{'target': 600, 'success': 582, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5880681818181818, 'errors': {'', "('John Travolta', 'voice type', '', 'She appeared on the radio show Last Night with John Travolta in 1970 , with Paul McCartney , John Lennon and David Bowie , for which she received the third solo award .')", "('David Bowie', 'voice type', '', 'Among his music works he also recorded two for the BBC with David Bowie ( The Secret of Night ) and George Harrison ( The Night of the Hunter ) .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 7072
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7172, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.57it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.59it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:08,  1.56it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:10,  1.65it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.66it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.61it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:16,  1.61it/s]Extractor Estimating: 27it [00:16,  1.67it/s]Extractor Estimating: 28it [00:17,  1.52it/s]Extractor Estimating: 29it [00:18,  1.58it/s]Extractor Estimating: 30it [00:18,  1.66it/s]Extractor Estimating: 31it [00:19,  1.65it/s]Extractor Estimating: 32it [00:19,  1.72it/s]Extractor Estimating: 33it [00:20,  1.80it/s]Extractor Estimating: 34it [00:20,  1.80it/s]Extractor Estimating: 35it [00:21,  1.76it/s]Extractor Estimating: 36it [00:22,  1.78it/s]Extractor Estimating: 37it [00:22,  1.64it/s]Extractor Estimating: 38it [00:23,  1.71it/s]Extractor Estimating: 39it [00:23,  1.73it/s]Extractor Estimating: 40it [00:24,  1.80it/s]Extractor Estimating: 41it [00:24,  1.85it/s]Extractor Estimating: 42it [00:25,  1.81it/s]Extractor Estimating: 43it [00:26,  1.81it/s]Extractor Estimating: 44it [00:26,  1.81it/s]Extractor Estimating: 45it [00:27,  1.80it/s]Extractor Estimating: 46it [00:27,  1.79it/s]Extractor Estimating: 47it [00:28,  1.75it/s]Extractor Estimating: 48it [00:28,  1.82it/s]Extractor Estimating: 49it [00:29,  1.83it/s]Extractor Estimating: 50it [00:30,  1.63it/s]Extractor Estimating: 51it [00:30,  1.68it/s]Extractor Estimating: 52it [00:31,  1.72it/s]Extractor Estimating: 53it [00:31,  1.79it/s]Extractor Estimating: 54it [00:32,  1.81it/s]Extractor Estimating: 55it [00:32,  1.78it/s]Extractor Estimating: 56it [00:33,  1.76it/s]Extractor Estimating: 57it [00:33,  1.79it/s]Extractor Estimating: 58it [00:34,  1.77it/s]Extractor Estimating: 59it [00:35,  1.82it/s]Extractor Estimating: 60it [00:35,  1.82it/s]Extractor Estimating: 61it [00:36,  1.79it/s]Extractor Estimating: 62it [00:36,  1.84it/s]Extractor Estimating: 63it [00:37,  1.83it/s]Extractor Estimating: 64it [00:37,  1.83it/s]Extractor Estimating: 65it [00:38,  1.73it/s]Extractor Estimating: 66it [00:39,  1.76it/s]Extractor Estimating: 67it [00:39,  1.77it/s]Extractor Estimating: 68it [00:40,  1.74it/s]Extractor Estimating: 69it [00:40,  1.74it/s]Extractor Estimating: 70it [00:41,  1.72it/s]Extractor Estimating: 71it [00:41,  1.75it/s]Extractor Estimating: 72it [00:42,  1.75it/s]Extractor Estimating: 73it [00:43,  1.68it/s]Extractor Estimating: 74it [00:43,  1.61it/s]Extractor Estimating: 75it [00:44,  1.63it/s]Extractor Estimating: 76it [00:44,  1.64it/s]Extractor Estimating: 77it [00:45,  1.68it/s]Extractor Estimating: 78it [00:46,  1.66it/s]Extractor Estimating: 79it [00:46,  1.59it/s]Extractor Estimating: 80it [00:47,  1.62it/s]Extractor Estimating: 81it [00:48,  1.63it/s]Extractor Estimating: 82it [00:48,  1.61it/s]Extractor Estimating: 83it [00:49,  1.65it/s]Extractor Estimating: 84it [00:49,  1.60it/s]Extractor Estimating: 85it [00:50,  1.62it/s]Extractor Estimating: 86it [00:51,  1.61it/s]Extractor Estimating: 87it [00:51,  1.66it/s]Extractor Estimating: 88it [00:52,  1.67it/s]Extractor Estimating: 89it [00:52,  1.65it/s]Extractor Estimating: 90it [00:53,  1.67it/s]Extractor Estimating: 91it [00:54,  1.68it/s]Extractor Estimating: 92it [00:54,  1.63it/s]Extractor Estimating: 93it [00:55,  1.69it/s]Extractor Estimating: 94it [00:55,  1.64it/s]Extractor Estimating: 95it [00:56,  1.66it/s]Extractor Estimating: 96it [00:57,  1.62it/s]Extractor Estimating: 97it [00:57,  1.56it/s]Extractor Estimating: 98it [00:58,  1.60it/s]Extractor Estimating: 99it [00:59,  1.46it/s]Extractor Estimating: 100it [00:59,  1.53it/s]Extractor Estimating: 101it [01:00,  1.56it/s]Extractor Estimating: 102it [01:01,  1.57it/s]Extractor Estimating: 103it [01:01,  1.58it/s]Extractor Estimating: 104it [01:02,  1.52it/s]Extractor Estimating: 105it [01:03,  1.55it/s]Extractor Estimating: 106it [01:03,  1.54it/s]Extractor Estimating: 107it [01:04,  1.57it/s]Extractor Estimating: 108it [01:04,  1.60it/s]Extractor Estimating: 109it [01:05,  1.54it/s]Extractor Estimating: 110it [01:06,  1.58it/s]Extractor Estimating: 111it [01:06,  1.55it/s]Extractor Estimating: 112it [01:07,  1.58it/s]Extractor Estimating: 113it [01:08,  1.59it/s]Extractor Estimating: 114it [01:08,  1.53it/s]Extractor Estimating: 115it [01:09,  1.57it/s]Extractor Estimating: 116it [01:10,  1.59it/s]Extractor Estimating: 117it [01:10,  1.60it/s]Extractor Estimating: 118it [01:11,  1.59it/s]Extractor Estimating: 119it [01:11,  1.58it/s]Extractor Estimating: 120it [01:12,  1.61it/s]Extractor Estimating: 121it [01:13,  1.62it/s]Extractor Estimating: 122it [01:13,  1.65it/s]Extractor Estimating: 123it [01:14,  1.66it/s]Extractor Estimating: 124it [01:15,  1.59it/s]Extractor Estimating: 125it [01:15,  1.58it/s]Extractor Estimating: 126it [01:16,  1.58it/s]Extractor Estimating: 127it [01:16,  1.55it/s]Extractor Estimating: 128it [01:17,  1.53it/s]Extractor Estimating: 129it [01:18,  1.55it/s]Extractor Estimating: 130it [01:18,  1.55it/s]Extractor Estimating: 131it [01:19,  1.57it/s]Extractor Estimating: 132it [01:20,  1.61it/s]Extractor Estimating: 133it [01:20,  1.60it/s]Extractor Estimating: 134it [01:21,  1.53it/s]Extractor Estimating: 135it [01:22,  1.57it/s]Extractor Estimating: 136it [01:22,  1.56it/s]Extractor Estimating: 137it [01:23,  1.59it/s]Extractor Estimating: 138it [01:23,  1.58it/s]Extractor Estimating: 139it [01:24,  1.54it/s]Extractor Estimating: 140it [01:25,  1.55it/s]Extractor Estimating: 141it [01:25,  1.53it/s]Extractor Estimating: 142it [01:26,  1.52it/s]Extractor Estimating: 143it [01:27,  1.54it/s]Extractor Estimating: 144it [01:27,  1.55it/s]Extractor Estimating: 145it [01:28,  1.58it/s]Extractor Estimating: 146it [01:29,  1.56it/s]Extractor Estimating: 147it [01:29,  1.50it/s]Extractor Estimating: 148it [01:30,  1.52it/s]Extractor Estimating: 149it [01:31,  1.53it/s]Extractor Estimating: 150it [01:31,  1.50it/s]Extractor Estimating: 151it [01:32,  1.53it/s]Extractor Estimating: 152it [01:33,  1.53it/s]Extractor Estimating: 153it [01:33,  1.57it/s]Extractor Estimating: 154it [01:34,  1.56it/s]Extractor Estimating: 155it [01:34,  1.57it/s]Extractor Estimating: 156it [01:35,  1.54it/s]Extractor Estimating: 157it [01:36,  1.52it/s]Extractor Estimating: 158it [01:37,  1.50it/s]Extractor Estimating: 159it [01:37,  1.59it/s]Extractor Estimating: 160it [01:38,  1.56it/s]Extractor Estimating: 161it [01:38,  1.59it/s]Extractor Estimating: 162it [01:39,  1.43it/s]Extractor Estimating: 163it [01:40,  1.47it/s]Extractor Estimating: 164it [01:40,  1.54it/s]Extractor Estimating: 165it [01:41,  1.59it/s]Extractor Estimating: 166it [01:42,  1.57it/s]Extractor Estimating: 167it [01:42,  1.52it/s]Extractor Estimating: 168it [01:43,  1.56it/s]Extractor Estimating: 169it [01:44,  1.55it/s]Extractor Estimating: 170it [01:44,  1.54it/s]Extractor Estimating: 171it [01:45,  1.55it/s]Extractor Estimating: 172it [01:46,  1.51it/s]Extractor Estimating: 173it [01:46,  1.55it/s]Extractor Estimating: 174it [01:47,  1.56it/s]Extractor Estimating: 175it [01:47,  1.57it/s]Extractor Estimating: 176it [01:48,  1.58it/s]Extractor Estimating: 177it [01:49,  1.51it/s]Extractor Estimating: 178it [01:49,  1.52it/s]Extractor Estimating: 179it [01:50,  1.54it/s]Extractor Estimating: 180it [01:51,  1.56it/s]Extractor Estimating: 181it [01:51,  1.62it/s]Extractor Estimating: 182it [01:52,  1.54it/s]Extractor Estimating: 183it [01:53,  1.61it/s]Extractor Estimating: 184it [01:53,  1.63it/s]Extractor Estimating: 185it [01:54,  1.60it/s]Extractor Estimating: 186it [01:54,  1.58it/s]Extractor Estimating: 187it [01:55,  1.48it/s]Extractor Estimating: 188it [01:56,  1.54it/s]Extractor Estimating: 189it [01:56,  1.58it/s]Extractor Estimating: 190it [01:57,  1.59it/s]Extractor Estimating: 191it [01:58,  1.59it/s]Extractor Estimating: 192it [01:58,  1.63it/s]Extractor Estimating: 193it [01:59,  1.62it/s]Extractor Estimating: 194it [02:00,  1.60it/s]Extractor Estimating: 195it [02:00,  1.58it/s]Extractor Estimating: 196it [02:01,  1.59it/s]Extractor Estimating: 197it [02:01,  1.61it/s]Extractor Estimating: 198it [02:02,  1.59it/s]Extractor Estimating: 199it [02:03,  1.58it/s]Extractor Estimating: 200it [02:03,  1.62it/s]Extractor Estimating: 201it [02:04,  1.67it/s]Extractor Estimating: 202it [02:04,  1.67it/s]Extractor Estimating: 203it [02:05,  1.69it/s]Extractor Estimating: 204it [02:06,  1.68it/s]Extractor Estimating: 205it [02:06,  1.72it/s]Extractor Estimating: 206it [02:07,  1.69it/s]Extractor Estimating: 207it [02:07,  1.73it/s]Extractor Estimating: 208it [02:08,  1.77it/s]Extractor Estimating: 209it [02:08,  1.76it/s]Extractor Estimating: 210it [02:09,  1.76it/s]Extractor Estimating: 211it [02:10,  1.78it/s]Extractor Estimating: 212it [02:10,  1.57it/s]Extractor Estimating: 213it [02:11,  1.64it/s]Extractor Estimating: 214it [02:11,  1.68it/s]Extractor Estimating: 215it [02:12,  1.72it/s]Extractor Estimating: 216it [02:13,  1.68it/s]Extractor Estimating: 217it [02:13,  1.64it/s]Extractor Estimating: 218it [02:14,  1.67it/s]Extractor Estimating: 219it [02:14,  1.68it/s]Extractor Estimating: 220it [02:15,  1.68it/s]Extractor Estimating: 221it [02:16,  1.70it/s]Extractor Estimating: 222it [02:16,  1.73it/s]Extractor Estimating: 223it [02:17,  1.67it/s]Extractor Estimating: 224it [02:17,  1.67it/s]Extractor Estimating: 225it [02:18,  1.72it/s]Extractor Estimating: 226it [02:19,  1.68it/s]Extractor Estimating: 227it [02:19,  1.64it/s]Extractor Estimating: 228it [02:20,  1.63it/s]Extractor Estimating: 229it [02:20,  1.60it/s]Extractor Estimating: 230it [02:21,  1.60it/s]Extractor Estimating: 231it [02:22,  1.48it/s]Extractor Estimating: 232it [02:22,  1.55it/s]Extractor Estimating: 233it [02:23,  1.53it/s]Extractor Estimating: 234it [02:24,  1.56it/s]Extractor Estimating: 235it [02:24,  1.61it/s]Extractor Estimating: 236it [02:25,  1.60it/s]Extractor Estimating: 237it [02:26,  1.60it/s]Extractor Estimating: 238it [02:26,  1.58it/s]Extractor Estimating: 239it [02:27,  1.58it/s]Extractor Estimating: 240it [02:27,  1.61it/s]Extractor Estimating: 241it [02:28,  1.59it/s]Extractor Estimating: 242it [02:29,  1.61it/s]Extractor Estimating: 243it [02:29,  1.55it/s]Extractor Estimating: 244it [02:30,  1.59it/s]Extractor Estimating: 245it [02:31,  1.57it/s]Extractor Estimating: 246it [02:31,  1.56it/s]Extractor Estimating: 247it [02:32,  1.56it/s]Extractor Estimating: 248it [02:33,  1.55it/s]Extractor Estimating: 249it [02:33,  1.57it/s]Extractor Estimating: 250it [02:34,  1.58it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:22,462 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:22,534 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:22,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:22,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:22,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:27:23,249 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:27:23,250 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:27:23,602 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:27:24,762 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:27:24,762 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:26,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:26,714 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:26,714 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:26,714 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:26,715 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:27:27,288 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:27:27,290 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:27:27,609 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:27:27,854 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:27:27,854 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:37:18,718 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:37:19,007 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 3999 mean pseudo reward: 0.9617744293423145
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 15861
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15961, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15961, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.032, loss:311.2230
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 33, avg_time 0.998, loss:310.9321
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 133, avg_time 1.015, loss:290.5750
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 66, avg_time 1.012, loss:273.1123
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 166, avg_time 1.042, loss:268.5123
>> valid entity prec:0.4657, rec:0.4466, f1:0.4560
>> valid relation prec:0.2400, rec:0.0998, f1:0.1410
>> valid relation with NER prec:0.2400, rec:0.0998, f1:0.1410
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 99, avg_time 2.492, loss:248.9590
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 32, avg_time 1.017, loss:245.5018
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 132, avg_time 1.007, loss:251.2241
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 65, avg_time 1.013, loss:255.8207
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 165, avg_time 1.005, loss:252.2340
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4561, rec:0.4508, f1:0.4534
>> valid relation prec:0.1730, rec:0.0898, f1:0.1182
>> valid relation with NER prec:0.1730, rec:0.0898, f1:0.1182
g_step 1100, step 98, avg_time 2.475, loss:236.9396
g_step 1200, step 31, avg_time 1.045, loss:223.2976
g_step 1300, step 131, avg_time 0.986, loss:229.9416
g_step 1400, step 64, avg_time 1.012, loss:211.7606
g_step 1500, step 164, avg_time 1.003, loss:195.1669
>> valid entity prec:0.4739, rec:0.5106, f1:0.4915
>> valid relation prec:0.2294, rec:0.1321, f1:0.1676
>> valid relation with NER prec:0.2294, rec:0.1321, f1:0.1676
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 97, avg_time 2.498, loss:197.2922
g_step 1700, step 30, avg_time 0.957, loss:182.0879
g_step 1800, step 130, avg_time 0.962, loss:189.9209
g_step 1900, step 63, avg_time 0.986, loss:177.0081
g_step 2000, step 163, avg_time 0.988, loss:194.0472
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4473, rec:0.4477, f1:0.4475
>> valid relation prec:0.1604, rec:0.0850, f1:0.1111
>> valid relation with NER prec:0.1604, rec:0.0850, f1:0.1111
g_step 2100, step 96, avg_time 2.403, loss:152.2435
g_step 2200, step 29, avg_time 0.958, loss:166.4416
g_step 2300, step 129, avg_time 0.974, loss:153.6423
g_step 2400, step 62, avg_time 0.987, loss:151.7372
g_step 2500, step 162, avg_time 0.972, loss:145.0698
>> valid entity prec:0.4763, rec:0.3919, f1:0.4300
>> valid relation prec:0.2145, rec:0.0824, f1:0.1191
>> valid relation with NER prec:0.2145, rec:0.0824, f1:0.1191
g_step 2600, step 95, avg_time 2.403, loss:137.4325
g_step 2700, step 28, avg_time 0.972, loss:137.9189
g_step 2800, step 128, avg_time 0.975, loss:127.4255
g_step 2900, step 61, avg_time 0.969, loss:130.3082
g_step 3000, step 161, avg_time 1.000, loss:142.8474
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4692, rec:0.4218, f1:0.4442
>> valid relation prec:0.2297, rec:0.1035, f1:0.1427
>> valid relation with NER prec:0.2297, rec:0.1035, f1:0.1427
g_step 3100, step 94, avg_time 2.373, loss:110.1196
g_step 3200, step 27, avg_time 0.958, loss:121.3221
g_step 3300, step 127, avg_time 0.972, loss:118.2407
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:37:19 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:37:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-37-18_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:37:20 - WARNING - datasets.builder -   Using custom data configuration default-354dbf6cc7ad9014
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-354dbf6cc7ad9014/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.81 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:37:23,804 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:37:23,805 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:37:23,806 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:37:23,807 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:37:24,027 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:24,126 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:37:24,661 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:37:27,906 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:37:27,906 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-354dbf6cc7ad9014/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.05ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.13ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.79ba/s]100%|██████████| 4/4 [00:01<00:00,  4.23ba/s]100%|██████████| 4/4 [00:01<00:00,  3.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.60ba/s] 40%|████      | 2/5 [00:00<00:00,  4.03ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.36ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.53ba/s]100%|██████████| 5/5 [00:01<00:00,  5.36ba/s]100%|██████████| 5/5 [00:01<00:00,  4.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.89ba/s]100%|██████████| 4/4 [00:00<00:00,  8.99ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.24ba/s] 40%|████      | 2/5 [00:00<00:00,  6.57ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.05ba/s]100%|██████████| 5/5 [00:00<00:00,  9.25ba/s]
[INFO|trainer.py:414] 2023-08-29 11:37:32,403 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:37:32,534 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:37:32,534 >>   Num examples = 4000
[INFO|trainer.py:1149] 2023-08-29 11:37:32,534 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:37:32,534 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:37:32,534 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:37:32,534 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:37:32,534 >>   Total optimization steps = 310
  0%|          | 0/310 [00:00<?, ?it/s]  0%|          | 1/310 [00:00<03:48,  1.35it/s]  1%|          | 2/310 [00:01<02:43,  1.88it/s]  1%|          | 3/310 [00:01<02:12,  2.31it/s]  1%|▏         | 4/310 [00:01<01:58,  2.58it/s]  2%|▏         | 5/310 [00:02<01:47,  2.84it/s]  2%|▏         | 6/310 [00:02<01:49,  2.78it/s]  2%|▏         | 7/310 [00:02<01:42,  2.97it/s]  3%|▎         | 8/310 [00:03<01:37,  3.11it/s]  3%|▎         | 9/310 [00:03<01:33,  3.20it/s]  3%|▎         | 10/310 [00:03<01:31,  3.28it/s]  4%|▎         | 11/310 [00:03<01:29,  3.32it/s]  4%|▍         | 12/310 [00:04<01:28,  3.36it/s]  4%|▍         | 13/310 [00:04<01:28,  3.36it/s]  5%|▍         | 14/310 [00:04<01:27,  3.39it/s]  5%|▍         | 15/310 [00:05<01:26,  3.40it/s]  5%|▌         | 16/310 [00:05<01:26,  3.42it/s]  5%|▌         | 17/310 [00:05<01:25,  3.42it/s]  6%|▌         | 18/310 [00:05<01:25,  3.43it/s]  6%|▌         | 19/310 [00:06<01:24,  3.44it/s]  6%|▋         | 20/310 [00:06<01:24,  3.44it/s]  7%|▋         | 21/310 [00:06<01:24,  3.44it/s]  7%|▋         | 22/310 [00:07<01:23,  3.45it/s]  7%|▋         | 23/310 [00:07<01:22,  3.46it/s]  8%|▊         | 24/310 [00:07<01:24,  3.37it/s]  8%|▊         | 25/310 [00:07<01:23,  3.41it/s]  8%|▊         | 26/310 [00:08<01:22,  3.44it/s]  9%|▊         | 27/310 [00:08<01:22,  3.44it/s]  9%|▉         | 28/310 [00:08<01:21,  3.44it/s]  9%|▉         | 29/310 [00:09<01:21,  3.44it/s] 10%|▉         | 30/310 [00:09<01:21,  3.44it/s] 10%|█         | 31/310 [00:09<01:21,  3.44it/s] 10%|█         | 32/310 [00:09<01:20,  3.44it/s] 11%|█         | 33/310 [00:10<01:20,  3.44it/s] 11%|█         | 34/310 [00:10<01:20,  3.44it/s] 11%|█▏        | 35/310 [00:10<01:19,  3.44it/s] 12%|█▏        | 36/310 [00:11<01:19,  3.44it/s] 12%|█▏        | 37/310 [00:11<01:19,  3.44it/s] 12%|█▏        | 38/310 [00:11<01:18,  3.45it/s] 13%|█▎        | 39/310 [00:12<01:18,  3.44it/s] 13%|█▎        | 40/310 [00:12<01:18,  3.44it/s] 13%|█▎        | 41/310 [00:12<01:20,  3.33it/s] 14%|█▎        | 42/310 [00:12<01:19,  3.36it/s] 14%|█▍        | 43/310 [00:13<01:18,  3.38it/s] 14%|█▍        | 44/310 [00:13<01:18,  3.40it/s] 15%|█▍        | 45/310 [00:13<01:17,  3.41it/s] 15%|█▍        | 46/310 [00:14<01:17,  3.42it/s] 15%|█▌        | 47/310 [00:14<01:16,  3.43it/s] 15%|█▌        | 48/310 [00:14<01:19,  3.30it/s] 16%|█▌        | 49/310 [00:14<01:18,  3.34it/s] 16%|█▌        | 50/310 [00:15<01:17,  3.37it/s] 16%|█▋        | 51/310 [00:15<01:16,  3.39it/s] 17%|█▋        | 52/310 [00:15<01:15,  3.40it/s] 17%|█▋        | 53/310 [00:16<01:15,  3.41it/s] 17%|█▋        | 54/310 [00:16<01:14,  3.42it/s] 18%|█▊        | 55/310 [00:16<01:14,  3.43it/s] 18%|█▊        | 56/310 [00:17<01:14,  3.43it/s] 18%|█▊        | 57/310 [00:17<01:13,  3.43it/s] 19%|█▊        | 58/310 [00:17<01:13,  3.43it/s] 19%|█▉        | 59/310 [00:17<01:18,  3.21it/s] 19%|█▉        | 60/310 [00:18<01:16,  3.27it/s] 20%|█▉        | 61/310 [00:18<01:14,  3.32it/s] 20%|██        | 62/310 [00:18<01:13,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 11:37:51,520 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:37:51,520 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:37:51,520 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.47it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.03it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.63it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.66it/s][A
  5%|▍         | 27/577 [00:00<00:11, 46.01it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.59it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.30it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.23it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.31it/s][A
  9%|▉         | 52/577 [00:01<00:13, 40.16it/s][A
 10%|▉         | 57/577 [00:01<00:12, 41.78it/s][A
 11%|█         | 62/577 [00:01<00:12, 42.91it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.77it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 44.36it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.64it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.75it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 44.78it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 44.64it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.61it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.93it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.10it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.35it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.38it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.25it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.27it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.03it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.90it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.88it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.94it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.85it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.19it/s][A
 28%|██▊       | 162/577 [00:03<00:10, 38.70it/s][A
 29%|██▉       | 167/577 [00:03<00:10, 40.62it/s][A
 30%|██▉       | 172/577 [00:03<00:09, 42.00it/s][A
 31%|███       | 177/577 [00:04<00:10, 38.18it/s][A
 32%|███▏      | 182/577 [00:04<00:09, 40.29it/s][A
 32%|███▏      | 187/577 [00:04<00:09, 41.77it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 42.93it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 43.61it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.96it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.26it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.74it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.59it/s][A
 38%|███▊      | 222/577 [00:05<00:07, 44.52it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.77it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.02it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.16it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.27it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.11it/s][A
 44%|████▎     | 252/577 [00:05<00:09, 35.41it/s][A
 45%|████▍     | 257/577 [00:05<00:08, 37.95it/s][A
 45%|████▌     | 262/577 [00:06<00:07, 39.93it/s][A
 46%|████▋     | 267/577 [00:06<00:07, 41.50it/s][A
 47%|████▋     | 272/577 [00:06<00:07, 42.65it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 43.46it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.04it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.45it/s][A
 51%|█████     | 292/577 [00:06<00:06, 44.31it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 44.51it/s][A
 52%|█████▏    | 302/577 [00:06<00:07, 39.12it/s][A
 53%|█████▎    | 307/577 [00:07<00:06, 40.92it/s][A
 54%|█████▍    | 312/577 [00:07<00:06, 42.27it/s][A
 55%|█████▍    | 317/577 [00:07<00:06, 43.29it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 44.04it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 44.49it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.84it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 44.91it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.61it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.41it/s][A
 61%|██████    | 352/577 [00:08<00:05, 44.56it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 44.87it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.05it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.26it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.48it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.48it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 42.02it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 42.84it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 43.41it/s][A
 69%|██████▉   | 397/577 [00:09<00:04, 43.84it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.26it/s][A
 71%|███████   | 407/577 [00:09<00:03, 44.58it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.87it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.05it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.93it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 40.67it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 42.09it/s][A
 76%|███████▌  | 437/577 [00:10<00:03, 43.10it/s][A
 77%|███████▋  | 442/577 [00:10<00:03, 43.83it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 44.29it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 44.59it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 44.82it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.92it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.62it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.55it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 44.78it/s][A
 84%|████████▎ | 482/577 [00:11<00:02, 45.00it/s][A
 84%|████████▍ | 487/577 [00:11<00:01, 45.17it/s][A
 85%|████████▌ | 492/577 [00:11<00:01, 45.23it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.37it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.34it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 45.21it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.91it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.80it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.92it/s][A
 91%|█████████▏| 527/577 [00:12<00:01, 45.03it/s][A
 92%|█████████▏| 532/577 [00:12<00:00, 45.20it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 45.37it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.44it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.36it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.23it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.95it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.90it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.91it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 44.91it/s][A
100%|██████████| 577/577 [00:13<00:00, 45.12it/s][A                                                
                                                 [A 20%|██        | 62/310 [00:32<01:13,  3.36it/s]
100%|██████████| 577/577 [00:13<00:00, 45.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:38:04,839 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62
[INFO|configuration_utils.py:351] 2023-08-29 11:38:05,071 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:38:08,536 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:38:08,861 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:38:08,965 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62/special_tokens_map.json
 20%|██        | 63/310 [00:45<33:50,  8.22s/it] 21%|██        | 64/310 [00:45<24:00,  5.85s/it] 21%|██        | 65/310 [00:46<17:05,  4.18s/it] 21%|██▏       | 66/310 [00:46<12:32,  3.08s/it] 22%|██▏       | 67/310 [00:46<09:05,  2.24s/it] 22%|██▏       | 68/310 [00:47<06:41,  1.66s/it] 22%|██▏       | 69/310 [00:47<05:00,  1.25s/it] 23%|██▎       | 70/310 [00:47<03:50,  1.04it/s] 23%|██▎       | 71/310 [00:48<03:01,  1.32it/s] 23%|██▎       | 72/310 [00:48<02:27,  1.62it/s] 24%|██▎       | 73/310 [00:48<02:03,  1.92it/s] 24%|██▍       | 74/310 [00:49<01:48,  2.17it/s] 24%|██▍       | 75/310 [00:49<01:36,  2.44it/s] 25%|██▍       | 76/310 [00:49<01:27,  2.68it/s] 25%|██▍       | 77/310 [00:49<01:21,  2.87it/s] 25%|██▌       | 78/310 [00:50<01:16,  3.02it/s] 25%|██▌       | 79/310 [00:50<01:13,  3.13it/s] 26%|██▌       | 80/310 [00:50<01:11,  3.22it/s] 26%|██▌       | 81/310 [00:51<01:09,  3.29it/s] 26%|██▋       | 82/310 [00:51<01:08,  3.33it/s] 27%|██▋       | 83/310 [00:51<01:07,  3.36it/s] 27%|██▋       | 84/310 [00:51<01:06,  3.38it/s] 27%|██▋       | 85/310 [00:52<01:09,  3.26it/s] 28%|██▊       | 86/310 [00:52<01:07,  3.31it/s] 28%|██▊       | 87/310 [00:52<01:06,  3.35it/s] 28%|██▊       | 88/310 [00:53<01:05,  3.38it/s] 29%|██▊       | 89/310 [00:53<01:05,  3.39it/s] 29%|██▉       | 90/310 [00:53<01:04,  3.41it/s] 29%|██▉       | 91/310 [00:54<01:04,  3.42it/s] 30%|██▉       | 92/310 [00:54<01:03,  3.43it/s] 30%|███       | 93/310 [00:54<01:03,  3.43it/s] 30%|███       | 94/310 [00:54<01:02,  3.44it/s] 31%|███       | 95/310 [00:55<01:02,  3.43it/s] 31%|███       | 96/310 [00:55<01:03,  3.37it/s] 31%|███▏      | 97/310 [00:55<01:02,  3.39it/s] 32%|███▏      | 98/310 [00:56<01:02,  3.41it/s] 32%|███▏      | 99/310 [00:56<01:01,  3.42it/s] 32%|███▏      | 100/310 [00:56<01:01,  3.42it/s] 33%|███▎      | 101/310 [00:56<01:00,  3.43it/s] 33%|███▎      | 102/310 [00:57<01:00,  3.43it/s] 33%|███▎      | 103/310 [00:57<01:00,  3.43it/s] 34%|███▎      | 104/310 [00:57<00:59,  3.44it/s] 34%|███▍      | 105/310 [00:58<00:59,  3.44it/s] 34%|███▍      | 106/310 [00:58<00:59,  3.44it/s] 35%|███▍      | 107/310 [00:58<01:00,  3.38it/s] 35%|███▍      | 108/310 [00:59<00:59,  3.39it/s] 35%|███▌      | 109/310 [00:59<00:59,  3.40it/s] 35%|███▌      | 110/310 [00:59<00:58,  3.42it/s] 36%|███▌      | 111/310 [00:59<00:58,  3.42it/s] 36%|███▌      | 112/310 [01:00<00:57,  3.43it/s] 36%|███▋      | 113/310 [01:00<00:57,  3.43it/s] 37%|███▋      | 114/310 [01:00<00:57,  3.40it/s] 37%|███▋      | 115/310 [01:01<00:57,  3.41it/s] 37%|███▋      | 116/310 [01:01<00:56,  3.42it/s] 38%|███▊      | 117/310 [01:01<00:56,  3.43it/s] 38%|███▊      | 118/310 [01:01<00:56,  3.43it/s] 38%|███▊      | 119/310 [01:02<00:55,  3.43it/s] 39%|███▊      | 120/310 [01:02<00:55,  3.43it/s] 39%|███▉      | 121/310 [01:02<00:54,  3.44it/s] 39%|███▉      | 122/310 [01:03<00:54,  3.43it/s] 40%|███▉      | 123/310 [01:03<00:54,  3.43it/s] 40%|████      | 124/310 [01:03<00:54,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 11:38:36,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:38:36,372 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:38:36,372 >>   Batch size = 8
{'eval_loss': 1.0951038599014282, 'eval_runtime': 13.1235, 'eval_samples_per_second': 351.201, 'eval_steps_per_second': 43.967, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.55it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.53it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.91it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.86it/s][A
  5%|▍         | 28/577 [00:00<00:11, 46.29it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.90it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.29it/s][A
  7%|▋         | 43/577 [00:00<00:11, 44.89it/s][A
  8%|▊         | 48/577 [00:01<00:11, 44.95it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.11it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.22it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.35it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.23it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.48it/s][A
 14%|█▎        | 78/577 [00:01<00:10, 45.44it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 45.31it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 45.05it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.03it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.08it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.16it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.13it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.28it/s][A
 20%|██        | 118/577 [00:02<00:10, 45.30it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 45.23it/s][A
 22%|██▏       | 128/577 [00:02<00:09, 44.96it/s][A
 23%|██▎       | 133/577 [00:02<00:09, 44.64it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 44.05it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 44.51it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 44.77it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.06it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.25it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.31it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 45.16it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.06it/s][A
 31%|███       | 178/577 [00:03<00:08, 44.88it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 44.70it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 44.99it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.17it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.29it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.42it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.37it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.30it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.08it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 44.89it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 44.98it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.09it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.27it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.35it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.41it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 45.35it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 45.26it/s][A
 46%|████▌     | 263/577 [00:05<00:06, 45.02it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 44.88it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 44.93it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 43.46it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 44.07it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 44.52it/s][A
 51%|█████     | 293/577 [00:06<00:06, 44.80it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.06it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.16it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 45.15it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.13it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 44.84it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 44.94it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.01it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.19it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.29it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.35it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.29it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.29it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 45.16it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 44.99it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 44.96it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 44.97it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.06it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.23it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.30it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.32it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.26it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.17it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.07it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 44.95it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 44.25it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 44.62it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 44.88it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.05it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.18it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 45.21it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 45.12it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 45.02it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 44.87it/s][A
 80%|████████  | 463/577 [00:10<00:02, 44.92it/s][A
 81%|████████  | 468/577 [00:10<00:02, 44.94it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.21it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.35it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.36it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.25it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 45.10it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 44.97it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 44.79it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 44.85it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 44.94it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 45.13it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.30it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.39it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.37it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 45.24it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 45.07it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 44.84it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 43.81it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 44.36it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 44.69it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 44.98it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 45.12it/s][A                                                 
                                                 [A 40%|████      | 124/310 [01:16<00:54,  3.43it/s]
100%|██████████| 577/577 [00:12<00:00, 45.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:38:49,287 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-29 11:38:49,425 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:38:54,165 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:38:54,505 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:38:54,692 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124/special_tokens_map.json
 40%|████      | 125/310 [01:32<26:52,  8.72s/it] 41%|████      | 126/310 [01:32<18:58,  6.19s/it] 41%|████      | 127/310 [01:32<13:28,  4.42s/it] 41%|████▏     | 128/310 [01:32<09:38,  3.18s/it] 42%|████▏     | 129/310 [01:33<06:58,  2.31s/it] 42%|████▏     | 130/310 [01:33<05:06,  1.70s/it] 42%|████▏     | 131/310 [01:33<03:48,  1.28s/it] 43%|████▎     | 132/310 [01:34<02:54,  1.02it/s] 43%|████▎     | 133/310 [01:34<02:16,  1.30it/s] 43%|████▎     | 134/310 [01:34<01:52,  1.57it/s] 44%|████▎     | 135/310 [01:34<01:33,  1.88it/s] 44%|████▍     | 136/310 [01:35<01:19,  2.18it/s] 44%|████▍     | 137/310 [01:35<01:10,  2.46it/s] 45%|████▍     | 138/310 [01:35<01:03,  2.70it/s] 45%|████▍     | 139/310 [01:36<01:00,  2.84it/s] 45%|████▌     | 140/310 [01:36<00:56,  3.01it/s] 45%|████▌     | 141/310 [01:36<00:53,  3.14it/s] 46%|████▌     | 142/310 [01:36<00:52,  3.23it/s] 46%|████▌     | 143/310 [01:37<00:50,  3.30it/s] 46%|████▋     | 144/310 [01:37<00:49,  3.36it/s] 47%|████▋     | 145/310 [01:37<00:49,  3.32it/s] 47%|████▋     | 146/310 [01:38<00:48,  3.37it/s] 47%|████▋     | 147/310 [01:38<00:47,  3.41it/s] 48%|████▊     | 148/310 [01:38<00:47,  3.43it/s] 48%|████▊     | 149/310 [01:38<00:46,  3.45it/s] 48%|████▊     | 150/310 [01:39<00:46,  3.46it/s] 49%|████▊     | 151/310 [01:39<00:45,  3.48it/s] 49%|████▉     | 152/310 [01:39<00:45,  3.48it/s] 49%|████▉     | 153/310 [01:40<00:45,  3.48it/s] 50%|████▉     | 154/310 [01:40<00:44,  3.48it/s] 50%|█████     | 155/310 [01:40<00:44,  3.49it/s] 50%|█████     | 156/310 [01:41<00:45,  3.39it/s] 51%|█████     | 157/310 [01:41<00:44,  3.42it/s] 51%|█████     | 158/310 [01:41<00:44,  3.44it/s] 51%|█████▏    | 159/310 [01:41<00:43,  3.46it/s] 52%|█████▏    | 160/310 [01:42<00:43,  3.47it/s] 52%|█████▏    | 161/310 [01:42<00:42,  3.48it/s] 52%|█████▏    | 162/310 [01:42<00:42,  3.48it/s] 53%|█████▎    | 163/310 [01:43<00:42,  3.48it/s] 53%|█████▎    | 164/310 [01:43<00:41,  3.49it/s] 53%|█████▎    | 165/310 [01:43<00:41,  3.48it/s] 54%|█████▎    | 166/310 [01:43<00:41,  3.49it/s] 54%|█████▍    | 167/310 [01:44<00:42,  3.37it/s] 54%|█████▍    | 168/310 [01:44<00:41,  3.41it/s] 55%|█████▍    | 169/310 [01:44<00:41,  3.43it/s] 55%|█████▍    | 170/310 [01:45<00:40,  3.45it/s] 55%|█████▌    | 171/310 [01:45<00:40,  3.46it/s] 55%|█████▌    | 172/310 [01:45<00:39,  3.47it/s] 56%|█████▌    | 173/310 [01:45<00:39,  3.48it/s] 56%|█████▌    | 174/310 [01:46<00:39,  3.48it/s] 56%|█████▋    | 175/310 [01:46<00:46,  2.88it/s] 57%|█████▋    | 176/310 [01:46<00:44,  3.04it/s] 57%|█████▋    | 177/310 [01:47<00:43,  3.05it/s] 57%|█████▋    | 178/310 [01:47<00:41,  3.17it/s] 58%|█████▊    | 179/310 [01:47<00:40,  3.26it/s] 58%|█████▊    | 180/310 [01:48<00:39,  3.32it/s] 58%|█████▊    | 181/310 [01:48<00:38,  3.37it/s] 59%|█████▊    | 182/310 [01:48<00:37,  3.41it/s] 59%|█████▉    | 183/310 [01:49<00:37,  3.43it/s] 59%|█████▉    | 184/310 [01:49<00:36,  3.45it/s] 60%|█████▉    | 185/310 [01:49<00:36,  3.46it/s] 60%|██████    | 186/310 [01:49<00:35,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 11:39:22,570 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:39:22,592 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:39:22,592 >>   Batch size = 8
{'eval_loss': 1.114255428314209, 'eval_runtime': 12.7965, 'eval_samples_per_second': 360.178, 'eval_steps_per_second': 45.091, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.89it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.45it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.46it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.65it/s][A
  5%|▍         | 28/577 [00:00<00:11, 46.00it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.84it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.56it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.20it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.24it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.25it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.36it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.41it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.24it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.23it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 45.05it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 45.01it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 44.76it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.13it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.21it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.33it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.32it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.36it/s][A
 20%|██        | 118/577 [00:02<00:10, 45.25it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 45.17it/s][A
 22%|██▏       | 128/577 [00:02<00:09, 45.09it/s][A
 23%|██▎       | 133/577 [00:02<00:09, 45.08it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 45.08it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 44.88it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 45.16it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.19it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.26it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.24it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 45.02it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.05it/s][A
 31%|███       | 178/577 [00:03<00:08, 45.07it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.04it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.17it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.25it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.30it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.31it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.21it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.13it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.07it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 44.99it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.00it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.10it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.11it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.24it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.25it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 45.13it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 45.11it/s][A
 46%|████▌     | 263/577 [00:05<00:06, 45.06it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 45.02it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 45.05it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 45.01it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 44.36it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 44.72it/s][A
 51%|█████     | 293/577 [00:06<00:06, 44.93it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.03it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.07it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 45.12it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.12it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 45.09it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 44.91it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 44.99it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.13it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.11it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.22it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.13it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.23it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 45.19it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 45.12it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 45.06it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 44.94it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.12it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.14it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.17it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.04it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.24it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.18it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.09it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.07it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 44.90it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 43.22it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 43.88it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 44.37it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 44.71it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 44.78it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 44.98it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 44.99it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 45.06it/s][A
 80%|████████  | 463/577 [00:10<00:02, 41.47it/s][A
 81%|████████▏ | 469/577 [00:10<00:02, 43.98it/s][A
 82%|████████▏ | 474/577 [00:10<00:02, 44.38it/s][A
 83%|████████▎ | 479/577 [00:10<00:02, 44.71it/s][A
 84%|████████▍ | 484/577 [00:10<00:02, 44.98it/s][A
 85%|████████▍ | 489/577 [00:10<00:01, 45.08it/s][A
 86%|████████▌ | 494/577 [00:10<00:01, 45.22it/s][A
 86%|████████▋ | 499/577 [00:11<00:01, 45.25it/s][A
 87%|████████▋ | 504/577 [00:11<00:01, 45.07it/s][A
 88%|████████▊ | 509/577 [00:11<00:01, 44.78it/s][A
 89%|████████▉ | 514/577 [00:11<00:01, 44.92it/s][A
 90%|████████▉ | 519/577 [00:11<00:01, 45.05it/s][A
 91%|█████████ | 524/577 [00:11<00:01, 45.23it/s][A
 92%|█████████▏| 529/577 [00:11<00:01, 45.28it/s][A
 93%|█████████▎| 534/577 [00:11<00:00, 45.20it/s][A
 93%|█████████▎| 539/577 [00:11<00:00, 45.28it/s][A
 94%|█████████▍| 544/577 [00:12<00:00, 45.21it/s][A
 95%|█████████▌| 549/577 [00:12<00:00, 45.06it/s][A
 96%|█████████▌| 554/577 [00:12<00:00, 44.89it/s][A
 97%|█████████▋| 559/577 [00:12<00:00, 44.84it/s][A
 98%|█████████▊| 564/577 [00:12<00:00, 44.99it/s][A
 99%|█████████▊| 569/577 [00:12<00:00, 45.08it/s][A
 99%|█████████▉| 574/577 [00:12<00:00, 45.19it/s][A                                                 
                                                 [A 60%|██████    | 186/310 [02:02<00:35,  3.47it/s]
100%|██████████| 577/577 [00:12<00:00, 45.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:39:35,561 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186
[INFO|configuration_utils.py:351] 2023-08-29 11:39:35,758 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:39:39,483 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:39:39,695 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:39:39,778 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186/special_tokens_map.json
 60%|██████    | 187/310 [02:16<16:29,  8.04s/it] 61%|██████    | 188/310 [02:16<11:38,  5.72s/it] 61%|██████    | 189/310 [02:16<08:15,  4.09s/it] 61%|██████▏   | 190/310 [02:16<05:54,  2.95s/it] 62%|██████▏   | 191/310 [02:17<04:16,  2.15s/it] 62%|██████▏   | 192/310 [02:17<03:08,  1.60s/it] 62%|██████▏   | 193/310 [02:17<02:20,  1.20s/it] 63%|██████▎   | 194/310 [02:18<01:47,  1.07it/s] 63%|██████▎   | 195/310 [02:18<01:24,  1.35it/s] 63%|██████▎   | 196/310 [02:18<01:08,  1.65it/s] 64%|██████▎   | 197/310 [02:18<00:57,  1.96it/s] 64%|██████▍   | 198/310 [02:19<00:49,  2.25it/s] 64%|██████▍   | 199/310 [02:19<00:46,  2.39it/s] 65%|██████▍   | 200/310 [02:19<00:41,  2.62it/s] 65%|██████▍   | 201/310 [02:20<00:38,  2.83it/s] 65%|██████▌   | 202/310 [02:20<00:36,  2.98it/s] 65%|██████▌   | 203/310 [02:20<00:34,  3.11it/s] 66%|██████▌   | 204/310 [02:21<00:33,  3.20it/s] 66%|██████▌   | 205/310 [02:21<00:32,  3.27it/s] 66%|██████▋   | 206/310 [02:21<00:31,  3.31it/s] 67%|██████▋   | 207/310 [02:21<00:30,  3.35it/s] 67%|██████▋   | 208/310 [02:22<00:30,  3.37it/s] 67%|██████▋   | 209/310 [02:22<00:29,  3.39it/s] 68%|██████▊   | 210/310 [02:22<00:31,  3.21it/s] 68%|██████▊   | 211/310 [02:23<00:30,  3.27it/s] 68%|██████▊   | 212/310 [02:23<00:29,  3.32it/s] 69%|██████▊   | 213/310 [02:23<00:28,  3.35it/s] 69%|██████▉   | 214/310 [02:24<00:28,  3.38it/s] 69%|██████▉   | 215/310 [02:24<00:27,  3.40it/s] 70%|██████▉   | 216/310 [02:24<00:27,  3.41it/s] 70%|███████   | 217/310 [02:24<00:27,  3.42it/s] 70%|███████   | 218/310 [02:25<00:26,  3.42it/s] 71%|███████   | 219/310 [02:25<00:26,  3.42it/s] 71%|███████   | 220/310 [02:25<00:26,  3.43it/s] 71%|███████▏  | 221/310 [02:26<00:27,  3.21it/s] 72%|███████▏  | 222/310 [02:26<00:26,  3.28it/s] 72%|███████▏  | 223/310 [02:26<00:26,  3.32it/s] 72%|███████▏  | 224/310 [02:27<00:25,  3.36it/s] 73%|███████▎  | 225/310 [02:27<00:25,  3.38it/s] 73%|███████▎  | 226/310 [02:27<00:24,  3.39it/s] 73%|███████▎  | 227/310 [02:27<00:24,  3.40it/s] 74%|███████▎  | 228/310 [02:28<00:24,  3.41it/s] 74%|███████▍  | 229/310 [02:28<00:23,  3.42it/s] 74%|███████▍  | 230/310 [02:28<00:23,  3.42it/s] 75%|███████▍  | 231/310 [02:29<00:23,  3.42it/s] 75%|███████▍  | 232/310 [02:29<00:24,  3.25it/s] 75%|███████▌  | 233/310 [02:29<00:23,  3.30it/s] 75%|███████▌  | 234/310 [02:29<00:22,  3.34it/s] 76%|███████▌  | 235/310 [02:30<00:22,  3.37it/s] 76%|███████▌  | 236/310 [02:30<00:21,  3.39it/s] 76%|███████▋  | 237/310 [02:30<00:21,  3.40it/s] 77%|███████▋  | 238/310 [02:31<00:21,  3.41it/s] 77%|███████▋  | 239/310 [02:31<00:20,  3.41it/s] 77%|███████▋  | 240/310 [02:31<00:20,  3.42it/s] 78%|███████▊  | 241/310 [02:32<00:20,  3.42it/s] 78%|███████▊  | 242/310 [02:32<00:19,  3.43it/s] 78%|███████▊  | 243/310 [02:32<00:20,  3.33it/s] 79%|███████▊  | 244/310 [02:32<00:19,  3.36it/s] 79%|███████▉  | 245/310 [02:33<00:19,  3.38it/s] 79%|███████▉  | 246/310 [02:33<00:18,  3.39it/s] 80%|███████▉  | 247/310 [02:33<00:18,  3.40it/s] 80%|████████  | 248/310 [02:34<00:18,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 11:40:06,777 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:40:06,777 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:40:06,777 >>   Batch size = 8
{'eval_loss': 1.1277697086334229, 'eval_runtime': 12.8279, 'eval_samples_per_second': 359.295, 'eval_steps_per_second': 44.98, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.09it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.53it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.10it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.28it/s][A
  5%|▍         | 27/577 [00:00<00:12, 45.68it/s][A
  6%|▌         | 32/577 [00:00<00:12, 45.41it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.36it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.40it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.44it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.31it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.45it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.25it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 43.07it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 43.55it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.17it/s][A
 14%|█▍        | 82/577 [00:01<00:11, 44.43it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.28it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.92it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.36it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.71it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 44.52it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 44.87it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.95it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.01it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.88it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.03it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.14it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.20it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.15it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.15it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.19it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.13it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.13it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.14it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.04it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.12it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.12it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.20it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.15it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 43.03it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 43.62it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 44.27it/s][A
 38%|███▊      | 217/577 [00:04<00:08, 44.54it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 44.72it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.96it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.02it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.07it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 44.89it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 44.90it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.96it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.15it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.18it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.23it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.19it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.24it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.15it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.10it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.01it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.07it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.15it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.08it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.18it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.11it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.22it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.13it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.09it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 43.71it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 44.20it/s][A
 60%|██████    | 347/577 [00:07<00:05, 44.51it/s][A
 61%|██████    | 352/577 [00:07<00:05, 44.70it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 44.82it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 44.90it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.04it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.04it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.81it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 44.86it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.06it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.16it/s][A
 69%|██████▉   | 397/577 [00:08<00:03, 45.21it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 45.15it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.09it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.17it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.13it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.95it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.05it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.12it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.22it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.09it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.21it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.15it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.16it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.01it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.06it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 43.21it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 43.97it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 44.37it/s][A
 84%|████████▍ | 487/577 [00:10<00:02, 44.65it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 44.80it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 44.83it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.86it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.88it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.75it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.83it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 45.08it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.19it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.27it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 45.27it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.13it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.09it/s][A
 96%|█████████▌| 552/577 [00:12<00:01, 20.34it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 24.51it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 28.48it/s][A
 98%|█████████▊| 567/577 [00:13<00:00, 32.12it/s][A
 99%|█████████▉| 572/577 [00:13<00:00, 35.23it/s][A
100%|██████████| 577/577 [00:13<00:00, 37.83it/s][A                                                 
                                                 [A 80%|████████  | 248/310 [02:47<00:18,  3.41it/s]
100%|██████████| 577/577 [00:13<00:00, 37.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:40:20,328 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-29 11:40:20,637 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:40:25,312 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:40:25,637 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:40:25,796 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248/special_tokens_map.json
 80%|████████  | 249/310 [03:02<08:50,  8.70s/it] 81%|████████  | 250/310 [03:02<06:11,  6.19s/it] 81%|████████  | 251/310 [03:03<04:20,  4.42s/it] 81%|████████▏ | 252/310 [03:03<03:04,  3.18s/it] 82%|████████▏ | 253/310 [03:03<02:13,  2.33s/it] 82%|████████▏ | 254/310 [03:03<01:36,  1.72s/it] 82%|████████▏ | 255/310 [03:04<01:11,  1.29s/it] 83%|████████▎ | 256/310 [03:04<00:53,  1.01it/s] 83%|████████▎ | 257/310 [03:04<00:41,  1.28it/s] 83%|████████▎ | 258/310 [03:05<00:32,  1.58it/s] 84%|████████▎ | 259/310 [03:05<00:27,  1.88it/s] 84%|████████▍ | 260/310 [03:05<00:22,  2.18it/s] 84%|████████▍ | 261/310 [03:06<00:20,  2.45it/s] 85%|████████▍ | 262/310 [03:06<00:17,  2.68it/s] 85%|████████▍ | 263/310 [03:06<00:16,  2.87it/s] 85%|████████▌ | 264/310 [03:06<00:15,  2.90it/s] 85%|████████▌ | 265/310 [03:07<00:14,  3.04it/s] 86%|████████▌ | 266/310 [03:07<00:13,  3.15it/s] 86%|████████▌ | 267/310 [03:07<00:13,  3.23it/s] 86%|████████▋ | 268/310 [03:08<00:12,  3.29it/s] 87%|████████▋ | 269/310 [03:08<00:12,  3.33it/s] 87%|████████▋ | 270/310 [03:08<00:11,  3.36it/s] 87%|████████▋ | 271/310 [03:08<00:11,  3.38it/s] 88%|████████▊ | 272/310 [03:09<00:11,  3.40it/s] 88%|████████▊ | 273/310 [03:09<00:10,  3.41it/s] 88%|████████▊ | 274/310 [03:09<00:10,  3.42it/s] 89%|████████▊ | 275/310 [03:10<00:10,  3.32it/s] 89%|████████▉ | 276/310 [03:10<00:10,  3.35it/s] 89%|████████▉ | 277/310 [03:10<00:09,  3.38it/s] 90%|████████▉ | 278/310 [03:11<00:09,  3.40it/s] 90%|█████████ | 279/310 [03:11<00:09,  3.41it/s] 90%|█████████ | 280/310 [03:11<00:08,  3.41it/s] 91%|█████████ | 281/310 [03:11<00:08,  3.42it/s] 91%|█████████ | 282/310 [03:12<00:08,  3.42it/s] 91%|█████████▏| 283/310 [03:12<00:07,  3.43it/s] 92%|█████████▏| 284/310 [03:12<00:07,  3.43it/s] 92%|█████████▏| 285/310 [03:13<00:07,  3.43it/s] 92%|█████████▏| 286/310 [03:13<00:07,  3.34it/s] 93%|█████████▎| 287/310 [03:13<00:06,  3.37it/s] 93%|█████████▎| 288/310 [03:13<00:06,  3.39it/s] 93%|█████████▎| 289/310 [03:14<00:06,  3.40it/s] 94%|█████████▎| 290/310 [03:14<00:05,  3.41it/s] 94%|█████████▍| 291/310 [03:14<00:05,  3.42it/s] 94%|█████████▍| 292/310 [03:15<00:05,  3.42it/s] 95%|█████████▍| 293/310 [03:15<00:04,  3.44it/s] 95%|█████████▍| 294/310 [03:15<00:04,  3.45it/s] 95%|█████████▌| 295/310 [03:15<00:04,  3.46it/s] 95%|█████████▌| 296/310 [03:16<00:04,  3.47it/s] 96%|█████████▌| 297/310 [03:16<00:03,  3.36it/s] 96%|█████████▌| 298/310 [03:16<00:03,  3.40it/s] 96%|█████████▋| 299/310 [03:17<00:03,  3.42it/s] 97%|█████████▋| 300/310 [03:17<00:02,  3.44it/s] 97%|█████████▋| 301/310 [03:17<00:02,  3.45it/s] 97%|█████████▋| 302/310 [03:18<00:02,  3.46it/s] 98%|█████████▊| 303/310 [03:18<00:02,  3.47it/s] 98%|█████████▊| 304/310 [03:18<00:01,  3.47it/s] 98%|█████████▊| 305/310 [03:18<00:01,  3.48it/s] 99%|█████████▊| 306/310 [03:19<00:01,  3.48it/s] 99%|█████████▉| 307/310 [03:19<00:00,  3.48it/s] 99%|█████████▉| 308/310 [03:19<00:00,  3.34it/s]100%|█████████▉| 309/310 [03:20<00:00,  3.38it/s]100%|██████████| 310/310 [03:20<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 11:40:52,915 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:40:52,916 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:40:52,916 >>   Batch size = 8
{'eval_loss': 1.1461408138275146, 'eval_runtime': 13.2867, 'eval_samples_per_second': 346.888, 'eval_steps_per_second': 43.427, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.62it/s][A
  2%|▏         | 12/577 [00:00<00:11, 48.79it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.11it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.41it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.89it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.65it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.38it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.36it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.35it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.42it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.21it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.21it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.14it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.12it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 44.93it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.17it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.20it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.32it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.35it/s][A
 18%|█▊        | 102/577 [00:02<00:12, 38.73it/s][A
 19%|█▊        | 107/577 [00:02<00:11, 40.66it/s][A
 19%|█▉        | 112/577 [00:02<00:11, 42.08it/s][A
 20%|██        | 117/577 [00:02<00:10, 43.12it/s][A
 21%|██        | 122/577 [00:02<00:10, 43.78it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.47it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 44.82it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 44.97it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 44.46it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 44.36it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 44.52it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 44.72it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.27it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.45it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.55it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 45.45it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.00it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.79it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 44.78it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 44.82it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 44.97it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.11it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.31it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.46it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 45.44it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.26it/s][A
 41%|████      | 237/577 [00:05<00:08, 42.31it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 43.15it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 43.71it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 44.21it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 44.54it/s][A
 45%|████▌     | 262/577 [00:05<00:07, 44.79it/s][A
 46%|████▋     | 267/577 [00:05<00:06, 45.11it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.15it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.88it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.80it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.81it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.05it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.08it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.19it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.19it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 45.30it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.23it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.09it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.00it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 44.98it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.03it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.17it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.20it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.29it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.33it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.21it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.05it/s][A
 64%|██████▍   | 372/577 [00:08<00:05, 39.73it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 41.29it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 42.48it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 43.35it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 44.05it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.48it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 44.91it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.01it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 44.69it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 44.39it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 44.52it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 44.75it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 44.87it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.13it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.20it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 45.43it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.41it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.13it/s][A
 80%|████████  | 462/577 [00:10<00:02, 44.85it/s][A
 81%|████████  | 467/577 [00:10<00:02, 44.83it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 44.95it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.04it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.15it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.27it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.38it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 45.34it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 45.18it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.08it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.32it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.49it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.72it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.97it/s][A
 92%|█████████▏| 532/577 [00:11<00:00, 45.11it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 45.31it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.26it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.07it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 44.92it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 44.86it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 44.89it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 44.95it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.14it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.30it/s][A                                                 
                                                 [A100%|██████████| 310/310 [03:33<00:00,  3.41it/s]
100%|██████████| 577/577 [00:12<00:00, 45.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:41:05,933 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310
[INFO|configuration_utils.py:351] 2023-08-29 11:41:06,113 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:41:08,724 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:41:08,932 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:41:09,031 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:41:17,540 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:41:17,566 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62 (score: 1.0951038599014282).
                                                 100%|██████████| 310/310 [03:55<00:00,  3.41it/s]100%|██████████| 310/310 [03:55<00:00,  1.32it/s]
[INFO|trainer.py:1894] 2023-08-29 11:41:28,525 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 11:41:28,729 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:41:32,313 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:41:32,510 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:41:32,585 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:41:33,047 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,047 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,048 >>   train_loss               =     0.4108
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,048 >>   train_runtime            = 0:03:55.44
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,048 >>   train_samples            =       4000
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,048 >>   train_samples_per_second =     84.947
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:33,048 >>   train_steps_per_second   =      1.317
{'eval_loss': 1.1498438119888306, 'eval_runtime': 12.9043, 'eval_samples_per_second': 357.168, 'eval_steps_per_second': 44.714, 'epoch': 4.99}
{'train_runtime': 235.4416, 'train_samples_per_second': 84.947, 'train_steps_per_second': 1.317, 'train_loss': 0.4107970453077747, 'epoch': 4.99}
08/29/2023 11:41:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:41:33,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:41:33,354 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 11:41:33,355 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 56.46it/s]  2%|▏         | 12/577 [00:00<00:11, 50.22it/s]  3%|▎         | 18/577 [00:00<00:11, 48.25it/s]  4%|▍         | 23/577 [00:00<00:11, 47.49it/s]  5%|▍         | 28/577 [00:00<00:11, 46.97it/s]  6%|▌         | 33/577 [00:00<00:11, 46.68it/s]  7%|▋         | 38/577 [00:00<00:11, 46.52it/s]  7%|▋         | 43/577 [00:00<00:11, 46.08it/s]  8%|▊         | 48/577 [00:01<00:11, 45.42it/s]  9%|▉         | 53/577 [00:01<00:11, 45.22it/s] 10%|█         | 58/577 [00:01<00:11, 45.33it/s] 11%|█         | 63/577 [00:01<00:11, 45.54it/s] 12%|█▏        | 68/577 [00:01<00:11, 45.62it/s] 13%|█▎        | 73/577 [00:01<00:11, 45.79it/s] 14%|█▎        | 78/577 [00:01<00:10, 45.85it/s] 14%|█▍        | 83/577 [00:01<00:10, 45.82it/s] 15%|█▌        | 88/577 [00:01<00:10, 45.58it/s] 16%|█▌        | 93/577 [00:02<00:10, 45.30it/s] 17%|█▋        | 98/577 [00:02<00:11, 43.26it/s] 18%|█▊        | 103/577 [00:02<00:10, 44.11it/s] 19%|█▊        | 108/577 [00:02<00:10, 44.58it/s] 20%|█▉        | 113/577 [00:02<00:10, 44.95it/s] 20%|██        | 118/577 [00:02<00:10, 45.21it/s] 21%|██▏       | 123/577 [00:02<00:09, 45.47it/s] 22%|██▏       | 128/577 [00:02<00:09, 45.53it/s] 23%|██▎       | 133/577 [00:02<00:09, 45.38it/s] 24%|██▍       | 138/577 [00:03<00:09, 45.06it/s] 25%|██▍       | 143/577 [00:03<00:09, 45.05it/s] 26%|██▌       | 148/577 [00:03<00:09, 45.14it/s] 27%|██▋       | 153/577 [00:03<00:09, 45.37it/s] 27%|██▋       | 158/577 [00:03<00:09, 45.46it/s] 28%|██▊       | 163/577 [00:03<00:09, 45.63it/s] 29%|██▉       | 168/577 [00:03<00:08, 45.56it/s] 30%|██▉       | 173/577 [00:03<00:08, 45.70it/s] 31%|███       | 178/577 [00:03<00:08, 45.66it/s] 32%|███▏      | 183/577 [00:04<00:08, 45.45it/s] 33%|███▎      | 188/577 [00:04<00:08, 45.32it/s] 33%|███▎      | 193/577 [00:04<00:08, 45.28it/s] 34%|███▍      | 198/577 [00:04<00:08, 45.43it/s] 35%|███▌      | 203/577 [00:04<00:08, 45.47it/s] 36%|███▌      | 208/577 [00:04<00:08, 45.48it/s] 37%|███▋      | 213/577 [00:04<00:07, 45.59it/s] 38%|███▊      | 218/577 [00:04<00:07, 45.59it/s] 39%|███▊      | 223/577 [00:04<00:07, 45.55it/s] 40%|███▉      | 228/577 [00:04<00:07, 45.42it/s] 40%|████      | 233/577 [00:05<00:07, 45.28it/s] 41%|████      | 238/577 [00:05<00:07, 45.31it/s] 42%|████▏     | 243/577 [00:05<00:07, 45.27it/s] 43%|████▎     | 248/577 [00:05<00:07, 45.38it/s] 44%|████▍     | 253/577 [00:05<00:07, 45.44it/s] 45%|████▍     | 258/577 [00:05<00:07, 45.46it/s] 46%|████▌     | 263/577 [00:05<00:06, 45.48it/s] 46%|████▋     | 268/577 [00:05<00:06, 45.38it/s] 47%|████▋     | 273/577 [00:05<00:06, 45.39it/s] 48%|████▊     | 278/577 [00:06<00:06, 45.35it/s] 49%|████▉     | 283/577 [00:06<00:06, 45.25it/s] 50%|████▉     | 288/577 [00:06<00:06, 45.32it/s] 51%|█████     | 293/577 [00:06<00:06, 45.37it/s] 52%|█████▏    | 298/577 [00:06<00:06, 45.42it/s] 53%|█████▎    | 303/577 [00:06<00:06, 45.49it/s] 53%|█████▎    | 308/577 [00:06<00:05, 45.47it/s] 54%|█████▍    | 313/577 [00:06<00:05, 45.43it/s] 55%|█████▌    | 318/577 [00:06<00:05, 45.33it/s] 56%|█████▌    | 323/577 [00:07<00:05, 44.00it/s] 57%|█████▋    | 328/577 [00:07<00:05, 44.44it/s] 58%|█████▊    | 333/577 [00:07<00:05, 44.68it/s] 59%|█████▊    | 338/577 [00:07<00:05, 44.99it/s] 59%|█████▉    | 343/577 [00:07<00:05, 45.17it/s] 60%|██████    | 348/577 [00:07<00:05, 45.30it/s] 61%|██████    | 353/577 [00:07<00:04, 45.29it/s] 62%|██████▏   | 358/577 [00:07<00:04, 45.24it/s] 63%|██████▎   | 363/577 [00:07<00:04, 45.02it/s] 64%|██████▍   | 368/577 [00:08<00:04, 44.91it/s] 65%|██████▍   | 373/577 [00:08<00:04, 45.03it/s] 66%|██████▌   | 378/577 [00:08<00:04, 45.08it/s] 66%|██████▋   | 383/577 [00:08<00:04, 45.28it/s] 67%|██████▋   | 388/577 [00:08<00:04, 45.46it/s] 68%|██████▊   | 393/577 [00:08<00:04, 45.52it/s] 69%|██████▉   | 398/577 [00:08<00:03, 45.59it/s] 70%|██████▉   | 403/577 [00:08<00:03, 45.49it/s] 71%|███████   | 408/577 [00:08<00:03, 45.28it/s] 72%|███████▏  | 413/577 [00:09<00:03, 45.21it/s] 72%|███████▏  | 418/577 [00:09<00:03, 45.08it/s] 73%|███████▎  | 423/577 [00:09<00:03, 45.23it/s] 74%|███████▍  | 428/577 [00:09<00:03, 45.29it/s] 75%|███████▌  | 433/577 [00:09<00:03, 45.52it/s] 76%|███████▌  | 438/577 [00:09<00:03, 45.61it/s] 77%|███████▋  | 443/577 [00:09<00:02, 45.57it/s] 78%|███████▊  | 448/577 [00:09<00:02, 45.45it/s] 79%|███████▊  | 453/577 [00:09<00:02, 45.32it/s] 79%|███████▉  | 458/577 [00:10<00:02, 45.24it/s] 80%|████████  | 463/577 [00:10<00:02, 43.10it/s] 81%|████████  | 468/577 [00:10<00:02, 43.89it/s] 82%|████████▏ | 473/577 [00:10<00:02, 44.45it/s] 83%|████████▎ | 478/577 [00:10<00:02, 44.82it/s] 84%|████████▎ | 483/577 [00:10<00:02, 45.08it/s] 85%|████████▍ | 488/577 [00:10<00:01, 45.27it/s] 85%|████████▌ | 493/577 [00:10<00:01, 45.33it/s] 86%|████████▋ | 498/577 [00:10<00:01, 45.30it/s] 87%|████████▋ | 503/577 [00:11<00:01, 45.03it/s] 88%|████████▊ | 508/577 [00:11<00:01, 45.02it/s] 89%|████████▉ | 513/577 [00:11<00:01, 45.18it/s] 90%|████████▉ | 518/577 [00:11<00:01, 45.30it/s] 91%|█████████ | 523/577 [00:11<00:01, 45.33it/s] 92%|█████████▏| 528/577 [00:11<00:01, 45.32it/s] 92%|█████████▏| 533/577 [00:11<00:00, 45.35it/s] 93%|█████████▎| 538/577 [00:11<00:00, 45.32it/s] 94%|█████████▍| 543/577 [00:11<00:00, 45.17it/s] 95%|█████████▍| 548/577 [00:12<00:00, 45.06it/s] 96%|█████████▌| 553/577 [00:12<00:00, 45.05it/s] 97%|█████████▋| 558/577 [00:12<00:00, 45.20it/s] 98%|█████████▊| 563/577 [00:12<00:00, 45.43it/s] 98%|█████████▊| 568/577 [00:12<00:00, 45.48it/s] 99%|█████████▉| 573/577 [00:12<00:00, 45.51it/s]100%|██████████| 577/577 [00:12<00:00, 45.37it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:41:46,090 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   eval_loss               =     1.0951
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   eval_runtime            = 0:00:12.73
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   eval_samples_per_second =    361.907
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   eval_steps_per_second   =     45.307
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:41:46,090 >>   perplexity              =     2.9895
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:55,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:55,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:55,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:55,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:55,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:41:55,865 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:41:55,866 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:56,199 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:57,263 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:57,263 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:58,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:58,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:58,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:58,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:58,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:41:59,266 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:41:59,267 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:59,587 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:59,745 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:59,745 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-310
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-186
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-62
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/checkpoint-248
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:11,  1.69it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:25,  1.56it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:27,  1.54it/s]Extractor Predicting: 46it [00:28,  1.55it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:34,  1.62it/s]Extractor Predicting: 57it [00:35,  1.61it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:36,  1.63it/s]Extractor Predicting: 60it [00:37,  1.61it/s]Extractor Predicting: 61it [00:37,  1.59it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:39,  1.55it/s]Extractor Predicting: 65it [00:40,  1.56it/s]Extractor Predicting: 66it [00:41,  1.61it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.66it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.64it/s]Extractor Predicting: 71it [00:44,  1.67it/s]Extractor Predicting: 72it [00:44,  1.72it/s]Extractor Predicting: 73it [00:45,  1.69it/s]Extractor Predicting: 74it [00:45,  1.71it/s]Extractor Predicting: 75it [00:46,  1.71it/s]Extractor Predicting: 76it [00:47,  1.67it/s]Extractor Predicting: 77it [00:47,  1.65it/s]Extractor Predicting: 78it [00:48,  1.68it/s]Extractor Predicting: 79it [00:48,  1.67it/s]Extractor Predicting: 80it [00:49,  1.67it/s]Extractor Predicting: 81it [00:50,  1.65it/s]Extractor Predicting: 82it [00:50,  1.65it/s]Extractor Predicting: 83it [00:51,  1.64it/s]Extractor Predicting: 84it [00:51,  1.69it/s]Extractor Predicting: 85it [00:52,  1.75it/s]Extractor Predicting: 86it [00:52,  1.69it/s]Extractor Predicting: 87it [00:53,  1.73it/s]Extractor Predicting: 88it [00:54,  1.65it/s]Extractor Predicting: 89it [00:54,  1.65it/s]Extractor Predicting: 90it [00:55,  1.63it/s]Extractor Predicting: 91it [00:56,  1.64it/s]Extractor Predicting: 92it [00:56,  1.65it/s]Extractor Predicting: 93it [00:57,  1.66it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:58,  1.64it/s]Extractor Predicting: 96it [00:59,  1.65it/s]Extractor Predicting: 97it [00:59,  1.66it/s]Extractor Predicting: 98it [01:00,  1.62it/s]Extractor Predicting: 99it [01:00,  1.63it/s]Extractor Predicting: 100it [01:01,  1.60it/s]Extractor Predicting: 101it [01:02,  1.60it/s]Extractor Predicting: 102it [01:02,  1.63it/s]Extractor Predicting: 103it [01:03,  1.46it/s]Extractor Predicting: 104it [01:04,  1.52it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.55it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.62it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.59it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:10,  1.62it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.66it/s]Extractor Predicting: 118it [01:12,  1.66it/s]Extractor Predicting: 119it [01:13,  1.69it/s]Extractor Predicting: 120it [01:13,  1.68it/s]Extractor Predicting: 121it [01:14,  1.67it/s]Extractor Predicting: 122it [01:15,  1.65it/s]Extractor Predicting: 123it [01:15,  1.64it/s]Extractor Predicting: 124it [01:16,  1.66it/s]Extractor Predicting: 125it [01:17,  1.62it/s]Extractor Predicting: 126it [01:17,  1.66it/s]Extractor Predicting: 127it [01:18,  1.66it/s]Extractor Predicting: 128it [01:18,  1.64it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.59it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.63it/s]Extractor Predicting: 133it [01:21,  1.64it/s]Extractor Predicting: 134it [01:22,  1.62it/s]Extractor Predicting: 135it [01:23,  1.62it/s]Extractor Predicting: 136it [01:23,  1.60it/s]Extractor Predicting: 137it [01:24,  1.63it/s]Extractor Predicting: 138it [01:25,  1.62it/s]Extractor Predicting: 139it [01:25,  1.59it/s]Extractor Predicting: 140it [01:26,  1.58it/s]Extractor Predicting: 141it [01:26,  1.57it/s]Extractor Predicting: 142it [01:27,  1.55it/s]Extractor Predicting: 143it [01:28,  1.57it/s]Extractor Predicting: 144it [01:28,  1.54it/s]Extractor Predicting: 145it [01:29,  1.55it/s]Extractor Predicting: 146it [01:30,  1.52it/s]Extractor Predicting: 147it [01:30,  1.53it/s]Extractor Predicting: 148it [01:31,  1.56it/s]Extractor Predicting: 149it [01:32,  1.59it/s]Extractor Predicting: 150it [01:32,  1.58it/s]Extractor Predicting: 151it [01:33,  1.61it/s]Extractor Predicting: 152it [01:33,  1.57it/s]Extractor Predicting: 153it [01:34,  1.60it/s]Extractor Predicting: 154it [01:35,  1.59it/s]Extractor Predicting: 155it [01:35,  1.59it/s]Extractor Predicting: 156it [01:36,  1.57it/s]Extractor Predicting: 157it [01:37,  1.60it/s]Extractor Predicting: 158it [01:37,  1.61it/s]Extractor Predicting: 159it [01:38,  1.60it/s]Extractor Predicting: 160it [01:38,  1.59it/s]Extractor Predicting: 161it [01:39,  1.63it/s]Extractor Predicting: 162it [01:40,  1.62it/s]Extractor Predicting: 163it [01:40,  1.57it/s]Extractor Predicting: 164it [01:41,  1.57it/s]Extractor Predicting: 165it [01:42,  1.58it/s]Extractor Predicting: 166it [01:42,  1.56it/s]Extractor Predicting: 167it [01:43,  1.58it/s]Extractor Predicting: 168it [01:44,  1.58it/s]Extractor Predicting: 169it [01:44,  1.58it/s]Extractor Predicting: 170it [01:45,  1.59it/s]Extractor Predicting: 171it [01:45,  1.85it/s]Extractor Predicting: 171it [01:45,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:43:57,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:43:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:43:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:43:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:43:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:43:57,833 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:43:57,834 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:43:58,243 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:43:59,393 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:43:59,393 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:44:01,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:44:01,519 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:44:01,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:44:01,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:44:01,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:44:02,138 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:44:02,139 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:44:02,887 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:44:03,157 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:44:03,157 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.23709740007760963,
  "recall": 0.13256671729225428,
  "score": 0.17005288060116894,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.55it/s]Extractor Predicting: 39it [00:23,  1.56it/s]Extractor Predicting: 40it [00:24,  1.60it/s]Extractor Predicting: 41it [00:24,  1.62it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:27,  1.67it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.65it/s]Extractor Predicting: 49it [00:29,  1.65it/s]Extractor Predicting: 50it [00:30,  1.65it/s]Extractor Predicting: 51it [00:30,  1.66it/s]Extractor Predicting: 52it [00:31,  1.67it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.62it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:34,  1.65it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:37,  1.60it/s]Extractor Predicting: 63it [00:38,  1.65it/s]Extractor Predicting: 64it [00:38,  1.64it/s]Extractor Predicting: 65it [00:39,  1.66it/s]Extractor Predicting: 66it [00:40,  1.65it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:41,  1.68it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.67it/s]Extractor Predicting: 71it [00:43,  1.59it/s]Extractor Predicting: 72it [00:43,  1.62it/s]Extractor Predicting: 73it [00:44,  1.62it/s]Extractor Predicting: 74it [00:44,  1.64it/s]Extractor Predicting: 75it [00:45,  1.65it/s]Extractor Predicting: 76it [00:46,  1.68it/s]Extractor Predicting: 77it [00:46,  1.71it/s]Extractor Predicting: 78it [00:47,  1.70it/s]Extractor Predicting: 79it [00:47,  1.67it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:49,  1.69it/s]Extractor Predicting: 82it [00:49,  1.69it/s]Extractor Predicting: 83it [00:50,  1.68it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.67it/s]Extractor Predicting: 86it [00:52,  1.68it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:53,  1.64it/s]Extractor Predicting: 89it [00:53,  1.65it/s]Extractor Predicting: 90it [00:54,  1.63it/s]Extractor Predicting: 91it [00:55,  1.59it/s]Extractor Predicting: 92it [00:55,  1.61it/s]Extractor Predicting: 93it [00:56,  1.63it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:57,  1.62it/s]Extractor Predicting: 96it [00:58,  1.54it/s]Extractor Predicting: 97it [00:58,  1.60it/s]Extractor Predicting: 98it [00:59,  1.62it/s]Extractor Predicting: 99it [01:00,  1.72it/s]Extractor Predicting: 100it [01:00,  1.72it/s]Extractor Predicting: 101it [01:01,  1.69it/s]Extractor Predicting: 102it [01:01,  1.66it/s]Extractor Predicting: 103it [01:02,  1.67it/s]Extractor Predicting: 104it [01:03,  1.68it/s]Extractor Predicting: 105it [01:03,  1.68it/s]Extractor Predicting: 106it [01:04,  1.70it/s]Extractor Predicting: 107it [01:04,  1.73it/s]Extractor Predicting: 108it [01:05,  1.68it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:06,  1.57it/s]Extractor Predicting: 111it [01:07,  1.56it/s]Extractor Predicting: 112it [01:07,  1.57it/s]Extractor Predicting: 113it [01:08,  1.61it/s]Extractor Predicting: 114it [01:09,  1.63it/s]Extractor Predicting: 115it [01:09,  1.61it/s]Extractor Predicting: 116it [01:10,  1.61it/s]Extractor Predicting: 117it [01:11,  1.61it/s]Extractor Predicting: 118it [01:11,  1.56it/s]Extractor Predicting: 119it [01:12,  1.59it/s]Extractor Predicting: 120it [01:13,  1.45it/s]Extractor Predicting: 121it [01:13,  1.47it/s]Extractor Predicting: 122it [01:14,  1.52it/s]Extractor Predicting: 123it [01:15,  1.53it/s]Extractor Predicting: 124it [01:15,  1.55it/s]Extractor Predicting: 125it [01:15,  1.93it/s]Extractor Predicting: 125it [01:15,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:30,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:31,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:31,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:31,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:31,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:45:31,645 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:45:31,646 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:45:32,227 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:45:33,254 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:45:33,255 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:36,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:36,368 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:36,368 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:36,368 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:45:36,368 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:45:37,060 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:45:37,061 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:45:37,662 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:45:37,847 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:45:37,847 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4048309178743961,
  "recall": 0.2813024504867405,
  "score": 0.33194692018221433,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  2.04it/s]Extractor Predicting: 6it [00:03,  1.73it/s]
[INFO|configuration_utils.py:515] 2023-08-29 11:45:42,894 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:45:42,895 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:45:42,936 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:45:42,937 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 11:45:42,953 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:45:53,267 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 11:45:53,282 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 11:45:53,349 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:45:53,349 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:45:53,384 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:45:53,405 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5052631578947369,
  "recall": 0.1889763779527559,
  "score": 0.27507163323782235,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 11:45:53,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:54,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:54,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:55,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:55,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:56,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:57,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:57,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:58,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:59,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:59,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:00,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:00,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:01,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:01,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:02,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:03,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:03,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:04,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:05,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:11<01:47, 11.95s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:05,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:06,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:06,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:07,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:07,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:08,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:08,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:09,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:10,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:10,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:11,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:11,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:11,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:12,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:13,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:13,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:14,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:14,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:15,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:15,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:22<01:29, 11.21s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:16,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:16,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:17,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:17,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:18,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:18,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:19,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:19,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:20,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:20,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:21,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:21,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:22,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:22,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:24,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:24,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:25,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:25,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:26,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:33<01:16, 10.92s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:26,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:27,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:28,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:29,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:30,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:30,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:31,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:31,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:32,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:33,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:33,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:34,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:34,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:35,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:35,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:36,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:37,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:37,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:38,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:45<01:07, 11.30s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:38,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:39,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:39,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:40,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:41,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:41,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:42,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:42,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:43,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:44,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:44,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:45,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:45,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:46,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:46,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:47,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:47,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:48,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:49,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:49,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [00:56<00:56, 11.39s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:50,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:50,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:51,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:52,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:52,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:53,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:53,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:54,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:54,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:55,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:55,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:56,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:57,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:57,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:58,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:58,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:59,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:00,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:00,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:01,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:08<00:45, 11.46s/it][WARNING|generation_utils.py:914] 2023-08-29 11:47:01,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:02,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:03,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:03,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:04,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:04,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:05,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:06,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:07,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:07,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:08,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:09,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:09,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:10,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:10,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:11,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:12,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:12,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:13,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:20<00:34, 11.65s/it][WARNING|generation_utils.py:914] 2023-08-29 11:47:13,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:14,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:15,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:15,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:16,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:16,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:17,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:17,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:18,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:19,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:19,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:20,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:20,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:21,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:21,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:22,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:22,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:23,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:23,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:24,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:31<00:22, 11.48s/it][WARNING|generation_utils.py:914] 2023-08-29 11:47:25,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:25,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:26,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:26,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:27,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:27,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:28,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:29,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:29,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:30,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:30,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:31,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:31,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:32,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:33,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:33,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:34,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:34,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:35,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:35,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:42<00:11, 11.34s/it][WARNING|generation_utils.py:914] 2023-08-29 11:47:36,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:36,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:37,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:37,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:38,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:38,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:39,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:39,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:40,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:40,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:41,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:41,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:42,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:42,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:43,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:43,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:44,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:44,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:45,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:45,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:46,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:47,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:47,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:48,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:48,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:49,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:49,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:50,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:50,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:51,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:47:52,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [01:59<00:00, 13.01s/it]Generating: 100%|██████████| 10/10 [01:59<00:00, 11.92s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:00,476 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:00,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:00,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:00,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:00,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:48:01,388 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:48:01,390 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:48:02,034 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:48:03,166 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:48:03,166 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:06,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:06,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:06,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:06,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:48:06,522 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:48:07,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:48:07,507 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:48:08,197 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:48:08,481 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:48:08,481 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : official language .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 444, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 506, 'raw': 512}
{'target': 600, 'success': 538, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 599, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.984375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : occupant .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : use .', 'success_rate': 0.9375, 'errors': {'', "('C11', 'use', '', 'The main source of the code is the GNU C Library by Martin Selassie and the latest version is version 9 . 6 . The official GNU C Library is available as a package in the open source software distribution packages , C11 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : voice type . Context : Later in the year , she became a guest star on The Tonight Show with John Oliver and was featured alongside George Takei on a variety show called The Tonight Show with Jimmy Fallon , alongside Fallon . Head Entity : Jimmy Fallon , Tail Entity : voice type .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 75, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 135, 'raw': 224}
{'target': 600, 'success': 155, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 209, 'raw': 352}
{'target': 600, 'success': 225, 'raw': 384}
{'target': 600, 'success': 244, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 285, 'raw': 480}
{'target': 600, 'success': 301, 'raw': 512}
{'target': 600, 'success': 322, 'raw': 544}
{'target': 600, 'success': 346, 'raw': 576}
{'target': 600, 'success': 364, 'raw': 608}
{'target': 600, 'success': 384, 'raw': 640}
{'target': 600, 'success': 401, 'raw': 672}
{'target': 600, 'success': 418, 'raw': 704}
{'target': 600, 'success': 436, 'raw': 736}
{'target': 600, 'success': 459, 'raw': 768}
{'target': 600, 'success': 479, 'raw': 800}
{'target': 600, 'success': 498, 'raw': 832}
{'target': 600, 'success': 521, 'raw': 864}
{'target': 600, 'success': 538, 'raw': 896}
{'target': 600, 'success': 557, 'raw': 928}
{'target': 600, 'success': 569, 'raw': 960}
{'target': 600, 'success': 592, 'raw': 992}
{'target': 600, 'success': 603, 'raw': 1024}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5888671875, 'errors': {'', "('EastEnders', 'voice type', '', 'Like his fellow actors , he is also known for his work with the BBC soap opera EastEnders , playing the role of a girl who auditioned to play the part of a detective .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 6241
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6341, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.71it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.65it/s]Extractor Estimating: 4it [00:02,  1.69it/s]Extractor Estimating: 5it [00:02,  1.74it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.67it/s]Extractor Estimating: 9it [00:05,  1.68it/s]Extractor Estimating: 10it [00:05,  1.68it/s]Extractor Estimating: 11it [00:06,  1.70it/s]Extractor Estimating: 12it [00:07,  1.76it/s]Extractor Estimating: 13it [00:07,  1.75it/s]Extractor Estimating: 14it [00:08,  1.78it/s]Extractor Estimating: 15it [00:08,  1.75it/s]Extractor Estimating: 16it [00:09,  1.75it/s]Extractor Estimating: 17it [00:09,  1.71it/s]Extractor Estimating: 18it [00:10,  1.73it/s]Extractor Estimating: 19it [00:11,  1.71it/s]Extractor Estimating: 20it [00:11,  1.70it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:12,  1.66it/s]Extractor Estimating: 23it [00:13,  1.67it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:14,  1.70it/s]Extractor Estimating: 26it [00:15,  1.75it/s]Extractor Estimating: 27it [00:15,  1.75it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:16,  1.75it/s]Extractor Estimating: 30it [00:17,  1.71it/s]Extractor Estimating: 31it [00:18,  1.75it/s]Extractor Estimating: 32it [00:18,  1.83it/s]Extractor Estimating: 33it [00:19,  1.78it/s]Extractor Estimating: 34it [00:19,  1.88it/s]Extractor Estimating: 35it [00:20,  1.92it/s]Extractor Estimating: 36it [00:20,  1.92it/s]Extractor Estimating: 37it [00:21,  1.88it/s]Extractor Estimating: 38it [00:21,  1.95it/s]Extractor Estimating: 39it [00:22,  1.92it/s]Extractor Estimating: 40it [00:22,  1.98it/s]Extractor Estimating: 41it [00:23,  2.02it/s]Extractor Estimating: 42it [00:23,  2.06it/s]Extractor Estimating: 43it [00:24,  1.86it/s]Extractor Estimating: 44it [00:24,  1.89it/s]Extractor Estimating: 45it [00:25,  1.87it/s]Extractor Estimating: 46it [00:25,  1.92it/s]Extractor Estimating: 47it [00:26,  1.90it/s]Extractor Estimating: 48it [00:26,  1.98it/s]Extractor Estimating: 49it [00:27,  1.97it/s]Extractor Estimating: 50it [00:27,  1.94it/s]Extractor Estimating: 51it [00:28,  1.95it/s]Extractor Estimating: 52it [00:28,  1.93it/s]Extractor Estimating: 53it [00:29,  1.89it/s]Extractor Estimating: 54it [00:29,  1.92it/s]Extractor Estimating: 55it [00:30,  1.91it/s]Extractor Estimating: 56it [00:30,  1.96it/s]Extractor Estimating: 57it [00:31,  1.99it/s]Extractor Estimating: 58it [00:31,  1.94it/s]Extractor Estimating: 59it [00:32,  1.92it/s]Extractor Estimating: 60it [00:33,  1.92it/s]Extractor Estimating: 61it [00:33,  1.95it/s]Extractor Estimating: 62it [00:34,  1.94it/s]Extractor Estimating: 63it [00:34,  1.91it/s]Extractor Estimating: 64it [00:35,  1.90it/s]Extractor Estimating: 65it [00:35,  1.95it/s]Extractor Estimating: 66it [00:36,  1.92it/s]Extractor Estimating: 67it [00:36,  1.91it/s]Extractor Estimating: 68it [00:37,  1.94it/s]Extractor Estimating: 69it [00:37,  1.93it/s]Extractor Estimating: 70it [00:38,  1.87it/s]Extractor Estimating: 71it [00:38,  1.84it/s]Extractor Estimating: 72it [00:39,  1.85it/s]Extractor Estimating: 73it [00:39,  1.84it/s]Extractor Estimating: 74it [00:40,  1.88it/s]Extractor Estimating: 75it [00:40,  1.86it/s]Extractor Estimating: 76it [00:41,  1.85it/s]Extractor Estimating: 77it [00:42,  1.80it/s]Extractor Estimating: 78it [00:42,  1.82it/s]Extractor Estimating: 79it [00:43,  1.79it/s]Extractor Estimating: 80it [00:43,  1.79it/s]Extractor Estimating: 81it [00:44,  1.82it/s]Extractor Estimating: 82it [00:44,  1.76it/s]Extractor Estimating: 83it [00:45,  1.82it/s]Extractor Estimating: 84it [00:45,  1.82it/s]Extractor Estimating: 85it [00:46,  1.81it/s]Extractor Estimating: 86it [00:47,  1.86it/s]Extractor Estimating: 87it [00:47,  1.80it/s]Extractor Estimating: 88it [00:48,  1.78it/s]Extractor Estimating: 89it [00:48,  1.73it/s]Extractor Estimating: 90it [00:49,  1.71it/s]Extractor Estimating: 91it [00:50,  1.67it/s]Extractor Estimating: 92it [00:50,  1.69it/s]Extractor Estimating: 93it [00:51,  1.76it/s]Extractor Estimating: 94it [00:51,  1.75it/s]Extractor Estimating: 95it [00:52,  1.64it/s]Extractor Estimating: 96it [00:53,  1.68it/s]Extractor Estimating: 97it [00:53,  1.74it/s]Extractor Estimating: 98it [00:54,  1.76it/s]Extractor Estimating: 99it [00:54,  1.84it/s]Extractor Estimating: 100it [00:55,  1.75it/s]Extractor Estimating: 101it [00:55,  1.73it/s]Extractor Estimating: 102it [00:56,  1.72it/s]Extractor Estimating: 103it [00:56,  1.71it/s]Extractor Estimating: 104it [00:57,  1.70it/s]Extractor Estimating: 105it [00:58,  1.70it/s]Extractor Estimating: 106it [00:58,  1.73it/s]Extractor Estimating: 107it [00:59,  1.72it/s]Extractor Estimating: 108it [00:59,  1.70it/s]Extractor Estimating: 109it [01:00,  1.65it/s]Extractor Estimating: 110it [01:01,  1.67it/s]Extractor Estimating: 111it [01:01,  1.67it/s]Extractor Estimating: 112it [01:02,  1.67it/s]Extractor Estimating: 113it [01:02,  1.69it/s]Extractor Estimating: 114it [01:03,  1.68it/s]Extractor Estimating: 115it [01:04,  1.66it/s]Extractor Estimating: 116it [01:04,  1.70it/s]Extractor Estimating: 117it [01:05,  1.69it/s]Extractor Estimating: 118it [01:05,  1.68it/s]Extractor Estimating: 119it [01:06,  1.68it/s]Extractor Estimating: 120it [01:07,  1.68it/s]Extractor Estimating: 121it [01:07,  1.68it/s]Extractor Estimating: 122it [01:08,  1.68it/s]Extractor Estimating: 123it [01:08,  1.68it/s]Extractor Estimating: 124it [01:09,  1.69it/s]Extractor Estimating: 125it [01:10,  1.65it/s]Extractor Estimating: 126it [01:10,  1.61it/s]Extractor Estimating: 127it [01:11,  1.58it/s]Extractor Estimating: 128it [01:12,  1.56it/s]Extractor Estimating: 129it [01:12,  1.61it/s]Extractor Estimating: 130it [01:13,  1.62it/s]Extractor Estimating: 131it [01:13,  1.65it/s]Extractor Estimating: 132it [01:14,  1.67it/s]Extractor Estimating: 133it [01:14,  1.69it/s]Extractor Estimating: 134it [01:15,  1.72it/s]Extractor Estimating: 135it [01:16,  1.70it/s]Extractor Estimating: 136it [01:16,  1.69it/s]Extractor Estimating: 137it [01:17,  1.68it/s]Extractor Estimating: 138it [01:17,  1.67it/s]Extractor Estimating: 139it [01:18,  1.60it/s]Extractor Estimating: 140it [01:19,  1.67it/s]Extractor Estimating: 141it [01:19,  1.65it/s]Extractor Estimating: 142it [01:20,  1.61it/s]Extractor Estimating: 143it [01:21,  1.60it/s]Extractor Estimating: 144it [01:21,  1.59it/s]Extractor Estimating: 145it [01:22,  1.60it/s]Extractor Estimating: 146it [01:23,  1.56it/s]Extractor Estimating: 147it [01:23,  1.56it/s]Extractor Estimating: 148it [01:24,  1.56it/s]Extractor Estimating: 149it [01:24,  1.60it/s]Extractor Estimating: 150it [01:25,  1.55it/s]Extractor Estimating: 151it [01:26,  1.55it/s]Extractor Estimating: 152it [01:26,  1.62it/s]Extractor Estimating: 153it [01:27,  1.63it/s]Extractor Estimating: 154it [01:28,  1.62it/s]Extractor Estimating: 155it [01:28,  1.63it/s]Extractor Estimating: 156it [01:29,  1.68it/s]Extractor Estimating: 157it [01:29,  1.68it/s]Extractor Estimating: 158it [01:30,  1.70it/s]Extractor Estimating: 159it [01:30,  1.69it/s]Extractor Estimating: 160it [01:31,  1.56it/s]Extractor Estimating: 161it [01:32,  1.60it/s]Extractor Estimating: 162it [01:32,  1.61it/s]Extractor Estimating: 163it [01:33,  1.62it/s]Extractor Estimating: 164it [01:34,  1.66it/s]Extractor Estimating: 165it [01:34,  1.70it/s]Extractor Estimating: 166it [01:35,  1.70it/s]Extractor Estimating: 167it [01:35,  1.70it/s]Extractor Estimating: 168it [01:36,  1.68it/s]Extractor Estimating: 169it [01:37,  1.64it/s]Extractor Estimating: 170it [01:37,  1.67it/s]Extractor Estimating: 171it [01:38,  1.70it/s]Extractor Estimating: 172it [01:38,  1.65it/s]Extractor Estimating: 173it [01:39,  1.67it/s]Extractor Estimating: 174it [01:40,  1.66it/s]Extractor Estimating: 175it [01:40,  1.69it/s]Extractor Estimating: 176it [01:41,  1.69it/s]Extractor Estimating: 177it [01:41,  1.68it/s]Extractor Estimating: 178it [01:42,  1.72it/s]Extractor Estimating: 179it [01:42,  1.69it/s]Extractor Estimating: 180it [01:43,  1.68it/s]Extractor Estimating: 181it [01:44,  1.69it/s]Extractor Estimating: 182it [01:44,  1.68it/s]Extractor Estimating: 183it [01:45,  1.68it/s]Extractor Estimating: 184it [01:45,  1.73it/s]Extractor Estimating: 185it [01:46,  1.75it/s]Extractor Estimating: 186it [01:47,  1.72it/s]Extractor Estimating: 187it [01:47,  1.70it/s]Extractor Estimating: 188it [01:48,  1.68it/s]Extractor Estimating: 189it [01:48,  1.71it/s]Extractor Estimating: 190it [01:49,  1.75it/s]Extractor Estimating: 191it [01:49,  1.74it/s]Extractor Estimating: 192it [01:50,  1.75it/s]Extractor Estimating: 193it [01:51,  1.73it/s]Extractor Estimating: 194it [01:51,  1.65it/s]Extractor Estimating: 195it [01:52,  1.68it/s]Extractor Estimating: 196it [01:52,  1.71it/s]Extractor Estimating: 197it [01:53,  1.70it/s]Extractor Estimating: 198it [01:54,  1.71it/s]Extractor Estimating: 199it [01:54,  1.72it/s]Extractor Estimating: 200it [01:55,  1.70it/s]Extractor Estimating: 201it [01:55,  1.74it/s]Extractor Estimating: 202it [01:56,  1.77it/s]Extractor Estimating: 203it [01:56,  1.82it/s]Extractor Estimating: 204it [01:57,  1.80it/s]Extractor Estimating: 205it [01:58,  1.75it/s]Extractor Estimating: 206it [01:58,  1.74it/s]Extractor Estimating: 207it [01:59,  1.80it/s]Extractor Estimating: 208it [01:59,  1.81it/s]Extractor Estimating: 209it [02:00,  1.83it/s]Extractor Estimating: 210it [02:00,  1.87it/s]Extractor Estimating: 211it [02:01,  1.84it/s]Extractor Estimating: 212it [02:01,  1.78it/s]Extractor Estimating: 213it [02:02,  1.77it/s]Extractor Estimating: 214it [02:03,  1.71it/s]Extractor Estimating: 215it [02:03,  1.74it/s]Extractor Estimating: 216it [02:04,  1.79it/s]Extractor Estimating: 217it [02:04,  1.83it/s]Extractor Estimating: 218it [02:05,  1.82it/s]Extractor Estimating: 219it [02:05,  1.93it/s]Extractor Estimating: 220it [02:06,  1.93it/s]Extractor Estimating: 221it [02:06,  1.91it/s]Extractor Estimating: 222it [02:07,  1.84it/s]Extractor Estimating: 223it [02:07,  1.84it/s]Extractor Estimating: 224it [02:08,  1.89it/s]Extractor Estimating: 225it [02:08,  1.86it/s]Extractor Estimating: 226it [02:09,  1.78it/s]Extractor Estimating: 227it [02:10,  1.75it/s]Extractor Estimating: 228it [02:10,  1.74it/s]Extractor Estimating: 229it [02:11,  1.71it/s]Extractor Estimating: 230it [02:11,  1.66it/s]Extractor Estimating: 231it [02:12,  1.71it/s]Extractor Estimating: 232it [02:13,  1.74it/s]Extractor Estimating: 233it [02:13,  1.74it/s]Extractor Estimating: 234it [02:14,  1.74it/s]Extractor Estimating: 235it [02:14,  1.70it/s]Extractor Estimating: 236it [02:15,  1.71it/s]Extractor Estimating: 237it [02:15,  1.72it/s]Extractor Estimating: 238it [02:16,  1.65it/s]Extractor Estimating: 239it [02:17,  1.59it/s]Extractor Estimating: 240it [02:18,  1.52it/s]Extractor Estimating: 241it [02:18,  1.50it/s]Extractor Estimating: 242it [02:19,  1.59it/s]Extractor Estimating: 243it [02:19,  1.64it/s]Extractor Estimating: 244it [02:20,  1.65it/s]Extractor Estimating: 245it [02:21,  1.69it/s]Extractor Estimating: 246it [02:21,  1.74it/s]Extractor Estimating: 247it [02:22,  1.70it/s]Extractor Estimating: 248it [02:22,  1.67it/s]Extractor Estimating: 249it [02:23,  1.68it/s]Extractor Estimating: 250it [02:23,  1.66it/s]Extractor Estimating: 250it [02:23,  1.74it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:51,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:51,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:51,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:51,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:51,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:50:52,467 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:50:52,469 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:50:52,806 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:50:53,947 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:50:53,948 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:50:55,890 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:50:55,891 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:50:56,194 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:50:56,441 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:50:56,441 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 13:15:00,505 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 13:15:00,782 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5000 mean pseudo reward: 0.9619547365780979
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl'}
train vocab size: 15932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16032, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.935, loss:389.8235
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.939, loss:336.0813
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.938, loss:311.6569
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.933, loss:309.2346
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.937, loss:281.0532
>> valid entity prec:0.4563, rec:0.4615, f1:0.4589
>> valid relation prec:0.2312, rec:0.0752, f1:0.1135
>> valid relation with NER prec:0.2312, rec:0.0752, f1:0.1135
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.363, loss:304.5774
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.950, loss:269.1584
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.932, loss:288.7334
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.932, loss:286.7745
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.943, loss:298.9171
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4633, rec:0.3753, f1:0.4147
>> valid relation prec:0.2636, rec:0.0656, f1:0.1051
>> valid relation with NER prec:0.2636, rec:0.0656, f1:0.1051
g_step 1100, step 55, avg_time 2.347, loss:275.4281
g_step 1200, step 155, avg_time 0.941, loss:274.6530
g_step 1300, step 46, avg_time 0.935, loss:272.4992
g_step 1400, step 146, avg_time 0.935, loss:252.6954
g_step 1500, step 37, avg_time 0.940, loss:260.3106
>> valid entity prec:0.4880, rec:0.3996, f1:0.4394
>> valid relation prec:0.2609, rec:0.0676, f1:0.1073
>> valid relation with NER prec:0.2609, rec:0.0676, f1:0.1073
g_step 1600, step 137, avg_time 2.354, loss:243.9641
g_step 1700, step 28, avg_time 0.947, loss:233.8268
g_step 1800, step 128, avg_time 0.946, loss:224.3037
g_step 1900, step 19, avg_time 0.939, loss:233.3962
g_step 2000, step 119, avg_time 0.944, loss:231.9182
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4755, rec:0.4051, f1:0.4375
>> valid relation prec:0.2589, rec:0.0922, f1:0.1360
>> valid relation with NER prec:0.2589, rec:0.0922, f1:0.1360
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 10, avg_time 2.359, loss:212.0074
g_step 2200, step 110, avg_time 0.933, loss:207.3458
g_step 2300, step 1, avg_time 0.946, loss:210.0870
g_step 2400, step 101, avg_time 0.946, loss:183.8038
g_step 2500, step 201, avg_time 0.933, loss:209.4920
>> valid entity prec:0.4656, rec:0.3963, f1:0.4282
>> valid relation prec:0.2084, rec:0.0830, f1:0.1188
>> valid relation with NER prec:0.2084, rec:0.0830, f1:0.1188
g_step 2600, step 92, avg_time 2.352, loss:191.9050
g_step 2700, step 192, avg_time 0.940, loss:186.4486
g_step 2800, step 83, avg_time 0.929, loss:184.6664
g_step 2900, step 183, avg_time 0.927, loss:178.9692
g_step 3000, step 74, avg_time 0.938, loss:173.9263
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4902, rec:0.3508, f1:0.4089
>> valid relation prec:0.1967, rec:0.0650, f1:0.0977
>> valid relation with NER prec:0.1967, rec:0.0650, f1:0.0977
g_step 3100, step 174, avg_time 2.334, loss:184.6124
g_step 3200, step 65, avg_time 0.930, loss:161.4824
g_step 3300, step 165, avg_time 0.940, loss:171.3892
g_step 3400, step 56, avg_time 0.931, loss:166.6717
g_step 3500, step 156, avg_time 0.932, loss:156.5990
>> valid entity prec:0.4762, rec:0.4267, f1:0.4501
>> valid relation prec:0.1768, rec:0.0706, f1:0.1009
>> valid relation with NER prec:0.1768, rec:0.0706, f1:0.1009
g_step 3600, step 47, avg_time 2.332, loss:159.6148
g_step 3700, step 147, avg_time 0.938, loss:145.3225
g_step 3800, step 38, avg_time 0.929, loss:153.9535
g_step 3900, step 138, avg_time 0.914, loss:150.2269
g_step 4000, step 29, avg_time 0.926, loss:156.7346
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4528, rec:0.4538, f1:0.4533
>> valid relation prec:0.1740, rec:0.0942, f1:0.1222
>> valid relation with NER prec:0.1740, rec:0.0942, f1:0.1222
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 129, avg_time 2.311, loss:144.2693
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 13:15:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 13:15:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_13-15-00_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 13:15:01 - WARNING - datasets.builder -   Using custom data configuration default-7f6651344c4bae4e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7f6651344c4bae4e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.93 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 13:15:04,331 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:15:04,332 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 13:15:04,333 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:15:04,334 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 13:15:04,410 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,464 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,464 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,465 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:15:04,465 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 13:15:04,907 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 13:15:08,114 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 13:15:08,114 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7f6651344c4bae4e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.80ba/s] 40%|████      | 2/5 [00:00<00:01,  2.76ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.55ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.06ba/s]100%|██████████| 5/5 [00:01<00:00,  4.43ba/s]100%|██████████| 5/5 [00:01<00:00,  3.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.78ba/s] 40%|████      | 2/5 [00:00<00:00,  3.75ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.22ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.29ba/s]100%|██████████| 5/5 [00:01<00:00,  4.18ba/s]100%|██████████| 5/5 [00:01<00:00,  3.87ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.30ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.13ba/s]100%|██████████| 5/5 [00:00<00:00,  9.08ba/s]100%|██████████| 5/5 [00:00<00:00,  7.90ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.34ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.14ba/s]100%|██████████| 5/5 [00:00<00:00,  9.79ba/s]100%|██████████| 5/5 [00:00<00:00,  8.32ba/s]
[INFO|trainer.py:414] 2023-08-29 13:15:13,947 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 13:15:14,209 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 13:15:14,209 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-29 13:15:14,209 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 13:15:14,209 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 13:15:14,209 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 13:15:14,209 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 13:15:14,209 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:01<12:33,  1.94s/it]  1%|          | 2/390 [00:02<06:15,  1.03it/s]  1%|          | 3/390 [00:02<04:14,  1.52it/s]  1%|          | 4/390 [00:02<03:18,  1.95it/s]  1%|▏         | 5/390 [00:03<02:46,  2.31it/s]  2%|▏         | 6/390 [00:03<02:27,  2.60it/s]  2%|▏         | 7/390 [00:03<02:15,  2.83it/s]  2%|▏         | 8/390 [00:03<02:07,  3.00it/s]  2%|▏         | 9/390 [00:04<02:01,  3.13it/s]  3%|▎         | 10/390 [00:04<01:57,  3.22it/s]  3%|▎         | 11/390 [00:04<01:58,  3.20it/s]  3%|▎         | 12/390 [00:05<01:55,  3.27it/s]  3%|▎         | 13/390 [00:05<01:53,  3.32it/s]  4%|▎         | 14/390 [00:05<01:51,  3.36it/s]  4%|▍         | 15/390 [00:06<01:50,  3.39it/s]  4%|▍         | 16/390 [00:06<01:49,  3.40it/s]  4%|▍         | 17/390 [00:06<01:49,  3.42it/s]  5%|▍         | 18/390 [00:06<01:48,  3.43it/s]  5%|▍         | 19/390 [00:07<01:48,  3.43it/s]  5%|▌         | 20/390 [00:07<01:47,  3.44it/s]  5%|▌         | 21/390 [00:07<01:47,  3.44it/s]  6%|▌         | 22/390 [00:08<01:46,  3.44it/s]  6%|▌         | 23/390 [00:08<01:46,  3.45it/s]  6%|▌         | 24/390 [00:08<01:46,  3.44it/s]  6%|▋         | 25/390 [00:08<01:45,  3.45it/s]  7%|▋         | 26/390 [00:09<01:45,  3.45it/s]  7%|▋         | 27/390 [00:09<01:45,  3.45it/s]  7%|▋         | 28/390 [00:09<01:45,  3.44it/s]  7%|▋         | 29/390 [00:10<01:48,  3.33it/s]  8%|▊         | 30/390 [00:10<01:46,  3.37it/s]  8%|▊         | 31/390 [00:10<01:45,  3.39it/s]  8%|▊         | 32/390 [00:10<01:45,  3.41it/s]  8%|▊         | 33/390 [00:11<01:44,  3.42it/s]  9%|▊         | 34/390 [00:11<01:43,  3.43it/s]  9%|▉         | 35/390 [00:11<01:43,  3.43it/s]  9%|▉         | 36/390 [00:12<01:43,  3.43it/s]  9%|▉         | 37/390 [00:12<01:42,  3.44it/s] 10%|▉         | 38/390 [00:12<01:42,  3.44it/s] 10%|█         | 39/390 [00:13<01:41,  3.44it/s] 10%|█         | 40/390 [00:13<01:41,  3.44it/s] 11%|█         | 41/390 [00:13<01:41,  3.44it/s] 11%|█         | 42/390 [00:13<01:41,  3.44it/s] 11%|█         | 43/390 [00:14<01:40,  3.45it/s] 11%|█▏        | 44/390 [00:14<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:14<01:40,  3.45it/s] 12%|█▏        | 46/390 [00:15<01:42,  3.35it/s] 12%|█▏        | 47/390 [00:15<01:41,  3.38it/s] 12%|█▏        | 48/390 [00:15<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:15<01:39,  3.41it/s] 13%|█▎        | 50/390 [00:16<01:39,  3.42it/s] 13%|█▎        | 51/390 [00:16<01:38,  3.43it/s] 13%|█▎        | 52/390 [00:16<01:38,  3.43it/s] 14%|█▎        | 53/390 [00:17<01:38,  3.44it/s] 14%|█▍        | 54/390 [00:17<01:37,  3.44it/s] 14%|█▍        | 55/390 [00:17<01:37,  3.44it/s] 14%|█▍        | 56/390 [00:17<01:37,  3.44it/s] 15%|█▍        | 57/390 [00:18<01:36,  3.44it/s] 15%|█▍        | 58/390 [00:18<01:36,  3.44it/s] 15%|█▌        | 59/390 [00:18<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:19<01:35,  3.44it/s] 16%|█▌        | 61/390 [00:19<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:19<01:34,  3.46it/s] 16%|█▌        | 63/390 [00:20<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:20<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:20<01:35,  3.42it/s] 17%|█▋        | 66/390 [00:20<01:34,  3.44it/s] 17%|█▋        | 67/390 [00:21<01:33,  3.46it/s] 17%|█▋        | 68/390 [00:21<01:32,  3.47it/s] 18%|█▊        | 69/390 [00:21<01:32,  3.48it/s] 18%|█▊        | 70/390 [00:22<01:31,  3.48it/s] 18%|█▊        | 71/390 [00:22<01:31,  3.48it/s] 18%|█▊        | 72/390 [00:22<01:31,  3.48it/s] 19%|█▊        | 73/390 [00:22<01:30,  3.49it/s] 19%|█▉        | 74/390 [00:23<01:30,  3.49it/s] 19%|█▉        | 75/390 [00:23<01:30,  3.49it/s] 19%|█▉        | 76/390 [00:23<01:29,  3.49it/s] 20%|█▉        | 77/390 [00:24<01:29,  3.50it/s] 20%|██        | 78/390 [00:24<01:29,  3.50it/s][INFO|trainer.py:2140] 2023-08-29 13:15:38,613 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:15:38,613 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:15:38,613 >>   Batch size = 8

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.87it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.88it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.96it/s][A
  4%|▍         | 23/577 [00:00<00:11, 47.10it/s][A
  5%|▍         | 28/577 [00:00<00:11, 46.43it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.58it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.59it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.15it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.05it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.16it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.26it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.37it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.41it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.52it/s][A
 14%|█▎        | 78/577 [00:01<00:10, 45.54it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 45.48it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 45.31it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.17it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.28it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.31it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.42it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.43it/s][A
 20%|██        | 118/577 [00:02<00:10, 45.46it/s][A
 21%|██▏       | 123/577 [00:02<00:09, 45.46it/s][A
 22%|██▏       | 128/577 [00:02<00:10, 41.98it/s][A
 23%|██▎       | 133/577 [00:02<00:10, 43.09it/s][A
 24%|██▍       | 138/577 [00:03<00:10, 43.85it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 44.25it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 44.60it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 44.88it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.09it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.19it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 44.98it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.00it/s][A
 31%|███       | 178/577 [00:03<00:08, 45.21it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.34it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.38it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.30it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.32it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.31it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.28it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.22it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.23it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 45.37it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.42it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.47it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.44it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.41it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.35it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 45.34it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 45.24it/s][A
 46%|████▌     | 263/577 [00:05<00:06, 44.96it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 45.12it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 45.27it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 45.35it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 45.44it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 45.40it/s][A
 51%|█████     | 293/577 [00:06<00:06, 45.24it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.02it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.19it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 45.21it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.33it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 45.39it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 45.43it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.47it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.51it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.36it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.27it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.17it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.24it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 45.33it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 45.46it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 45.44it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 45.47it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.42it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.38it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.23it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.16it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.16it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.28it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.33it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.44it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 45.40it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 45.40it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 45.27it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.16it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.14it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 45.18it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 45.29it/s][A
 79%|███████▊  | 453/577 [00:09<00:02, 45.36it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 45.46it/s][A
 80%|████████  | 463/577 [00:10<00:02, 45.45it/s][A
 81%|████████  | 468/577 [00:10<00:02, 45.46it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.13it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.28it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.08it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.21it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 45.35it/s][A
 86%|████████▋ | 498/577 [00:10<00:01, 44.65it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 45.02it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 45.12it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 45.13it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 44.91it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.11it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.09it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.14it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 45.22it/s][A
 94%|█████████▍| 543/577 [00:11<00:00, 45.23it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 45.40it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 45.44it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 45.49it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 45.40it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 45.24it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 45.25it/s][A
                                                 [A                                                
100%|██████████| 577/577 [00:12<00:00, 45.25it/s][A 20%|██        | 78/390 [00:37<01:29,  3.50it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:15:51,493 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-29 13:15:51,657 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:15:54,785 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:15:54,922 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:15:54,995 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:47<37:48,  7.29s/it] 21%|██        | 80/390 [00:48<26:50,  5.20s/it] 21%|██        | 81/390 [00:48<19:10,  3.72s/it] 21%|██        | 82/390 [00:48<13:49,  2.69s/it] 21%|██▏       | 83/390 [00:49<10:05,  1.97s/it] 22%|██▏       | 84/390 [00:49<07:29,  1.47s/it] 22%|██▏       | 85/390 [00:49<05:40,  1.11s/it] 22%|██▏       | 86/390 [00:50<04:23,  1.15it/s] 22%|██▏       | 87/390 [00:50<03:30,  1.44it/s] 23%|██▎       | 88/390 [00:50<02:53,  1.74it/s] 23%|██▎       | 89/390 [00:50<02:27,  2.05it/s] 23%|██▎       | 90/390 [00:51<02:08,  2.33it/s] 23%|██▎       | 91/390 [00:51<01:58,  2.52it/s] 24%|██▎       | 92/390 [00:51<01:48,  2.74it/s] 24%|██▍       | 93/390 [00:52<01:41,  2.92it/s] 24%|██▍       | 94/390 [00:52<01:36,  3.06it/s] 24%|██▍       | 95/390 [00:52<01:33,  3.17it/s] 25%|██▍       | 96/390 [00:52<01:30,  3.24it/s] 25%|██▍       | 97/390 [00:53<01:28,  3.30it/s] 25%|██▌       | 98/390 [00:53<01:27,  3.35it/s] 25%|██▌       | 99/390 [00:53<01:26,  3.38it/s] 26%|██▌       | 100/390 [00:54<01:25,  3.40it/s] 26%|██▌       | 101/390 [00:54<01:24,  3.41it/s] 26%|██▌       | 102/390 [00:54<01:28,  3.25it/s] 26%|██▋       | 103/390 [00:55<01:26,  3.31it/s] 27%|██▋       | 104/390 [00:55<01:25,  3.35it/s] 27%|██▋       | 105/390 [00:55<01:23,  3.39it/s] 27%|██▋       | 106/390 [00:55<01:22,  3.43it/s] 27%|██▋       | 107/390 [00:56<01:22,  3.45it/s] 28%|██▊       | 108/390 [00:56<01:21,  3.46it/s] 28%|██▊       | 109/390 [00:56<01:20,  3.48it/s] 28%|██▊       | 110/390 [00:57<01:20,  3.48it/s] 28%|██▊       | 111/390 [00:57<01:19,  3.49it/s] 29%|██▊       | 112/390 [00:57<01:19,  3.49it/s] 29%|██▉       | 113/390 [00:57<01:24,  3.30it/s] 29%|██▉       | 114/390 [00:58<01:22,  3.36it/s] 29%|██▉       | 115/390 [00:58<01:20,  3.40it/s] 30%|██▉       | 116/390 [00:58<01:19,  3.43it/s] 30%|███       | 117/390 [00:59<01:19,  3.45it/s] 30%|███       | 118/390 [00:59<01:18,  3.46it/s] 31%|███       | 119/390 [00:59<01:18,  3.47it/s] 31%|███       | 120/390 [00:59<01:17,  3.48it/s] 31%|███       | 121/390 [01:00<01:17,  3.49it/s] 31%|███▏      | 122/390 [01:00<01:16,  3.49it/s] 32%|███▏      | 123/390 [01:00<01:16,  3.49it/s] 32%|███▏      | 124/390 [01:01<01:19,  3.35it/s] 32%|███▏      | 125/390 [01:01<01:17,  3.40it/s] 32%|███▏      | 126/390 [01:01<01:17,  3.43it/s] 33%|███▎      | 127/390 [01:01<01:16,  3.45it/s] 33%|███▎      | 128/390 [01:02<01:15,  3.46it/s] 33%|███▎      | 129/390 [01:02<01:15,  3.47it/s] 33%|███▎      | 130/390 [01:02<01:14,  3.48it/s] 34%|███▎      | 131/390 [01:03<01:14,  3.49it/s] 34%|███▍      | 132/390 [01:03<01:13,  3.49it/s] 34%|███▍      | 133/390 [01:03<01:13,  3.49it/s] 34%|███▍      | 134/390 [01:03<01:13,  3.50it/s] 35%|███▍      | 135/390 [01:04<01:18,  3.26it/s] 35%|███▍      | 136/390 [01:04<01:16,  3.33it/s] 35%|███▌      | 137/390 [01:04<01:14,  3.38it/s] 35%|███▌      | 138/390 [01:05<01:13,  3.41it/s] 36%|███▌      | 139/390 [01:05<01:13,  3.44it/s] 36%|███▌      | 140/390 [01:05<01:12,  3.46it/s] 36%|███▌      | 141/390 [01:06<01:11,  3.47it/s] 36%|███▋      | 142/390 [01:06<01:11,  3.48it/s] 37%|███▋      | 143/390 [01:06<01:10,  3.49it/s] 37%|███▋      | 144/390 [01:06<01:10,  3.49it/s] 37%|███▋      | 145/390 [01:07<01:10,  3.49it/s] 37%|███▋      | 146/390 [01:07<01:12,  3.35it/s] 38%|███▊      | 147/390 [01:07<01:11,  3.39it/s] 38%|███▊      | 148/390 [01:08<01:10,  3.42it/s] 38%|███▊      | 149/390 [01:08<01:10,  3.39it/s] 38%|███▊      | 150/390 [01:08<01:10,  3.43it/s] 39%|███▊      | 151/390 [01:08<01:09,  3.45it/s] 39%|███▉      | 152/390 [01:09<01:08,  3.46it/s] 39%|███▉      | 153/390 [01:09<01:08,  3.47it/s] 39%|███▉      | 154/390 [01:09<01:07,  3.48it/s] 40%|███▉      | 155/390 [01:10<01:07,  3.48it/s] 40%|████      | 156/390 [01:10<01:07,  3.49it/s][INFO|trainer.py:2140] 2023-08-29 13:16:24,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:16:24,650 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:16:24,651 >>   Batch size = 8
{'eval_loss': 1.1234697103500366, 'eval_runtime': 12.7585, 'eval_samples_per_second': 361.249, 'eval_steps_per_second': 45.225, 'epoch': 0.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.43it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.01it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.12it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.47it/s][A
  5%|▍         | 27/577 [00:00<00:11, 45.97it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.72it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.09it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.22it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.31it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.51it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.56it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.50it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.44it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.38it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.24it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.18it/s][A
 15%|█▌        | 87/577 [00:01<00:10, 45.21it/s][A
 16%|█▌        | 92/577 [00:02<00:10, 45.37it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 45.40it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 45.51it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 45.43it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 45.46it/s][A
 20%|██        | 117/577 [00:02<00:10, 45.36it/s][A
 21%|██        | 122/577 [00:02<00:10, 45.29it/s][A
 22%|██▏       | 127/577 [00:02<00:09, 45.16it/s][A
 23%|██▎       | 132/577 [00:02<00:09, 45.10it/s][A
 24%|██▎       | 137/577 [00:03<00:09, 45.25it/s][A
 25%|██▍       | 142/577 [00:03<00:09, 45.39it/s][A
 25%|██▌       | 147/577 [00:03<00:09, 45.44it/s][A
 26%|██▋       | 152/577 [00:03<00:09, 45.51it/s][A
 27%|██▋       | 157/577 [00:03<00:09, 45.46it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 45.41it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 45.25it/s][A
 30%|██▉       | 172/577 [00:03<00:08, 45.20it/s][A
 31%|███       | 177/577 [00:03<00:08, 45.24it/s][A
 32%|███▏      | 182/577 [00:03<00:08, 45.26it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 45.40it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 45.39it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.54it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.49it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.45it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 45.23it/s][A
 38%|███▊      | 217/577 [00:04<00:07, 45.20it/s][A
 38%|███▊      | 222/577 [00:04<00:07, 45.18it/s][A
 39%|███▉      | 227/577 [00:04<00:07, 45.37it/s][A
 40%|████      | 232/577 [00:05<00:07, 45.39it/s][A
 41%|████      | 237/577 [00:05<00:07, 45.48it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.47it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.53it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.38it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.20it/s][A
 45%|████▌     | 262/577 [00:05<00:06, 45.20it/s][A
 46%|████▋     | 267/577 [00:05<00:07, 42.96it/s][A
 47%|████▋     | 272/577 [00:05<00:06, 43.79it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 44.37it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 44.74it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 44.96it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.17it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.25it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.18it/s][A
 53%|█████▎    | 307/577 [00:06<00:05, 45.00it/s][A
 54%|█████▍    | 312/577 [00:06<00:05, 44.88it/s][A
 55%|█████▍    | 317/577 [00:06<00:05, 45.09it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.20it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.32it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.40it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.47it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.50it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.38it/s][A
 61%|██████    | 352/577 [00:07<00:04, 45.18it/s][A
 62%|██████▏   | 357/577 [00:07<00:04, 45.04it/s][A
 63%|██████▎   | 362/577 [00:07<00:04, 43.61it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 44.30it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 44.69it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 44.96it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.07it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.19it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.05it/s][A
 69%|██████▉   | 397/577 [00:08<00:04, 44.88it/s][A
 70%|██████▉   | 402/577 [00:08<00:03, 44.76it/s][A
 71%|███████   | 407/577 [00:08<00:03, 44.85it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.12it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.21it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.39it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.40it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.52it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.45it/s][A
 77%|███████▋  | 442/577 [00:09<00:02, 45.26it/s][A
 77%|███████▋  | 447/577 [00:09<00:02, 45.06it/s][A
 78%|███████▊  | 452/577 [00:09<00:02, 45.05it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.19it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.28it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.42it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.47it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.50it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.51it/s][A
 84%|████████▍ | 487/577 [00:10<00:01, 45.33it/s][A
 85%|████████▌ | 492/577 [00:10<00:01, 45.15it/s][A
 86%|████████▌ | 497/577 [00:10<00:01, 44.28it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.67it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.91it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 45.15it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 42.79it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 43.69it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 44.34it/s][A
 92%|█████████▏| 532/577 [00:11<00:01, 44.59it/s][A
 93%|█████████▎| 537/577 [00:11<00:00, 44.65it/s][A
 94%|█████████▍| 542/577 [00:11<00:00, 44.79it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.08it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.10it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.11it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.12it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.27it/s][A
100%|██████████| 577/577 [00:12<00:00, 45.41it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:12<00:00, 45.41it/s][A 40%|████      | 156/390 [01:23<01:07,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:16:37,549 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 13:16:37,719 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:16:41,405 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:16:41,529 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:16:41,575 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:34<29:00,  7.47s/it] 41%|████      | 158/390 [01:34<20:36,  5.33s/it] 41%|████      | 159/390 [01:35<14:41,  3.82s/it] 41%|████      | 160/390 [01:35<10:34,  2.76s/it] 41%|████▏     | 161/390 [01:35<07:42,  2.02s/it] 42%|████▏     | 162/390 [01:36<05:41,  1.50s/it] 42%|████▏     | 163/390 [01:36<04:18,  1.14s/it] 42%|████▏     | 164/390 [01:36<03:19,  1.13it/s] 42%|████▏     | 165/390 [01:36<02:38,  1.42it/s] 43%|████▎     | 166/390 [01:37<02:10,  1.72it/s] 43%|████▎     | 167/390 [01:37<01:50,  2.03it/s] 43%|████▎     | 168/390 [01:37<01:36,  2.31it/s] 43%|████▎     | 169/390 [01:38<01:28,  2.50it/s] 44%|████▎     | 170/390 [01:38<01:20,  2.73it/s] 44%|████▍     | 171/390 [01:38<01:15,  2.91it/s] 44%|████▍     | 172/390 [01:39<01:11,  3.05it/s] 44%|████▍     | 173/390 [01:39<01:08,  3.16it/s] 45%|████▍     | 174/390 [01:39<01:06,  3.24it/s] 45%|████▍     | 175/390 [01:39<01:05,  3.30it/s] 45%|████▌     | 176/390 [01:40<01:04,  3.34it/s] 45%|████▌     | 177/390 [01:40<01:03,  3.37it/s] 46%|████▌     | 178/390 [01:40<01:02,  3.39it/s] 46%|████▌     | 179/390 [01:41<01:01,  3.41it/s] 46%|████▌     | 180/390 [01:41<01:02,  3.34it/s] 46%|████▋     | 181/390 [01:41<01:01,  3.37it/s] 47%|████▋     | 182/390 [01:41<01:01,  3.39it/s] 47%|████▋     | 183/390 [01:42<01:00,  3.41it/s] 47%|████▋     | 184/390 [01:42<01:00,  3.42it/s] 47%|████▋     | 185/390 [01:42<00:59,  3.43it/s] 48%|████▊     | 186/390 [01:43<00:59,  3.43it/s] 48%|████▊     | 187/390 [01:43<00:59,  3.44it/s] 48%|████▊     | 188/390 [01:43<00:58,  3.44it/s] 48%|████▊     | 189/390 [01:44<00:58,  3.44it/s] 49%|████▊     | 190/390 [01:44<00:58,  3.44it/s] 49%|████▉     | 191/390 [01:44<00:59,  3.34it/s] 49%|████▉     | 192/390 [01:44<00:58,  3.37it/s] 49%|████▉     | 193/390 [01:45<00:58,  3.39it/s] 50%|████▉     | 194/390 [01:45<00:57,  3.41it/s] 50%|█████     | 195/390 [01:45<00:57,  3.42it/s] 50%|█████     | 196/390 [01:46<00:56,  3.42it/s] 51%|█████     | 197/390 [01:46<00:56,  3.43it/s] 51%|█████     | 198/390 [01:46<00:55,  3.43it/s] 51%|█████     | 199/390 [01:46<00:57,  3.33it/s] 51%|█████▏    | 200/390 [01:47<00:56,  3.36it/s] 52%|█████▏    | 201/390 [01:47<01:11,  2.64it/s] 52%|█████▏    | 202/390 [01:48<01:08,  2.75it/s] 52%|█████▏    | 203/390 [01:48<01:03,  2.93it/s] 52%|█████▏    | 204/390 [01:48<01:00,  3.07it/s] 53%|█████▎    | 205/390 [01:49<00:58,  3.17it/s] 53%|█████▎    | 206/390 [01:49<00:56,  3.25it/s] 53%|█████▎    | 207/390 [01:49<00:55,  3.30it/s] 53%|█████▎    | 208/390 [01:49<00:54,  3.34it/s] 54%|█████▎    | 209/390 [01:50<00:53,  3.37it/s] 54%|█████▍    | 210/390 [01:50<00:53,  3.39it/s] 54%|█████▍    | 211/390 [01:50<00:52,  3.41it/s] 54%|█████▍    | 212/390 [01:51<00:52,  3.42it/s] 55%|█████▍    | 213/390 [01:51<00:52,  3.38it/s] 55%|█████▍    | 214/390 [01:51<00:51,  3.40it/s] 55%|█████▌    | 215/390 [01:51<00:51,  3.41it/s] 55%|█████▌    | 216/390 [01:52<00:50,  3.42it/s] 56%|█████▌    | 217/390 [01:52<00:50,  3.43it/s] 56%|█████▌    | 218/390 [01:52<00:50,  3.43it/s] 56%|█████▌    | 219/390 [01:53<00:49,  3.43it/s] 56%|█████▋    | 220/390 [01:53<00:49,  3.44it/s] 57%|█████▋    | 221/390 [01:53<00:49,  3.44it/s] 57%|█████▋    | 222/390 [01:53<00:48,  3.44it/s] 57%|█████▋    | 223/390 [01:54<00:48,  3.44it/s] 57%|█████▋    | 224/390 [01:54<00:48,  3.44it/s] 58%|█████▊    | 225/390 [01:54<00:47,  3.44it/s] 58%|█████▊    | 226/390 [01:55<00:47,  3.43it/s] 58%|█████▊    | 227/390 [01:55<00:47,  3.43it/s] 58%|█████▊    | 228/390 [01:55<00:47,  3.43it/s] 59%|█████▊    | 229/390 [01:56<00:46,  3.43it/s] 59%|█████▉    | 230/390 [01:56<00:46,  3.43it/s] 59%|█████▉    | 231/390 [01:56<00:46,  3.43it/s] 59%|█████▉    | 232/390 [01:56<00:46,  3.43it/s] 60%|█████▉    | 233/390 [01:57<00:45,  3.43it/s] 60%|██████    | 234/390 [01:57<00:48,  3.24it/s][INFO|trainer.py:2140] 2023-08-29 13:17:11,802 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:17:11,802 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:17:11,802 >>   Batch size = 8
{'eval_loss': 1.1337954998016357, 'eval_runtime': 12.7859, 'eval_samples_per_second': 360.474, 'eval_steps_per_second': 45.128, 'epoch': 1.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.63it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.47it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.92it/s][A
  4%|▍         | 23/577 [00:00<00:11, 47.26it/s][A
  5%|▍         | 28/577 [00:00<00:11, 46.30it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.65it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.42it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.18it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.32it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.51it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.63it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.73it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.71it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.45it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 45.13it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 44.91it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 44.90it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.09it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.28it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.36it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.41it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.52it/s][A
 20%|██        | 118/577 [00:02<00:10, 45.53it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 45.37it/s][A
 22%|██▏       | 128/577 [00:02<00:10, 41.54it/s][A
 23%|██▎       | 133/577 [00:02<00:10, 42.80it/s][A
 24%|██▍       | 138/577 [00:03<00:10, 43.65it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 44.21it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 44.61it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 44.88it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.13it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 45.24it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 44.95it/s][A
 30%|██▉       | 173/577 [00:03<00:09, 44.85it/s][A
 31%|███       | 178/577 [00:03<00:08, 45.01it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.14it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.36it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.46it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.59it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.55it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.43it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.19it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.14it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 45.06it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.25it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.33it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.43it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.46it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.52it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 45.42it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 45.26it/s][A
 46%|████▌     | 263/577 [00:05<00:07, 42.49it/s][A
 46%|████▋     | 268/577 [00:05<00:07, 43.52it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 44.11it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 44.57it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 44.84it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 45.06it/s][A
 51%|█████     | 293/577 [00:06<00:06, 45.19it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.25it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 44.85it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 44.87it/s][A
 54%|█████▍    | 313/577 [00:06<00:06, 41.52it/s][A
 55%|█████▌    | 318/577 [00:07<00:06, 42.73it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 43.62it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 44.28it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 44.76it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.07it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.12it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.05it/s][A
 61%|██████    | 353/577 [00:07<00:05, 44.79it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 44.72it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 44.94it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 45.11it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 45.27it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.35it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.46it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.48it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.42it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.20it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.07it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.13it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.21it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 45.29it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 45.30it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 45.45it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.49it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.41it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 45.24it/s][A
 78%|███████▊  | 448/577 [00:09<00:03, 42.09it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 43.15it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 43.88it/s][A
 80%|████████  | 463/577 [00:10<00:02, 44.30it/s][A
 81%|████████  | 468/577 [00:10<00:02, 44.71it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.02it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.15it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.33it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.04it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 44.85it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 45.01it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 45.13it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 45.31it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 45.34it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 45.40it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.46it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.36it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.05it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 44.83it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 44.93it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 45.11it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 45.27it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 45.23it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 45.36it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 45.43it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 45.33it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:12<00:00, 45.33it/s][A 60%|██████    | 234/390 [02:10<00:48,  3.24it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:17:24,864 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 13:17:25,043 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:17:28,421 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:17:28,582 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:17:28,666 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:22<19:49,  7.67s/it] 61%|██████    | 236/390 [02:22<14:01,  5.46s/it] 61%|██████    | 237/390 [02:23<09:58,  3.91s/it] 61%|██████    | 238/390 [02:23<07:09,  2.83s/it] 61%|██████▏   | 239/390 [02:23<05:11,  2.06s/it] 62%|██████▏   | 240/390 [02:23<03:49,  1.53s/it] 62%|██████▏   | 241/390 [02:24<02:52,  1.16s/it] 62%|██████▏   | 242/390 [02:24<02:13,  1.11it/s] 62%|██████▏   | 243/390 [02:24<01:45,  1.40it/s] 63%|██████▎   | 244/390 [02:25<01:25,  1.70it/s] 63%|██████▎   | 245/390 [02:25<01:12,  2.00it/s] 63%|██████▎   | 246/390 [02:25<01:02,  2.29it/s] 63%|██████▎   | 247/390 [02:25<00:57,  2.49it/s] 64%|██████▎   | 248/390 [02:26<00:52,  2.72it/s] 64%|██████▍   | 249/390 [02:26<00:48,  2.91it/s] 64%|██████▍   | 250/390 [02:26<00:45,  3.07it/s] 64%|██████▍   | 251/390 [02:27<00:43,  3.19it/s] 65%|██████▍   | 252/390 [02:27<00:42,  3.28it/s] 65%|██████▍   | 253/390 [02:27<00:40,  3.34it/s] 65%|██████▌   | 254/390 [02:27<00:40,  3.39it/s] 65%|██████▌   | 255/390 [02:28<00:39,  3.43it/s] 66%|██████▌   | 256/390 [02:28<00:38,  3.45it/s] 66%|██████▌   | 257/390 [02:28<00:38,  3.47it/s] 66%|██████▌   | 258/390 [02:29<00:39,  3.38it/s] 66%|██████▋   | 259/390 [02:29<00:38,  3.42it/s] 67%|██████▋   | 260/390 [02:29<00:37,  3.44it/s] 67%|██████▋   | 261/390 [02:29<00:37,  3.46it/s] 67%|██████▋   | 262/390 [02:30<00:36,  3.47it/s] 67%|██████▋   | 263/390 [02:30<00:36,  3.48it/s] 68%|██████▊   | 264/390 [02:30<00:36,  3.49it/s] 68%|██████▊   | 265/390 [02:31<00:35,  3.49it/s] 68%|██████▊   | 266/390 [02:31<00:35,  3.49it/s] 68%|██████▊   | 267/390 [02:31<00:35,  3.50it/s] 69%|██████▊   | 268/390 [02:31<00:34,  3.50it/s] 69%|██████▉   | 269/390 [02:32<00:35,  3.42it/s] 69%|██████▉   | 270/390 [02:32<00:34,  3.45it/s] 69%|██████▉   | 271/390 [02:32<00:34,  3.46it/s] 70%|██████▉   | 272/390 [02:33<00:33,  3.47it/s] 70%|███████   | 273/390 [02:33<00:33,  3.48it/s] 70%|███████   | 274/390 [02:33<00:33,  3.49it/s] 71%|███████   | 275/390 [02:33<00:32,  3.50it/s] 71%|███████   | 276/390 [02:34<00:32,  3.49it/s] 71%|███████   | 277/390 [02:34<00:32,  3.50it/s] 71%|███████▏  | 278/390 [02:34<00:32,  3.50it/s] 72%|███████▏  | 279/390 [02:35<00:31,  3.50it/s] 72%|███████▏  | 280/390 [02:35<00:32,  3.43it/s] 72%|███████▏  | 281/390 [02:35<00:31,  3.45it/s] 72%|███████▏  | 282/390 [02:36<00:31,  3.47it/s] 73%|███████▎  | 283/390 [02:36<00:30,  3.48it/s] 73%|███████▎  | 284/390 [02:36<00:30,  3.48it/s] 73%|███████▎  | 285/390 [02:36<00:30,  3.49it/s] 73%|███████▎  | 286/390 [02:37<00:29,  3.50it/s] 74%|███████▎  | 287/390 [02:37<00:29,  3.50it/s] 74%|███████▍  | 288/390 [02:37<00:29,  3.50it/s] 74%|███████▍  | 289/390 [02:38<00:28,  3.50it/s] 74%|███████▍  | 290/390 [02:38<00:28,  3.50it/s] 75%|███████▍  | 291/390 [02:38<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:38<00:28,  3.43it/s] 75%|███████▌  | 293/390 [02:39<00:28,  3.45it/s] 75%|███████▌  | 294/390 [02:39<00:27,  3.46it/s] 76%|███████▌  | 295/390 [02:39<00:27,  3.48it/s] 76%|███████▌  | 296/390 [02:40<00:27,  3.48it/s] 76%|███████▌  | 297/390 [02:40<00:26,  3.49it/s] 76%|███████▋  | 298/390 [02:40<00:26,  3.49it/s] 77%|███████▋  | 299/390 [02:40<00:26,  3.49it/s] 77%|███████▋  | 300/390 [02:41<00:25,  3.49it/s] 77%|███████▋  | 301/390 [02:41<00:25,  3.50it/s] 77%|███████▋  | 302/390 [02:41<00:25,  3.50it/s] 78%|███████▊  | 303/390 [02:42<00:24,  3.50it/s] 78%|███████▊  | 304/390 [02:42<00:24,  3.50it/s] 78%|███████▊  | 305/390 [02:42<00:24,  3.50it/s] 78%|███████▊  | 306/390 [02:42<00:24,  3.50it/s] 79%|███████▊  | 307/390 [02:43<00:23,  3.50it/s] 79%|███████▉  | 308/390 [02:43<00:23,  3.50it/s] 79%|███████▉  | 309/390 [02:43<00:23,  3.40it/s] 79%|███████▉  | 310/390 [02:44<00:23,  3.43it/s] 80%|███████▉  | 311/390 [02:44<00:22,  3.45it/s] 80%|████████  | 312/390 [02:44<00:22,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 13:17:58,876 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:17:58,876 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:17:58,876 >>   Batch size = 8
{'eval_loss': 1.1523497104644775, 'eval_runtime': 12.8336, 'eval_samples_per_second': 359.136, 'eval_steps_per_second': 44.96, 'epoch': 2.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:10, 56.46it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.16it/s][A
  3%|▎         | 17/577 [00:00<00:11, 47.48it/s][A
  4%|▍         | 22/577 [00:00<00:11, 46.65it/s][A
  5%|▍         | 27/577 [00:00<00:11, 46.19it/s][A
  6%|▌         | 32/577 [00:00<00:11, 45.81it/s][A
  6%|▋         | 37/577 [00:00<00:11, 45.68it/s][A
  7%|▋         | 42/577 [00:00<00:11, 45.43it/s][A
  8%|▊         | 47/577 [00:01<00:11, 45.37it/s][A
  9%|▉         | 52/577 [00:01<00:11, 45.43it/s][A
 10%|▉         | 57/577 [00:01<00:11, 45.48it/s][A
 11%|█         | 62/577 [00:01<00:11, 45.58it/s][A
 12%|█▏        | 67/577 [00:01<00:11, 45.59it/s][A
 12%|█▏        | 72/577 [00:01<00:11, 45.52it/s][A
 13%|█▎        | 77/577 [00:01<00:11, 45.40it/s][A
 14%|█▍        | 82/577 [00:01<00:10, 45.38it/s][A
 15%|█▌        | 87/577 [00:01<00:11, 43.22it/s][A
 16%|█▌        | 92/577 [00:02<00:11, 43.95it/s][A
 17%|█▋        | 97/577 [00:02<00:10, 44.38it/s][A
 18%|█▊        | 102/577 [00:02<00:10, 44.79it/s][A
 19%|█▊        | 107/577 [00:02<00:10, 43.05it/s][A
 19%|█▉        | 112/577 [00:02<00:10, 43.83it/s][A
 20%|██        | 117/577 [00:02<00:10, 44.37it/s][A
 21%|██        | 122/577 [00:02<00:10, 44.74it/s][A
 22%|██▏       | 127/577 [00:02<00:10, 44.63it/s][A
 23%|██▎       | 132/577 [00:03<00:17, 25.80it/s][A
 24%|██▎       | 137/577 [00:03<00:14, 29.80it/s][A
 25%|██▍       | 142/577 [00:03<00:13, 33.30it/s][A
 25%|██▌       | 147/577 [00:03<00:11, 36.29it/s][A
 26%|██▋       | 152/577 [00:03<00:10, 38.73it/s][A
 27%|██▋       | 157/577 [00:03<00:10, 40.60it/s][A
 28%|██▊       | 162/577 [00:03<00:09, 42.05it/s][A
 29%|██▉       | 167/577 [00:03<00:09, 43.13it/s][A
 30%|██▉       | 172/577 [00:04<00:09, 43.68it/s][A
 31%|███       | 177/577 [00:04<00:09, 43.77it/s][A
 32%|███▏      | 182/577 [00:04<00:08, 44.01it/s][A
 32%|███▏      | 187/577 [00:04<00:08, 44.40it/s][A
 33%|███▎      | 192/577 [00:04<00:08, 44.74it/s][A
 34%|███▍      | 197/577 [00:04<00:08, 45.03it/s][A
 35%|███▌      | 202/577 [00:04<00:08, 45.13it/s][A
 36%|███▌      | 207/577 [00:04<00:08, 45.33it/s][A
 37%|███▋      | 212/577 [00:04<00:08, 43.28it/s][A
 38%|███▊      | 217/577 [00:05<00:08, 44.04it/s][A
 38%|███▊      | 222/577 [00:05<00:08, 44.33it/s][A
 39%|███▉      | 227/577 [00:05<00:07, 44.53it/s][A
 40%|████      | 232/577 [00:05<00:07, 44.74it/s][A
 41%|████      | 237/577 [00:05<00:07, 44.93it/s][A
 42%|████▏     | 242/577 [00:05<00:07, 45.15it/s][A
 43%|████▎     | 247/577 [00:05<00:07, 45.24it/s][A
 44%|████▎     | 252/577 [00:05<00:07, 45.23it/s][A
 45%|████▍     | 257/577 [00:05<00:07, 45.26it/s][A
 45%|████▌     | 262/577 [00:06<00:06, 45.36it/s][A
 46%|████▋     | 267/577 [00:06<00:06, 45.35it/s][A
 47%|████▋     | 272/577 [00:06<00:06, 45.18it/s][A
 48%|████▊     | 277/577 [00:06<00:06, 45.20it/s][A
 49%|████▉     | 282/577 [00:06<00:06, 45.23it/s][A
 50%|████▉     | 287/577 [00:06<00:06, 45.28it/s][A
 51%|█████     | 292/577 [00:06<00:06, 45.37it/s][A
 51%|█████▏    | 297/577 [00:06<00:06, 45.42it/s][A
 52%|█████▏    | 302/577 [00:06<00:06, 45.41it/s][A
 53%|█████▎    | 307/577 [00:07<00:05, 45.41it/s][A
 54%|█████▍    | 312/577 [00:07<00:05, 45.40it/s][A
 55%|█████▍    | 317/577 [00:07<00:05, 45.36it/s][A
 56%|█████▌    | 322/577 [00:07<00:05, 45.25it/s][A
 57%|█████▋    | 327/577 [00:07<00:05, 45.02it/s][A
 58%|█████▊    | 332/577 [00:07<00:05, 45.15it/s][A
 58%|█████▊    | 337/577 [00:07<00:05, 45.25it/s][A
 59%|█████▉    | 342/577 [00:07<00:05, 45.38it/s][A
 60%|██████    | 347/577 [00:07<00:05, 45.44it/s][A
 61%|██████    | 352/577 [00:08<00:05, 44.83it/s][A
 62%|██████▏   | 357/577 [00:08<00:04, 45.03it/s][A
 63%|██████▎   | 362/577 [00:08<00:04, 45.13it/s][A
 64%|██████▎   | 367/577 [00:08<00:04, 45.11it/s][A
 64%|██████▍   | 372/577 [00:08<00:04, 45.15it/s][A
 65%|██████▌   | 377/577 [00:08<00:04, 45.11it/s][A
 66%|██████▌   | 382/577 [00:08<00:04, 45.25it/s][A
 67%|██████▋   | 387/577 [00:08<00:04, 45.30it/s][A
 68%|██████▊   | 392/577 [00:08<00:04, 45.38it/s][A
 69%|██████▉   | 397/577 [00:09<00:03, 45.44it/s][A
 70%|██████▉   | 402/577 [00:09<00:03, 45.48it/s][A
 71%|███████   | 407/577 [00:09<00:03, 45.40it/s][A
 71%|███████▏  | 412/577 [00:09<00:03, 45.35it/s][A
 72%|███████▏  | 417/577 [00:09<00:03, 45.21it/s][A
 73%|███████▎  | 422/577 [00:09<00:03, 45.24it/s][A
 74%|███████▍  | 427/577 [00:09<00:03, 45.25it/s][A
 75%|███████▍  | 432/577 [00:09<00:03, 45.34it/s][A
 76%|███████▌  | 437/577 [00:09<00:03, 45.41it/s][A
 77%|███████▋  | 442/577 [00:10<00:02, 45.44it/s][A
 77%|███████▋  | 447/577 [00:10<00:02, 45.47it/s][A
 78%|███████▊  | 452/577 [00:10<00:02, 45.45it/s][A
 79%|███████▉  | 457/577 [00:10<00:02, 45.18it/s][A
 80%|████████  | 462/577 [00:10<00:02, 45.22it/s][A
 81%|████████  | 467/577 [00:10<00:02, 45.18it/s][A
 82%|████████▏ | 472/577 [00:10<00:02, 45.11it/s][A
 83%|████████▎ | 477/577 [00:10<00:02, 45.17it/s][A
 84%|████████▎ | 482/577 [00:10<00:02, 45.42it/s][A
 84%|████████▍ | 487/577 [00:11<00:01, 45.29it/s][A
 85%|████████▌ | 492/577 [00:11<00:02, 42.29it/s][A
 86%|████████▌ | 497/577 [00:11<00:01, 43.31it/s][A
 87%|████████▋ | 502/577 [00:11<00:01, 44.04it/s][A
 88%|████████▊ | 507/577 [00:11<00:01, 44.47it/s][A
 89%|████████▊ | 512/577 [00:11<00:01, 44.69it/s][A
 90%|████████▉ | 517/577 [00:11<00:01, 44.83it/s][A
 90%|█████████ | 522/577 [00:11<00:01, 44.99it/s][A
 91%|█████████▏| 527/577 [00:11<00:01, 45.20it/s][A
 92%|█████████▏| 532/577 [00:12<00:01, 45.00it/s][A
 93%|█████████▎| 537/577 [00:12<00:00, 44.98it/s][A
 94%|█████████▍| 542/577 [00:12<00:00, 45.05it/s][A
 95%|█████████▍| 547/577 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 552/577 [00:12<00:00, 45.35it/s][A
 97%|█████████▋| 557/577 [00:12<00:00, 45.42it/s][A
 97%|█████████▋| 562/577 [00:12<00:00, 45.46it/s][A
 98%|█████████▊| 567/577 [00:12<00:00, 45.47it/s][A
 99%|█████████▉| 572/577 [00:12<00:00, 45.35it/s][A
100%|██████████| 577/577 [00:13<00:00, 45.24it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:13<00:00, 45.24it/s][A 80%|████████  | 312/390 [02:57<00:22,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:18:12,226 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 13:18:12,531 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:18:16,918 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:18:17,121 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:18:17,234 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:11<10:35,  8.25s/it] 81%|████████  | 314/390 [03:11<07:25,  5.86s/it] 81%|████████  | 315/390 [03:12<05:14,  4.19s/it] 81%|████████  | 316/390 [03:12<03:43,  3.02s/it] 81%|████████▏ | 317/390 [03:12<02:40,  2.20s/it] 82%|████████▏ | 318/390 [03:12<01:57,  1.63s/it] 82%|████████▏ | 319/390 [03:13<01:27,  1.23s/it] 82%|████████▏ | 320/390 [03:13<01:06,  1.06it/s] 82%|████████▏ | 321/390 [03:13<00:51,  1.33it/s] 83%|████████▎ | 322/390 [03:14<00:41,  1.63it/s] 83%|████████▎ | 323/390 [03:14<00:34,  1.92it/s] 83%|████████▎ | 324/390 [03:14<00:29,  2.21it/s] 83%|████████▎ | 325/390 [03:14<00:26,  2.48it/s] 84%|████████▎ | 326/390 [03:15<00:23,  2.71it/s] 84%|████████▍ | 327/390 [03:15<00:21,  2.89it/s] 84%|████████▍ | 328/390 [03:15<00:20,  3.04it/s] 84%|████████▍ | 329/390 [03:16<00:19,  3.15it/s] 85%|████████▍ | 330/390 [03:16<00:18,  3.23it/s] 85%|████████▍ | 331/390 [03:16<00:17,  3.30it/s] 85%|████████▌ | 332/390 [03:17<00:17,  3.34it/s] 85%|████████▌ | 333/390 [03:17<00:16,  3.37it/s] 86%|████████▌ | 334/390 [03:17<00:16,  3.32it/s] 86%|████████▌ | 335/390 [03:17<00:16,  3.38it/s] 86%|████████▌ | 336/390 [03:18<00:15,  3.41it/s] 86%|████████▋ | 337/390 [03:18<00:15,  3.44it/s] 87%|████████▋ | 338/390 [03:18<00:15,  3.46it/s] 87%|████████▋ | 339/390 [03:19<00:14,  3.47it/s] 87%|████████▋ | 340/390 [03:19<00:14,  3.48it/s] 87%|████████▋ | 341/390 [03:19<00:14,  3.49it/s] 88%|████████▊ | 342/390 [03:19<00:13,  3.50it/s] 88%|████████▊ | 343/390 [03:20<00:13,  3.50it/s] 88%|████████▊ | 344/390 [03:20<00:13,  3.50it/s] 88%|████████▊ | 345/390 [03:20<00:13,  3.43it/s] 89%|████████▊ | 346/390 [03:21<00:12,  3.45it/s] 89%|████████▉ | 347/390 [03:21<00:12,  3.46it/s] 89%|████████▉ | 348/390 [03:21<00:12,  3.48it/s] 89%|████████▉ | 349/390 [03:21<00:12,  3.41it/s] 90%|████████▉ | 350/390 [03:22<00:11,  3.44it/s] 90%|█████████ | 351/390 [03:22<00:11,  3.46it/s] 90%|█████████ | 352/390 [03:22<00:10,  3.47it/s] 91%|█████████ | 353/390 [03:23<00:10,  3.48it/s] 91%|█████████ | 354/390 [03:23<00:10,  3.49it/s] 91%|█████████ | 355/390 [03:23<00:10,  3.49it/s] 91%|█████████▏| 356/390 [03:23<00:09,  3.42it/s] 92%|█████████▏| 357/390 [03:24<00:09,  3.45it/s] 92%|█████████▏| 358/390 [03:24<00:09,  3.46it/s] 92%|█████████▏| 359/390 [03:24<00:08,  3.47it/s] 92%|█████████▏| 360/390 [03:25<00:08,  3.48it/s] 93%|█████████▎| 361/390 [03:25<00:08,  3.48it/s] 93%|█████████▎| 362/390 [03:25<00:08,  3.49it/s] 93%|█████████▎| 363/390 [03:25<00:07,  3.49it/s] 93%|█████████▎| 364/390 [03:26<00:07,  3.49it/s] 94%|█████████▎| 365/390 [03:26<00:07,  3.50it/s] 94%|█████████▍| 366/390 [03:26<00:06,  3.50it/s] 94%|█████████▍| 367/390 [03:27<00:06,  3.42it/s] 94%|█████████▍| 368/390 [03:27<00:06,  3.45it/s] 95%|█████████▍| 369/390 [03:27<00:06,  3.46it/s] 95%|█████████▍| 370/390 [03:27<00:05,  3.47it/s] 95%|█████████▌| 371/390 [03:28<00:05,  3.47it/s] 95%|█████████▌| 372/390 [03:28<00:05,  3.48it/s] 96%|█████████▌| 373/390 [03:28<00:04,  3.49it/s] 96%|█████████▌| 374/390 [03:29<00:04,  3.49it/s] 96%|█████████▌| 375/390 [03:29<00:04,  3.49it/s] 96%|█████████▋| 376/390 [03:29<00:04,  3.49it/s] 97%|█████████▋| 377/390 [03:29<00:03,  3.50it/s] 97%|█████████▋| 378/390 [03:30<00:03,  3.36it/s] 97%|█████████▋| 379/390 [03:30<00:03,  3.40it/s] 97%|█████████▋| 380/390 [03:30<00:02,  3.43it/s] 98%|█████████▊| 381/390 [03:31<00:02,  3.45it/s] 98%|█████████▊| 382/390 [03:31<00:02,  3.46it/s] 98%|█████████▊| 383/390 [03:31<00:02,  3.47it/s] 98%|█████████▊| 384/390 [03:32<00:01,  3.48it/s] 99%|█████████▊| 385/390 [03:32<00:01,  3.49it/s] 99%|█████████▉| 386/390 [03:32<00:01,  3.49it/s] 99%|█████████▉| 387/390 [03:32<00:00,  3.49it/s] 99%|█████████▉| 388/390 [03:33<00:00,  3.49it/s]100%|█████████▉| 389/390 [03:33<00:00,  3.35it/s]100%|██████████| 390/390 [03:33<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 13:18:47,981 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:18:47,981 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:18:47,981 >>   Batch size = 8
{'eval_loss': 1.1641619205474854, 'eval_runtime': 13.0682, 'eval_samples_per_second': 352.687, 'eval_steps_per_second': 44.153, 'epoch': 3.99}

  0%|          | 0/577 [00:00<?, ?it/s][A
  1%|          | 6/577 [00:00<00:09, 57.13it/s][A
  2%|▏         | 12/577 [00:00<00:11, 49.87it/s][A
  3%|▎         | 18/577 [00:00<00:11, 47.35it/s][A
  4%|▍         | 23/577 [00:00<00:11, 46.26it/s][A
  5%|▍         | 28/577 [00:00<00:11, 45.76it/s][A
  6%|▌         | 33/577 [00:00<00:11, 45.54it/s][A
  7%|▋         | 38/577 [00:00<00:11, 45.54it/s][A
  7%|▋         | 43/577 [00:00<00:11, 45.56it/s][A
  8%|▊         | 48/577 [00:01<00:11, 45.63it/s][A
  9%|▉         | 53/577 [00:01<00:11, 45.73it/s][A
 10%|█         | 58/577 [00:01<00:11, 45.69it/s][A
 11%|█         | 63/577 [00:01<00:11, 45.64it/s][A
 12%|█▏        | 68/577 [00:01<00:11, 45.36it/s][A
 13%|█▎        | 73/577 [00:01<00:11, 45.17it/s][A
 14%|█▎        | 78/577 [00:01<00:11, 45.17it/s][A
 14%|█▍        | 83/577 [00:01<00:10, 45.19it/s][A
 15%|█▌        | 88/577 [00:01<00:10, 45.31it/s][A
 16%|█▌        | 93/577 [00:02<00:10, 45.43it/s][A
 17%|█▋        | 98/577 [00:02<00:10, 45.46it/s][A
 18%|█▊        | 103/577 [00:02<00:10, 45.50it/s][A
 19%|█▊        | 108/577 [00:02<00:10, 45.51it/s][A
 20%|█▉        | 113/577 [00:02<00:10, 45.35it/s][A
 20%|██        | 118/577 [00:02<00:10, 42.49it/s][A
 21%|██▏       | 123/577 [00:02<00:10, 43.40it/s][A
 22%|██▏       | 128/577 [00:02<00:10, 44.02it/s][A
 23%|██▎       | 133/577 [00:02<00:09, 44.50it/s][A
 24%|██▍       | 138/577 [00:03<00:09, 44.82it/s][A
 25%|██▍       | 143/577 [00:03<00:09, 45.01it/s][A
 26%|██▌       | 148/577 [00:03<00:09, 45.18it/s][A
 27%|██▋       | 153/577 [00:03<00:09, 45.30it/s][A
 27%|██▋       | 158/577 [00:03<00:09, 45.02it/s][A
 28%|██▊       | 163/577 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 168/577 [00:03<00:09, 45.06it/s][A
 30%|██▉       | 173/577 [00:03<00:08, 45.26it/s][A
 31%|███       | 178/577 [00:03<00:08, 45.44it/s][A
 32%|███▏      | 183/577 [00:04<00:08, 45.47it/s][A
 33%|███▎      | 188/577 [00:04<00:08, 45.54it/s][A
 33%|███▎      | 193/577 [00:04<00:08, 45.49it/s][A
 34%|███▍      | 198/577 [00:04<00:08, 45.47it/s][A
 35%|███▌      | 203/577 [00:04<00:08, 45.20it/s][A
 36%|███▌      | 208/577 [00:04<00:08, 45.08it/s][A
 37%|███▋      | 213/577 [00:04<00:08, 45.08it/s][A
 38%|███▊      | 218/577 [00:04<00:07, 45.22it/s][A
 39%|███▊      | 223/577 [00:04<00:07, 45.31it/s][A
 40%|███▉      | 228/577 [00:05<00:07, 45.39it/s][A
 40%|████      | 233/577 [00:05<00:07, 45.49it/s][A
 41%|████      | 238/577 [00:05<00:07, 45.63it/s][A
 42%|████▏     | 243/577 [00:05<00:07, 45.49it/s][A
 43%|████▎     | 248/577 [00:05<00:07, 45.33it/s][A
 44%|████▍     | 253/577 [00:05<00:07, 43.85it/s][A
 45%|████▍     | 258/577 [00:05<00:07, 44.34it/s][A
 46%|████▌     | 263/577 [00:05<00:07, 44.62it/s][A
 46%|████▋     | 268/577 [00:05<00:06, 44.90it/s][A
 47%|████▋     | 273/577 [00:06<00:06, 45.08it/s][A
 48%|████▊     | 278/577 [00:06<00:06, 45.26it/s][A
 49%|████▉     | 283/577 [00:06<00:06, 45.26it/s][A
 50%|████▉     | 288/577 [00:06<00:06, 45.33it/s][A
 51%|█████     | 293/577 [00:06<00:06, 45.12it/s][A
 52%|█████▏    | 298/577 [00:06<00:06, 45.11it/s][A
 53%|█████▎    | 303/577 [00:06<00:06, 45.14it/s][A
 53%|█████▎    | 308/577 [00:06<00:05, 45.14it/s][A
 54%|█████▍    | 313/577 [00:06<00:05, 45.29it/s][A
 55%|█████▌    | 318/577 [00:07<00:05, 45.37it/s][A
 56%|█████▌    | 323/577 [00:07<00:05, 45.46it/s][A
 57%|█████▋    | 328/577 [00:07<00:05, 45.49it/s][A
 58%|█████▊    | 333/577 [00:07<00:05, 45.53it/s][A
 59%|█████▊    | 338/577 [00:07<00:05, 45.36it/s][A
 59%|█████▉    | 343/577 [00:07<00:05, 45.34it/s][A
 60%|██████    | 348/577 [00:07<00:05, 45.20it/s][A
 61%|██████    | 353/577 [00:07<00:04, 45.23it/s][A
 62%|██████▏   | 358/577 [00:07<00:04, 45.24it/s][A
 63%|██████▎   | 363/577 [00:08<00:04, 43.90it/s][A
 64%|██████▍   | 368/577 [00:08<00:04, 44.45it/s][A
 65%|██████▍   | 373/577 [00:08<00:04, 44.89it/s][A
 66%|██████▌   | 378/577 [00:08<00:04, 45.04it/s][A
 66%|██████▋   | 383/577 [00:08<00:04, 45.12it/s][A
 67%|██████▋   | 388/577 [00:08<00:04, 45.04it/s][A
 68%|██████▊   | 393/577 [00:08<00:04, 45.08it/s][A
 69%|██████▉   | 398/577 [00:08<00:03, 45.11it/s][A
 70%|██████▉   | 403/577 [00:08<00:03, 45.07it/s][A
 71%|███████   | 408/577 [00:09<00:03, 45.11it/s][A
 72%|███████▏  | 413/577 [00:09<00:03, 45.15it/s][A
 72%|███████▏  | 418/577 [00:09<00:03, 45.35it/s][A
 73%|███████▎  | 423/577 [00:09<00:03, 45.44it/s][A
 74%|███████▍  | 428/577 [00:09<00:03, 45.54it/s][A
 75%|███████▌  | 433/577 [00:09<00:03, 45.40it/s][A
 76%|███████▌  | 438/577 [00:09<00:03, 45.40it/s][A
 77%|███████▋  | 443/577 [00:09<00:02, 45.29it/s][A
 78%|███████▊  | 448/577 [00:09<00:02, 45.25it/s][A
 79%|███████▊  | 453/577 [00:10<00:02, 45.18it/s][A
 79%|███████▉  | 458/577 [00:10<00:02, 45.25it/s][A
 80%|████████  | 463/577 [00:10<00:02, 45.32it/s][A
 81%|████████  | 468/577 [00:10<00:02, 45.44it/s][A
 82%|████████▏ | 473/577 [00:10<00:02, 45.44it/s][A
 83%|████████▎ | 478/577 [00:10<00:02, 45.50it/s][A
 84%|████████▎ | 483/577 [00:10<00:02, 45.37it/s][A
 85%|████████▍ | 488/577 [00:10<00:01, 45.27it/s][A
 85%|████████▌ | 493/577 [00:10<00:01, 45.27it/s][A
 86%|████████▋ | 498/577 [00:11<00:01, 44.73it/s][A
 87%|████████▋ | 503/577 [00:11<00:01, 44.91it/s][A
 88%|████████▊ | 508/577 [00:11<00:01, 45.11it/s][A
 89%|████████▉ | 513/577 [00:11<00:01, 45.22it/s][A
 90%|████████▉ | 518/577 [00:11<00:01, 45.33it/s][A
 91%|█████████ | 523/577 [00:11<00:01, 45.33it/s][A
 92%|█████████▏| 528/577 [00:11<00:01, 45.37it/s][A
 92%|█████████▏| 533/577 [00:11<00:00, 45.24it/s][A
 93%|█████████▎| 538/577 [00:11<00:00, 45.18it/s][A
 94%|█████████▍| 543/577 [00:12<00:00, 45.09it/s][A
 95%|█████████▍| 548/577 [00:12<00:00, 45.26it/s][A
 96%|█████████▌| 553/577 [00:12<00:00, 45.33it/s][A
 97%|█████████▋| 558/577 [00:12<00:00, 45.42it/s][A
 98%|█████████▊| 563/577 [00:12<00:00, 45.45it/s][A
 98%|█████████▊| 568/577 [00:12<00:00, 45.44it/s][A
 99%|█████████▉| 573/577 [00:12<00:00, 45.36it/s][A
                                                 [A                                                 
100%|██████████| 577/577 [00:12<00:00, 45.36it/s][A100%|██████████| 390/390 [03:46<00:00,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:19:00,812 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-29 13:19:00,969 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:19:03,724 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:19:03,925 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:19:03,992 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 13:19:10,904 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 13:19:10,932 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78 (score: 1.1234697103500366).
                                                 100%|██████████| 390/390 [04:10<00:00,  3.40it/s]100%|██████████| 390/390 [04:10<00:00,  1.55it/s]
[INFO|trainer.py:1894] 2023-08-29 13:19:25,355 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 13:19:25,523 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:19:28,923 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:19:29,035 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:19:29,104 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:19:29,567 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   train_loss               =     0.3848
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   train_runtime            = 0:04:10.88
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   train_samples_per_second =     99.646
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:29,567 >>   train_steps_per_second   =      1.554
{'eval_loss': 1.1707136631011963, 'eval_runtime': 12.7607, 'eval_samples_per_second': 361.188, 'eval_steps_per_second': 45.217, 'epoch': 4.99}
{'train_runtime': 250.888, 'train_samples_per_second': 99.646, 'train_steps_per_second': 1.554, 'train_loss': 0.38482266939603366, 'epoch': 4.99}
08/29/2023 13:19:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 13:19:29,839 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:19:29,839 >>   Num examples = 4609
[INFO|trainer.py:2145] 2023-08-29 13:19:29,839 >>   Batch size = 8
  0%|          | 0/577 [00:00<?, ?it/s]  1%|          | 6/577 [00:00<00:10, 56.51it/s]  2%|▏         | 12/577 [00:00<00:11, 49.88it/s]  3%|▎         | 18/577 [00:00<00:11, 48.40it/s]  4%|▍         | 23/577 [00:00<00:11, 47.52it/s]  5%|▍         | 28/577 [00:00<00:11, 47.03it/s]  6%|▌         | 33/577 [00:00<00:11, 46.74it/s]  7%|▋         | 38/577 [00:00<00:11, 46.59it/s]  7%|▋         | 43/577 [00:00<00:11, 46.12it/s]  8%|▊         | 48/577 [00:01<00:11, 45.58it/s]  9%|▉         | 53/577 [00:01<00:11, 45.30it/s] 10%|█         | 58/577 [00:01<00:11, 45.41it/s] 11%|█         | 63/577 [00:01<00:11, 45.50it/s] 12%|█▏        | 68/577 [00:01<00:11, 45.75it/s] 13%|█▎        | 73/577 [00:01<00:11, 45.80it/s] 14%|█▎        | 78/577 [00:01<00:10, 45.93it/s] 14%|█▍        | 83/577 [00:01<00:10, 45.96it/s] 15%|█▌        | 88/577 [00:01<00:10, 45.77it/s] 16%|█▌        | 93/577 [00:02<00:10, 45.30it/s] 17%|█▋        | 98/577 [00:02<00:10, 45.20it/s] 18%|█▊        | 103/577 [00:02<00:10, 45.24it/s] 19%|█▊        | 108/577 [00:02<00:10, 45.32it/s] 20%|█▉        | 113/577 [00:02<00:10, 45.48it/s] 20%|██        | 118/577 [00:02<00:10, 45.67it/s] 21%|██▏       | 123/577 [00:02<00:09, 45.78it/s] 22%|██▏       | 128/577 [00:02<00:09, 45.89it/s] 23%|██▎       | 133/577 [00:02<00:09, 45.74it/s] 24%|██▍       | 138/577 [00:02<00:09, 45.45it/s] 25%|██▍       | 143/577 [00:03<00:09, 44.43it/s] 26%|██▌       | 148/577 [00:03<00:09, 44.70it/s] 27%|██▋       | 153/577 [00:03<00:09, 44.91it/s] 27%|██▋       | 158/577 [00:03<00:09, 45.14it/s] 28%|██▊       | 163/577 [00:03<00:09, 45.35it/s] 29%|██▉       | 168/577 [00:03<00:08, 45.50it/s] 30%|██▉       | 173/577 [00:03<00:08, 45.68it/s] 31%|███       | 178/577 [00:03<00:08, 45.63it/s] 32%|███▏      | 183/577 [00:03<00:08, 45.37it/s] 33%|███▎      | 188/577 [00:04<00:08, 45.25it/s] 33%|███▎      | 193/577 [00:04<00:08, 45.30it/s] 34%|███▍      | 198/577 [00:04<00:08, 45.40it/s] 35%|███▌      | 203/577 [00:04<00:08, 45.41it/s] 36%|███▌      | 208/577 [00:04<00:08, 45.49it/s] 37%|███▋      | 213/577 [00:04<00:07, 45.62it/s] 38%|███▊      | 218/577 [00:04<00:07, 45.76it/s] 39%|███▊      | 223/577 [00:04<00:07, 45.62it/s] 40%|███▉      | 228/577 [00:04<00:07, 45.51it/s] 40%|████      | 233/577 [00:05<00:07, 45.33it/s] 41%|████      | 238/577 [00:05<00:07, 45.25it/s] 42%|████▏     | 243/577 [00:05<00:07, 45.35it/s] 43%|████▎     | 248/577 [00:05<00:07, 45.45it/s] 44%|████▍     | 253/577 [00:05<00:07, 45.54it/s] 45%|████▍     | 258/577 [00:05<00:06, 45.66it/s] 46%|████▌     | 263/577 [00:05<00:06, 45.66it/s] 46%|████▋     | 268/577 [00:05<00:06, 45.64it/s] 47%|████▋     | 273/577 [00:05<00:06, 45.48it/s] 48%|████▊     | 278/577 [00:06<00:06, 45.31it/s] 49%|████▉     | 283/577 [00:06<00:06, 44.60it/s] 50%|████▉     | 288/577 [00:06<00:06, 43.89it/s] 51%|█████     | 293/577 [00:06<00:06, 44.51it/s] 52%|█████▏    | 298/577 [00:06<00:06, 44.93it/s] 53%|█████▎    | 303/577 [00:06<00:06, 45.20it/s] 53%|█████▎    | 308/577 [00:06<00:05, 45.42it/s] 54%|█████▍    | 313/577 [00:06<00:05, 45.38it/s] 55%|█████▌    | 318/577 [00:06<00:05, 45.32it/s] 56%|█████▌    | 323/577 [00:07<00:05, 45.16it/s] 57%|█████▋    | 328/577 [00:07<00:05, 45.13it/s] 58%|█████▊    | 333/577 [00:07<00:05, 45.13it/s] 59%|█████▊    | 338/577 [00:07<00:05, 45.32it/s] 59%|█████▉    | 343/577 [00:07<00:05, 45.44it/s] 60%|██████    | 348/577 [00:07<00:05, 45.59it/s] 61%|██████    | 353/577 [00:07<00:04, 45.61it/s] 62%|██████▏   | 358/577 [00:07<00:04, 45.60it/s] 63%|██████▎   | 363/577 [00:07<00:04, 45.54it/s] 64%|██████▍   | 368/577 [00:08<00:04, 45.48it/s] 65%|██████▍   | 373/577 [00:08<00:04, 45.29it/s] 66%|██████▌   | 378/577 [00:08<00:04, 45.28it/s] 66%|██████▋   | 383/577 [00:08<00:04, 45.32it/s] 67%|██████▋   | 388/577 [00:08<00:04, 45.50it/s] 68%|██████▊   | 393/577 [00:08<00:04, 45.61it/s] 69%|██████▉   | 398/577 [00:08<00:03, 45.62it/s] 70%|██████▉   | 403/577 [00:08<00:03, 45.56it/s] 71%|███████   | 408/577 [00:08<00:03, 45.48it/s] 72%|███████▏  | 413/577 [00:09<00:03, 45.37it/s] 72%|███████▏  | 418/577 [00:09<00:03, 45.32it/s] 73%|███████▎  | 423/577 [00:09<00:03, 44.44it/s] 74%|███████▍  | 428/577 [00:09<00:03, 44.71it/s] 75%|███████▌  | 433/577 [00:09<00:03, 45.06it/s] 76%|███████▌  | 438/577 [00:09<00:03, 45.24it/s] 77%|███████▋  | 443/577 [00:09<00:02, 45.26it/s] 78%|███████▊  | 448/577 [00:09<00:02, 45.41it/s] 79%|███████▊  | 453/577 [00:09<00:02, 45.38it/s] 79%|███████▉  | 458/577 [00:10<00:02, 45.28it/s] 80%|████████  | 463/577 [00:10<00:02, 45.19it/s] 81%|████████  | 468/577 [00:10<00:02, 45.15it/s] 82%|████████▏ | 473/577 [00:10<00:02, 45.29it/s] 83%|████████▎ | 478/577 [00:10<00:02, 45.35it/s] 84%|████████▎ | 483/577 [00:10<00:02, 45.53it/s] 85%|████████▍ | 488/577 [00:10<00:01, 45.58it/s] 85%|████████▌ | 493/577 [00:10<00:01, 45.59it/s] 86%|████████▋ | 498/577 [00:10<00:01, 45.46it/s] 87%|████████▋ | 503/577 [00:11<00:01, 45.39it/s] 88%|████████▊ | 508/577 [00:11<00:01, 45.24it/s] 89%|████████▉ | 513/577 [00:11<00:01, 45.18it/s] 90%|████████▉ | 518/577 [00:11<00:01, 45.15it/s] 91%|█████████ | 523/577 [00:11<00:01, 45.27it/s] 92%|█████████▏| 528/577 [00:11<00:01, 45.31it/s] 92%|█████████▏| 533/577 [00:11<00:00, 45.48it/s] 93%|█████████▎| 538/577 [00:11<00:00, 45.55it/s] 94%|█████████▍| 543/577 [00:11<00:00, 45.59it/s] 95%|█████████▍| 548/577 [00:12<00:00, 45.51it/s] 96%|█████████▌| 553/577 [00:12<00:00, 45.44it/s] 97%|█████████▋| 558/577 [00:12<00:00, 45.29it/s] 98%|█████████▊| 563/577 [00:12<00:00, 41.92it/s] 98%|█████████▊| 568/577 [00:12<00:00, 43.08it/s] 99%|█████████▉| 573/577 [00:12<00:00, 43.81it/s]100%|██████████| 577/577 [00:12<00:00, 45.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:19:42,570 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   eval_loss               =     1.1235
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   eval_runtime            = 0:00:12.73
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   eval_samples            =       4609
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   eval_samples_per_second =    362.022
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   eval_steps_per_second   =     45.322
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:19:42,570 >>   perplexity              =     3.0755
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:53,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:53,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:53,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:53,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:53,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:19:54,189 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:19:54,190 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:19:54,798 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:19:55,939 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:19:55,939 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:58,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:58,871 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:58,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:58,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:19:58,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:19:59,618 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:19:59,619 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:20:00,237 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:20:00,461 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:20:00,483 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/dev.jsonl', 'labels': ['made from material', 'manufacturer', 'mouth of the watercourse', 'official language', 'sports discipline competed in'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13127
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13227, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.58it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.68it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:12,  1.71it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:13,  1.62it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.68it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.70it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:21,  1.69it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.67it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:26,  1.59it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:27,  1.58it/s]Extractor Predicting: 47it [00:28,  1.56it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:29,  1.64it/s]Extractor Predicting: 50it [00:30,  1.64it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:31,  1.63it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:36,  1.64it/s]Extractor Predicting: 61it [00:37,  1.62it/s]Extractor Predicting: 62it [00:37,  1.59it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:39,  1.56it/s]Extractor Predicting: 65it [00:39,  1.57it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.68it/s]Extractor Predicting: 69it [00:42,  1.69it/s]Extractor Predicting: 70it [00:42,  1.67it/s]Extractor Predicting: 71it [00:43,  1.71it/s]Extractor Predicting: 72it [00:43,  1.73it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.65it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.67it/s]Extractor Predicting: 79it [00:48,  1.67it/s]Extractor Predicting: 80it [00:48,  1.66it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:49,  1.64it/s]Extractor Predicting: 83it [00:50,  1.64it/s]Extractor Predicting: 84it [00:51,  1.69it/s]Extractor Predicting: 85it [00:51,  1.75it/s]Extractor Predicting: 86it [00:52,  1.75it/s]Extractor Predicting: 87it [00:52,  1.78it/s]Extractor Predicting: 88it [00:53,  1.69it/s]Extractor Predicting: 89it [00:54,  1.67it/s]Extractor Predicting: 90it [00:54,  1.65it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:55,  1.67it/s]Extractor Predicting: 93it [00:56,  1.69it/s]Extractor Predicting: 94it [00:57,  1.69it/s]Extractor Predicting: 95it [00:57,  1.69it/s]Extractor Predicting: 96it [00:58,  1.69it/s]Extractor Predicting: 97it [00:58,  1.70it/s]Extractor Predicting: 98it [00:59,  1.63it/s]Extractor Predicting: 99it [01:00,  1.65it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:01,  1.64it/s]Extractor Predicting: 103it [01:02,  1.62it/s]Extractor Predicting: 104it [01:03,  1.66it/s]Extractor Predicting: 105it [01:03,  1.70it/s]Extractor Predicting: 106it [01:04,  1.62it/s]Extractor Predicting: 107it [01:04,  1.68it/s]Extractor Predicting: 108it [01:05,  1.71it/s]Extractor Predicting: 109it [01:06,  1.67it/s]Extractor Predicting: 110it [01:06,  1.67it/s]Extractor Predicting: 111it [01:07,  1.65it/s]Extractor Predicting: 112it [01:07,  1.64it/s]Extractor Predicting: 113it [01:08,  1.65it/s]Extractor Predicting: 114it [01:09,  1.66it/s]Extractor Predicting: 115it [01:09,  1.64it/s]Extractor Predicting: 116it [01:10,  1.65it/s]Extractor Predicting: 117it [01:10,  1.69it/s]Extractor Predicting: 118it [01:11,  1.69it/s]Extractor Predicting: 119it [01:11,  1.75it/s]Extractor Predicting: 120it [01:12,  1.74it/s]Extractor Predicting: 121it [01:13,  1.72it/s]Extractor Predicting: 122it [01:13,  1.70it/s]Extractor Predicting: 123it [01:14,  1.63it/s]Extractor Predicting: 124it [01:15,  1.67it/s]Extractor Predicting: 125it [01:15,  1.63it/s]Extractor Predicting: 126it [01:16,  1.69it/s]Extractor Predicting: 127it [01:16,  1.72it/s]Extractor Predicting: 128it [01:17,  1.69it/s]Extractor Predicting: 129it [01:17,  1.68it/s]Extractor Predicting: 130it [01:18,  1.64it/s]Extractor Predicting: 131it [01:19,  1.64it/s]Extractor Predicting: 132it [01:19,  1.64it/s]Extractor Predicting: 133it [01:20,  1.66it/s]Extractor Predicting: 134it [01:21,  1.64it/s]Extractor Predicting: 135it [01:21,  1.64it/s]Extractor Predicting: 136it [01:22,  1.64it/s]Extractor Predicting: 137it [01:22,  1.67it/s]Extractor Predicting: 138it [01:23,  1.66it/s]Extractor Predicting: 139it [01:24,  1.63it/s]Extractor Predicting: 140it [01:24,  1.60it/s]Extractor Predicting: 141it [01:25,  1.59it/s]Extractor Predicting: 142it [01:26,  1.57it/s]Extractor Predicting: 143it [01:26,  1.60it/s]Extractor Predicting: 144it [01:27,  1.59it/s]Extractor Predicting: 145it [01:27,  1.59it/s]Extractor Predicting: 146it [01:28,  1.55it/s]Extractor Predicting: 147it [01:29,  1.56it/s]Extractor Predicting: 148it [01:29,  1.57it/s]Extractor Predicting: 149it [01:30,  1.60it/s]Extractor Predicting: 150it [01:31,  1.60it/s]Extractor Predicting: 151it [01:31,  1.63it/s]Extractor Predicting: 152it [01:32,  1.62it/s]Extractor Predicting: 153it [01:33,  1.51it/s]Extractor Predicting: 154it [01:33,  1.54it/s]Extractor Predicting: 155it [01:34,  1.56it/s]Extractor Predicting: 156it [01:34,  1.54it/s]Extractor Predicting: 157it [01:35,  1.57it/s]Extractor Predicting: 158it [01:36,  1.58it/s]Extractor Predicting: 159it [01:36,  1.58it/s]Extractor Predicting: 160it [01:37,  1.60it/s]Extractor Predicting: 161it [01:38,  1.63it/s]Extractor Predicting: 162it [01:38,  1.63it/s]Extractor Predicting: 163it [01:39,  1.58it/s]Extractor Predicting: 164it [01:39,  1.57it/s]Extractor Predicting: 165it [01:40,  1.58it/s]Extractor Predicting: 166it [01:41,  1.57it/s]Extractor Predicting: 167it [01:41,  1.59it/s]Extractor Predicting: 168it [01:42,  1.59it/s]Extractor Predicting: 169it [01:43,  1.60it/s]Extractor Predicting: 170it [01:43,  1.60it/s]Extractor Predicting: 171it [01:43,  1.91it/s]Extractor Predicting: 171it [01:43,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:54,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:54,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:54,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:54,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:54,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:21:54,973 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:21:54,975 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:21:55,255 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:21:56,375 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:21:56,375 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:58,606 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:58,640 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:58,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:58,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:21:58,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:21:59,082 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:21:59,084 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:21:59,787 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:22:00,021 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:22:00,021 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.183375104427736,
  "recall": 0.09524842699067043,
  "score": 0.1253748393545623,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.77it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:13,  1.71it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:14,  1.70it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:15,  1.74it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:19,  1.69it/s]Extractor Predicting: 34it [00:20,  1.68it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:21,  1.65it/s]Extractor Predicting: 38it [00:22,  1.67it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:24,  1.70it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:26,  1.72it/s]Extractor Predicting: 46it [00:27,  1.72it/s]Extractor Predicting: 47it [00:27,  1.67it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:30,  1.71it/s]Extractor Predicting: 52it [00:30,  1.71it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:31,  1.68it/s]Extractor Predicting: 55it [00:32,  1.54it/s]Extractor Predicting: 56it [00:33,  1.59it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:36,  1.63it/s]Extractor Predicting: 63it [00:37,  1.66it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:38,  1.68it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.70it/s]Extractor Predicting: 69it [00:41,  1.72it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.63it/s]Extractor Predicting: 72it [00:42,  1.65it/s]Extractor Predicting: 73it [00:43,  1.64it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:45,  1.71it/s]Extractor Predicting: 77it [00:45,  1.75it/s]Extractor Predicting: 78it [00:46,  1.74it/s]Extractor Predicting: 79it [00:46,  1.72it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.70it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:49,  1.72it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.69it/s]Extractor Predicting: 86it [00:51,  1.71it/s]Extractor Predicting: 87it [00:51,  1.69it/s]Extractor Predicting: 88it [00:52,  1.69it/s]Extractor Predicting: 89it [00:52,  1.67it/s]Extractor Predicting: 90it [00:53,  1.64it/s]Extractor Predicting: 91it [00:54,  1.66it/s]Extractor Predicting: 92it [00:54,  1.64it/s]Extractor Predicting: 93it [00:55,  1.66it/s]Extractor Predicting: 94it [00:55,  1.65it/s]Extractor Predicting: 95it [00:56,  1.65it/s]Extractor Predicting: 96it [00:57,  1.62it/s]Extractor Predicting: 97it [00:57,  1.64it/s]Extractor Predicting: 98it [00:58,  1.66it/s]Extractor Predicting: 99it [00:58,  1.75it/s]Extractor Predicting: 100it [00:59,  1.75it/s]Extractor Predicting: 101it [01:00,  1.73it/s]Extractor Predicting: 102it [01:00,  1.74it/s]Extractor Predicting: 103it [01:01,  1.73it/s]Extractor Predicting: 104it [01:01,  1.74it/s]Extractor Predicting: 105it [01:02,  1.72it/s]Extractor Predicting: 106it [01:02,  1.73it/s]Extractor Predicting: 107it [01:03,  1.75it/s]Extractor Predicting: 108it [01:04,  1.73it/s]Extractor Predicting: 109it [01:04,  1.68it/s]Extractor Predicting: 110it [01:05,  1.61it/s]Extractor Predicting: 111it [01:06,  1.59it/s]Extractor Predicting: 112it [01:06,  1.60it/s]Extractor Predicting: 113it [01:07,  1.52it/s]Extractor Predicting: 114it [01:08,  1.56it/s]Extractor Predicting: 115it [01:08,  1.58it/s]Extractor Predicting: 116it [01:09,  1.61it/s]Extractor Predicting: 117it [01:09,  1.61it/s]Extractor Predicting: 118it [01:10,  1.59it/s]Extractor Predicting: 119it [01:11,  1.61it/s]Extractor Predicting: 120it [01:11,  1.59it/s]Extractor Predicting: 121it [01:12,  1.58it/s]Extractor Predicting: 122it [01:12,  1.61it/s]Extractor Predicting: 123it [01:13,  1.61it/s]Extractor Predicting: 124it [01:14,  1.62it/s]Extractor Predicting: 125it [01:14,  2.01it/s]Extractor Predicting: 125it [01:14,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:31,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:31,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:31,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:31,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:31,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:23:32,459 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:23:32,460 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:23:32,770 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:23:33,856 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:23:33,856 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:36,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:36,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:36,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:36,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:23:36,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:23:36,623 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:23:36,624 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:23:37,078 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:23:37,330 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:23:37,330 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.414058209774849,
  "recall": 0.2531050688150386,
  "score": 0.31416666666666665,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  2.09it/s]Extractor Predicting: 6it [00:03,  1.77it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5232558139534884,
  "recall": 0.17716535433070865,
  "score": 0.2647058823529412,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/', 'labels': ['lyrics by', 'occupant', 'screenwriter', 'use', 'voice type'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
