Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:22<05:17, 22.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:39<04:10, 19.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:56<03:37, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:14<03:20, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:30<02:52, 17.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:50<02:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:10<02:30, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:30<02:15, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:50<01:57, 19.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:08<01:34, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:22<01:10, 17.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:40<00:53, 17.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:58<00:35, 17.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:16<00:17, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:32<00:00, 17.38s/it]Generating: 100%|██████████| 15/15 [04:32<00:00, 18.20s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : genre . Context : Later in Life , he wrote his own comedy songs , adapted and remastered by a different generation of American artists . Head Entity : comedy , Tail Entity : anthology .\n']
['Relation : genre . Context : Later in Life , he wrote his own comedy songs , adapted and remastered by a different generation of American artists . Head Entity : comedy , Tail Entity : anthology .\n', "Relation : genre . Context : After the death of his son Richard , his nephew Richard Henry entered the Academy of Music , where his father James ' grandfather Robert was a singer . Head Entity : Richard Henry , Tail Entity : academy of music .\n"]
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 74, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 115, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 160, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 272, 'raw': 448}
{'target': 600, 'success': 291, 'raw': 480}
{'target': 600, 'success': 313, 'raw': 512}
{'target': 600, 'success': 334, 'raw': 544}
{'target': 600, 'success': 348, 'raw': 576}
{'target': 600, 'success': 367, 'raw': 608}
{'target': 600, 'success': 389, 'raw': 640}
{'target': 600, 'success': 411, 'raw': 672}
{'target': 600, 'success': 431, 'raw': 704}
{'target': 600, 'success': 450, 'raw': 736}
{'target': 600, 'success': 469, 'raw': 768}
{'target': 600, 'success': 489, 'raw': 800}
{'target': 600, 'success': 513, 'raw': 832}
{'target': 600, 'success': 531, 'raw': 864}
{'target': 600, 'success': 555, 'raw': 896}
{'target': 600, 'success': 572, 'raw': 928}
{'target': 600, 'success': 595, 'raw': 960}
{'target': 600, 'success': 617, 'raw': 992}
{'prompt': 'Relation : genre .', 'success_rate': 0.6219758064516129, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : participant in . Context : Later in the year ( October 1887 ) , a young French journalist who had formerly represented the government had been invited to join Napoleon III in the battle against the Ottoman Empire . Head Entity : Battle , Tail Entity : Napoleon III .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 186, 'raw': 288}
{'target': 600, 'success': 213, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 261, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 371, 'raw': 544}
{'target': 600, 'success': 395, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 487, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 582, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : participant in .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : competition class . Context : Following his leadership in the 1972 elections under the leadership of former Premier Jack Liddell , the Labor Party won the popular vote for the first time against a Labor led Coalition Government , and lost the election following a brutal Labor loss at the 2006 State Government election . Head Entity : 1974 election , Tail Entity : Labor Party .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 216, 'raw': 352}
{'target': 600, 'success': 235, 'raw': 384}
{'target': 600, 'success': 256, 'raw': 416}
{'target': 600, 'success': 273, 'raw': 448}
{'target': 600, 'success': 290, 'raw': 480}
{'target': 600, 'success': 310, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 346, 'raw': 576}
{'target': 600, 'success': 364, 'raw': 608}
{'target': 600, 'success': 384, 'raw': 640}
{'target': 600, 'success': 405, 'raw': 672}
{'target': 600, 'success': 422, 'raw': 704}
{'target': 600, 'success': 441, 'raw': 736}
{'target': 600, 'success': 458, 'raw': 768}
{'target': 600, 'success': 479, 'raw': 800}
{'target': 600, 'success': 497, 'raw': 832}
{'target': 600, 'success': 514, 'raw': 864}
{'target': 600, 'success': 530, 'raw': 896}
{'target': 600, 'success': 554, 'raw': 928}
{'target': 600, 'success': 572, 'raw': 960}
{'target': 600, 'success': 588, 'raw': 992}
{'target': 600, 'success': 606, 'raw': 1024}
{'prompt': 'Relation : competition class .', 'success_rate': 0.591796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 123, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 226, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 269, 'raw': 416}
{'target': 600, 'success': 291, 'raw': 448}
{'target': 600, 'success': 311, 'raw': 480}
{'target': 600, 'success': 332, 'raw': 512}
{'target': 600, 'success': 353, 'raw': 544}
{'target': 600, 'success': 376, 'raw': 576}
{'target': 600, 'success': 390, 'raw': 608}
{'target': 600, 'success': 414, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 454, 'raw': 704}
{'target': 600, 'success': 472, 'raw': 736}
{'target': 600, 'success': 493, 'raw': 768}
{'target': 600, 'success': 514, 'raw': 800}
{'target': 600, 'success': 532, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 595, 'raw': 928}
{'target': 600, 'success': 613, 'raw': 960}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6385416666666667, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 393, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 434, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 480, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 523, 'raw': 736}
{'target': 600, 'success': 548, 'raw': 768}
{'target': 600, 'success': 572, 'raw': 800}
{'target': 600, 'success': 592, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : father .', 'success_rate': 0.7083333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 177, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 223, 'raw': 320}
{'target': 600, 'success': 243, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 331, 'raw': 480}
{'target': 600, 'success': 353, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 399, 'raw': 576}
{'target': 600, 'success': 418, 'raw': 608}
{'target': 600, 'success': 442, 'raw': 640}
{'target': 600, 'success': 467, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 535, 'raw': 768}
{'target': 600, 'success': 557, 'raw': 800}
{'target': 600, 'success': 580, 'raw': 832}
{'target': 600, 'success': 604, 'raw': 864}
{'prompt': 'Relation : field of work .', 'success_rate': 0.6990740740740741, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : heritage designation . Context : Later in the year ( 1168–1174 ) he married Brigadier General Henry de Bruyne , son of Alexander de Bruyne , the King of France . Head Entity : Henry de Bruyne , Tail Entity : French Empire .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n']
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n', 'Relation : licensed to broadcast to . Context : After the death of Cephas Mater ( 9 January 2012 ) - his first wife was his elder brother Amalia Cephensi , who he still considers the best of all men . Head Entity : Amalia Cephensi , Tail Entity : licensed to network .\n']
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n', 'Relation : licensed to broadcast to . Context : After the death of Cephas Mater ( 9 January 2012 ) - his first wife was his elder brother Amalia Cephensi , who he still considers the best of all men . Head Entity : Amalia Cephensi , Tail Entity : licensed to network .\n', 'Relation : licensed to broadcast to . Context : This film was released on October 8 , 1998 starring Chris Rock and Joe Pesci . Head Entity : Joe Pesci , Tail Entity : CBS .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8151041666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : located in the administrative territorial entity . Context : The city of Marchec is under the Administration of the President of the Republic at the end of 2010 , led by General Roman Pontak , and under the supervision of Chief of Staff Aleister Chirakov and National Defence Minister Ivan Popov . Head Entity : Marchec , Tail Entity : Moscow .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 493, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 537, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 168, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 293, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 337, 'raw': 480}
{'target': 600, 'success': 364, 'raw': 512}
{'target': 600, 'success': 387, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 461, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 532, 'raw': 736}
{'target': 600, 'success': 556, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7211538461538461, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('2008 Republican primaries', 'occupation', '', 'He was one of the leading candidates for president of the United States in the 2008 Republican primaries .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 14940
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15040, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.35s/it]Extractor Estimating: 2it [00:20,  9.33s/it]Extractor Estimating: 3it [00:20,  5.38s/it]Extractor Estimating: 4it [00:22,  3.75s/it]Extractor Estimating: 5it [00:22,  2.64s/it]Extractor Estimating: 6it [00:23,  1.95s/it]Extractor Estimating: 7it [00:24,  1.54s/it]Extractor Estimating: 8it [00:24,  1.29s/it]Extractor Estimating: 9it [00:25,  1.10s/it]Extractor Estimating: 10it [00:27,  1.32s/it]Extractor Estimating: 11it [00:28,  1.14s/it]Extractor Estimating: 12it [00:28,  1.00it/s]Extractor Estimating: 13it [00:30,  1.08s/it]Extractor Estimating: 14it [00:30,  1.04it/s]Extractor Estimating: 15it [00:31,  1.17it/s]Extractor Estimating: 16it [00:31,  1.29it/s]Extractor Estimating: 17it [00:32,  1.37it/s]Extractor Estimating: 18it [00:33,  1.42it/s]Extractor Estimating: 19it [00:33,  1.35it/s]Extractor Estimating: 20it [00:34,  1.36it/s]Extractor Estimating: 21it [00:35,  1.37it/s]Extractor Estimating: 22it [00:36,  1.40it/s]Extractor Estimating: 23it [00:36,  1.39it/s]Extractor Estimating: 24it [00:37,  1.37it/s]Extractor Estimating: 25it [00:38,  1.38it/s]Extractor Estimating: 26it [00:38,  1.44it/s]Extractor Estimating: 27it [00:39,  1.50it/s]Extractor Estimating: 28it [00:40,  1.53it/s]Extractor Estimating: 29it [00:40,  1.56it/s]Extractor Estimating: 30it [00:41,  1.54it/s]Extractor Estimating: 31it [00:41,  1.60it/s]Extractor Estimating: 32it [00:42,  1.57it/s]Extractor Estimating: 33it [00:43,  1.61it/s]Extractor Estimating: 34it [00:43,  1.57it/s]Extractor Estimating: 35it [00:44,  1.61it/s]Extractor Estimating: 36it [00:45,  1.55it/s]Extractor Estimating: 37it [00:45,  1.54it/s]Extractor Estimating: 38it [00:46,  1.59it/s]Extractor Estimating: 39it [00:46,  1.68it/s]Extractor Estimating: 40it [00:47,  1.66it/s]Extractor Estimating: 41it [00:48,  1.63it/s]Extractor Estimating: 42it [00:48,  1.66it/s]Extractor Estimating: 43it [00:49,  1.66it/s]Extractor Estimating: 44it [00:50,  1.61it/s]Extractor Estimating: 45it [00:50,  1.64it/s]Extractor Estimating: 46it [00:51,  1.59it/s]Extractor Estimating: 47it [00:51,  1.56it/s]Extractor Estimating: 48it [00:52,  1.62it/s]Extractor Estimating: 49it [00:53,  1.63it/s]Extractor Estimating: 50it [00:53,  1.58it/s]Extractor Estimating: 51it [00:54,  1.60it/s]Extractor Estimating: 52it [00:55,  1.63it/s]Extractor Estimating: 53it [00:56,  1.16it/s]Extractor Estimating: 54it [00:57,  1.24it/s]Extractor Estimating: 55it [00:57,  1.33it/s]Extractor Estimating: 56it [00:58,  1.40it/s]Extractor Estimating: 57it [00:59,  1.42it/s]Extractor Estimating: 58it [00:59,  1.45it/s]Extractor Estimating: 59it [01:00,  1.48it/s]Extractor Estimating: 60it [01:00,  1.54it/s]Extractor Estimating: 61it [01:01,  1.44it/s]Extractor Estimating: 62it [01:02,  1.50it/s]Extractor Estimating: 63it [01:02,  1.54it/s]Extractor Estimating: 64it [01:03,  1.57it/s]Extractor Estimating: 65it [01:04,  1.56it/s]Extractor Estimating: 66it [01:04,  1.50it/s]Extractor Estimating: 67it [01:05,  1.56it/s]Extractor Estimating: 68it [01:06,  1.51it/s]Extractor Estimating: 69it [01:06,  1.58it/s]Extractor Estimating: 70it [01:07,  1.53it/s]Extractor Estimating: 71it [01:08,  1.55it/s]Extractor Estimating: 72it [01:08,  1.45it/s]Extractor Estimating: 73it [01:09,  1.45it/s]Extractor Estimating: 74it [01:10,  1.52it/s]Extractor Estimating: 75it [01:10,  1.45it/s]Extractor Estimating: 76it [01:11,  1.46it/s]Extractor Estimating: 77it [01:12,  1.47it/s]Extractor Estimating: 78it [01:13,  1.44it/s]Extractor Estimating: 79it [01:13,  1.42it/s]Extractor Estimating: 80it [01:14,  1.44it/s]Extractor Estimating: 81it [01:15,  1.46it/s]Extractor Estimating: 82it [01:15,  1.49it/s]Extractor Estimating: 83it [01:16,  1.45it/s]Extractor Estimating: 84it [01:17,  1.49it/s]Extractor Estimating: 85it [01:17,  1.55it/s]Extractor Estimating: 86it [01:18,  1.58it/s]Extractor Estimating: 87it [01:18,  1.57it/s]Extractor Estimating: 88it [01:19,  1.48it/s]Extractor Estimating: 89it [01:20,  1.49it/s]Extractor Estimating: 90it [01:21,  1.48it/s]Extractor Estimating: 91it [01:21,  1.51it/s]Extractor Estimating: 92it [01:22,  1.49it/s]Extractor Estimating: 93it [01:23,  1.47it/s]Extractor Estimating: 94it [01:23,  1.47it/s]Extractor Estimating: 95it [01:24,  1.49it/s]Extractor Estimating: 96it [01:25,  1.44it/s]Extractor Estimating: 97it [01:25,  1.48it/s]Extractor Estimating: 98it [01:26,  1.46it/s]Extractor Estimating: 99it [01:27,  1.44it/s]Extractor Estimating: 100it [01:27,  1.46it/s]Extractor Estimating: 101it [01:28,  1.51it/s]Extractor Estimating: 102it [01:29,  1.51it/s]Extractor Estimating: 103it [01:29,  1.53it/s]Extractor Estimating: 104it [01:30,  1.60it/s]Extractor Estimating: 105it [01:30,  1.58it/s]Extractor Estimating: 106it [01:31,  1.65it/s]Extractor Estimating: 107it [01:32,  1.63it/s]Extractor Estimating: 108it [01:32,  1.60it/s]Extractor Estimating: 109it [01:33,  1.60it/s]Extractor Estimating: 110it [01:34,  1.60it/s]Extractor Estimating: 111it [01:34,  1.60it/s]Extractor Estimating: 112it [01:35,  1.58it/s]Extractor Estimating: 113it [01:35,  1.62it/s]Extractor Estimating: 114it [01:36,  1.65it/s]Extractor Estimating: 115it [01:37,  1.62it/s]Extractor Estimating: 116it [01:37,  1.49it/s]Extractor Estimating: 117it [01:38,  1.59it/s]Extractor Estimating: 118it [01:39,  1.62it/s]Extractor Estimating: 119it [01:39,  1.52it/s]Extractor Estimating: 120it [01:40,  1.51it/s]Extractor Estimating: 121it [01:41,  1.58it/s]Extractor Estimating: 122it [01:41,  1.59it/s]Extractor Estimating: 123it [01:42,  1.65it/s]Extractor Estimating: 124it [01:42,  1.67it/s]Extractor Estimating: 125it [01:43,  1.63it/s]Extractor Estimating: 126it [01:44,  1.57it/s]Extractor Estimating: 127it [01:44,  1.59it/s]Extractor Estimating: 128it [01:45,  1.58it/s]Extractor Estimating: 129it [01:46,  1.59it/s]Extractor Estimating: 130it [01:46,  1.56it/s]Extractor Estimating: 131it [01:47,  1.51it/s]Extractor Estimating: 132it [01:48,  1.55it/s]Extractor Estimating: 133it [01:48,  1.58it/s]Extractor Estimating: 134it [01:49,  1.57it/s]Extractor Estimating: 135it [01:49,  1.57it/s]Extractor Estimating: 136it [01:50,  1.62it/s]Extractor Estimating: 137it [01:51,  1.65it/s]Extractor Estimating: 138it [01:51,  1.64it/s]Extractor Estimating: 139it [01:52,  1.58it/s]Extractor Estimating: 140it [01:53,  1.56it/s]Extractor Estimating: 141it [01:53,  1.57it/s]Extractor Estimating: 142it [01:54,  1.55it/s]Extractor Estimating: 143it [01:54,  1.55it/s]Extractor Estimating: 144it [01:55,  1.61it/s]Extractor Estimating: 145it [01:56,  1.58it/s]Extractor Estimating: 146it [01:56,  1.55it/s]Extractor Estimating: 147it [01:57,  1.50it/s]Extractor Estimating: 148it [01:58,  1.51it/s]Extractor Estimating: 149it [01:58,  1.54it/s]Extractor Estimating: 150it [01:59,  1.55it/s]Extractor Estimating: 151it [02:00,  1.57it/s]Extractor Estimating: 152it [02:00,  1.55it/s]Extractor Estimating: 153it [02:01,  1.62it/s]Extractor Estimating: 154it [02:01,  1.61it/s]Extractor Estimating: 155it [02:02,  1.59it/s]Extractor Estimating: 156it [02:03,  1.59it/s]Extractor Estimating: 157it [02:03,  1.65it/s]Extractor Estimating: 158it [02:04,  1.66it/s]Extractor Estimating: 159it [02:05,  1.56it/s]Extractor Estimating: 160it [02:05,  1.58it/s]Extractor Estimating: 161it [02:06,  1.62it/s]Extractor Estimating: 162it [02:06,  1.60it/s]Extractor Estimating: 163it [02:07,  1.58it/s]Extractor Estimating: 164it [02:08,  1.54it/s]Extractor Estimating: 165it [02:08,  1.58it/s]Extractor Estimating: 166it [02:09,  1.57it/s]Extractor Estimating: 167it [02:10,  1.18it/s]Extractor Estimating: 168it [02:11,  1.30it/s]Extractor Estimating: 169it [02:12,  1.36it/s]Extractor Estimating: 170it [02:12,  1.45it/s]Extractor Estimating: 171it [02:13,  1.50it/s]Extractor Estimating: 172it [02:13,  1.53it/s]Extractor Estimating: 173it [02:14,  1.54it/s]Extractor Estimating: 174it [02:15,  1.61it/s]Extractor Estimating: 175it [02:15,  1.61it/s]Extractor Estimating: 176it [02:16,  1.60it/s]Extractor Estimating: 177it [02:16,  1.64it/s]Extractor Estimating: 178it [02:17,  1.63it/s]Extractor Estimating: 179it [02:18,  1.56it/s]Extractor Estimating: 180it [02:18,  1.52it/s]Extractor Estimating: 181it [02:19,  1.56it/s]Extractor Estimating: 182it [02:20,  1.55it/s]Extractor Estimating: 183it [02:20,  1.53it/s]Extractor Estimating: 184it [02:21,  1.56it/s]Extractor Estimating: 185it [02:22,  1.52it/s]Extractor Estimating: 186it [02:22,  1.51it/s]Extractor Estimating: 187it [02:23,  1.46it/s]Extractor Estimating: 188it [02:24,  1.42it/s]Extractor Estimating: 189it [02:25,  1.45it/s]Extractor Estimating: 190it [02:25,  1.48it/s]Extractor Estimating: 191it [02:26,  1.57it/s]Extractor Estimating: 192it [02:26,  1.53it/s]Extractor Estimating: 193it [02:27,  1.58it/s]Extractor Estimating: 194it [02:28,  1.60it/s]Extractor Estimating: 195it [02:28,  1.63it/s]Extractor Estimating: 196it [02:29,  1.50it/s]Extractor Estimating: 197it [02:30,  1.50it/s]Extractor Estimating: 198it [02:30,  1.54it/s]Extractor Estimating: 199it [02:31,  1.35it/s]Extractor Estimating: 200it [02:32,  1.38it/s]Extractor Estimating: 201it [02:33,  1.41it/s]Extractor Estimating: 202it [02:33,  1.43it/s]Extractor Estimating: 203it [02:34,  1.41it/s]Extractor Estimating: 204it [02:35,  1.41it/s]Extractor Estimating: 205it [02:35,  1.37it/s]Extractor Estimating: 206it [02:36,  1.42it/s]Extractor Estimating: 207it [02:37,  1.46it/s]Extractor Estimating: 208it [02:38,  1.37it/s]Extractor Estimating: 209it [02:38,  1.44it/s]Extractor Estimating: 210it [02:39,  1.49it/s]Extractor Estimating: 211it [02:39,  1.52it/s]Extractor Estimating: 212it [02:40,  1.43it/s]Extractor Estimating: 213it [02:41,  1.45it/s]Extractor Estimating: 214it [02:41,  1.51it/s]Extractor Estimating: 215it [02:42,  1.36it/s]Extractor Estimating: 216it [02:43,  1.39it/s]Extractor Estimating: 217it [02:44,  1.44it/s]Extractor Estimating: 218it [02:44,  1.48it/s]Extractor Estimating: 219it [02:45,  1.52it/s]Extractor Estimating: 220it [02:46,  1.52it/s]Extractor Estimating: 221it [02:46,  1.55it/s]Extractor Estimating: 222it [02:47,  1.51it/s]Extractor Estimating: 223it [02:48,  1.48it/s]Extractor Estimating: 224it [02:48,  1.47it/s]Extractor Estimating: 225it [02:49,  1.49it/s]Extractor Estimating: 226it [02:50,  1.49it/s]Extractor Estimating: 227it [02:50,  1.49it/s]Extractor Estimating: 228it [02:51,  1.51it/s]Extractor Estimating: 229it [02:52,  1.50it/s]Extractor Estimating: 230it [02:52,  1.53it/s]Extractor Estimating: 231it [02:53,  1.57it/s]Extractor Estimating: 232it [02:54,  1.55it/s]Extractor Estimating: 233it [02:54,  1.54it/s]Extractor Estimating: 234it [02:55,  1.59it/s]Extractor Estimating: 235it [02:55,  1.58it/s]Extractor Estimating: 236it [02:56,  1.57it/s]Extractor Estimating: 237it [02:57,  1.53it/s]Extractor Estimating: 238it [02:57,  1.59it/s]Extractor Estimating: 239it [02:58,  1.58it/s]Extractor Estimating: 240it [02:59,  1.56it/s]Extractor Estimating: 241it [02:59,  1.49it/s]Extractor Estimating: 242it [03:00,  1.54it/s]Extractor Estimating: 243it [03:01,  1.52it/s]Extractor Estimating: 244it [03:01,  1.49it/s]Extractor Estimating: 245it [03:02,  1.47it/s]Extractor Estimating: 246it [03:03,  1.55it/s]Extractor Estimating: 247it [03:03,  1.54it/s]Extractor Estimating: 248it [03:04,  1.56it/s]Extractor Estimating: 249it [03:05,  1.53it/s]Extractor Estimating: 250it [03:05,  1.50it/s]Extractor Estimating: 251it [03:06,  1.50it/s]Extractor Estimating: 252it [03:06,  1.58it/s]Extractor Estimating: 253it [03:07,  1.61it/s]Extractor Estimating: 254it [03:08,  1.54it/s]Extractor Estimating: 255it [03:08,  1.53it/s]Extractor Estimating: 256it [03:09,  1.60it/s]Extractor Estimating: 257it [03:10,  1.61it/s]Extractor Estimating: 258it [03:10,  1.65it/s]Extractor Estimating: 259it [03:11,  1.64it/s]Extractor Estimating: 260it [03:12,  1.53it/s]Extractor Estimating: 261it [03:13,  1.08it/s]Extractor Estimating: 262it [03:14,  1.18it/s]Extractor Estimating: 263it [03:14,  1.25it/s]Extractor Estimating: 264it [03:15,  1.33it/s]Extractor Estimating: 265it [03:16,  1.36it/s]Extractor Estimating: 266it [03:16,  1.40it/s]Extractor Estimating: 267it [03:17,  1.42it/s]Extractor Estimating: 268it [03:18,  1.42it/s]Extractor Estimating: 269it [03:18,  1.47it/s]Extractor Estimating: 270it [03:19,  1.50it/s]Extractor Estimating: 271it [03:20,  1.49it/s]Extractor Estimating: 272it [03:20,  1.52it/s]Extractor Estimating: 273it [03:21,  1.55it/s]Extractor Estimating: 274it [03:22,  1.53it/s]Extractor Estimating: 275it [03:22,  1.57it/s]Extractor Estimating: 276it [03:23,  1.64it/s]Extractor Estimating: 277it [03:24,  1.61it/s]Extractor Estimating: 278it [03:24,  1.42it/s]Extractor Estimating: 279it [03:25,  1.45it/s]Extractor Estimating: 280it [03:26,  1.42it/s]Extractor Estimating: 281it [03:27,  1.40it/s]Extractor Estimating: 282it [03:27,  1.49it/s]Extractor Estimating: 283it [03:28,  1.51it/s]Extractor Estimating: 284it [03:28,  1.54it/s]Extractor Estimating: 285it [03:29,  1.57it/s]Extractor Estimating: 286it [03:30,  1.59it/s]Extractor Estimating: 287it [03:30,  1.54it/s]Extractor Estimating: 288it [03:31,  1.63it/s]Extractor Estimating: 289it [03:31,  1.58it/s]Extractor Estimating: 290it [03:32,  1.59it/s]Extractor Estimating: 291it [03:33,  1.65it/s]Extractor Estimating: 292it [03:33,  1.67it/s]Extractor Estimating: 293it [03:34,  1.66it/s]Extractor Estimating: 294it [03:35,  1.59it/s]Extractor Estimating: 295it [03:35,  1.54it/s]Extractor Estimating: 296it [03:36,  1.53it/s]Extractor Estimating: 297it [03:37,  1.58it/s]Extractor Estimating: 298it [03:37,  1.55it/s]Extractor Estimating: 299it [03:38,  1.55it/s]Extractor Estimating: 300it [03:38,  1.58it/s]Extractor Estimating: 301it [03:39,  1.58it/s]Extractor Estimating: 302it [03:40,  1.51it/s]Extractor Estimating: 303it [03:40,  1.52it/s]Extractor Estimating: 304it [03:41,  1.52it/s]Extractor Estimating: 305it [03:42,  1.54it/s]Extractor Estimating: 306it [03:42,  1.60it/s]Extractor Estimating: 307it [03:43,  1.57it/s]Extractor Estimating: 308it [03:44,  1.47it/s]Extractor Estimating: 309it [03:44,  1.48it/s]Extractor Estimating: 310it [03:45,  1.50it/s]Extractor Estimating: 311it [03:46,  1.46it/s]Extractor Estimating: 312it [03:46,  1.46it/s]Extractor Estimating: 313it [03:47,  1.51it/s]Extractor Estimating: 314it [03:48,  1.51it/s]Extractor Estimating: 315it [03:48,  1.48it/s]Extractor Estimating: 316it [03:49,  1.55it/s]Extractor Estimating: 317it [03:50,  1.53it/s]Extractor Estimating: 318it [03:50,  1.50it/s]Extractor Estimating: 319it [03:51,  1.50it/s]Extractor Estimating: 320it [03:52,  1.47it/s]Extractor Estimating: 321it [03:52,  1.48it/s]Extractor Estimating: 322it [03:53,  1.51it/s]Extractor Estimating: 323it [03:54,  1.53it/s]Extractor Estimating: 324it [03:54,  1.52it/s]Extractor Estimating: 325it [03:55,  1.52it/s]Extractor Estimating: 326it [03:56,  1.57it/s]Extractor Estimating: 327it [03:56,  1.56it/s]Extractor Estimating: 328it [03:57,  1.57it/s]Extractor Estimating: 329it [03:58,  1.54it/s]Extractor Estimating: 330it [03:58,  1.47it/s]Extractor Estimating: 331it [03:59,  1.50it/s]Extractor Estimating: 332it [04:00,  1.48it/s]Extractor Estimating: 333it [04:00,  1.46it/s]Extractor Estimating: 334it [04:01,  1.42it/s]Extractor Estimating: 335it [04:02,  1.48it/s]Extractor Estimating: 336it [04:02,  1.43it/s]Extractor Estimating: 337it [04:03,  1.47it/s]Extractor Estimating: 338it [04:04,  1.41it/s]Extractor Estimating: 339it [04:04,  1.47it/s]Extractor Estimating: 340it [04:05,  1.48it/s]Extractor Estimating: 341it [04:06,  1.52it/s]Extractor Estimating: 342it [04:06,  1.50it/s]Extractor Estimating: 343it [04:07,  1.54it/s]Extractor Estimating: 344it [04:08,  1.56it/s]Extractor Estimating: 345it [04:08,  1.56it/s]Extractor Estimating: 346it [04:09,  1.56it/s]Extractor Estimating: 347it [04:10,  1.56it/s]Extractor Estimating: 348it [04:10,  1.61it/s]Extractor Estimating: 349it [04:11,  1.56it/s]Extractor Estimating: 350it [04:12,  1.55it/s]Extractor Estimating: 351it [04:12,  1.61it/s]Extractor Estimating: 352it [04:13,  1.62it/s]Extractor Estimating: 353it [04:13,  1.55it/s]Extractor Estimating: 354it [04:14,  1.54it/s]Extractor Estimating: 355it [04:15,  1.27it/s]Extractor Estimating: 356it [04:16,  1.31it/s]Extractor Estimating: 357it [04:17,  1.30it/s]Extractor Estimating: 358it [04:17,  1.39it/s]Extractor Estimating: 359it [04:18,  1.40it/s]Extractor Estimating: 360it [04:19,  1.35it/s]Extractor Estimating: 361it [04:19,  1.41it/s]Extractor Estimating: 362it [04:20,  1.46it/s]Extractor Estimating: 363it [04:21,  1.45it/s]Extractor Estimating: 364it [04:21,  1.47it/s]Extractor Estimating: 365it [04:22,  1.41it/s]Extractor Estimating: 366it [04:23,  1.46it/s]Extractor Estimating: 367it [04:23,  1.47it/s]Extractor Estimating: 368it [04:24,  1.39it/s]Extractor Estimating: 369it [04:25,  1.48it/s]Extractor Estimating: 370it [04:26,  1.50it/s]Extractor Estimating: 371it [04:26,  1.52it/s]Extractor Estimating: 372it [04:27,  1.42it/s]Extractor Estimating: 373it [04:28,  1.44it/s]Extractor Estimating: 374it [04:28,  1.51it/s]Extractor Estimating: 375it [04:29,  1.45it/s]Extractor Estimating: 375it [04:29,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1532 mean pseudo reward: 0.9686513004621375
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 13999
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14099, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14099, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 36, avg_time 1.486, loss:292.3223
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 8, avg_time 1.089, loss:225.7706
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 44, avg_time 1.100, loss:186.9457
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 16, avg_time 1.079, loss:164.8858
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 52, avg_time 1.103, loss:147.0390
>> valid entity prec:0.5473, rec:0.4942, f1:0.5194
>> valid relation prec:0.2196, rec:0.1409, f1:0.1717
>> valid relation with NER prec:0.2196, rec:0.1409, f1:0.1717
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 24, avg_time 2.390, loss:141.0109
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 60, avg_time 1.105, loss:139.6589
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 32, avg_time 1.111, loss:121.0973
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 4, avg_time 1.107, loss:134.4388
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 40, avg_time 1.132, loss:126.7353
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5754, rec:0.5082, f1:0.5397
>> valid relation prec:0.2035, rec:0.1263, f1:0.1559
>> valid relation with NER prec:0.2035, rec:0.1263, f1:0.1559
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 12, avg_time 2.441, loss:125.1639
g_step 1200, step 48, avg_time 1.073, loss:115.1319
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:25:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:25:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-25-44_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:25:47 - WARNING - datasets.builder -   Using custom data configuration default-8bab0c4a9415b6c9
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8bab0c4a9415b6c9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:25:57,089 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:25:57,124 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:25:57,124 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:25:57,125 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:25:58,803 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:25:59,069 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:26:00,175 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:26:03,409 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:26:03,891 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8bab0c4a9415b6c9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 18:26:03 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x15450ba6c0e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:02<00:02,  2.44s/ba]100%|██████████| 2/2 [00:02<00:00,  1.21s/ba]100%|██████████| 2/2 [00:02<00:00,  1.40s/ba]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.34ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.79ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.43ba/s]100%|██████████| 4/4 [00:01<00:00,  4.56ba/s]100%|██████████| 4/4 [00:01<00:00,  3.78ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:01<00:01,  1.02s/ba]100%|██████████| 2/2 [00:01<00:00,  1.85ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.30ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.61ba/s]100%|██████████| 4/4 [00:01<00:00,  5.39ba/s]100%|██████████| 4/4 [00:01<00:00,  3.93ba/s]
[INFO|trainer.py:414] 2023-08-28 18:26:17,160 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:26:18,212 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:26:18,212 >>   Num examples = 1532
[INFO|trainer.py:1149] 2023-08-28 18:26:18,212 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:26:18,212 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:26:18,212 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:26:18,212 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:26:18,212 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:01<02:26,  1.23s/it]  2%|▏         | 2/120 [00:02<01:58,  1.00s/it]  2%|▎         | 3/120 [00:02<01:19,  1.47it/s]  3%|▎         | 4/120 [00:02<01:01,  1.90it/s]  4%|▍         | 5/120 [00:02<00:50,  2.26it/s]  5%|▌         | 6/120 [00:03<00:44,  2.56it/s]  6%|▌         | 7/120 [00:03<00:40,  2.79it/s]  7%|▋         | 8/120 [00:03<00:37,  2.96it/s]  8%|▊         | 9/120 [00:04<00:35,  3.09it/s]  8%|▊         | 10/120 [00:04<00:39,  2.79it/s]  9%|▉         | 11/120 [00:04<00:36,  2.96it/s] 10%|█         | 12/120 [00:05<00:35,  3.08it/s] 11%|█         | 13/120 [00:06<00:53,  1.98it/s] 12%|█▏        | 14/120 [00:06<00:46,  2.27it/s] 12%|█▎        | 15/120 [00:06<00:41,  2.52it/s] 13%|█▎        | 16/120 [00:06<00:37,  2.74it/s] 14%|█▍        | 17/120 [00:07<00:35,  2.91it/s] 15%|█▌        | 18/120 [00:07<00:35,  2.86it/s] 16%|█▌        | 19/120 [00:07<00:33,  3.01it/s] 17%|█▋        | 20/120 [00:08<00:32,  3.12it/s] 18%|█▊        | 21/120 [00:08<00:30,  3.21it/s] 18%|█▊        | 22/120 [00:08<00:30,  3.27it/s] 19%|█▉        | 23/120 [00:09<00:29,  3.31it/s] 20%|██        | 24/120 [00:09<00:28,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 18:26:27,558 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:26:27,558 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:26:27,558 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.36it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.21it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.57it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.42it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.96it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.39it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.01it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/436 [00:01<00:10, 37.97it/s][A
 12%|█▏        | 51/436 [00:01<00:20, 18.77it/s][A
 13%|█▎        | 56/436 [00:01<00:16, 23.09it/s][A
 14%|█▍        | 60/436 [00:02<00:21, 17.74it/s][A
 15%|█▍        | 65/436 [00:02<00:16, 22.09it/s][A
 16%|█▌        | 70/436 [00:02<00:13, 26.33it/s][A
 17%|█▋        | 75/436 [00:02<00:11, 30.30it/s][A
 18%|█▊        | 80/436 [00:02<00:10, 33.70it/s][A
 19%|█▉        | 85/436 [00:02<00:09, 36.50it/s][A
 21%|██        | 90/436 [00:02<00:08, 38.82it/s][A
 22%|██▏       | 95/436 [00:02<00:08, 40.57it/s][A
 23%|██▎       | 100/436 [00:02<00:08, 41.46it/s][A
 24%|██▍       | 105/436 [00:03<00:07, 42.19it/s][A
 25%|██▌       | 110/436 [00:03<00:07, 42.80it/s][A
 26%|██▋       | 115/436 [00:03<00:07, 43.32it/s][A
 28%|██▊       | 120/436 [00:03<00:07, 43.97it/s][A
 29%|██▊       | 125/436 [00:03<00:07, 44.31it/s][A
 30%|██▉       | 130/436 [00:03<00:06, 44.62it/s][A
 31%|███       | 135/436 [00:03<00:06, 44.81it/s][A
 32%|███▏      | 140/436 [00:03<00:06, 44.75it/s][A
 33%|███▎      | 145/436 [00:04<00:06, 44.57it/s][A
 34%|███▍      | 150/436 [00:04<00:08, 33.45it/s][A
 36%|███▌      | 155/436 [00:04<00:07, 36.31it/s][A
 37%|███▋      | 160/436 [00:04<00:07, 38.64it/s][A
 38%|███▊      | 165/436 [00:04<00:06, 40.44it/s][A
 39%|███▉      | 170/436 [00:04<00:06, 41.71it/s][A
 40%|████      | 175/436 [00:04<00:07, 34.38it/s][A
 41%|████▏     | 180/436 [00:04<00:06, 37.31it/s][A
 42%|████▏     | 185/436 [00:05<00:06, 38.44it/s][A
 44%|████▎     | 190/436 [00:05<00:06, 40.28it/s][A
 45%|████▍     | 195/436 [00:05<00:05, 41.73it/s][A
 46%|████▌     | 200/436 [00:05<00:05, 42.72it/s][A
 47%|████▋     | 205/436 [00:05<00:05, 43.42it/s][A
 48%|████▊     | 210/436 [00:05<00:05, 44.00it/s][A
 49%|████▉     | 215/436 [00:05<00:05, 44.04it/s][A
 50%|█████     | 220/436 [00:05<00:04, 43.90it/s][A
 52%|█████▏    | 225/436 [00:05<00:04, 43.86it/s][A
 53%|█████▎    | 230/436 [00:06<00:04, 44.20it/s][A
 54%|█████▍    | 235/436 [00:06<00:04, 44.41it/s][A
 55%|█████▌    | 240/436 [00:06<00:04, 44.69it/s][A
 56%|█████▌    | 245/436 [00:06<00:04, 44.87it/s][A
 57%|█████▋    | 250/436 [00:06<00:04, 45.07it/s][A
 58%|█████▊    | 255/436 [00:06<00:04, 45.06it/s][A
 60%|█████▉    | 260/436 [00:06<00:03, 44.77it/s][A
 61%|██████    | 265/436 [00:06<00:04, 40.60it/s][A
 62%|██████▏   | 270/436 [00:07<00:03, 42.64it/s][A
 63%|██████▎   | 275/436 [00:07<00:03, 41.74it/s][A
 64%|██████▍   | 280/436 [00:07<00:03, 42.73it/s][A
 65%|██████▌   | 285/436 [00:07<00:03, 43.60it/s][A
 67%|██████▋   | 290/436 [00:07<00:03, 44.17it/s][A
 68%|██████▊   | 295/436 [00:07<00:03, 44.49it/s][A
 69%|██████▉   | 300/436 [00:07<00:05, 26.72it/s][A
 70%|██████▉   | 305/436 [00:08<00:04, 30.58it/s][A
 71%|███████   | 309/436 [00:08<00:04, 27.92it/s][A
 72%|███████▏  | 314/436 [00:08<00:03, 31.77it/s][A
 73%|███████▎  | 319/436 [00:08<00:03, 34.99it/s][A
 74%|███████▍  | 324/436 [00:08<00:02, 37.59it/s][A
 75%|███████▌  | 329/436 [00:08<00:02, 39.69it/s][A
 77%|███████▋  | 334/436 [00:08<00:02, 41.22it/s][A
 78%|███████▊  | 339/436 [00:08<00:02, 42.35it/s][A
 79%|███████▉  | 344/436 [00:09<00:02, 43.14it/s][A
 80%|████████  | 349/436 [00:09<00:02, 43.22it/s][A
 81%|████████  | 354/436 [00:09<00:01, 43.29it/s][A
 82%|████████▏ | 359/436 [00:09<00:01, 43.70it/s][A
 83%|████████▎ | 364/436 [00:09<00:01, 44.07it/s][A
 85%|████████▍ | 369/436 [00:09<00:01, 44.45it/s][A
 86%|████████▌ | 374/436 [00:09<00:01, 44.75it/s][A
 87%|████████▋ | 379/436 [00:09<00:01, 44.75it/s][A
 88%|████████▊ | 384/436 [00:09<00:01, 44.97it/s][A
 89%|████████▉ | 389/436 [00:10<00:01, 44.80it/s][A
 90%|█████████ | 394/436 [00:10<00:00, 44.55it/s][A
 92%|█████████▏| 399/436 [00:10<00:00, 42.27it/s][A
 93%|█████████▎| 404/436 [00:10<00:00, 43.04it/s][A
 94%|█████████▍| 409/436 [00:10<00:00, 43.64it/s][A
 95%|█████████▍| 414/436 [00:10<00:00, 44.13it/s][A
 96%|█████████▌| 419/436 [00:10<00:00, 44.37it/s][A
 97%|█████████▋| 424/436 [00:10<00:00, 44.71it/s][A
 98%|█████████▊| 429/436 [00:10<00:00, 44.75it/s][A
100%|█████████▉| 434/436 [00:11<00:00, 44.59it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:20<00:28,  3.39it/s]
100%|██████████| 436/436 [00:11<00:00, 44.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:26:38,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 18:26:39,796 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:26:47,210 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:26:48,357 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:26:49,721 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [00:59<24:07, 15.24s/it] 22%|██▏       | 26/120 [00:59<16:56, 10.81s/it] 22%|██▎       | 27/120 [01:00<11:51,  7.65s/it] 23%|██▎       | 28/120 [01:00<08:20,  5.45s/it] 24%|██▍       | 29/120 [01:00<05:54,  3.90s/it] 25%|██▌       | 30/120 [01:01<04:13,  2.82s/it] 26%|██▌       | 31/120 [01:01<03:03,  2.06s/it] 27%|██▋       | 32/120 [01:01<02:14,  1.53s/it] 28%|██▊       | 33/120 [01:01<01:40,  1.16s/it] 28%|██▊       | 34/120 [01:02<01:17,  1.11it/s] 29%|██▉       | 35/120 [01:02<01:00,  1.40it/s] 30%|███       | 36/120 [01:02<00:51,  1.62it/s] 31%|███       | 37/120 [01:03<00:43,  1.92it/s] 32%|███▏      | 38/120 [01:03<00:37,  2.21it/s] 32%|███▎      | 39/120 [01:03<00:32,  2.47it/s] 33%|███▎      | 40/120 [01:04<00:29,  2.69it/s] 34%|███▍      | 41/120 [01:04<00:27,  2.88it/s] 35%|███▌      | 42/120 [01:04<00:25,  3.02it/s] 36%|███▌      | 43/120 [01:04<00:24,  3.13it/s] 37%|███▋      | 44/120 [01:05<00:23,  3.21it/s] 38%|███▊      | 45/120 [01:05<00:30,  2.46it/s] 38%|███▊      | 46/120 [01:06<00:31,  2.32it/s] 39%|███▉      | 47/120 [01:06<00:28,  2.57it/s] 40%|████      | 48/120 [01:06<00:25,  2.81it/s][INFO|trainer.py:2140] 2023-08-28 18:27:25,190 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:27:25,190 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:27:25,190 >>   Batch size = 8
{'eval_loss': 1.0222519636154175, 'eval_runtime': 11.1061, 'eval_samples_per_second': 314.062, 'eval_steps_per_second': 39.258, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.72it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.09it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.47it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.40it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.87it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.33it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.93it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.80it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.90it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.97it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.16it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 45.12it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 45.10it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.94it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.82it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.75it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.55it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.73it/s][A
 22%|██▏       | 97/436 [00:02<00:08, 40.30it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 41.75it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 42.80it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.54it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.33it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.30it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.19it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.25it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.52it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.83it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.99it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 45.03it/s][A
 38%|███▊      | 167/436 [00:03<00:05, 44.91it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.73it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.56it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.42it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.63it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.87it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.96it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 45.16it/s][A
 49%|████▊     | 212/436 [00:04<00:04, 45.12it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.81it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.62it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.50it/s][A
 53%|█████▎    | 232/436 [00:05<00:05, 36.20it/s][A
 54%|█████▍    | 237/436 [00:05<00:05, 38.50it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 40.34it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 41.81it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 42.88it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.64it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.03it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.36it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.13it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.00it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.01it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.66it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.96it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.99it/s][A
 70%|███████   | 307/436 [00:06<00:02, 45.21it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 45.02it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.58it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.44it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.41it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.51it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.76it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.83it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 45.06it/s][A
 81%|████████  | 352/436 [00:07<00:01, 45.10it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.93it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.72it/s][A
 84%|████████▍ | 367/436 [00:08<00:02, 24.20it/s][A
 85%|████████▌ | 372/436 [00:08<00:02, 28.16it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 31.75it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 34.87it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 37.39it/s][A
 90%|████████▉ | 392/436 [00:09<00:01, 39.45it/s][A
 91%|█████████ | 397/436 [00:09<00:01, 29.21it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 33.92it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 36.58it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 38.72it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 40.42it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 41.74it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 42.77it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 43.55it/s][A                                                
                                                 [A 40%|████      | 48/120 [01:17<00:25,  2.81it/s]
100%|██████████| 436/436 [00:10<00:00, 43.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:27:35,663 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 18:27:35,943 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:27:46,725 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:47,410 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:47,723 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [02:01<19:34, 16.54s/it] 42%|████▏     | 50/120 [02:01<13:38, 11.69s/it] 42%|████▎     | 51/120 [02:01<09:30,  8.27s/it] 43%|████▎     | 52/120 [02:02<06:39,  5.88s/it] 44%|████▍     | 53/120 [02:02<04:41,  4.20s/it] 45%|████▌     | 54/120 [02:02<03:19,  3.03s/it] 46%|████▌     | 55/120 [02:03<02:23,  2.21s/it] 47%|████▋     | 56/120 [02:03<01:44,  1.63s/it] 48%|████▊     | 57/120 [02:03<01:17,  1.23s/it] 48%|████▊     | 58/120 [02:03<00:58,  1.05it/s] 49%|████▉     | 59/120 [02:04<00:48,  1.25it/s] 50%|█████     | 60/120 [02:04<00:38,  1.55it/s] 51%|█████     | 61/120 [02:05<00:31,  1.85it/s] 52%|█████▏    | 62/120 [02:05<00:27,  2.15it/s] 52%|█████▎    | 63/120 [02:05<00:25,  2.26it/s] 53%|█████▎    | 64/120 [02:05<00:22,  2.52it/s] 54%|█████▍    | 65/120 [02:06<00:20,  2.73it/s] 55%|█████▌    | 66/120 [02:06<00:18,  2.91it/s] 56%|█████▌    | 67/120 [02:06<00:17,  3.04it/s] 57%|█████▋    | 68/120 [02:07<00:16,  3.15it/s] 57%|█████▊    | 69/120 [02:07<00:19,  2.61it/s] 58%|█████▊    | 70/120 [02:07<00:17,  2.81it/s] 59%|█████▉    | 71/120 [02:08<00:16,  2.97it/s] 60%|██████    | 72/120 [02:08<00:15,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 18:28:26,777 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:28:26,777 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:28:26,777 >>   Batch size = 8
{'eval_loss': 0.9933077096939087, 'eval_runtime': 10.3071, 'eval_samples_per_second': 338.409, 'eval_steps_per_second': 42.301, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.32it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.25it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.34it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.47it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.82it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.33it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.06it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.86it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.98it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.12it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.20it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.98it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.99it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.97it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.82it/s][A
 19%|█▉        | 82/436 [00:02<00:12, 28.99it/s][A
 20%|█▉        | 87/436 [00:02<00:10, 32.58it/s][A
 21%|██        | 92/436 [00:02<00:09, 35.57it/s][A
 22%|██▏       | 97/436 [00:02<00:08, 37.97it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 40.00it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 41.45it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 42.65it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.31it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.43it/s][A
 29%|██▉       | 127/436 [00:03<00:07, 43.50it/s][A
 30%|███       | 132/436 [00:03<00:06, 43.86it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.19it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.57it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.86it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.10it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 45.15it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 45.02it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.71it/s][A
 39%|███▉      | 172/436 [00:04<00:05, 44.43it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.57it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.59it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.73it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.92it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 45.13it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 45.24it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 45.13it/s][A
 49%|████▊     | 212/436 [00:05<00:07, 29.07it/s][A
 50%|████▉     | 217/436 [00:05<00:06, 32.56it/s][A
 51%|█████     | 222/436 [00:05<00:06, 35.53it/s][A
 52%|█████▏    | 227/436 [00:05<00:05, 37.93it/s][A
 53%|█████▎    | 232/436 [00:05<00:05, 39.97it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 41.47it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 42.61it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.37it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.35it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 43.54it/s][A
 60%|██████    | 262/436 [00:06<00:03, 43.79it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.21it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.51it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.67it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.66it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.97it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.99it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.68it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 44.41it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.51it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.65it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.85it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.86it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 45.01it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 45.07it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.71it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 43.32it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 43.85it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.10it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.47it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.64it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.81it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.87it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.57it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.66it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.75it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.76it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.74it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.79it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.88it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.97it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.88it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.57it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.74it/s][A                                                
                                                 [A 60%|██████    | 72/120 [02:18<00:15,  3.13it/s]
100%|██████████| 436/436 [00:10<00:00, 44.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:28:37,441 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 18:28:37,789 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:28:41,863 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:28:42,006 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:28:42,066 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [02:40<07:39,  9.77s/it] 62%|██████▏   | 74/120 [02:40<05:19,  6.94s/it] 62%|██████▎   | 75/120 [02:41<03:42,  4.94s/it] 63%|██████▎   | 76/120 [02:41<02:36,  3.55s/it] 64%|██████▍   | 77/120 [02:41<01:50,  2.57s/it] 65%|██████▌   | 78/120 [02:41<01:19,  1.89s/it] 66%|██████▌   | 79/120 [02:42<00:57,  1.41s/it] 67%|██████▋   | 80/120 [02:42<00:42,  1.07s/it] 68%|██████▊   | 81/120 [02:42<00:32,  1.20it/s] 68%|██████▊   | 82/120 [02:43<00:25,  1.49it/s] 69%|██████▉   | 83/120 [02:43<00:20,  1.80it/s] 70%|███████   | 84/120 [02:43<00:17,  2.10it/s] 71%|███████   | 85/120 [02:44<00:16,  2.13it/s] 72%|███████▏  | 86/120 [02:44<00:14,  2.41it/s] 72%|███████▎  | 87/120 [02:44<00:12,  2.65it/s] 73%|███████▎  | 88/120 [02:44<00:11,  2.86it/s] 74%|███████▍  | 89/120 [02:45<00:10,  3.02it/s] 75%|███████▌  | 90/120 [02:45<00:09,  3.14it/s] 76%|███████▌  | 91/120 [02:45<00:08,  3.24it/s] 77%|███████▋  | 92/120 [02:46<00:08,  3.30it/s] 78%|███████▊  | 93/120 [02:46<00:08,  3.35it/s] 78%|███████▊  | 94/120 [02:46<00:07,  3.38it/s] 79%|███████▉  | 95/120 [02:47<00:08,  2.86it/s] 80%|████████  | 96/120 [02:47<00:07,  3.06it/s][INFO|trainer.py:2140] 2023-08-28 18:29:05,613 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:29:05,613 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:29:05,613 >>   Batch size = 8
{'eval_loss': 1.000370740890503, 'eval_runtime': 10.1306, 'eval_samples_per_second': 344.304, 'eval_steps_per_second': 43.038, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.44it/s][A
  4%|▍         | 18/436 [00:00<00:08, 46.89it/s][A
  5%|▌         | 23/436 [00:00<00:09, 45.76it/s][A
  6%|▋         | 28/436 [00:00<00:08, 45.38it/s][A
  8%|▊         | 33/436 [00:00<00:08, 45.13it/s][A
  9%|▊         | 38/436 [00:00<00:08, 45.01it/s][A
 10%|▉         | 43/436 [00:00<00:08, 44.88it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.07it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.19it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 44.76it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 44.60it/s][A
 17%|█▋        | 73/436 [00:01<00:08, 44.53it/s][A
 18%|█▊        | 78/436 [00:01<00:08, 44.58it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 44.70it/s][A
 20%|██        | 88/436 [00:01<00:07, 44.77it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 44.94it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.05it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 44.99it/s][A
 25%|██▍       | 108/436 [00:02<00:10, 30.94it/s][A
 26%|██▌       | 113/436 [00:02<00:09, 34.18it/s][A
 27%|██▋       | 118/436 [00:02<00:08, 36.85it/s][A
 28%|██▊       | 123/436 [00:02<00:08, 39.06it/s][A
 29%|██▉       | 128/436 [00:02<00:07, 40.73it/s][A
 31%|███       | 133/436 [00:03<00:07, 42.00it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 42.87it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 43.48it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 43.56it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 43.83it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 163/436 [00:03<00:06, 44.47it/s][A
 39%|███▊      | 168/436 [00:03<00:06, 44.65it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 44.67it/s][A
 41%|████      | 178/436 [00:04<00:05, 44.74it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 44.86it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 44.75it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 44.56it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 44.44it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 44.57it/s][A
 48%|████▊     | 208/436 [00:04<00:05, 44.76it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 44.87it/s][A
 50%|█████     | 218/436 [00:05<00:04, 44.81it/s][A
 51%|█████     | 223/436 [00:05<00:04, 44.87it/s][A
 52%|█████▏    | 228/436 [00:05<00:04, 44.90it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 44.77it/s][A
 55%|█████▍    | 238/436 [00:05<00:08, 23.35it/s][A
 56%|█████▌    | 243/436 [00:05<00:07, 27.34it/s][A
 57%|█████▋    | 248/436 [00:06<00:06, 31.02it/s][A
 58%|█████▊    | 253/436 [00:06<00:05, 34.27it/s][A
 59%|█████▉    | 258/436 [00:06<00:04, 37.01it/s][A
 60%|██████    | 263/436 [00:06<00:04, 39.05it/s][A
 61%|██████▏   | 268/436 [00:06<00:04, 40.78it/s][A
 63%|██████▎   | 273/436 [00:06<00:03, 41.94it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 42.41it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 42.80it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 43.36it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 43.87it/s][A
 68%|██████▊   | 298/436 [00:07<00:03, 44.34it/s][A
 69%|██████▉   | 303/436 [00:07<00:02, 44.53it/s][A
 71%|███████   | 308/436 [00:07<00:02, 44.82it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 44.87it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 44.68it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 44.47it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 44.35it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 44.46it/s][A
 78%|███████▊  | 338/436 [00:08<00:02, 44.67it/s][A
 79%|███████▊  | 343/436 [00:08<00:02, 44.82it/s][A
 80%|███████▉  | 348/436 [00:08<00:01, 44.86it/s][A
 81%|████████  | 353/436 [00:08<00:01, 44.93it/s][A
 82%|████████▏ | 358/436 [00:08<00:03, 25.16it/s][A
 83%|████████▎ | 363/436 [00:08<00:02, 29.08it/s][A
 84%|████████▍ | 368/436 [00:08<00:02, 32.57it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 35.55it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 38.03it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 39.97it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 41.39it/s][A
 90%|█████████ | 393/436 [00:09<00:01, 42.43it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 42.76it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 43.06it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 43.40it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 43.77it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 44.21it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 44.27it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 44.78it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 44.93it/s][A                                                
                                                 [A 80%|████████  | 96/120 [02:57<00:07,  3.06it/s]
100%|██████████| 436/436 [00:10<00:00, 44.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:29:17,457 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 18:29:20,238 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:29:35,145 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:29:35,304 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:29:35,388 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [03:35<05:39, 14.75s/it] 82%|████████▏ | 98/120 [03:36<03:49, 10.43s/it] 82%|████████▎ | 99/120 [03:36<02:35,  7.39s/it] 83%|████████▎ | 100/120 [03:36<01:45,  5.26s/it] 84%|████████▍ | 101/120 [03:37<01:11,  3.77s/it] 85%|████████▌ | 102/120 [03:37<00:49,  2.73s/it] 86%|████████▌ | 103/120 [03:37<00:33,  2.00s/it] 87%|████████▋ | 104/120 [03:37<00:23,  1.48s/it] 88%|████████▊ | 105/120 [03:38<00:16,  1.13s/it] 88%|████████▊ | 106/120 [03:38<00:12,  1.14it/s] 89%|████████▉ | 107/120 [03:38<00:09,  1.43it/s] 90%|█████████ | 108/120 [03:39<00:06,  1.73it/s] 91%|█████████ | 109/120 [03:39<00:05,  2.03it/s] 92%|█████████▏| 110/120 [03:39<00:05,  1.91it/s] 92%|█████████▎| 111/120 [03:40<00:04,  2.20it/s] 93%|█████████▎| 112/120 [03:40<00:03,  2.47it/s] 94%|█████████▍| 113/120 [03:40<00:02,  2.69it/s] 95%|█████████▌| 114/120 [03:41<00:02,  2.88it/s] 96%|█████████▌| 115/120 [03:41<00:01,  3.02it/s] 97%|█████████▋| 116/120 [03:41<00:01,  3.13it/s] 98%|█████████▊| 117/120 [03:41<00:00,  3.21it/s] 98%|█████████▊| 118/120 [03:42<00:00,  3.28it/s] 99%|█████████▉| 119/120 [03:42<00:00,  3.34it/s]100%|██████████| 120/120 [03:43<00:00,  2.57it/s][INFO|trainer.py:2140] 2023-08-28 18:30:01,384 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:30:01,385 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:30:01,385 >>   Batch size = 8
{'eval_loss': 1.003836989402771, 'eval_runtime': 10.519, 'eval_samples_per_second': 331.59, 'eval_steps_per_second': 41.449, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.53it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.16it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.41it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.70it/s][A
  6%|▌         | 27/436 [00:00<00:08, 46.26it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.51it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.92it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.68it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.80it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.83it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.93it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 45.09it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 45.22it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 45.26it/s][A
 18%|█▊        | 77/436 [00:01<00:07, 44.96it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.69it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.60it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.63it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.58it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.81it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 45.01it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 45.16it/s][A
 27%|██▋       | 117/436 [00:02<00:13, 24.23it/s][A
 28%|██▊       | 122/436 [00:03<00:11, 28.12it/s][A
 29%|██▉       | 127/436 [00:03<00:09, 31.72it/s][A
 30%|███       | 132/436 [00:03<00:08, 34.84it/s][A
 31%|███▏      | 137/436 [00:03<00:07, 37.45it/s][A
 33%|███▎      | 142/436 [00:03<00:07, 39.42it/s][A
 34%|███▎      | 147/436 [00:03<00:07, 41.06it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 42.17it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 42.65it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.07it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 43.62it/s][A
 39%|███▉      | 172/436 [00:04<00:05, 44.08it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.36it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.49it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.59it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.57it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.68it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.48it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.41it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 44.56it/s][A
 50%|████▉     | 217/436 [00:05<00:04, 44.75it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.69it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.57it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.53it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.48it/s][A
 56%|█████▌    | 242/436 [00:06<00:08, 21.66it/s][A
 57%|█████▋    | 247/436 [00:06<00:07, 25.64it/s][A
 58%|█████▊    | 252/436 [00:06<00:06, 29.49it/s][A
 59%|█████▉    | 257/436 [00:06<00:05, 32.95it/s][A
 60%|██████    | 262/436 [00:06<00:04, 35.86it/s][A
 61%|██████    | 267/436 [00:06<00:04, 38.20it/s][A
 62%|██████▏   | 272/436 [00:06<00:04, 40.03it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 41.44it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 42.06it/s][A
 66%|██████▌   | 287/436 [00:07<00:03, 42.70it/s][A
 67%|██████▋   | 292/436 [00:07<00:03, 43.24it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 43.75it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 44.22it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.45it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.31it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.75it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.63it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.52it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 44.36it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 44.50it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 44.60it/s][A
 80%|███████▉  | 347/436 [00:08<00:01, 44.68it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.92it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.87it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 37.39it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 39.42it/s][A
 85%|████████▌ | 372/436 [00:09<00:01, 41.00it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 42.26it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 43.17it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 43.78it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.35it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.38it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.10it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.90it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.00it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 44.28it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 44.59it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 44.66it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.88it/s][A                                                 
                                                 [A100%|██████████| 120/120 [03:53<00:00,  2.57it/s]
100%|██████████| 436/436 [00:10<00:00, 44.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:30:12,110 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 18:30:13,148 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:30:23,821 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:30:25,782 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:30:25,986 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:30:43,549 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:30:43,594 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48 (score: 0.9933077096939087).
                                                 100%|██████████| 120/120 [05:03<00:00,  2.57it/s]100%|██████████| 120/120 [05:03<00:00,  2.53s/it]
[INFO|trainer.py:1894] 2023-08-28 18:31:22,289 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 18:31:23,515 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:31:32,756 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:31:32,969 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:31:33,075 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:31:34,705 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   train_loss               =     0.6246
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   train_runtime            = 0:05:03.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   train_samples            =       1532
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   train_samples_per_second =     25.257
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:34,705 >>   train_steps_per_second   =      0.396
{'eval_loss': 1.005182147026062, 'eval_runtime': 10.5096, 'eval_samples_per_second': 331.886, 'eval_steps_per_second': 41.486, 'epoch': 5.0}
{'train_runtime': 303.2789, 'train_samples_per_second': 25.257, 'train_steps_per_second': 0.396, 'train_loss': 0.6246086120605469, 'epoch': 5.0}
08/28/2023 18:31:35 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:31:35,556 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:31:35,556 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 18:31:35,557 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 56.46it/s]  3%|▎         | 12/436 [00:00<00:08, 49.55it/s]  4%|▍         | 18/436 [00:00<00:10, 39.28it/s]  5%|▌         | 23/436 [00:00<00:09, 41.32it/s]  6%|▋         | 28/436 [00:00<00:09, 42.56it/s]  8%|▊         | 33/436 [00:00<00:09, 43.41it/s]  9%|▊         | 38/436 [00:00<00:09, 44.04it/s] 10%|▉         | 43/436 [00:00<00:08, 44.40it/s] 11%|█         | 48/436 [00:01<00:08, 44.73it/s] 12%|█▏        | 53/436 [00:01<00:08, 44.82it/s] 13%|█▎        | 58/436 [00:01<00:08, 44.61it/s] 14%|█▍        | 63/436 [00:01<00:08, 44.82it/s] 16%|█▌        | 68/436 [00:01<00:08, 45.06it/s] 17%|█▋        | 73/436 [00:01<00:08, 45.17it/s] 18%|█▊        | 78/436 [00:01<00:07, 45.31it/s] 19%|█▉        | 83/436 [00:01<00:07, 45.36it/s] 20%|██        | 88/436 [00:01<00:07, 45.45it/s] 21%|██▏       | 93/436 [00:02<00:07, 45.48it/s] 22%|██▏       | 98/436 [00:02<00:07, 45.22it/s] 24%|██▎       | 103/436 [00:02<00:07, 45.03it/s] 25%|██▍       | 108/436 [00:02<00:07, 45.10it/s] 26%|██▌       | 113/436 [00:02<00:07, 45.15it/s] 27%|██▋       | 118/436 [00:02<00:07, 45.17it/s] 28%|██▊       | 123/436 [00:02<00:06, 45.23it/s] 29%|██▉       | 128/436 [00:02<00:06, 45.39it/s] 31%|███       | 133/436 [00:02<00:06, 45.41it/s] 32%|███▏      | 138/436 [00:03<00:06, 45.30it/s] 33%|███▎      | 143/436 [00:03<00:06, 45.21it/s] 34%|███▍      | 148/436 [00:03<00:06, 45.11it/s] 35%|███▌      | 153/436 [00:03<00:07, 37.14it/s] 36%|███▌      | 158/436 [00:03<00:07, 39.45it/s] 37%|███▋      | 163/436 [00:03<00:06, 41.22it/s] 39%|███▊      | 168/436 [00:03<00:06, 42.53it/s] 40%|███▉      | 173/436 [00:03<00:06, 43.43it/s] 41%|████      | 178/436 [00:04<00:05, 44.11it/s] 42%|████▏     | 183/436 [00:04<00:05, 44.36it/s] 43%|████▎     | 188/436 [00:04<00:05, 44.79it/s] 44%|████▍     | 193/436 [00:04<00:05, 44.48it/s] 45%|████▌     | 198/436 [00:04<00:05, 44.20it/s] 47%|████▋     | 203/436 [00:04<00:05, 44.49it/s] 48%|████▊     | 208/436 [00:04<00:05, 44.82it/s] 49%|████▉     | 213/436 [00:04<00:04, 45.07it/s] 50%|█████     | 218/436 [00:04<00:04, 45.35it/s] 51%|█████     | 223/436 [00:05<00:04, 45.44it/s] 52%|█████▏    | 228/436 [00:05<00:04, 45.56it/s] 53%|█████▎    | 233/436 [00:05<00:04, 45.40it/s] 55%|█████▍    | 238/436 [00:05<00:04, 44.99it/s] 56%|█████▌    | 243/436 [00:05<00:04, 44.74it/s] 57%|█████▋    | 248/436 [00:05<00:04, 44.91it/s] 58%|█████▊    | 253/436 [00:05<00:04, 45.14it/s] 59%|█████▉    | 258/436 [00:05<00:03, 45.25it/s] 60%|██████    | 263/436 [00:05<00:03, 45.34it/s] 61%|██████▏   | 268/436 [00:06<00:03, 45.37it/s] 63%|██████▎   | 273/436 [00:06<00:03, 45.43it/s] 64%|██████▍   | 278/436 [00:06<00:03, 45.17it/s] 65%|██████▍   | 283/436 [00:06<00:03, 44.87it/s] 66%|██████▌   | 288/436 [00:06<00:03, 43.49it/s] 67%|██████▋   | 293/436 [00:06<00:03, 44.04it/s] 68%|██████▊   | 298/436 [00:06<00:03, 44.47it/s] 69%|██████▉   | 303/436 [00:06<00:02, 44.81it/s] 71%|███████   | 308/436 [00:06<00:02, 44.98it/s] 72%|███████▏  | 313/436 [00:07<00:02, 45.31it/s] 73%|███████▎  | 318/436 [00:07<00:02, 45.37it/s] 74%|███████▍  | 323/436 [00:09<00:14,  7.82it/s] 75%|███████▌  | 328/436 [00:09<00:10, 10.46it/s] 76%|███████▋  | 333/436 [00:09<00:07, 13.62it/s] 78%|███████▊  | 338/436 [00:09<00:05, 17.25it/s] 79%|███████▊  | 343/436 [00:09<00:04, 21.22it/s] 80%|███████▉  | 348/436 [00:09<00:03, 24.78it/s] 81%|████████  | 353/436 [00:09<00:02, 28.69it/s] 82%|████████▏ | 358/436 [00:09<00:02, 32.32it/s] 83%|████████▎ | 363/436 [00:09<00:02, 35.25it/s] 84%|████████▍ | 368/436 [00:10<00:01, 37.63it/s] 86%|████████▌ | 373/436 [00:10<00:01, 39.62it/s] 87%|████████▋ | 378/436 [00:10<00:01, 41.28it/s] 88%|████████▊ | 383/436 [00:10<00:01, 42.39it/s] 89%|████████▉ | 388/436 [00:10<00:01, 43.16it/s] 90%|█████████ | 393/436 [00:10<00:00, 43.62it/s] 91%|█████████▏| 398/436 [00:10<00:00, 44.23it/s] 92%|█████████▏| 403/436 [00:10<00:00, 44.59it/s] 94%|█████████▎| 408/436 [00:10<00:00, 44.58it/s] 95%|█████████▍| 413/436 [00:11<00:00, 44.60it/s] 96%|█████████▌| 418/436 [00:11<00:00, 44.68it/s] 97%|█████████▋| 423/436 [00:11<00:00, 44.96it/s] 98%|█████████▊| 428/436 [00:11<00:00, 45.07it/s] 99%|█████████▉| 433/436 [00:11<00:00, 45.07it/s]100%|██████████| 436/436 [00:11<00:00, 37.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:31:47,106 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,106 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,106 >>   eval_loss               =     0.9933
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,106 >>   eval_runtime            = 0:00:11.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,106 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,106 >>   eval_samples_per_second =    302.001
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,107 >>   eval_steps_per_second   =      37.75
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:47,107 >>   perplexity              =     2.7002
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:01,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:01,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:01,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:01,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:01,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:32:01,894 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:32:01,895 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:32:02,290 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:32:03,337 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:32:03,337 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:05,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:05,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:05,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:05,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:32:05,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:32:06,296 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:32:06,297 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:32:06,588 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:32:06,747 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:32:06,747 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-48
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-72
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-24
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-120
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.30it/s]Extractor Predicting: 17it [00:11,  1.38it/s]Extractor Predicting: 18it [00:11,  1.39it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.58it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:18,  1.62it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:19,  1.60it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.54it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:29,  1.49it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.34it/s]Extractor Predicting: 57it [00:37,  1.43it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.38it/s]Extractor Predicting: 62it [00:40,  1.42it/s]Extractor Predicting: 63it [00:41,  1.47it/s]Extractor Predicting: 64it [00:41,  1.48it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.41it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.51it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.47it/s]Extractor Predicting: 73it [00:48,  1.49it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.57it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:53,  1.55it/s]Extractor Predicting: 82it [00:53,  1.48it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.55it/s]Extractor Predicting: 87it [00:57,  1.43it/s]Extractor Predicting: 88it [00:58,  1.37it/s]Extractor Predicting: 89it [00:58,  1.43it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [00:59,  1.57it/s]Extractor Predicting: 92it [01:00,  1.58it/s]Extractor Predicting: 93it [01:01,  1.57it/s]Extractor Predicting: 94it [01:01,  1.61it/s]Extractor Predicting: 95it [01:02,  1.62it/s]Extractor Predicting: 96it [01:02,  1.62it/s]Extractor Predicting: 97it [01:03,  1.32it/s]Extractor Predicting: 98it [01:04,  1.38it/s]Extractor Predicting: 99it [01:05,  1.42it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.30it/s]Extractor Predicting: 102it [01:07,  1.41it/s]Extractor Predicting: 103it [01:08,  1.47it/s]Extractor Predicting: 104it [01:08,  1.53it/s]Extractor Predicting: 105it [01:09,  1.55it/s]Extractor Predicting: 106it [01:10,  1.45it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:11,  1.53it/s]Extractor Predicting: 109it [01:11,  1.56it/s]Extractor Predicting: 110it [01:12,  1.57it/s]Extractor Predicting: 111it [01:13,  1.47it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.61it/s]Extractor Predicting: 114it [01:15,  1.63it/s]Extractor Predicting: 115it [01:15,  1.65it/s]Extractor Predicting: 116it [01:16,  1.66it/s]Extractor Predicting: 117it [01:16,  1.64it/s]Extractor Predicting: 118it [01:17,  1.62it/s]Extractor Predicting: 119it [01:18,  1.55it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:19,  1.59it/s]Extractor Predicting: 122it [01:20,  1.60it/s]Extractor Predicting: 123it [01:20,  1.59it/s]Extractor Predicting: 124it [01:21,  1.52it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:22,  1.51it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:23,  1.60it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:25,  1.56it/s]Extractor Predicting: 131it [01:25,  1.56it/s]Extractor Predicting: 132it [01:26,  1.57it/s]Extractor Predicting: 133it [01:27,  1.57it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:28,  1.52it/s]Extractor Predicting: 136it [01:29,  1.55it/s]Extractor Predicting: 137it [01:29,  1.57it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:31,  1.54it/s]Extractor Predicting: 140it [01:31,  1.58it/s]Extractor Predicting: 141it [01:32,  1.57it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 143it [01:33,  1.57it/s]Extractor Predicting: 144it [01:33,  1.88it/s]Extractor Predicting: 144it [01:33,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:05,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:05,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:05,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:05,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:05,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:34:05,993 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:34:05,994 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:34:06,788 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:34:08,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:34:08,124 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:12,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:12,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:12,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:12,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:12,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:34:13,930 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:34:13,932 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:34:14,674 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:34:14,897 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:34:14,897 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.24725877192982457,
  "recall": 0.12930045871559634,
  "score": 0.1698042168674699,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.34it/s]Extractor Predicting: 17it [00:11,  1.38it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.58it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:22,  1.67it/s]Extractor Predicting: 35it [00:22,  1.72it/s]Extractor Predicting: 36it [00:23,  1.73it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:24,  1.68it/s]Extractor Predicting: 39it [00:25,  1.74it/s]Extractor Predicting: 40it [00:25,  1.79it/s]Extractor Predicting: 41it [00:26,  1.78it/s]Extractor Predicting: 42it [00:26,  1.74it/s]Extractor Predicting: 43it [00:27,  1.74it/s]Extractor Predicting: 44it [00:27,  1.80it/s]Extractor Predicting: 45it [00:28,  1.80it/s]Extractor Predicting: 46it [00:29,  1.75it/s]Extractor Predicting: 47it [00:29,  1.68it/s]Extractor Predicting: 48it [00:30,  1.75it/s]Extractor Predicting: 49it [00:30,  1.79it/s]Extractor Predicting: 50it [00:31,  1.79it/s]Extractor Predicting: 51it [00:31,  1.83it/s]Extractor Predicting: 52it [00:32,  1.81it/s]Extractor Predicting: 53it [00:33,  1.46it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.58it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:35,  1.67it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:37,  1.54it/s]Extractor Predicting: 61it [00:38,  1.52it/s]Extractor Predicting: 62it [00:39,  1.49it/s]Extractor Predicting: 63it [00:39,  1.36it/s]Extractor Predicting: 64it [00:40,  1.39it/s]Extractor Predicting: 65it [00:41,  1.46it/s]Extractor Predicting: 66it [00:41,  1.49it/s]Extractor Predicting: 67it [00:42,  1.51it/s]Extractor Predicting: 68it [00:43,  1.43it/s]Extractor Predicting: 69it [00:44,  1.44it/s]Extractor Predicting: 70it [00:44,  1.45it/s]Extractor Predicting: 71it [00:45,  1.43it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:46,  1.32it/s]Extractor Predicting: 74it [00:47,  1.39it/s]Extractor Predicting: 75it [00:48,  1.44it/s]Extractor Predicting: 76it [00:48,  1.47it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:50,  1.41it/s]Extractor Predicting: 79it [00:51,  1.42it/s]Extractor Predicting: 80it [00:51,  1.45it/s]Extractor Predicting: 81it [00:52,  1.46it/s]Extractor Predicting: 82it [00:53,  1.47it/s]Extractor Predicting: 83it [00:53,  1.40it/s]Extractor Predicting: 84it [00:54,  1.46it/s]Extractor Predicting: 85it [00:55,  1.47it/s]Extractor Predicting: 86it [00:55,  1.45it/s]Extractor Predicting: 87it [00:56,  1.47it/s]Extractor Predicting: 88it [00:57,  1.27it/s]Extractor Predicting: 89it [00:58,  1.34it/s]Extractor Predicting: 90it [00:58,  1.30it/s]Extractor Predicting: 91it [00:59,  1.34it/s]Extractor Predicting: 92it [01:00,  1.12it/s]Extractor Predicting: 93it [01:01,  1.23it/s]Extractor Predicting: 94it [01:02,  1.31it/s]Extractor Predicting: 95it [01:02,  1.40it/s]Extractor Predicting: 96it [01:03,  1.35it/s]Extractor Predicting: 97it [01:04,  1.41it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:06,  1.54it/s]Extractor Predicting: 101it [01:06,  1.39it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.50it/s]Extractor Predicting: 107it [01:10,  1.53it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:12,  1.53it/s]Extractor Predicting: 110it [01:12,  1.52it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:14,  1.52it/s]Extractor Predicting: 113it [01:14,  1.55it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.54it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:17,  1.57it/s]Extractor Predicting: 118it [01:17,  1.64it/s]Extractor Predicting: 119it [01:18,  1.67it/s]Extractor Predicting: 120it [01:18,  1.69it/s]Extractor Predicting: 121it [01:19,  1.72it/s]Extractor Predicting: 122it [01:20,  1.61it/s]Extractor Predicting: 123it [01:20,  1.66it/s]Extractor Predicting: 124it [01:21,  1.69it/s]Extractor Predicting: 125it [01:21,  1.72it/s]Extractor Predicting: 126it [01:22,  1.71it/s]Extractor Predicting: 127it [01:23,  1.71it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.59it/s]Extractor Predicting: 130it [01:24,  1.71it/s]Extractor Predicting: 131it [01:25,  1.77it/s]Extractor Predicting: 132it [01:26,  1.74it/s]Extractor Predicting: 133it [01:26,  1.71it/s]Extractor Predicting: 134it [01:27,  1.44it/s]Extractor Predicting: 135it [01:28,  1.39it/s]Extractor Predicting: 136it [01:28,  1.51it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:30,  1.65it/s]Extractor Predicting: 139it [01:30,  1.65it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:31,  1.64it/s]Extractor Predicting: 142it [01:32,  1.72it/s]Extractor Predicting: 143it [01:33,  1.73it/s]Extractor Predicting: 144it [01:33,  1.73it/s]Extractor Predicting: 145it [01:34,  1.72it/s]Extractor Predicting: 146it [01:34,  1.56it/s]Extractor Predicting: 147it [01:35,  1.58it/s]Extractor Predicting: 148it [01:36,  1.61it/s]Extractor Predicting: 149it [01:36,  1.60it/s]Extractor Predicting: 150it [01:37,  1.60it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:38,  1.57it/s]Extractor Predicting: 153it [01:39,  1.55it/s]Extractor Predicting: 154it [01:39,  1.59it/s]Extractor Predicting: 155it [01:40,  1.63it/s]Extractor Predicting: 156it [01:41,  1.36it/s]Extractor Predicting: 157it [01:42,  1.44it/s]Extractor Predicting: 158it [01:42,  1.47it/s]Extractor Predicting: 159it [01:43,  1.50it/s]Extractor Predicting: 160it [01:44,  1.52it/s]Extractor Predicting: 161it [01:44,  1.43it/s]Extractor Predicting: 162it [01:45,  1.46it/s]Extractor Predicting: 163it [01:46,  1.52it/s]Extractor Predicting: 164it [01:46,  1.55it/s]Extractor Predicting: 165it [01:47,  1.55it/s]Extractor Predicting: 166it [01:48,  1.45it/s]Extractor Predicting: 167it [01:48,  1.51it/s]Extractor Predicting: 168it [01:49,  1.52it/s]Extractor Predicting: 169it [01:50,  1.56it/s]Extractor Predicting: 170it [01:50,  1.56it/s]Extractor Predicting: 171it [01:51,  1.53it/s]Extractor Predicting: 172it [01:51,  1.57it/s]Extractor Predicting: 173it [01:52,  1.55it/s]Extractor Predicting: 174it [01:53,  1.56it/s]Extractor Predicting: 175it [01:53,  1.59it/s]Extractor Predicting: 176it [01:54,  1.54it/s]Extractor Predicting: 177it [01:55,  1.53it/s]Extractor Predicting: 178it [01:55,  1.56it/s]Extractor Predicting: 179it [01:56,  1.57it/s]Extractor Predicting: 180it [01:57,  1.57it/s]Extractor Predicting: 181it [01:57,  1.55it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:59,  1.56it/s]Extractor Predicting: 184it [01:59,  1.57it/s]Extractor Predicting: 185it [02:00,  1.59it/s]Extractor Predicting: 186it [02:00,  1.56it/s]Extractor Predicting: 187it [02:01,  1.59it/s]Extractor Predicting: 188it [02:02,  1.46it/s]Extractor Predicting: 189it [02:02,  1.52it/s]Extractor Predicting: 190it [02:03,  1.39it/s]Extractor Predicting: 191it [02:04,  1.46it/s]Extractor Predicting: 192it [02:05,  1.50it/s]Extractor Predicting: 193it [02:05,  1.53it/s]Extractor Predicting: 194it [02:06,  1.54it/s]Extractor Predicting: 195it [02:06,  1.56it/s]Extractor Predicting: 196it [02:07,  1.57it/s]Extractor Predicting: 197it [02:08,  1.58it/s]Extractor Predicting: 198it [02:09,  1.45it/s]Extractor Predicting: 199it [02:09,  1.49it/s]Extractor Predicting: 200it [02:10,  1.52it/s]Extractor Predicting: 201it [02:10,  1.58it/s]Extractor Predicting: 202it [02:11,  1.56it/s]Extractor Predicting: 203it [02:12,  1.36it/s]Extractor Predicting: 204it [02:13,  1.42it/s]Extractor Predicting: 205it [02:13,  1.46it/s]Extractor Predicting: 206it [02:14,  1.55it/s]Extractor Predicting: 207it [02:14,  1.58it/s]Extractor Predicting: 208it [02:15,  1.51it/s]Extractor Predicting: 209it [02:16,  1.54it/s]Extractor Predicting: 210it [02:16,  1.56it/s]Extractor Predicting: 211it [02:17,  1.56it/s]Extractor Predicting: 212it [02:18,  1.54it/s]Extractor Predicting: 213it [02:18,  1.52it/s]Extractor Predicting: 214it [02:19,  1.53it/s]Extractor Predicting: 215it [02:20,  1.53it/s]Extractor Predicting: 216it [02:20,  1.54it/s]Extractor Predicting: 217it [02:21,  1.51it/s]Extractor Predicting: 218it [02:22,  1.33it/s]Extractor Predicting: 219it [02:23,  1.40it/s]Extractor Predicting: 220it [02:23,  1.47it/s]Extractor Predicting: 221it [02:24,  1.50it/s]Extractor Predicting: 222it [02:24,  1.52it/s]Extractor Predicting: 223it [02:25,  1.44it/s]Extractor Predicting: 224it [02:26,  1.47it/s]Extractor Predicting: 225it [02:27,  1.49it/s]Extractor Predicting: 226it [02:27,  1.53it/s]Extractor Predicting: 227it [02:28,  1.53it/s]Extractor Predicting: 228it [02:29,  1.46it/s]Extractor Predicting: 229it [02:29,  1.48it/s]Extractor Predicting: 230it [02:30,  1.52it/s]Extractor Predicting: 231it [02:30,  1.51it/s]Extractor Predicting: 232it [02:31,  1.52it/s]Extractor Predicting: 233it [02:32,  1.53it/s]Extractor Predicting: 234it [02:32,  1.52it/s]Extractor Predicting: 235it [02:33,  1.55it/s]Extractor Predicting: 236it [02:34,  1.57it/s]Extractor Predicting: 237it [02:34,  1.51it/s]Extractor Predicting: 238it [02:35,  1.50it/s]Extractor Predicting: 239it [02:36,  1.38it/s]Extractor Predicting: 240it [02:37,  1.43it/s]Extractor Predicting: 241it [02:37,  1.46it/s]Extractor Predicting: 242it [02:38,  1.50it/s]Extractor Predicting: 243it [02:38,  1.52it/s]Extractor Predicting: 244it [02:39,  1.34it/s]Extractor Predicting: 245it [02:40,  1.40it/s]Extractor Predicting: 246it [02:41,  1.42it/s]Extractor Predicting: 247it [02:41,  1.41it/s]Extractor Predicting: 248it [02:42,  1.45it/s]Extractor Predicting: 249it [02:43,  1.50it/s]Extractor Predicting: 250it [02:43,  1.54it/s]Extractor Predicting: 251it [02:44,  1.54it/s]Extractor Predicting: 252it [02:45,  1.57it/s]Extractor Predicting: 253it [02:45,  1.59it/s]Extractor Predicting: 254it [02:46,  1.53it/s]Extractor Predicting: 255it [02:47,  1.54it/s]Extractor Predicting: 256it [02:47,  1.52it/s]Extractor Predicting: 257it [02:48,  1.53it/s]Extractor Predicting: 258it [02:49,  1.53it/s]Extractor Predicting: 259it [02:49,  1.51it/s]Extractor Predicting: 260it [02:50,  1.54it/s]Extractor Predicting: 261it [02:50,  1.57it/s]Extractor Predicting: 262it [02:51,  1.58it/s]Extractor Predicting: 263it [02:52,  1.57it/s]Extractor Predicting: 264it [02:52,  1.52it/s]Extractor Predicting: 265it [02:53,  1.54it/s]Extractor Predicting: 266it [02:54,  1.56it/s]Extractor Predicting: 267it [02:54,  1.60it/s]Extractor Predicting: 268it [02:55,  1.60it/s]Extractor Predicting: 269it [02:56,  1.51it/s]Extractor Predicting: 270it [02:56,  1.51it/s]Extractor Predicting: 271it [02:57,  1.57it/s]Extractor Predicting: 272it [02:57,  1.59it/s]Extractor Predicting: 273it [02:58,  1.62it/s]Extractor Predicting: 274it [02:59,  1.62it/s]Extractor Predicting: 275it [02:59,  1.64it/s]Extractor Predicting: 276it [03:00,  1.59it/s]Extractor Predicting: 277it [03:01,  1.60it/s]Extractor Predicting: 278it [03:01,  1.62it/s]Extractor Predicting: 279it [03:02,  1.62it/s]Extractor Predicting: 280it [03:02,  1.60it/s]Extractor Predicting: 281it [03:03,  1.61it/s]Extractor Predicting: 282it [03:04,  1.60it/s]Extractor Predicting: 283it [03:04,  1.56it/s]Extractor Predicting: 284it [03:05,  1.81it/s]Extractor Predicting: 284it [03:05,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:50,259 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:50,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:50,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:50,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:50,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:37:51,646 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:37:51,647 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:37:52,439 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:37:53,606 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:37:53,718 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:55,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:55,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:55,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:55,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:55,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:37:56,573 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:37:56,574 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:37:57,072 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:37:57,396 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:37:57,396 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2921061041866411,
  "recall": 0.13437224345780652,
  "score": 0.18407008357667906,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:02,  2.02it/s]Extractor Predicting: 5it [00:02,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:38:04,269 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:38:04,270 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:38:04,459 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:38:04,460 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:38:04,510 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:38:42,695 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:38:42,753 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:38:43,167 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:38:43,168 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:38:43,276 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:38:43,348 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2682926829268293,
  "recall": 0.05555555555555555,
  "score": 0.09205020920502091,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:38:43,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:44,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:44,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:45,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:46,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:48,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:49,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:49,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:50,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:52,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:53,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:53,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:54,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:55,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:56,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:57,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:57,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:58,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:58,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:59,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:59,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:54, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:00,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:01,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:01,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:02,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:02,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:03,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:03,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:04,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:06,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:06,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:07,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:09,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:09,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:11,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:11,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:01, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:12,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:13,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:14,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:14,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:15,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:16,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:16,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:17,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:18,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:18,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:19,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:20,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:20,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:21,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:22,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:23,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:24,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:25,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:26,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:26,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:27,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:27,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:58, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:28,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:29,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:29,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:30,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:30,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:32,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:32,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:33,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:33,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:34,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:35,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:36,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:36,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:37,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:38,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:39,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:39,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:40,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:40,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:41,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:42,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:59<02:41, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:42,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:43,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:44,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:44,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:45,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:45,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:46,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:46,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:47,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:48,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:49,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:50,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:50,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:51,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:52,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:53,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:53,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:54,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:54,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:55,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:55,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:56,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:13<02:24, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:57,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:57,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:58,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:58,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:59,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:00,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:00,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:01,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:02,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:02,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:03,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:03,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:04,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:05,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:06,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:06,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:07,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:07,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:08,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:09,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:09,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:10,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:11,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:11,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:12,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:29<02:15, 15.02s/it][WARNING|generation_utils.py:914] 2023-08-28 18:40:13,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:13,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:14,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:14,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:16,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:16,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:17,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:17,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:18,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:19,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:19,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:20,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:20,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:21,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:22,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:23,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:23,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:24,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:25,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:25,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:26,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:27,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:28,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:28,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:45<02:02, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-28 18:40:29,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:30,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:30,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:31,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:32,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:33,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:33,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:34,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:35,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:36,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:37,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:38,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:38,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:39,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:39,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:41,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:41,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:42,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:43,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:43,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:45,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:45,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:46,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:46,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:04<01:55, 16.49s/it][WARNING|generation_utils.py:914] 2023-08-28 18:40:48,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:48,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:49,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:50,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:50,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:51,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:52,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:52,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:53,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:54,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:55,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:55,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:56,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:57,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:57,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:58,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:59,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:40:59,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:00,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:01,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:02,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:03,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:04,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:04,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:21<01:40, 16.74s/it][WARNING|generation_utils.py:914] 2023-08-28 18:41:05,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:05,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:06,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:07,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:07,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:08,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:09,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:10,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:10,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:11,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:12,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:12,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:13,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:14,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:15,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:15,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:17,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:17,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:18,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:18,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:20,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:21,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:37<01:23, 16.64s/it][WARNING|generation_utils.py:914] 2023-08-28 18:41:21,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:22,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:22,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:23,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:24,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:24,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:25,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:25,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:26,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:27,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:28,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:28,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:29,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:30,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:30,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:31,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:31,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:32,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:33,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:34,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:35,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:36,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:53<01:04, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 18:41:37,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:37,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:38,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:39,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:39,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:40,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:41,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:42,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:42,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:43,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:43,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:44,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:45,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:45,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:46,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:47,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:47,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:48,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:49,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:49,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:50,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:51,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:52,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:09<00:48, 16.11s/it][WARNING|generation_utils.py:914] 2023-08-28 18:41:52,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:53,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:54,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:54,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:55,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:55,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:56,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:57,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:57,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:58,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:59,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:41:59,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:00,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:00,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:01,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:02,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:03,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:03,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:05,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:06,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:06,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:07,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:08,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:24<00:32, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:42:08,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:09,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:09,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:10,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:11,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:11,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:12,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:13,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:13,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:14,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:15,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:15,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:16,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:16,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:17,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:18,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:18,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:19,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:20,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:21,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:21,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:22,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:39<00:15, 15.64s/it][WARNING|generation_utils.py:914] 2023-08-28 18:42:23,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:24,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:25,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:26,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:26,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:27,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:27,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:28,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:28,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:29,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:30,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:30,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:31,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:32,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:33,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:33,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:34,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:35,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:36,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:36,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:42:37,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:54<00:00, 15.37s/it]Generating: 100%|██████████| 15/15 [03:54<00:00, 15.63s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:51,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:51,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:51,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:51,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:51,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:42:51,838 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:42:51,839 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:42:52,145 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:42:53,338 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:42:53,339 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:54,862 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:54,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:54,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:54,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:54,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:42:56,129 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:42:56,375 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:42:56,828 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:42:57,181 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:42:57,181 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 524, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 566, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7307692307692307, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : competition class . Context : Following his promotion to the U.S. Open , he played in four career Major Finals in Los Angeles , including four finals at the United States Open , two at the 1994 Summer Open and two at the 1985 Summer Olympics . Head Entity : 1986 Summer Olympic , Tail Entity : U.S. Open .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 11307
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11407, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.33it/s]Extractor Estimating: 7it [00:04,  1.42it/s]Extractor Estimating: 8it [00:05,  1.42it/s]Extractor Estimating: 9it [00:06,  1.46it/s]Extractor Estimating: 10it [00:07,  1.42it/s]Extractor Estimating: 11it [00:07,  1.29it/s]Extractor Estimating: 12it [00:08,  1.34it/s]Extractor Estimating: 13it [00:09,  1.38it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:11,  1.36it/s]Extractor Estimating: 17it [00:12,  1.39it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:13,  1.50it/s]Extractor Estimating: 20it [00:14,  1.48it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:15,  1.49it/s]Extractor Estimating: 23it [00:16,  1.52it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:17,  1.54it/s]Extractor Estimating: 26it [00:17,  1.58it/s]Extractor Estimating: 27it [00:18,  1.66it/s]Extractor Estimating: 28it [00:19,  1.70it/s]Extractor Estimating: 29it [00:19,  1.80it/s]Extractor Estimating: 30it [00:20,  1.75it/s]Extractor Estimating: 31it [00:20,  1.82it/s]Extractor Estimating: 32it [00:21,  1.66it/s]Extractor Estimating: 33it [00:22,  1.41it/s]Extractor Estimating: 34it [00:22,  1.50it/s]Extractor Estimating: 35it [00:23,  1.55it/s]Extractor Estimating: 36it [00:24,  1.61it/s]Extractor Estimating: 37it [00:24,  1.73it/s]Extractor Estimating: 38it [00:25,  1.38it/s]Extractor Estimating: 39it [00:26,  1.50it/s]Extractor Estimating: 40it [00:26,  1.62it/s]Extractor Estimating: 41it [00:27,  1.62it/s]Extractor Estimating: 42it [00:27,  1.73it/s]Extractor Estimating: 43it [00:28,  1.73it/s]Extractor Estimating: 44it [00:28,  1.73it/s]Extractor Estimating: 45it [00:29,  1.80it/s]Extractor Estimating: 46it [00:29,  1.78it/s]Extractor Estimating: 47it [00:30,  1.86it/s]Extractor Estimating: 48it [00:30,  1.88it/s]Extractor Estimating: 49it [00:31,  1.48it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:33,  1.56it/s]Extractor Estimating: 52it [00:33,  1.48it/s]Extractor Estimating: 53it [00:34,  1.49it/s]Extractor Estimating: 54it [00:35,  1.40it/s]Extractor Estimating: 55it [00:35,  1.48it/s]Extractor Estimating: 56it [00:36,  1.50it/s]Extractor Estimating: 57it [00:37,  1.58it/s]Extractor Estimating: 58it [00:37,  1.60it/s]Extractor Estimating: 59it [00:38,  1.54it/s]Extractor Estimating: 60it [00:39,  1.60it/s]Extractor Estimating: 61it [00:39,  1.64it/s]Extractor Estimating: 62it [00:40,  1.66it/s]Extractor Estimating: 63it [00:40,  1.66it/s]Extractor Estimating: 64it [00:41,  1.58it/s]Extractor Estimating: 65it [00:42,  1.64it/s]Extractor Estimating: 66it [00:42,  1.48it/s]Extractor Estimating: 67it [00:43,  1.53it/s]Extractor Estimating: 68it [00:44,  1.51it/s]Extractor Estimating: 69it [00:45,  1.40it/s]Extractor Estimating: 70it [00:45,  1.49it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:46,  1.60it/s]Extractor Estimating: 73it [00:47,  1.62it/s]Extractor Estimating: 74it [00:48,  1.55it/s]Extractor Estimating: 75it [00:48,  1.62it/s]Extractor Estimating: 76it [00:49,  1.62it/s]Extractor Estimating: 77it [00:49,  1.61it/s]Extractor Estimating: 78it [00:50,  1.59it/s]Extractor Estimating: 79it [00:51,  1.51it/s]Extractor Estimating: 80it [00:51,  1.52it/s]Extractor Estimating: 81it [00:52,  1.52it/s]Extractor Estimating: 82it [00:53,  1.55it/s]Extractor Estimating: 83it [00:53,  1.60it/s]Extractor Estimating: 84it [00:54,  1.63it/s]Extractor Estimating: 85it [00:54,  1.63it/s]Extractor Estimating: 86it [00:55,  1.53it/s]Extractor Estimating: 87it [00:56,  1.56it/s]Extractor Estimating: 88it [00:56,  1.61it/s]Extractor Estimating: 89it [00:57,  1.61it/s]Extractor Estimating: 90it [00:58,  1.58it/s]Extractor Estimating: 91it [00:59,  1.41it/s]Extractor Estimating: 92it [00:59,  1.49it/s]Extractor Estimating: 93it [01:00,  1.46it/s]Extractor Estimating: 94it [01:00,  1.54it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:02,  1.45it/s]Extractor Estimating: 97it [01:03,  1.47it/s]Extractor Estimating: 98it [01:03,  1.51it/s]Extractor Estimating: 99it [01:04,  1.49it/s]Extractor Estimating: 100it [01:04,  1.54it/s]Extractor Estimating: 101it [01:05,  1.37it/s]Extractor Estimating: 102it [01:06,  1.44it/s]Extractor Estimating: 103it [01:07,  1.50it/s]Extractor Estimating: 104it [01:07,  1.60it/s]Extractor Estimating: 105it [01:08,  1.62it/s]Extractor Estimating: 106it [01:08,  1.68it/s]Extractor Estimating: 107it [01:09,  1.70it/s]Extractor Estimating: 108it [01:09,  1.71it/s]Extractor Estimating: 109it [01:10,  1.68it/s]Extractor Estimating: 110it [01:11,  1.63it/s]Extractor Estimating: 111it [01:11,  1.53it/s]Extractor Estimating: 112it [01:12,  1.45it/s]Extractor Estimating: 113it [01:13,  1.53it/s]Extractor Estimating: 114it [01:13,  1.56it/s]Extractor Estimating: 115it [01:14,  1.63it/s]Extractor Estimating: 116it [01:14,  1.65it/s]Extractor Estimating: 117it [01:15,  1.44it/s]Extractor Estimating: 118it [01:16,  1.50it/s]Extractor Estimating: 119it [01:17,  1.55it/s]Extractor Estimating: 120it [01:17,  1.64it/s]Extractor Estimating: 121it [01:18,  1.70it/s]Extractor Estimating: 122it [01:18,  1.66it/s]Extractor Estimating: 123it [01:19,  1.73it/s]Extractor Estimating: 124it [01:19,  1.75it/s]Extractor Estimating: 125it [01:20,  1.81it/s]Extractor Estimating: 126it [01:21,  1.74it/s]Extractor Estimating: 127it [01:21,  1.76it/s]Extractor Estimating: 128it [01:22,  1.71it/s]Extractor Estimating: 129it [01:22,  1.77it/s]Extractor Estimating: 130it [01:23,  1.61it/s]Extractor Estimating: 131it [01:24,  1.60it/s]Extractor Estimating: 132it [01:24,  1.68it/s]Extractor Estimating: 133it [01:25,  1.69it/s]Extractor Estimating: 134it [01:25,  1.77it/s]Extractor Estimating: 135it [01:26,  1.68it/s]Extractor Estimating: 136it [01:26,  1.71it/s]Extractor Estimating: 137it [01:27,  1.71it/s]Extractor Estimating: 138it [01:28,  1.59it/s]Extractor Estimating: 139it [01:28,  1.63it/s]Extractor Estimating: 140it [01:29,  1.50it/s]Extractor Estimating: 141it [01:30,  1.60it/s]Extractor Estimating: 142it [01:30,  1.67it/s]Extractor Estimating: 143it [01:31,  1.63it/s]Extractor Estimating: 144it [01:31,  1.66it/s]Extractor Estimating: 145it [01:32,  1.52it/s]Extractor Estimating: 146it [01:33,  1.56it/s]Extractor Estimating: 147it [01:33,  1.63it/s]Extractor Estimating: 148it [01:34,  1.67it/s]Extractor Estimating: 149it [01:35,  1.64it/s]Extractor Estimating: 150it [01:35,  1.49it/s]Extractor Estimating: 151it [01:36,  1.56it/s]Extractor Estimating: 152it [01:36,  1.63it/s]Extractor Estimating: 153it [01:37,  1.69it/s]Extractor Estimating: 154it [01:38,  1.71it/s]Extractor Estimating: 155it [01:38,  1.54it/s]Extractor Estimating: 156it [01:39,  1.56it/s]Extractor Estimating: 157it [01:40,  1.63it/s]Extractor Estimating: 158it [01:40,  1.62it/s]Extractor Estimating: 159it [01:41,  1.67it/s]Extractor Estimating: 160it [01:41,  1.59it/s]Extractor Estimating: 161it [01:42,  1.62it/s]Extractor Estimating: 162it [01:43,  1.70it/s]Extractor Estimating: 163it [01:43,  1.78it/s]Extractor Estimating: 164it [01:44,  1.76it/s]Extractor Estimating: 165it [01:44,  1.74it/s]Extractor Estimating: 166it [01:45,  1.50it/s]Extractor Estimating: 167it [01:46,  1.57it/s]Extractor Estimating: 168it [01:46,  1.62it/s]Extractor Estimating: 169it [01:47,  1.64it/s]Extractor Estimating: 170it [01:47,  1.67it/s]Extractor Estimating: 171it [01:48,  1.51it/s]Extractor Estimating: 172it [01:49,  1.38it/s]Extractor Estimating: 173it [01:50,  1.45it/s]Extractor Estimating: 174it [01:50,  1.56it/s]Extractor Estimating: 175it [01:51,  1.64it/s]Extractor Estimating: 176it [01:52,  1.46it/s]Extractor Estimating: 177it [01:52,  1.51it/s]Extractor Estimating: 178it [01:53,  1.62it/s]Extractor Estimating: 179it [01:53,  1.68it/s]Extractor Estimating: 180it [01:54,  1.61it/s]Extractor Estimating: 181it [01:55,  1.67it/s]Extractor Estimating: 182it [01:55,  1.64it/s]Extractor Estimating: 183it [01:56,  1.73it/s]Extractor Estimating: 184it [01:56,  1.72it/s]Extractor Estimating: 185it [01:57,  1.56it/s]Extractor Estimating: 186it [01:58,  1.57it/s]Extractor Estimating: 187it [01:58,  1.48it/s]Extractor Estimating: 188it [01:59,  1.53it/s]Extractor Estimating: 189it [02:00,  1.57it/s]Extractor Estimating: 190it [02:00,  1.57it/s]Extractor Estimating: 191it [02:01,  1.59it/s]Extractor Estimating: 192it [02:01,  1.65it/s]Extractor Estimating: 193it [02:02,  1.70it/s]Extractor Estimating: 194it [02:03,  1.61it/s]Extractor Estimating: 195it [02:03,  1.56it/s]Extractor Estimating: 196it [02:04,  1.62it/s]Extractor Estimating: 197it [02:04,  1.71it/s]Extractor Estimating: 198it [02:05,  1.62it/s]Extractor Estimating: 199it [02:06,  1.68it/s]Extractor Estimating: 200it [02:06,  1.72it/s]Extractor Estimating: 201it [02:07,  1.34it/s]Extractor Estimating: 202it [02:08,  1.42it/s]Extractor Estimating: 203it [02:09,  1.44it/s]Extractor Estimating: 204it [02:09,  1.46it/s]Extractor Estimating: 205it [02:10,  1.44it/s]Extractor Estimating: 206it [02:11,  1.47it/s]Extractor Estimating: 207it [02:11,  1.47it/s]Extractor Estimating: 208it [02:12,  1.45it/s]Extractor Estimating: 209it [02:13,  1.45it/s]Extractor Estimating: 210it [02:14,  1.36it/s]Extractor Estimating: 211it [02:14,  1.42it/s]Extractor Estimating: 212it [02:15,  1.43it/s]Extractor Estimating: 213it [02:16,  1.48it/s]Extractor Estimating: 214it [02:16,  1.50it/s]Extractor Estimating: 215it [02:17,  1.25it/s]Extractor Estimating: 216it [02:18,  1.34it/s]Extractor Estimating: 217it [02:19,  1.36it/s]Extractor Estimating: 218it [02:19,  1.40it/s]Extractor Estimating: 219it [02:20,  1.24it/s]Extractor Estimating: 220it [02:21,  1.30it/s]Extractor Estimating: 221it [02:22,  1.30it/s]Extractor Estimating: 222it [02:22,  1.42it/s]Extractor Estimating: 223it [02:23,  1.38it/s]Extractor Estimating: 224it [02:24,  1.42it/s]Extractor Estimating: 225it [02:25,  1.04it/s]Extractor Estimating: 226it [02:26,  1.17it/s]Extractor Estimating: 227it [02:26,  1.31it/s]Extractor Estimating: 228it [02:27,  1.39it/s]Extractor Estimating: 229it [02:28,  1.27it/s]Extractor Estimating: 230it [02:29,  1.37it/s]Extractor Estimating: 231it [02:29,  1.42it/s]Extractor Estimating: 232it [02:30,  1.52it/s]Extractor Estimating: 233it [02:30,  1.50it/s]Extractor Estimating: 234it [02:32,  1.25it/s]Extractor Estimating: 235it [02:32,  1.40it/s]Extractor Estimating: 236it [02:33,  1.51it/s]Extractor Estimating: 237it [02:33,  1.62it/s]Extractor Estimating: 238it [02:34,  1.68it/s]Extractor Estimating: 239it [02:35,  1.50it/s]Extractor Estimating: 240it [02:35,  1.50it/s]Extractor Estimating: 241it [02:36,  1.57it/s]Extractor Estimating: 242it [02:36,  1.60it/s]Extractor Estimating: 243it [02:37,  1.62it/s]Extractor Estimating: 244it [02:38,  1.50it/s]Extractor Estimating: 245it [02:38,  1.60it/s]Extractor Estimating: 246it [02:39,  1.62it/s]Extractor Estimating: 247it [02:39,  1.64it/s]Extractor Estimating: 248it [02:40,  1.67it/s]Extractor Estimating: 249it [02:41,  1.58it/s]Extractor Estimating: 250it [02:41,  1.58it/s]Extractor Estimating: 251it [02:42,  1.54it/s]Extractor Estimating: 252it [02:43,  1.57it/s]Extractor Estimating: 253it [02:43,  1.62it/s]Extractor Estimating: 254it [02:44,  1.52it/s]Extractor Estimating: 255it [02:45,  1.54it/s]Extractor Estimating: 256it [02:45,  1.61it/s]Extractor Estimating: 257it [02:46,  1.60it/s]Extractor Estimating: 258it [02:46,  1.62it/s]Extractor Estimating: 259it [02:47,  1.53it/s]Extractor Estimating: 260it [02:48,  1.56it/s]Extractor Estimating: 261it [02:48,  1.62it/s]Extractor Estimating: 262it [02:49,  1.63it/s]Extractor Estimating: 263it [02:50,  1.54it/s]Extractor Estimating: 264it [02:50,  1.42it/s]Extractor Estimating: 265it [02:51,  1.48it/s]Extractor Estimating: 266it [02:52,  1.50it/s]Extractor Estimating: 267it [02:52,  1.48it/s]Extractor Estimating: 268it [02:53,  1.51it/s]Extractor Estimating: 269it [02:54,  1.24it/s]Extractor Estimating: 270it [02:55,  1.35it/s]Extractor Estimating: 271it [02:55,  1.44it/s]Extractor Estimating: 272it [02:56,  1.48it/s]Extractor Estimating: 273it [02:57,  1.42it/s]Extractor Estimating: 274it [02:57,  1.47it/s]Extractor Estimating: 275it [02:58,  1.49it/s]Extractor Estimating: 276it [02:59,  1.56it/s]Extractor Estimating: 277it [02:59,  1.59it/s]Extractor Estimating: 278it [03:00,  1.64it/s]Extractor Estimating: 279it [03:00,  1.70it/s]Extractor Estimating: 280it [03:01,  1.74it/s]Extractor Estimating: 281it [03:02,  1.62it/s]Extractor Estimating: 282it [03:02,  1.68it/s]Extractor Estimating: 283it [03:03,  1.66it/s]Extractor Estimating: 284it [03:03,  1.73it/s]Extractor Estimating: 285it [03:04,  1.78it/s]Extractor Estimating: 286it [03:04,  1.71it/s]Extractor Estimating: 287it [03:05,  1.69it/s]Extractor Estimating: 288it [03:06,  1.74it/s]Extractor Estimating: 289it [03:06,  1.72it/s]Extractor Estimating: 290it [03:07,  1.73it/s]Extractor Estimating: 291it [03:07,  1.82it/s]Extractor Estimating: 292it [03:08,  1.72it/s]Extractor Estimating: 293it [03:09,  1.57it/s]Extractor Estimating: 294it [03:09,  1.53it/s]Extractor Estimating: 295it [03:10,  1.58it/s]Extractor Estimating: 296it [03:11,  1.64it/s]Extractor Estimating: 297it [03:11,  1.62it/s]Extractor Estimating: 298it [03:12,  1.50it/s]Extractor Estimating: 299it [03:13,  1.56it/s]Extractor Estimating: 300it [03:13,  1.67it/s]Extractor Estimating: 301it [03:14,  1.70it/s]Extractor Estimating: 302it [03:14,  1.70it/s]Extractor Estimating: 303it [03:15,  1.59it/s]Extractor Estimating: 304it [03:15,  1.63it/s]Extractor Estimating: 305it [03:16,  1.61it/s]Extractor Estimating: 306it [03:17,  1.64it/s]Extractor Estimating: 307it [03:17,  1.64it/s]Extractor Estimating: 308it [03:18,  1.46it/s]Extractor Estimating: 309it [03:19,  1.55it/s]Extractor Estimating: 310it [03:19,  1.56it/s]Extractor Estimating: 311it [03:20,  1.58it/s]Extractor Estimating: 312it [03:21,  1.57it/s]Extractor Estimating: 313it [03:21,  1.51it/s]Extractor Estimating: 314it [03:22,  1.53it/s]Extractor Estimating: 315it [03:23,  1.61it/s]Extractor Estimating: 316it [03:23,  1.60it/s]Extractor Estimating: 317it [03:24,  1.64it/s]Extractor Estimating: 318it [03:25,  1.44it/s]Extractor Estimating: 319it [03:25,  1.48it/s]Extractor Estimating: 320it [03:26,  1.55it/s]Extractor Estimating: 321it [03:26,  1.59it/s]Extractor Estimating: 322it [03:27,  1.62it/s]Extractor Estimating: 323it [03:28,  1.66it/s]Extractor Estimating: 324it [03:28,  1.71it/s]Extractor Estimating: 325it [03:29,  1.76it/s]Extractor Estimating: 326it [03:29,  1.68it/s]Extractor Estimating: 327it [03:30,  1.68it/s]Extractor Estimating: 328it [03:30,  1.71it/s]Extractor Estimating: 329it [03:31,  1.70it/s]Extractor Estimating: 330it [03:32,  1.67it/s]Extractor Estimating: 331it [03:32,  1.62it/s]Extractor Estimating: 332it [03:33,  1.66it/s]Extractor Estimating: 333it [03:33,  1.66it/s]Extractor Estimating: 334it [03:34,  1.60it/s]Extractor Estimating: 335it [03:35,  1.67it/s]Extractor Estimating: 336it [03:36,  1.49it/s]Extractor Estimating: 337it [03:36,  1.57it/s]Extractor Estimating: 338it [03:37,  1.64it/s]Extractor Estimating: 339it [03:37,  1.67it/s]Extractor Estimating: 340it [03:38,  1.64it/s]Extractor Estimating: 341it [03:39,  1.51it/s]Extractor Estimating: 342it [03:39,  1.54it/s]Extractor Estimating: 343it [03:40,  1.59it/s]Extractor Estimating: 344it [03:40,  1.61it/s]Extractor Estimating: 345it [03:41,  1.63it/s]Extractor Estimating: 346it [03:42,  1.60it/s]Extractor Estimating: 347it [03:42,  1.63it/s]Extractor Estimating: 348it [03:43,  1.68it/s]Extractor Estimating: 349it [03:44,  1.51it/s]Extractor Estimating: 350it [03:44,  1.47it/s]Extractor Estimating: 351it [03:45,  1.39it/s]Extractor Estimating: 352it [03:46,  1.42it/s]Extractor Estimating: 353it [03:46,  1.49it/s]Extractor Estimating: 354it [03:47,  1.47it/s]Extractor Estimating: 355it [03:48,  1.47it/s]Extractor Estimating: 356it [03:49,  1.47it/s]Extractor Estimating: 357it [03:49,  1.52it/s]Extractor Estimating: 358it [03:50,  1.52it/s]Extractor Estimating: 359it [03:50,  1.56it/s]Extractor Estimating: 360it [03:51,  1.57it/s]Extractor Estimating: 361it [03:52,  1.37it/s]Extractor Estimating: 362it [03:53,  1.42it/s]Extractor Estimating: 363it [03:53,  1.41it/s]Extractor Estimating: 364it [03:54,  1.44it/s]Extractor Estimating: 365it [03:55,  1.47it/s]Extractor Estimating: 366it [03:55,  1.42it/s]Extractor Estimating: 367it [03:56,  1.50it/s]Extractor Estimating: 368it [03:57,  1.58it/s]Extractor Estimating: 369it [03:57,  1.60it/s]Extractor Estimating: 370it [03:58,  1.59it/s]Extractor Estimating: 371it [03:58,  1.59it/s]Extractor Estimating: 372it [03:59,  1.49it/s]Extractor Estimating: 373it [04:00,  1.55it/s]Extractor Estimating: 374it [04:01,  1.33it/s]Extractor Estimating: 375it [04:02,  1.32it/s]Extractor Estimating: 375it [04:02,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:23,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:23,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:23,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:23,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:23,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:47:24,244 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:47:24,245 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:47:25,197 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:47:26,412 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:47:26,463 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:30,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:30,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:30,705 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:30,705 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:47:30,705 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:47:31,801 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:47:31,802 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:47:32,549 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:47:32,818 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:47:32,818 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:38:35,147 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:38:35,216 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3000 mean pseudo reward: 0.9716098172994878
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 14638
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14738, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14738, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.957, loss:504.3345
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 0.959, loss:405.3154
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.970, loss:384.3612
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 0.976, loss:327.6883
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 0.971, loss:312.9715
>> valid entity prec:0.5651, rec:0.5708, f1:0.5679
>> valid relation prec:0.2007, rec:0.1501, f1:0.1718
>> valid relation with NER prec:0.2007, rec:0.1501, f1:0.1718
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.968, loss:277.2093
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 0.977, loss:269.5445
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.969, loss:264.3031
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 0.991, loss:265.6772
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 0.990, loss:253.4108
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5423, rec:0.5976, f1:0.5686
>> valid relation prec:0.1868, rec:0.1309, f1:0.1539
>> valid relation with NER prec:0.1868, rec:0.1309, f1:0.1539
new max entity f1 on valid!
g_step 1100, step 100, avg_time 0.972, loss:231.6680
g_step 1200, step 75, avg_time 0.967, loss:235.5165
g_step 1300, step 50, avg_time 0.957, loss:214.3656
g_step 1400, step 25, avg_time 0.995, loss:209.9473
g_step 1500, step 125, avg_time 0.973, loss:196.9057
>> valid entity prec:0.5571, rec:0.5130, f1:0.5341
>> valid relation prec:0.1939, rec:0.1223, f1:0.1500
>> valid relation with NER prec:0.1939, rec:0.1223, f1:0.1500
g_step 1600, step 100, avg_time 0.973, loss:169.8181
g_step 1700, step 75, avg_time 0.973, loss:178.9091
g_step 1800, step 50, avg_time 0.964, loss:166.1017
g_step 1900, step 25, avg_time 0.974, loss:165.8536
g_step 2000, step 125, avg_time 0.978, loss:158.3884
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5564, rec:0.5903, f1:0.5728
>> valid relation prec:0.2118, rec:0.1573, f1:0.1805
>> valid relation with NER prec:0.2118, rec:0.1573, f1:0.1805
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 0.979, loss:139.3480
g_step 2200, step 75, avg_time 0.969, loss:139.1097
g_step 2300, step 50, avg_time 0.981, loss:124.5163
g_step 2400, step 25, avg_time 0.966, loss:125.0236
g_step 2500, step 125, avg_time 0.974, loss:132.4101
>> valid entity prec:0.5750, rec:0.5438, f1:0.5589
>> valid relation prec:0.1806, rec:0.1329, f1:0.1531
>> valid relation with NER prec:0.1806, rec:0.1329, f1:0.1531
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:38:35 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:38:35 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-38-35_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:38:39 - WARNING - datasets.builder -   Using custom data configuration default-13b4a26e71d55fb2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-13b4a26e71d55fb2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:38:47,232 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:38:47,289 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:38:47,289 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:38:47,290 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:38:47,525 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,670 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,671 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,671 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,671 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,671 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:38:47,671 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:38:49,598 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:38:52,904 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:38:52,904 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-13b4a26e71d55fb2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:01<00:02,  1.16s/ba] 67%|██████▋   | 2/3 [00:01<00:00,  1.69ba/s]100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]100%|██████████| 3/3 [00:01<00:00,  1.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.61ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.62ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.27ba/s]100%|██████████| 4/4 [00:01<00:00,  4.40ba/s]100%|██████████| 4/4 [00:01<00:00,  3.46ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:01,  1.16ba/s]100%|██████████| 3/3 [00:01<00:00,  3.44ba/s]100%|██████████| 3/3 [00:01<00:00,  2.88ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.81ba/s] 50%|█████     | 2/4 [00:00<00:00,  5.72ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.04ba/s]100%|██████████| 4/4 [00:00<00:00,  7.64ba/s]
[INFO|trainer.py:414] 2023-08-28 19:39:02,942 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:39:03,631 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:39:03,631 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-28 19:39:03,631 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:39:03,631 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:39:03,631 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:39:03,632 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:39:03,632 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:11,  3.25it/s]  1%|          | 2/235 [00:00<01:09,  3.35it/s]  1%|▏         | 3/235 [00:00<01:08,  3.38it/s]  2%|▏         | 4/235 [00:01<01:07,  3.40it/s]  2%|▏         | 5/235 [00:01<01:07,  3.41it/s]  3%|▎         | 6/235 [00:01<01:07,  3.41it/s]  3%|▎         | 7/235 [00:02<01:06,  3.42it/s]  3%|▎         | 8/235 [00:02<01:14,  3.03it/s]  4%|▍         | 9/235 [00:02<01:11,  3.14it/s]  4%|▍         | 10/235 [00:03<01:09,  3.22it/s]  5%|▍         | 11/235 [00:03<01:08,  3.29it/s]  5%|▌         | 12/235 [00:03<01:07,  3.32it/s]  6%|▌         | 13/235 [00:03<01:06,  3.35it/s]  6%|▌         | 14/235 [00:04<01:05,  3.38it/s]  6%|▋         | 15/235 [00:04<01:04,  3.39it/s]  7%|▋         | 16/235 [00:04<01:04,  3.40it/s]  7%|▋         | 17/235 [00:05<01:04,  3.40it/s]  8%|▊         | 18/235 [00:05<01:03,  3.40it/s]  8%|▊         | 19/235 [00:05<01:03,  3.41it/s]  9%|▊         | 20/235 [00:05<01:03,  3.41it/s]  9%|▉         | 21/235 [00:06<01:02,  3.41it/s]  9%|▉         | 22/235 [00:06<01:02,  3.41it/s] 10%|▉         | 23/235 [00:06<01:02,  3.41it/s] 10%|█         | 24/235 [00:07<01:01,  3.41it/s] 11%|█         | 25/235 [00:07<01:01,  3.41it/s] 11%|█         | 26/235 [00:07<01:01,  3.41it/s] 11%|█▏        | 27/235 [00:08<01:00,  3.42it/s] 12%|█▏        | 28/235 [00:08<01:00,  3.41it/s] 12%|█▏        | 29/235 [00:08<01:00,  3.42it/s] 13%|█▎        | 30/235 [00:08<01:00,  3.42it/s] 13%|█▎        | 31/235 [00:09<00:59,  3.41it/s] 14%|█▎        | 32/235 [00:09<00:59,  3.42it/s] 14%|█▍        | 33/235 [00:09<00:59,  3.42it/s] 14%|█▍        | 34/235 [00:10<00:58,  3.42it/s] 15%|█▍        | 35/235 [00:10<00:58,  3.42it/s] 15%|█▌        | 36/235 [00:10<00:58,  3.42it/s] 16%|█▌        | 37/235 [00:10<00:57,  3.42it/s] 16%|█▌        | 38/235 [00:11<00:57,  3.42it/s] 17%|█▋        | 39/235 [00:11<00:57,  3.43it/s] 17%|█▋        | 40/235 [00:11<00:56,  3.44it/s] 17%|█▋        | 41/235 [00:12<00:56,  3.45it/s] 18%|█▊        | 42/235 [00:12<00:55,  3.45it/s] 18%|█▊        | 43/235 [00:12<00:55,  3.45it/s] 19%|█▊        | 44/235 [00:12<00:55,  3.45it/s] 19%|█▉        | 45/235 [00:13<00:54,  3.46it/s] 20%|█▉        | 46/235 [00:13<00:54,  3.46it/s] 20%|██        | 47/235 [00:13<00:52,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 19:39:17,453 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:39:17,454 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:39:17,454 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.52it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.84it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.01it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.11it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.55it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.20it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.86it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.79it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.91it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 39.67it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 41.30it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.42it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.20it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.73it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.37it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.50it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.17it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.15it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.29it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.61it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.77it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.86it/s][A
 28%|██▊       | 122/436 [00:02<00:06, 44.98it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.94it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.81it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.50it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.47it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.79it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.94it/s][A
 38%|███▊      | 167/436 [00:03<00:05, 45.07it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.96it/s][A
 41%|████      | 177/436 [00:04<00:07, 35.58it/s][A
 42%|████▏     | 182/436 [00:04<00:07, 35.08it/s][A
 43%|████▎     | 187/436 [00:04<00:06, 38.21it/s][A
 44%|████▍     | 192/436 [00:04<00:06, 40.20it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 41.63it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.73it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.50it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.87it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.73it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.91it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.76it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.14it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.39it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.71it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.83it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 45.11it/s][A
 59%|█████▉    | 257/436 [00:05<00:03, 44.98it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.76it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.58it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.42it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.46it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.58it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.69it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.85it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.88it/s][A
 69%|██████▉   | 302/436 [00:07<00:07, 17.29it/s][A
 70%|███████   | 306/436 [00:07<00:07, 16.56it/s][A
 71%|███████▏  | 311/436 [00:07<00:05, 20.86it/s][A
 72%|███████▏  | 316/436 [00:07<00:04, 25.07it/s][A
 74%|███████▎  | 321/436 [00:08<00:03, 29.02it/s][A
 75%|███████▍  | 326/436 [00:08<00:03, 32.60it/s][A
 76%|███████▌  | 331/436 [00:08<00:02, 35.68it/s][A
 77%|███████▋  | 336/436 [00:08<00:02, 38.14it/s][A
 78%|███████▊  | 341/436 [00:09<00:06, 15.19it/s][A
 79%|███████▉  | 346/436 [00:09<00:04, 18.99it/s][A
 81%|████████  | 351/436 [00:09<00:03, 23.00it/s][A
 82%|████████▏ | 356/436 [00:09<00:02, 26.99it/s][A
 83%|████████▎ | 361/436 [00:09<00:02, 30.68it/s][A
 84%|████████▍ | 366/436 [00:09<00:02, 33.99it/s][A
 85%|████████▌ | 371/436 [00:09<00:01, 36.73it/s][A
 86%|████████▌ | 376/436 [00:09<00:01, 38.93it/s][A
 87%|████████▋ | 381/436 [00:10<00:01, 40.14it/s][A
 89%|████████▊ | 386/436 [00:10<00:01, 41.15it/s][A
 90%|████████▉ | 391/436 [00:10<00:01, 42.19it/s][A
 91%|█████████ | 396/436 [00:10<00:00, 42.86it/s][A
 92%|█████████▏| 401/436 [00:10<00:00, 43.58it/s][A
 93%|█████████▎| 406/436 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 411/436 [00:10<00:00, 44.42it/s][A
 95%|█████████▌| 416/436 [00:10<00:00, 44.72it/s][A
 97%|█████████▋| 421/436 [00:10<00:00, 44.51it/s][A
 98%|█████████▊| 426/436 [00:11<00:00, 44.37it/s][A
 99%|█████████▉| 431/436 [00:11<00:00, 44.31it/s][A
100%|██████████| 436/436 [00:11<00:00, 44.32it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:11<00:00, 44.32it/s][A 20%|██        | 47/235 [00:25<00:52,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:39:29,875 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 19:39:32,048 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:39:48,190 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:39:51,170 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:39:51,589 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [01:12<55:42, 17.87s/it] 21%|██        | 49/235 [01:13<39:28, 12.73s/it] 21%|██▏       | 50/235 [01:13<27:45,  9.00s/it] 22%|██▏       | 51/235 [01:14<19:35,  6.39s/it] 22%|██▏       | 52/235 [01:14<13:54,  4.56s/it] 23%|██▎       | 53/235 [01:14<09:56,  3.28s/it] 23%|██▎       | 54/235 [01:14<07:11,  2.38s/it] 23%|██▎       | 55/235 [01:15<05:16,  1.76s/it] 24%|██▍       | 56/235 [01:15<03:55,  1.32s/it] 24%|██▍       | 57/235 [01:15<02:59,  1.01s/it] 25%|██▍       | 58/235 [01:16<02:35,  1.14it/s] 25%|██▌       | 59/235 [01:16<02:03,  1.43it/s] 26%|██▌       | 60/235 [01:16<01:40,  1.74it/s] 26%|██▌       | 61/235 [01:17<01:25,  2.04it/s] 26%|██▋       | 62/235 [01:17<01:14,  2.33it/s] 27%|██▋       | 63/235 [01:17<01:06,  2.59it/s] 27%|██▋       | 64/235 [01:18<01:01,  2.80it/s] 28%|██▊       | 65/235 [01:18<00:57,  2.98it/s] 28%|██▊       | 66/235 [01:18<00:54,  3.11it/s] 29%|██▊       | 67/235 [01:18<00:52,  3.21it/s] 29%|██▉       | 68/235 [01:19<00:57,  2.92it/s] 29%|██▉       | 69/235 [01:19<00:54,  3.07it/s] 30%|██▉       | 70/235 [01:19<00:51,  3.18it/s] 30%|███       | 71/235 [01:20<00:50,  3.26it/s] 31%|███       | 72/235 [01:20<00:49,  3.32it/s] 31%|███       | 73/235 [01:20<00:48,  3.37it/s] 31%|███▏      | 74/235 [01:21<00:47,  3.40it/s] 32%|███▏      | 75/235 [01:21<00:46,  3.42it/s] 32%|███▏      | 76/235 [01:21<00:46,  3.44it/s] 33%|███▎      | 77/235 [01:21<00:45,  3.45it/s] 33%|███▎      | 78/235 [01:22<00:55,  2.81it/s] 34%|███▎      | 79/235 [01:22<00:52,  2.98it/s] 34%|███▍      | 80/235 [01:23<00:49,  3.11it/s] 34%|███▍      | 81/235 [01:23<00:47,  3.21it/s] 35%|███▍      | 82/235 [01:23<00:46,  3.29it/s] 35%|███▌      | 83/235 [01:23<00:45,  3.34it/s] 36%|███▌      | 84/235 [01:24<00:44,  3.38it/s] 36%|███▌      | 85/235 [01:24<00:44,  3.40it/s] 37%|███▋      | 86/235 [01:24<00:43,  3.43it/s] 37%|███▋      | 87/235 [01:25<00:43,  3.44it/s] 37%|███▋      | 88/235 [01:25<01:01,  2.40it/s] 38%|███▊      | 89/235 [01:26<00:55,  2.64it/s] 38%|███▊      | 90/235 [01:26<00:50,  2.85it/s] 39%|███▊      | 91/235 [01:26<00:47,  3.01it/s] 39%|███▉      | 92/235 [01:26<00:45,  3.14it/s] 40%|███▉      | 93/235 [01:27<00:43,  3.23it/s] 40%|████      | 94/235 [01:27<00:41,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 19:40:31,133 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:40:31,133 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:40:31,133 >>   Batch size = 8
{'eval_loss': 1.0421258211135864, 'eval_runtime': 11.3398, 'eval_samples_per_second': 307.589, 'eval_steps_per_second': 38.449, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.01it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.70it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.82it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.91it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.56it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.27it/s][A
  8%|▊         | 37/436 [00:00<00:11, 35.29it/s][A
 10%|▉         | 42/436 [00:01<00:10, 37.92it/s][A
 11%|█         | 47/436 [00:01<00:09, 40.00it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 41.51it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 42.68it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.52it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.07it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.20it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.09it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.93it/s][A
 20%|█▉        | 87/436 [00:02<00:07, 43.98it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.66it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.92it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 45.09it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 45.27it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.96it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.68it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.38it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.31it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.53it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.74it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.99it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.12it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 45.18it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 45.01it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 44.69it/s][A
 39%|███▉      | 172/436 [00:04<00:09, 27.31it/s][A
 41%|████      | 177/436 [00:04<00:08, 31.01it/s][A
 42%|████▏     | 182/436 [00:04<00:07, 34.24it/s][A
 43%|████▎     | 187/436 [00:04<00:06, 36.90it/s][A
 44%|████▍     | 192/436 [00:04<00:06, 39.08it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 40.67it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.01it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 42.85it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 42.83it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 43.46it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.83it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.16it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.63it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.77it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.92it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.82it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 44.64it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.54it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.49it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.53it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.78it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.90it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.99it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 45.02it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 42.04it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 42.94it/s][A
 70%|███████   | 307/436 [00:07<00:02, 43.21it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.67it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.04it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.25it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.51it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.74it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.57it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.59it/s][A
 80%|███████▉  | 347/436 [00:08<00:01, 44.69it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.69it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.62it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.48it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.78it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.72it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.90it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.71it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.62it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.67it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.63it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.51it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.55it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.79it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.96it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.89it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 44.81it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 19.80it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:10<00:00, 19.80it/s][A 40%|████      | 94/235 [01:38<00:41,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:40:42,000 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 19:40:42,596 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:41:05,091 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:41:07,316 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:41:07,721 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [02:29<43:57, 18.84s/it] 41%|████      | 96/235 [02:30<30:58, 13.37s/it] 41%|████▏     | 97/235 [02:30<21:43,  9.45s/it] 42%|████▏     | 98/235 [02:30<15:18,  6.70s/it] 42%|████▏     | 99/235 [02:31<10:49,  4.78s/it] 43%|████▎     | 100/235 [02:31<07:43,  3.43s/it] 43%|████▎     | 101/235 [02:31<05:33,  2.49s/it] 43%|████▎     | 102/235 [02:31<04:03,  1.83s/it] 44%|████▍     | 103/235 [02:32<03:00,  1.37s/it] 44%|████▍     | 104/235 [02:32<02:17,  1.05s/it] 45%|████▍     | 105/235 [02:32<01:46,  1.22it/s] 45%|████▌     | 106/235 [02:33<01:30,  1.43it/s] 46%|████▌     | 107/235 [02:33<01:13,  1.73it/s] 46%|████▌     | 108/235 [02:33<01:02,  2.03it/s] 46%|████▋     | 109/235 [02:34<00:54,  2.31it/s] 47%|████▋     | 110/235 [02:34<00:48,  2.56it/s] 47%|████▋     | 111/235 [02:34<00:44,  2.77it/s] 48%|████▊     | 112/235 [02:35<00:41,  2.94it/s] 48%|████▊     | 113/235 [02:35<00:39,  3.07it/s] 49%|████▊     | 114/235 [02:35<00:38,  3.17it/s] 49%|████▉     | 115/235 [02:35<00:37,  3.24it/s] 49%|████▉     | 116/235 [02:36<00:45,  2.63it/s] 50%|████▉     | 117/235 [02:36<00:41,  2.83it/s] 50%|█████     | 118/235 [02:37<00:39,  2.98it/s] 51%|█████     | 119/235 [02:37<00:37,  3.10it/s] 51%|█████     | 120/235 [02:37<00:36,  3.19it/s] 51%|█████▏    | 121/235 [02:37<00:35,  3.26it/s] 52%|█████▏    | 122/235 [02:38<00:34,  3.30it/s] 52%|█████▏    | 123/235 [02:38<00:33,  3.34it/s] 53%|█████▎    | 124/235 [02:38<00:33,  3.36it/s] 53%|█████▎    | 125/235 [02:39<00:32,  3.38it/s] 54%|█████▎    | 126/235 [02:39<00:36,  2.98it/s] 54%|█████▍    | 127/235 [02:39<00:34,  3.10it/s] 54%|█████▍    | 128/235 [02:40<00:33,  3.20it/s] 55%|█████▍    | 129/235 [02:40<00:32,  3.28it/s] 55%|█████▌    | 130/235 [02:40<00:31,  3.34it/s] 56%|█████▌    | 131/235 [02:41<00:35,  2.92it/s] 56%|█████▌    | 132/235 [02:41<00:33,  3.06it/s] 57%|█████▋    | 133/235 [02:41<00:32,  3.18it/s] 57%|█████▋    | 134/235 [02:41<00:30,  3.26it/s] 57%|█████▋    | 135/235 [02:42<00:30,  3.32it/s] 58%|█████▊    | 136/235 [02:42<00:29,  3.37it/s] 58%|█████▊    | 137/235 [02:42<00:28,  3.40it/s] 59%|█████▊    | 138/235 [02:43<00:28,  3.42it/s] 59%|█████▉    | 139/235 [02:43<00:27,  3.43it/s] 60%|█████▉    | 140/235 [02:43<00:27,  3.44it/s] 60%|██████    | 141/235 [02:44<00:30,  3.05it/s][INFO|trainer.py:2140] 2023-08-28 19:41:47,756 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:41:47,756 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:41:47,756 >>   Batch size = 8
{'eval_loss': 1.0578539371490479, 'eval_runtime': 10.5533, 'eval_samples_per_second': 330.513, 'eval_steps_per_second': 41.314, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.42it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.30it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.45it/s][A
  5%|▌         | 22/436 [00:00<00:10, 40.95it/s][A
  6%|▌         | 27/436 [00:00<00:09, 42.63it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.29it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.64it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.01it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.37it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.68it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.94it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.79it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.53it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.56it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.60it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.65it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.75it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.91it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 45.00it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 45.15it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.73it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.59it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 40.34it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 41.80it/s][A
 30%|███       | 132/436 [00:02<00:07, 42.79it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.50it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.06it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.40it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.73it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.70it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.41it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.22it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.44it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.61it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.78it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.82it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.91it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 45.03it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.79it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.46it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.34it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.41it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.70it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.78it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.97it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 45.06it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 45.13it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.94it/s][A
 58%|█████▊    | 252/436 [00:06<00:04, 44.53it/s][A
 59%|█████▉    | 257/436 [00:06<00:08, 20.68it/s][A
 60%|██████    | 262/436 [00:06<00:07, 24.74it/s][A
 61%|██████    | 267/436 [00:06<00:05, 28.66it/s][A
 62%|██████▏   | 272/436 [00:06<00:05, 32.22it/s][A
 64%|██████▎   | 277/436 [00:06<00:04, 35.27it/s][A
 65%|██████▍   | 282/436 [00:06<00:04, 37.84it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 39.77it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 41.06it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 41.77it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 42.40it/s][A
 70%|███████   | 307/436 [00:07<00:02, 43.00it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.53it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.07it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.57it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.76it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 45.01it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.83it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 44.47it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 44.38it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.40it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.55it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.71it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 372/436 [00:09<00:01, 44.99it/s][A
 86%|████████▋ | 377/436 [00:09<00:04, 13.97it/s][A
 88%|████████▊ | 382/436 [00:09<00:03, 17.64it/s][A
 89%|████████▉ | 387/436 [00:09<00:02, 21.62it/s][A
 90%|████████▉ | 392/436 [00:10<00:01, 25.66it/s][A
 91%|█████████ | 397/436 [00:10<00:01, 29.54it/s][A
 92%|█████████▏| 402/436 [00:10<00:01, 32.96it/s][A
 93%|█████████▎| 407/436 [00:10<00:00, 35.88it/s][A
 94%|█████████▍| 412/436 [00:10<00:00, 38.23it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 39.65it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 40.77it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 41.79it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 42.74it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:11<00:00, 42.74it/s][A 60%|██████    | 141/235 [02:55<00:30,  3.05it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:41:59,781 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 19:42:00,892 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:42:10,346 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:42:11,042 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:11,375 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [03:24<19:03, 12.30s/it] 61%|██████    | 143/235 [03:24<13:22,  8.72s/it] 61%|██████▏   | 144/235 [03:25<09:23,  6.19s/it] 62%|██████▏   | 145/235 [03:25<06:38,  4.42s/it] 62%|██████▏   | 146/235 [03:25<04:43,  3.18s/it] 63%|██████▎   | 147/235 [03:25<03:23,  2.32s/it] 63%|██████▎   | 148/235 [03:26<02:28,  1.71s/it] 63%|██████▎   | 149/235 [03:26<01:50,  1.28s/it] 64%|██████▍   | 150/235 [03:26<01:23,  1.01it/s] 64%|██████▍   | 151/235 [03:27<01:05,  1.28it/s] 65%|██████▍   | 152/235 [03:27<00:52,  1.58it/s] 65%|██████▌   | 153/235 [03:28<00:52,  1.57it/s] 66%|██████▌   | 154/235 [03:28<00:43,  1.87it/s] 66%|██████▌   | 155/235 [03:28<00:36,  2.16it/s] 66%|██████▋   | 156/235 [03:28<00:32,  2.43it/s] 67%|██████▋   | 157/235 [03:29<00:29,  2.66it/s] 67%|██████▋   | 158/235 [03:29<00:27,  2.85it/s] 68%|██████▊   | 159/235 [03:29<00:25,  3.00it/s] 68%|██████▊   | 160/235 [03:30<00:24,  3.11it/s] 69%|██████▊   | 161/235 [03:30<00:23,  3.20it/s] 69%|██████▉   | 162/235 [03:30<00:22,  3.26it/s] 69%|██████▉   | 163/235 [03:30<00:22,  3.17it/s] 70%|██████▉   | 164/235 [03:31<00:21,  3.24it/s] 70%|███████   | 165/235 [03:31<00:21,  3.29it/s] 71%|███████   | 166/235 [03:31<00:20,  3.32it/s] 71%|███████   | 167/235 [03:32<00:20,  3.35it/s] 71%|███████▏  | 168/235 [03:32<00:19,  3.37it/s] 72%|███████▏  | 169/235 [03:32<00:19,  3.38it/s] 72%|███████▏  | 170/235 [03:33<00:19,  3.39it/s] 73%|███████▎  | 171/235 [03:33<00:18,  3.40it/s] 73%|███████▎  | 172/235 [03:33<00:18,  3.40it/s] 74%|███████▎  | 173/235 [03:33<00:18,  3.40it/s] 74%|███████▍  | 174/235 [03:34<00:18,  3.24it/s] 74%|███████▍  | 175/235 [03:34<00:18,  3.29it/s] 75%|███████▍  | 176/235 [03:34<00:17,  3.32it/s] 75%|███████▌  | 177/235 [03:35<00:17,  3.34it/s] 76%|███████▌  | 178/235 [03:35<00:16,  3.37it/s] 76%|███████▌  | 179/235 [03:35<00:16,  3.39it/s] 77%|███████▋  | 180/235 [03:36<00:16,  3.42it/s] 77%|███████▋  | 181/235 [03:36<00:15,  3.43it/s] 77%|███████▋  | 182/235 [03:36<00:15,  3.43it/s] 78%|███████▊  | 183/235 [03:36<00:15,  3.44it/s] 78%|███████▊  | 184/235 [03:37<00:14,  3.45it/s] 79%|███████▊  | 185/235 [03:37<00:14,  3.34it/s] 79%|███████▉  | 186/235 [03:37<00:14,  3.37it/s] 80%|███████▉  | 187/235 [03:38<00:14,  3.40it/s] 80%|████████  | 188/235 [03:38<00:13,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 19:42:41,955 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:42:41,955 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:42:41,955 >>   Batch size = 8
{'eval_loss': 1.073145866394043, 'eval_runtime': 11.0451, 'eval_samples_per_second': 315.796, 'eval_steps_per_second': 39.474, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.36it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.94it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.89it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.08it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.59it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.16it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.02it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.89it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.85it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.01it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.10it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.90it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.79it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.51it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.46it/s][A
 20%|█▉        | 87/436 [00:02<00:07, 44.51it/s][A
 21%|██        | 92/436 [00:02<00:15, 22.75it/s][A
 22%|██▏       | 97/436 [00:02<00:12, 26.79it/s][A
 23%|██▎       | 102/436 [00:02<00:10, 30.52it/s][A
 25%|██▍       | 107/436 [00:02<00:09, 33.86it/s][A
 26%|██▌       | 112/436 [00:02<00:08, 36.68it/s][A
 27%|██▋       | 117/436 [00:02<00:08, 38.94it/s][A
 28%|██▊       | 122/436 [00:03<00:07, 40.67it/s][A
 29%|██▉       | 127/436 [00:03<00:07, 41.78it/s][A
 30%|███       | 132/436 [00:03<00:07, 42.34it/s][A
 31%|███▏      | 137/436 [00:03<00:07, 42.64it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 43.25it/s][A
 34%|███▎      | 147/436 [00:04<00:06, 43.70it/s][A
 35%|███▍      | 152/436 [00:04<00:25, 11.15it/s][A
 36%|███▌      | 157/436 [00:04<00:19, 14.40it/s][A
 37%|███▋      | 162/436 [00:05<00:15, 18.10it/s][A
 38%|███▊      | 167/436 [00:05<00:12, 22.07it/s][A
 39%|███▉      | 172/436 [00:05<00:10, 26.09it/s][A
 41%|████      | 177/436 [00:05<00:08, 29.86it/s][A
 42%|████▏     | 182/436 [00:05<00:07, 33.21it/s][A
 43%|████▎     | 187/436 [00:05<00:06, 36.04it/s][A
 44%|████▍     | 192/436 [00:05<00:06, 38.02it/s][A
 45%|████▌     | 197/436 [00:05<00:06, 39.66it/s][A
 46%|████▋     | 202/436 [00:05<00:05, 41.09it/s][A
 47%|████▋     | 207/436 [00:06<00:05, 42.16it/s][A
 49%|████▊     | 212/436 [00:06<00:05, 43.02it/s][A
 50%|████▉     | 217/436 [00:06<00:09, 23.75it/s][A
 51%|█████     | 221/436 [00:06<00:09, 22.21it/s][A
 52%|█████▏    | 226/436 [00:06<00:07, 26.48it/s][A
 53%|█████▎    | 231/436 [00:07<00:06, 30.40it/s][A
 54%|█████▍    | 236/436 [00:07<00:05, 33.84it/s][A
 55%|█████▌    | 241/436 [00:07<00:05, 36.60it/s][A
 56%|█████▋    | 246/436 [00:07<00:04, 38.84it/s][A
 58%|█████▊    | 251/436 [00:07<00:04, 40.56it/s][A
 59%|█████▊    | 256/436 [00:07<00:04, 41.82it/s][A
 60%|█████▉    | 261/436 [00:07<00:04, 42.37it/s][A
 61%|██████    | 266/436 [00:07<00:03, 42.70it/s][A
 62%|██████▏   | 271/436 [00:07<00:03, 43.27it/s][A
 63%|██████▎   | 276/436 [00:08<00:03, 43.72it/s][A
 64%|██████▍   | 281/436 [00:08<00:03, 44.15it/s][A
 66%|██████▌   | 286/436 [00:08<00:03, 44.56it/s][A
 67%|██████▋   | 291/436 [00:08<00:03, 44.78it/s][A
 68%|██████▊   | 296/436 [00:08<00:03, 44.80it/s][A
 69%|██████▉   | 301/436 [00:08<00:03, 44.76it/s][A
 70%|███████   | 306/436 [00:08<00:02, 44.47it/s][A
 71%|███████▏  | 311/436 [00:08<00:02, 44.34it/s][A
 72%|███████▏  | 316/436 [00:08<00:02, 44.19it/s][A
 74%|███████▎  | 321/436 [00:09<00:02, 44.46it/s][A
 75%|███████▍  | 326/436 [00:09<00:02, 44.63it/s][A
 76%|███████▌  | 331/436 [00:09<00:02, 44.81it/s][A
 77%|███████▋  | 336/436 [00:09<00:02, 44.89it/s][A
 78%|███████▊  | 341/436 [00:09<00:02, 45.05it/s][A
 79%|███████▉  | 346/436 [00:09<00:02, 44.87it/s][A
 81%|████████  | 351/436 [00:09<00:03, 27.14it/s][A
 82%|████████▏ | 356/436 [00:10<00:02, 30.89it/s][A
 83%|████████▎ | 361/436 [00:10<00:02, 34.11it/s][A
 84%|████████▍ | 366/436 [00:10<00:01, 36.81it/s][A
 85%|████████▌ | 371/436 [00:10<00:01, 39.00it/s][A
 86%|████████▌ | 376/436 [00:10<00:01, 40.68it/s][A
 87%|████████▋ | 381/436 [00:10<00:01, 41.94it/s][A
 89%|████████▊ | 386/436 [00:10<00:01, 42.74it/s][A
 90%|████████▉ | 391/436 [00:10<00:01, 42.87it/s][A
 91%|█████████ | 396/436 [00:10<00:00, 43.12it/s][A
 92%|█████████▏| 401/436 [00:11<00:00, 43.49it/s][A
 93%|█████████▎| 406/436 [00:11<00:00, 43.99it/s][A
 94%|█████████▍| 411/436 [00:11<00:00, 44.37it/s][A
 95%|█████████▌| 416/436 [00:11<00:00, 44.66it/s][A
 97%|█████████▋| 421/436 [00:11<00:00, 44.81it/s][A
 98%|█████████▊| 426/436 [00:11<00:00, 44.87it/s][A
 99%|█████████▉| 431/436 [00:11<00:00, 44.65it/s][A
100%|██████████| 436/436 [00:11<00:00, 44.47it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:11<00:00, 44.47it/s][A 80%|████████  | 188/235 [03:50<00:13,  3.54it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:42:54,471 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 19:42:55,816 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:43:07,023 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:43:08,956 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:43:09,363 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [04:30<12:11, 15.90s/it] 81%|████████  | 190/235 [04:31<08:27, 11.29s/it] 81%|████████▏ | 191/235 [04:31<05:51,  7.99s/it] 82%|████████▏ | 192/235 [04:31<04:04,  5.68s/it] 82%|████████▏ | 193/235 [04:32<02:50,  4.06s/it] 83%|████████▎ | 194/235 [04:32<02:00,  2.93s/it] 83%|████████▎ | 195/235 [04:32<01:25,  2.14s/it] 83%|████████▎ | 196/235 [04:32<01:01,  1.59s/it] 84%|████████▍ | 197/235 [04:33<00:45,  1.20s/it] 84%|████████▍ | 198/235 [04:33<00:34,  1.08it/s] 85%|████████▍ | 199/235 [04:33<00:26,  1.36it/s] 85%|████████▌ | 200/235 [04:34<00:22,  1.58it/s] 86%|████████▌ | 201/235 [04:34<00:18,  1.88it/s] 86%|████████▌ | 202/235 [04:34<00:15,  2.17it/s] 86%|████████▋ | 203/235 [04:35<00:13,  2.44it/s] 87%|████████▋ | 204/235 [04:35<00:11,  2.67it/s] 87%|████████▋ | 205/235 [04:35<00:10,  2.86it/s] 88%|████████▊ | 206/235 [04:35<00:09,  3.00it/s] 88%|████████▊ | 207/235 [04:36<00:08,  3.11it/s] 89%|████████▊ | 208/235 [04:36<00:08,  3.19it/s] 89%|████████▉ | 209/235 [04:36<00:07,  3.25it/s] 89%|████████▉ | 210/235 [04:37<00:08,  3.02it/s] 90%|████████▉ | 211/235 [04:37<00:07,  3.12it/s] 90%|█████████ | 212/235 [04:37<00:07,  3.20it/s] 91%|█████████ | 213/235 [04:38<00:06,  3.26it/s] 91%|█████████ | 214/235 [04:38<00:06,  3.30it/s] 91%|█████████▏| 215/235 [04:38<00:05,  3.34it/s] 92%|█████████▏| 216/235 [04:38<00:05,  3.36it/s] 92%|█████████▏| 217/235 [04:39<00:05,  3.36it/s] 93%|█████████▎| 218/235 [04:39<00:05,  3.37it/s] 93%|█████████▎| 219/235 [04:39<00:04,  3.38it/s] 94%|█████████▎| 220/235 [04:40<00:04,  3.20it/s] 94%|█████████▍| 221/235 [04:40<00:04,  3.26it/s] 94%|█████████▍| 222/235 [04:40<00:03,  3.30it/s] 95%|█████████▍| 223/235 [04:41<00:03,  3.33it/s] 95%|█████████▌| 224/235 [04:41<00:03,  3.35it/s] 96%|█████████▌| 225/235 [04:41<00:02,  3.37it/s] 96%|█████████▌| 226/235 [04:41<00:02,  3.38it/s] 97%|█████████▋| 227/235 [04:42<00:02,  3.39it/s] 97%|█████████▋| 228/235 [04:42<00:02,  3.39it/s] 97%|█████████▋| 229/235 [04:42<00:01,  3.40it/s] 98%|█████████▊| 230/235 [04:43<00:01,  3.40it/s] 98%|█████████▊| 231/235 [04:43<00:01,  3.17it/s] 99%|█████████▊| 232/235 [04:43<00:00,  3.24it/s] 99%|█████████▉| 233/235 [04:44<00:00,  3.29it/s]100%|█████████▉| 234/235 [04:44<00:00,  3.32it/s]100%|██████████| 235/235 [04:44<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 19:43:48,315 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:43:48,315 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:43:48,315 >>   Batch size = 8
{'eval_loss': 1.084823489189148, 'eval_runtime': 11.8915, 'eval_samples_per_second': 293.318, 'eval_steps_per_second': 36.665, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.44it/s][A
  3%|▎         | 12/436 [00:00<00:12, 34.34it/s][A
  4%|▍         | 17/436 [00:00<00:10, 38.84it/s][A
  5%|▌         | 22/436 [00:00<00:10, 41.17it/s][A
  6%|▌         | 27/436 [00:00<00:09, 42.41it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.35it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.90it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.35it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.66it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.56it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.28it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.27it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.50it/s][A
 17%|█▋        | 72/436 [00:01<00:10, 35.10it/s][A
 18%|█▊        | 77/436 [00:01<00:09, 37.63it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 39.66it/s][A
 20%|█▉        | 87/436 [00:02<00:08, 41.11it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.35it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.20it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.88it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.09it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.93it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.90it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.95it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.32it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.61it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.82it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.99it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 45.00it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.88it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.56it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.35it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 44.39it/s][A
 39%|███▉      | 172/436 [00:04<00:12, 20.98it/s][A
 41%|████      | 177/436 [00:04<00:10, 25.03it/s][A
 42%|████▏     | 182/436 [00:04<00:08, 28.88it/s][A
 43%|████▎     | 187/436 [00:04<00:07, 32.42it/s][A
 44%|████▍     | 192/436 [00:04<00:06, 35.49it/s][A
 45%|████▌     | 197/436 [00:04<00:06, 37.94it/s][A
 46%|████▋     | 202/436 [00:05<00:05, 39.84it/s][A
 47%|████▋     | 207/436 [00:05<00:05, 41.21it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 41.91it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 42.46it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.01it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.58it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.07it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.48it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.66it/s][A
 57%|█████▋    | 247/436 [00:06<00:04, 44.86it/s][A
 58%|█████▊    | 252/436 [00:06<00:04, 44.74it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 44.37it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.22it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.27it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.24it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.68it/s][A
 65%|██████▍   | 282/436 [00:07<00:03, 44.96it/s][A
 66%|██████▌   | 287/436 [00:07<00:07, 21.19it/s][A
 67%|██████▋   | 292/436 [00:07<00:05, 25.17it/s][A
 68%|██████▊   | 297/436 [00:07<00:04, 29.05it/s][A
 69%|██████▉   | 302/436 [00:07<00:04, 32.55it/s][A
 70%|███████   | 307/436 [00:07<00:03, 35.51it/s][A
 72%|███████▏  | 312/436 [00:07<00:03, 37.93it/s][A
 73%|███████▎  | 317/436 [00:08<00:02, 39.85it/s][A
 74%|███████▍  | 322/436 [00:08<00:02, 41.23it/s][A
 75%|███████▌  | 327/436 [00:08<00:02, 41.94it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 42.63it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 43.24it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 43.89it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 44.19it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.45it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.65it/s][A
 83%|████████▎ | 362/436 [00:09<00:01, 44.77it/s][A
 84%|████████▍ | 367/436 [00:09<00:01, 44.62it/s][A
 85%|████████▌ | 372/436 [00:09<00:01, 44.32it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 44.45it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 44.54it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.72it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.85it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.90it/s][A
 92%|█████████▏| 402/436 [00:10<00:00, 44.91it/s][A
 93%|█████████▎| 407/436 [00:10<00:00, 33.68it/s][A
 94%|█████████▍| 412/436 [00:10<00:00, 36.57it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 38.81it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 40.59it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 41.93it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 42.84it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:10<00:00, 42.84it/s][A100%|██████████| 235/235 [04:55<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:43:59,377 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 19:44:00,746 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:44:16,321 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:44:17,173 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:44:17,285 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:44:41,700 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:44:41,826 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47 (score: 1.0421258211135864).
                                                 100%|██████████| 235/235 [06:21<00:00,  3.46it/s]100%|██████████| 235/235 [06:21<00:00,  1.62s/it]
[INFO|trainer.py:1894] 2023-08-28 19:45:26,775 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 19:45:27,609 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:45:37,349 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:45:38,111 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:45:38,362 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:45:39,542 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,542 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,542 >>   train_loss               =     0.4711
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,542 >>   train_runtime            = 0:06:21.58
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,542 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,542 >>   train_samples_per_second =      39.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:39,543 >>   train_steps_per_second   =      0.616
{'eval_loss': 1.0875166654586792, 'eval_runtime': 10.8583, 'eval_samples_per_second': 321.229, 'eval_steps_per_second': 40.154, 'epoch': 5.0}
{'train_runtime': 381.5816, 'train_samples_per_second': 39.31, 'train_steps_per_second': 0.616, 'train_loss': 0.4710698553856383, 'epoch': 5.0}
08/28/2023 19:45:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:45:40,748 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:45:40,748 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 19:45:40,749 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.89it/s]  3%|▎         | 12/436 [00:00<00:08, 49.36it/s]  4%|▍         | 17/436 [00:00<00:08, 47.70it/s]  5%|▌         | 22/436 [00:00<00:08, 46.71it/s]  6%|▌         | 27/436 [00:00<00:08, 46.36it/s]  7%|▋         | 32/436 [00:00<00:08, 46.02it/s]  8%|▊         | 37/436 [00:00<00:08, 45.80it/s] 10%|▉         | 42/436 [00:00<00:08, 45.51it/s] 11%|█         | 47/436 [00:01<00:08, 44.98it/s] 12%|█▏        | 52/436 [00:01<00:08, 45.02it/s] 13%|█▎        | 57/436 [00:01<00:08, 45.05it/s] 14%|█▍        | 62/436 [00:01<00:08, 45.08it/s] 15%|█▌        | 67/436 [00:01<00:08, 45.15it/s] 17%|█▋        | 72/436 [00:01<00:08, 45.24it/s] 18%|█▊        | 77/436 [00:01<00:07, 45.20it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.16it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.99it/s] 21%|██        | 92/436 [00:02<00:07, 44.84it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.83it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.84it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.93it/s] 26%|██▌       | 112/436 [00:02<00:07, 45.02it/s] 27%|██▋       | 117/436 [00:02<00:07, 45.12it/s] 28%|██▊       | 122/436 [00:02<00:06, 45.10it/s] 29%|██▉       | 127/436 [00:02<00:06, 45.08it/s] 30%|███       | 132/436 [00:02<00:06, 44.98it/s] 31%|███▏      | 137/436 [00:03<00:11, 26.60it/s] 33%|███▎      | 142/436 [00:03<00:09, 30.38it/s] 34%|███▎      | 147/436 [00:03<00:08, 33.73it/s] 35%|███▍      | 152/436 [00:03<00:07, 36.51it/s] 36%|███▌      | 157/436 [00:03<00:07, 38.82it/s] 37%|███▋      | 162/436 [00:03<00:06, 40.64it/s] 38%|███▊      | 167/436 [00:03<00:06, 41.94it/s] 39%|███▉      | 172/436 [00:04<00:06, 42.85it/s] 41%|████      | 177/436 [00:04<00:06, 42.97it/s] 42%|████▏     | 182/436 [00:04<00:05, 43.47it/s] 43%|████▎     | 187/436 [00:04<00:05, 43.90it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.24it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.57it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.85it/s] 47%|████▋     | 207/436 [00:04<00:05, 45.08it/s] 49%|████▊     | 212/436 [00:04<00:04, 45.15it/s] 50%|████▉     | 217/436 [00:05<00:04, 44.84it/s] 51%|█████     | 222/436 [00:05<00:04, 44.66it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.53it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.63it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.82it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.93it/s] 57%|█████▋    | 247/436 [00:05<00:04, 45.14it/s] 58%|█████▊    | 252/436 [00:05<00:04, 45.25it/s] 59%|█████▉    | 257/436 [00:05<00:03, 45.21it/s] 60%|██████    | 262/436 [00:06<00:05, 29.69it/s] 61%|██████    | 267/436 [00:06<00:05, 33.16it/s] 62%|██████▏   | 272/436 [00:06<00:04, 36.00it/s] 64%|██████▎   | 277/436 [00:06<00:04, 38.30it/s] 65%|██████▍   | 282/436 [00:06<00:03, 40.21it/s] 66%|██████▌   | 287/436 [00:06<00:03, 41.67it/s] 67%|██████▋   | 292/436 [00:06<00:03, 42.76it/s] 68%|██████▊   | 297/436 [00:07<00:03, 43.51it/s] 69%|██████▉   | 302/436 [00:07<00:03, 43.63it/s] 70%|███████   | 307/436 [00:07<00:02, 43.79it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.24it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.45it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.70it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.90it/s] 76%|███████▌  | 332/436 [00:07<00:02, 45.24it/s] 77%|███████▋  | 337/436 [00:07<00:02, 45.31it/s] 78%|███████▊  | 342/436 [00:08<00:02, 45.24it/s] 80%|███████▉  | 347/436 [00:08<00:01, 44.94it/s] 81%|████████  | 352/436 [00:08<00:01, 44.80it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.89it/s] 83%|████████▎ | 362/436 [00:08<00:02, 32.30it/s] 84%|████████▍ | 368/436 [00:08<00:01, 36.65it/s] 86%|████████▌ | 373/436 [00:08<00:01, 38.79it/s] 87%|████████▋ | 378/436 [00:08<00:01, 40.62it/s] 88%|████████▊ | 383/436 [00:09<00:01, 33.31it/s] 89%|████████▉ | 388/436 [00:09<00:01, 36.21it/s] 90%|█████████ | 393/436 [00:09<00:01, 38.34it/s] 91%|█████████▏| 398/436 [00:09<00:00, 40.54it/s] 92%|█████████▏| 403/436 [00:09<00:00, 41.95it/s] 94%|█████████▎| 408/436 [00:09<00:00, 42.99it/s] 95%|█████████▍| 413/436 [00:09<00:00, 43.67it/s] 96%|█████████▌| 418/436 [00:09<00:00, 44.18it/s] 97%|█████████▋| 423/436 [00:10<00:00, 44.08it/s] 98%|█████████▊| 428/436 [00:10<00:00, 43.98it/s] 99%|█████████▉| 433/436 [00:10<00:00, 44.14it/s]100%|██████████| 436/436 [00:10<00:00, 42.16it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:45:51,108 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   eval_loss               =     1.0421
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   eval_runtime            = 0:00:10.35
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   eval_samples_per_second =    336.706
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   eval_steps_per_second   =     42.088
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:45:51,108 >>   perplexity              =     2.8352
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:21,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:21,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:21,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:21,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:21,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:46:22,554 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:46:22,555 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:23,384 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:24,415 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:24,415 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:29,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:29,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:29,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:29,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:29,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:46:30,526 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:46:30,527 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:31,707 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:31,880 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:31,881 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-47
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-235
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-141
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.64it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:15,  1.66it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.47it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:19,  1.13it/s]Extractor Predicting: 30it [00:19,  1.24it/s]Extractor Predicting: 31it [00:20,  1.32it/s]Extractor Predicting: 32it [00:21,  1.41it/s]Extractor Predicting: 33it [00:21,  1.38it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.24it/s]Extractor Predicting: 39it [00:26,  1.33it/s]Extractor Predicting: 40it [00:26,  1.38it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.42it/s]Extractor Predicting: 43it [00:28,  1.44it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:31,  1.26it/s]Extractor Predicting: 48it [00:32,  1.33it/s]Extractor Predicting: 49it [00:33,  1.42it/s]Extractor Predicting: 50it [00:33,  1.43it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.53it/s]Extractor Predicting: 55it [00:37,  1.54it/s]Extractor Predicting: 56it [00:37,  1.55it/s]Extractor Predicting: 57it [00:38,  1.27it/s]Extractor Predicting: 58it [00:39,  1.35it/s]Extractor Predicting: 59it [00:40,  1.39it/s]Extractor Predicting: 60it [00:40,  1.42it/s]Extractor Predicting: 61it [00:41,  1.43it/s]Extractor Predicting: 62it [00:42,  1.46it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:43,  1.50it/s]Extractor Predicting: 65it [00:43,  1.54it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:45,  1.52it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:46,  1.54it/s]Extractor Predicting: 70it [00:47,  1.56it/s]Extractor Predicting: 71it [00:48,  1.42it/s]Extractor Predicting: 72it [00:48,  1.47it/s]Extractor Predicting: 73it [00:49,  1.48it/s]Extractor Predicting: 74it [00:49,  1.52it/s]Extractor Predicting: 75it [00:50,  1.54it/s]Extractor Predicting: 76it [00:51,  1.56it/s]Extractor Predicting: 77it [00:51,  1.59it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:53,  1.51it/s]Extractor Predicting: 80it [00:53,  1.51it/s]Extractor Predicting: 81it [00:54,  1.52it/s]Extractor Predicting: 82it [00:55,  1.53it/s]Extractor Predicting: 83it [00:55,  1.55it/s]Extractor Predicting: 84it [00:56,  1.54it/s]Extractor Predicting: 85it [00:57,  1.53it/s]Extractor Predicting: 86it [00:57,  1.55it/s]Extractor Predicting: 87it [00:58,  1.56it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.56it/s]Extractor Predicting: 91it [01:00,  1.61it/s]Extractor Predicting: 92it [01:01,  1.63it/s]Extractor Predicting: 93it [01:02,  1.52it/s]Extractor Predicting: 94it [01:02,  1.57it/s]Extractor Predicting: 95it [01:03,  1.60it/s]Extractor Predicting: 96it [01:04,  1.61it/s]Extractor Predicting: 97it [01:04,  1.61it/s]Extractor Predicting: 98it [01:05,  1.56it/s]Extractor Predicting: 99it [01:06,  1.54it/s]Extractor Predicting: 100it [01:06,  1.54it/s]Extractor Predicting: 101it [01:07,  1.60it/s]Extractor Predicting: 102it [01:07,  1.65it/s]Extractor Predicting: 103it [01:08,  1.55it/s]Extractor Predicting: 104it [01:09,  1.59it/s]Extractor Predicting: 105it [01:09,  1.60it/s]Extractor Predicting: 106it [01:10,  1.63it/s]Extractor Predicting: 107it [01:11,  1.62it/s]Extractor Predicting: 108it [01:11,  1.57it/s]Extractor Predicting: 109it [01:12,  1.59it/s]Extractor Predicting: 110it [01:12,  1.59it/s]Extractor Predicting: 111it [01:13,  1.64it/s]Extractor Predicting: 112it [01:14,  1.64it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.58it/s]Extractor Predicting: 116it [01:16,  1.60it/s]Extractor Predicting: 117it [01:17,  1.59it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:19,  1.51it/s]Extractor Predicting: 121it [01:20,  1.54it/s]Extractor Predicting: 122it [01:20,  1.56it/s]Extractor Predicting: 123it [01:21,  1.57it/s]Extractor Predicting: 124it [01:22,  1.56it/s]Extractor Predicting: 125it [01:22,  1.55it/s]Extractor Predicting: 126it [01:23,  1.34it/s]Extractor Predicting: 127it [01:24,  1.40it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:26,  1.55it/s]Extractor Predicting: 131it [01:27,  1.28it/s]Extractor Predicting: 132it [01:27,  1.36it/s]Extractor Predicting: 133it [01:28,  1.41it/s]Extractor Predicting: 134it [01:29,  1.43it/s]Extractor Predicting: 135it [01:30,  1.29it/s]Extractor Predicting: 136it [01:30,  1.37it/s]Extractor Predicting: 137it [01:31,  1.43it/s]Extractor Predicting: 138it [01:31,  1.47it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:33,  1.47it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:35,  1.52it/s]Extractor Predicting: 144it [01:35,  1.86it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:31,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:31,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:31,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:31,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:31,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:48:32,753 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:48:32,754 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:48:33,750 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:48:35,195 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:48:35,300 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:37,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:37,633 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:37,633 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:37,633 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:37,633 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:48:38,430 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:48:38,505 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:48:39,063 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:48:39,471 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:48:39,471 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26362366092221706,
  "recall": 0.16227064220183487,
  "score": 0.20088731144631766,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.45it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.27it/s]Extractor Predicting: 17it [00:11,  1.32it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.53it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.58it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:22,  1.67it/s]Extractor Predicting: 35it [00:23,  1.70it/s]Extractor Predicting: 36it [00:23,  1.72it/s]Extractor Predicting: 37it [00:24,  1.68it/s]Extractor Predicting: 38it [00:24,  1.73it/s]Extractor Predicting: 39it [00:25,  1.78it/s]Extractor Predicting: 40it [00:25,  1.80it/s]Extractor Predicting: 41it [00:26,  1.80it/s]Extractor Predicting: 42it [00:26,  1.82it/s]Extractor Predicting: 43it [00:27,  1.73it/s]Extractor Predicting: 44it [00:28,  1.79it/s]Extractor Predicting: 45it [00:28,  1.79it/s]Extractor Predicting: 46it [00:29,  1.74it/s]Extractor Predicting: 47it [00:29,  1.78it/s]Extractor Predicting: 48it [00:30,  1.83it/s]Extractor Predicting: 49it [00:30,  1.75it/s]Extractor Predicting: 50it [00:31,  1.76it/s]Extractor Predicting: 51it [00:32,  1.81it/s]Extractor Predicting: 52it [00:32,  1.79it/s]Extractor Predicting: 53it [00:33,  1.79it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:36,  1.64it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:37,  1.41it/s]Extractor Predicting: 61it [00:38,  1.42it/s]Extractor Predicting: 62it [00:39,  1.42it/s]Extractor Predicting: 63it [00:40,  1.46it/s]Extractor Predicting: 64it [00:40,  1.45it/s]Extractor Predicting: 65it [00:41,  1.43it/s]Extractor Predicting: 66it [00:42,  1.46it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.47it/s]Extractor Predicting: 69it [00:44,  1.46it/s]Extractor Predicting: 70it [00:45,  1.27it/s]Extractor Predicting: 71it [00:45,  1.30it/s]Extractor Predicting: 72it [00:46,  1.38it/s]Extractor Predicting: 73it [00:47,  1.40it/s]Extractor Predicting: 74it [00:48,  1.32it/s]Extractor Predicting: 75it [00:48,  1.39it/s]Extractor Predicting: 76it [00:49,  1.43it/s]Extractor Predicting: 77it [00:50,  1.44it/s]Extractor Predicting: 78it [00:50,  1.44it/s]Extractor Predicting: 79it [00:51,  1.38it/s]Extractor Predicting: 80it [00:52,  1.42it/s]Extractor Predicting: 81it [00:52,  1.43it/s]Extractor Predicting: 82it [00:53,  1.44it/s]Extractor Predicting: 83it [00:54,  1.46it/s]Extractor Predicting: 84it [00:54,  1.40it/s]Extractor Predicting: 85it [00:55,  1.42it/s]Extractor Predicting: 86it [00:56,  1.42it/s]Extractor Predicting: 87it [00:57,  1.44it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.45it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [00:59,  1.49it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:01,  1.43it/s]Extractor Predicting: 95it [01:02,  1.49it/s]Extractor Predicting: 96it [01:02,  1.55it/s]Extractor Predicting: 97it [01:03,  1.55it/s]Extractor Predicting: 98it [01:04,  1.60it/s]Extractor Predicting: 99it [01:04,  1.50it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:06,  1.61it/s]Extractor Predicting: 102it [01:06,  1.62it/s]Extractor Predicting: 103it [01:07,  1.59it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:12,  1.20it/s]Extractor Predicting: 110it [01:12,  1.28it/s]Extractor Predicting: 111it [01:13,  1.37it/s]Extractor Predicting: 112it [01:13,  1.40it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.40it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:16,  1.53it/s]Extractor Predicting: 117it [01:17,  1.55it/s]Extractor Predicting: 118it [01:17,  1.62it/s]Extractor Predicting: 119it [01:18,  1.42it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:19,  1.58it/s]Extractor Predicting: 122it [01:20,  1.60it/s]Extractor Predicting: 123it [01:20,  1.65it/s]Extractor Predicting: 124it [01:21,  1.47it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:22,  1.58it/s]Extractor Predicting: 127it [01:23,  1.62it/s]Extractor Predicting: 128it [01:24,  1.70it/s]Extractor Predicting: 129it [01:25,  1.44it/s]Extractor Predicting: 130it [01:25,  1.59it/s]Extractor Predicting: 131it [01:26,  1.67it/s]Extractor Predicting: 132it [01:26,  1.68it/s]Extractor Predicting: 133it [01:27,  1.66it/s]Extractor Predicting: 134it [01:27,  1.63it/s]Extractor Predicting: 135it [01:28,  1.68it/s]Extractor Predicting: 136it [01:28,  1.73it/s]Extractor Predicting: 137it [01:29,  1.74it/s]Extractor Predicting: 138it [01:30,  1.76it/s]Extractor Predicting: 139it [01:30,  1.73it/s]Extractor Predicting: 140it [01:31,  1.73it/s]Extractor Predicting: 141it [01:31,  1.73it/s]Extractor Predicting: 142it [01:32,  1.79it/s]Extractor Predicting: 143it [01:32,  1.78it/s]Extractor Predicting: 144it [01:33,  1.76it/s]Extractor Predicting: 145it [01:34,  1.75it/s]Extractor Predicting: 146it [01:34,  1.53it/s]Extractor Predicting: 147it [01:35,  1.55it/s]Extractor Predicting: 148it [01:36,  1.58it/s]Extractor Predicting: 149it [01:36,  1.58it/s]Extractor Predicting: 150it [01:37,  1.59it/s]Extractor Predicting: 151it [01:38,  1.38it/s]Extractor Predicting: 152it [01:38,  1.45it/s]Extractor Predicting: 153it [01:39,  1.47it/s]Extractor Predicting: 154it [01:40,  1.53it/s]Extractor Predicting: 155it [01:40,  1.58it/s]Extractor Predicting: 156it [01:41,  1.48it/s]Extractor Predicting: 157it [01:42,  1.53it/s]Extractor Predicting: 158it [01:43,  1.27it/s]Extractor Predicting: 159it [01:43,  1.35it/s]Extractor Predicting: 160it [01:44,  1.41it/s]Extractor Predicting: 161it [01:45,  1.46it/s]Extractor Predicting: 162it [01:45,  1.41it/s]Extractor Predicting: 163it [01:46,  1.49it/s]Extractor Predicting: 164it [01:47,  1.52it/s]Extractor Predicting: 165it [01:47,  1.53it/s]Extractor Predicting: 166it [01:48,  1.51it/s]Extractor Predicting: 167it [01:49,  1.41it/s]Extractor Predicting: 168it [01:49,  1.45it/s]Extractor Predicting: 169it [01:50,  1.50it/s]Extractor Predicting: 170it [01:51,  1.52it/s]Extractor Predicting: 171it [01:51,  1.54it/s]Extractor Predicting: 172it [01:52,  1.48it/s]Extractor Predicting: 173it [01:53,  1.49it/s]Extractor Predicting: 174it [01:54,  1.37it/s]Extractor Predicting: 175it [01:54,  1.44it/s]Extractor Predicting: 176it [01:55,  1.48it/s]Extractor Predicting: 177it [01:56,  1.41it/s]Extractor Predicting: 178it [01:56,  1.45it/s]Extractor Predicting: 179it [01:57,  1.49it/s]Extractor Predicting: 180it [01:58,  1.50it/s]Extractor Predicting: 181it [01:58,  1.51it/s]Extractor Predicting: 182it [01:59,  1.38it/s]Extractor Predicting: 183it [02:00,  1.42it/s]Extractor Predicting: 184it [02:00,  1.47it/s]Extractor Predicting: 185it [02:01,  1.50it/s]Extractor Predicting: 186it [02:02,  1.51it/s]Extractor Predicting: 187it [02:02,  1.47it/s]Extractor Predicting: 188it [02:03,  1.50it/s]Extractor Predicting: 189it [02:04,  1.55it/s]Extractor Predicting: 190it [02:04,  1.55it/s]Extractor Predicting: 191it [02:05,  1.58it/s]Extractor Predicting: 192it [02:06,  1.40it/s]Extractor Predicting: 193it [02:06,  1.48it/s]Extractor Predicting: 194it [02:07,  1.50it/s]Extractor Predicting: 195it [02:08,  1.53it/s]Extractor Predicting: 196it [02:08,  1.55it/s]Extractor Predicting: 197it [02:09,  1.48it/s]Extractor Predicting: 198it [02:10,  1.48it/s]Extractor Predicting: 199it [02:10,  1.51it/s]Extractor Predicting: 200it [02:11,  1.53it/s]Extractor Predicting: 201it [02:12,  1.58it/s]Extractor Predicting: 202it [02:12,  1.44it/s]Extractor Predicting: 203it [02:13,  1.51it/s]Extractor Predicting: 204it [02:14,  1.35it/s]Extractor Predicting: 205it [02:15,  1.41it/s]Extractor Predicting: 206it [02:15,  1.51it/s]Extractor Predicting: 207it [02:16,  1.54it/s]Extractor Predicting: 208it [02:16,  1.52it/s]Extractor Predicting: 209it [02:17,  1.53it/s]Extractor Predicting: 210it [02:18,  1.56it/s]Extractor Predicting: 211it [02:18,  1.56it/s]Extractor Predicting: 212it [02:19,  1.54it/s]Extractor Predicting: 213it [02:20,  1.34it/s]Extractor Predicting: 214it [02:21,  1.41it/s]Extractor Predicting: 215it [02:21,  1.44it/s]Extractor Predicting: 216it [02:22,  1.47it/s]Extractor Predicting: 217it [02:23,  1.46it/s]Extractor Predicting: 218it [02:23,  1.34it/s]Extractor Predicting: 219it [02:24,  1.40it/s]Extractor Predicting: 220it [02:25,  1.48it/s]Extractor Predicting: 221it [02:25,  1.50it/s]Extractor Predicting: 222it [02:26,  1.52it/s]Extractor Predicting: 223it [02:27,  1.54it/s]Extractor Predicting: 224it [02:27,  1.55it/s]Extractor Predicting: 225it [02:28,  1.55it/s]Extractor Predicting: 226it [02:28,  1.57it/s]Extractor Predicting: 227it [02:29,  1.55it/s]Extractor Predicting: 228it [02:30,  1.57it/s]Extractor Predicting: 229it [02:30,  1.55it/s]Extractor Predicting: 230it [02:31,  1.57it/s]Extractor Predicting: 231it [02:32,  1.54it/s]Extractor Predicting: 232it [02:32,  1.53it/s]Extractor Predicting: 233it [02:33,  1.40it/s]Extractor Predicting: 234it [02:34,  1.47it/s]Extractor Predicting: 235it [02:34,  1.51it/s]Extractor Predicting: 236it [02:35,  1.53it/s]Extractor Predicting: 237it [02:36,  1.49it/s]Extractor Predicting: 238it [02:37,  1.45it/s]Extractor Predicting: 239it [02:37,  1.46it/s]Extractor Predicting: 240it [02:38,  1.50it/s]Extractor Predicting: 241it [02:38,  1.51it/s]Extractor Predicting: 242it [02:39,  1.54it/s]Extractor Predicting: 243it [02:40,  1.50it/s]Extractor Predicting: 244it [02:40,  1.52it/s]Extractor Predicting: 245it [02:41,  1.53it/s]Extractor Predicting: 246it [02:42,  1.51it/s]Extractor Predicting: 247it [02:42,  1.47it/s]Extractor Predicting: 248it [02:43,  1.36it/s]Extractor Predicting: 249it [02:44,  1.32it/s]Extractor Predicting: 250it [02:45,  1.40it/s]Extractor Predicting: 251it [02:45,  1.44it/s]Extractor Predicting: 252it [02:46,  1.50it/s]Extractor Predicting: 253it [02:47,  1.53it/s]Extractor Predicting: 254it [02:48,  1.25it/s]Extractor Predicting: 255it [02:48,  1.33it/s]Extractor Predicting: 256it [02:49,  1.38it/s]Extractor Predicting: 257it [02:50,  1.42it/s]Extractor Predicting: 258it [02:50,  1.41it/s]Extractor Predicting: 259it [02:51,  1.48it/s]Extractor Predicting: 260it [02:52,  1.52it/s]Extractor Predicting: 261it [02:52,  1.56it/s]Extractor Predicting: 262it [02:53,  1.57it/s]Extractor Predicting: 263it [02:54,  1.54it/s]Extractor Predicting: 264it [02:54,  1.57it/s]Extractor Predicting: 265it [02:55,  1.59it/s]Extractor Predicting: 266it [02:55,  1.59it/s]Extractor Predicting: 267it [02:56,  1.62it/s]Extractor Predicting: 268it [02:57,  1.54it/s]Extractor Predicting: 269it [02:57,  1.56it/s]Extractor Predicting: 270it [02:58,  1.54it/s]Extractor Predicting: 271it [02:59,  1.59it/s]Extractor Predicting: 272it [02:59,  1.60it/s]Extractor Predicting: 273it [03:00,  1.58it/s]Extractor Predicting: 274it [03:00,  1.60it/s]Extractor Predicting: 275it [03:01,  1.63it/s]Extractor Predicting: 276it [03:02,  1.44it/s]Extractor Predicting: 277it [03:03,  1.49it/s]Extractor Predicting: 278it [03:04,  1.24it/s]Extractor Predicting: 279it [03:04,  1.33it/s]Extractor Predicting: 280it [03:05,  1.38it/s]Extractor Predicting: 281it [03:06,  1.47it/s]Extractor Predicting: 282it [03:06,  1.50it/s]Extractor Predicting: 283it [03:07,  1.43it/s]Extractor Predicting: 284it [03:07,  1.67it/s]Extractor Predicting: 284it [03:07,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:17,374 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:17,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:52:18,833 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:52:18,834 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:52:19,561 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:52:20,969 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:52:21,064 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:24,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:24,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:24,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:24,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:52:24,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:52:26,796 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:52:26,798 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:52:27,595 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:52:27,933 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:52:27,981 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25,
  "recall": 0.14583945898265216,
  "score": 0.18421541318477253,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:02,  1.96it/s]Extractor Predicting: 5it [00:02,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:52:39,744 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:39,913 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:52:40,143 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:40,145 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:52:40,195 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:53:27,881 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:53:28,035 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:53:28,638 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:53:28,639 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:53:28,757 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:28,822 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3023255813953488,
  "recall": 0.06565656565656566,
  "score": 0.10788381742738591,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:53:29,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:30,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:30,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:36,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:38,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:38,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:39,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:41,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:51, 16.52s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:46,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:46,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:49,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:51,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:52,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:54,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:57,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:57,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:02, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:58,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:00,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:00,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:03,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:03,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:09,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:11,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:13,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:57, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:14,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:15,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:18,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:21,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:22,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:22,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:27,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:27,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:46, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:29,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:31,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:35,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:39,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:22, 14.26s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:42,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:44,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:44,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:49,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:50,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:50,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:51,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:51,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:52,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:07, 14.15s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:56,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:00,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:07,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:07,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:12,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:45<02:04, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:15,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:16,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:24,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:30,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:01<01:50, 15.77s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:31,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:42,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:18<01:36, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:47,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:50,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:51,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:53,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:56,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:59,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:00,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:31<01:15, 15.15s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:00,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:07,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:07,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:44<00:58, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:14,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:17,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:17,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:18,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:20,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:23,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:25,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:25,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:57<00:42, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:27,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:28,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:31,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:31,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:33,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:33,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:34,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:35,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:35,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:37,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:38,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:38,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:39,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:40,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:40,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:11<00:28, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:41,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:41,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:43,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:45,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:45,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:46,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:46,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:47,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:48,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:49,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:49,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:50,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:51,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:51,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:52,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:52,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:53,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:54,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:25<00:14, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:55,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:55,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:56,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:56,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:57,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:58,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:58,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:59,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:59,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:00,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:00,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:02,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:03,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:03,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:04,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:04,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:05,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:05,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:06,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:07,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:07,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:39<00:00, 13.91s/it]Generating: 100%|██████████| 15/15 [03:39<00:00, 14.62s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:30,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:30,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:30,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:30,881 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:30,881 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:57:32,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:57:32,984 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:57:33,747 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:57:35,043 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:57:35,043 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:38,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:38,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:38,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:38,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:38,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:57:40,311 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:57:40,313 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:57:41,010 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:57:41,780 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:57:42,042 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 561, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8943452380952381, 'errors': {''}}
['Relation : located in the administrative territorial entity . Context : On 31 March 2014 , the municipality announced that it would be expanding its municipal boundaries at the end of 2016 , bringing it to a total of 26 municipalities . Head Entity : Municipality , Tail Entity : Municipality .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 9168
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9268, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.60it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:04,  1.27it/s]Extractor Estimating: 7it [00:04,  1.39it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:07,  1.53it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.51it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.53it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.58it/s]Extractor Estimating: 20it [00:13,  1.59it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:15,  1.55it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:17,  1.32it/s]Extractor Estimating: 27it [00:17,  1.48it/s]Extractor Estimating: 28it [00:18,  1.65it/s]Extractor Estimating: 29it [00:18,  1.81it/s]Extractor Estimating: 30it [00:19,  1.85it/s]Extractor Estimating: 31it [00:19,  1.90it/s]Extractor Estimating: 32it [00:20,  1.73it/s]Extractor Estimating: 33it [00:20,  1.91it/s]Extractor Estimating: 34it [00:21,  1.95it/s]Extractor Estimating: 35it [00:21,  2.01it/s]Extractor Estimating: 36it [00:22,  1.44it/s]Extractor Estimating: 37it [00:23,  1.64it/s]Extractor Estimating: 38it [00:23,  1.73it/s]Extractor Estimating: 39it [00:24,  1.88it/s]Extractor Estimating: 40it [00:24,  1.92it/s]Extractor Estimating: 41it [00:25,  2.02it/s]Extractor Estimating: 42it [00:26,  1.53it/s]Extractor Estimating: 43it [00:26,  1.68it/s]Extractor Estimating: 44it [00:27,  1.77it/s]Extractor Estimating: 45it [00:27,  1.86it/s]Extractor Estimating: 46it [00:28,  1.99it/s]Extractor Estimating: 47it [00:28,  1.99it/s]Extractor Estimating: 48it [00:29,  1.78it/s]Extractor Estimating: 49it [00:29,  1.87it/s]Extractor Estimating: 50it [00:30,  1.91it/s]Extractor Estimating: 51it [00:30,  1.86it/s]Extractor Estimating: 52it [00:31,  1.75it/s]Extractor Estimating: 53it [00:32,  1.78it/s]Extractor Estimating: 54it [00:33,  1.47it/s]Extractor Estimating: 55it [00:33,  1.49it/s]Extractor Estimating: 56it [00:34,  1.53it/s]Extractor Estimating: 57it [00:34,  1.62it/s]Extractor Estimating: 58it [00:35,  1.72it/s]Extractor Estimating: 59it [00:36,  1.50it/s]Extractor Estimating: 60it [00:36,  1.59it/s]Extractor Estimating: 61it [00:37,  1.63it/s]Extractor Estimating: 62it [00:37,  1.68it/s]Extractor Estimating: 63it [00:38,  1.66it/s]Extractor Estimating: 64it [00:39,  1.67it/s]Extractor Estimating: 65it [00:39,  1.62it/s]Extractor Estimating: 66it [00:40,  1.65it/s]Extractor Estimating: 67it [00:40,  1.64it/s]Extractor Estimating: 68it [00:41,  1.69it/s]Extractor Estimating: 69it [00:42,  1.51it/s]Extractor Estimating: 70it [00:43,  1.45it/s]Extractor Estimating: 71it [00:43,  1.52it/s]Extractor Estimating: 72it [00:44,  1.34it/s]Extractor Estimating: 73it [00:45,  1.42it/s]Extractor Estimating: 74it [00:45,  1.51it/s]Extractor Estimating: 75it [00:46,  1.58it/s]Extractor Estimating: 76it [00:46,  1.65it/s]Extractor Estimating: 77it [00:47,  1.57it/s]Extractor Estimating: 78it [00:48,  1.56it/s]Extractor Estimating: 79it [00:48,  1.54it/s]Extractor Estimating: 80it [00:49,  1.58it/s]Extractor Estimating: 81it [00:50,  1.58it/s]Extractor Estimating: 82it [00:50,  1.65it/s]Extractor Estimating: 83it [00:51,  1.66it/s]Extractor Estimating: 84it [00:52,  1.50it/s]Extractor Estimating: 85it [00:52,  1.60it/s]Extractor Estimating: 86it [00:53,  1.57it/s]Extractor Estimating: 87it [00:54,  1.49it/s]Extractor Estimating: 88it [00:54,  1.52it/s]Extractor Estimating: 89it [00:55,  1.57it/s]Extractor Estimating: 90it [00:55,  1.61it/s]Extractor Estimating: 91it [00:56,  1.44it/s]Extractor Estimating: 92it [00:57,  1.36it/s]Extractor Estimating: 93it [00:58,  1.43it/s]Extractor Estimating: 94it [00:58,  1.48it/s]Extractor Estimating: 95it [00:59,  1.54it/s]Extractor Estimating: 96it [01:00,  1.38it/s]Extractor Estimating: 97it [01:00,  1.45it/s]Extractor Estimating: 98it [01:01,  1.52it/s]Extractor Estimating: 99it [01:02,  1.57it/s]Extractor Estimating: 100it [01:02,  1.57it/s]Extractor Estimating: 101it [01:03,  1.56it/s]Extractor Estimating: 102it [01:03,  1.67it/s]Extractor Estimating: 103it [01:04,  1.72it/s]Extractor Estimating: 104it [01:04,  1.73it/s]Extractor Estimating: 105it [01:05,  1.75it/s]Extractor Estimating: 106it [01:06,  1.76it/s]Extractor Estimating: 107it [01:06,  1.62it/s]Extractor Estimating: 108it [01:07,  1.75it/s]Extractor Estimating: 109it [01:07,  1.77it/s]Extractor Estimating: 110it [01:08,  1.76it/s]Extractor Estimating: 111it [01:08,  1.82it/s]Extractor Estimating: 112it [01:09,  1.86it/s]Extractor Estimating: 113it [01:10,  1.56it/s]Extractor Estimating: 114it [01:10,  1.63it/s]Extractor Estimating: 115it [01:11,  1.74it/s]Extractor Estimating: 116it [01:11,  1.77it/s]Extractor Estimating: 117it [01:12,  1.80it/s]Extractor Estimating: 118it [01:12,  1.85it/s]Extractor Estimating: 119it [01:13,  1.62it/s]Extractor Estimating: 120it [01:14,  1.67it/s]Extractor Estimating: 121it [01:14,  1.76it/s]Extractor Estimating: 122it [01:15,  1.78it/s]Extractor Estimating: 123it [01:15,  1.80it/s]Extractor Estimating: 124it [01:16,  1.80it/s]Extractor Estimating: 125it [01:16,  1.83it/s]Extractor Estimating: 126it [01:17,  1.81it/s]Extractor Estimating: 127it [01:17,  1.84it/s]Extractor Estimating: 128it [01:18,  1.84it/s]Extractor Estimating: 129it [01:19,  1.81it/s]Extractor Estimating: 130it [01:19,  1.72it/s]Extractor Estimating: 131it [01:20,  1.54it/s]Extractor Estimating: 132it [01:21,  1.65it/s]Extractor Estimating: 133it [01:21,  1.70it/s]Extractor Estimating: 134it [01:22,  1.77it/s]Extractor Estimating: 135it [01:22,  1.83it/s]Extractor Estimating: 136it [01:23,  1.75it/s]Extractor Estimating: 137it [01:24,  1.29it/s]Extractor Estimating: 138it [01:25,  1.39it/s]Extractor Estimating: 139it [01:25,  1.51it/s]Extractor Estimating: 140it [01:26,  1.58it/s]Extractor Estimating: 141it [01:26,  1.65it/s]Extractor Estimating: 142it [01:28,  1.19it/s]Extractor Estimating: 143it [01:28,  1.36it/s]Extractor Estimating: 144it [01:29,  1.51it/s]Extractor Estimating: 145it [01:29,  1.62it/s]Extractor Estimating: 146it [01:30,  1.72it/s]Extractor Estimating: 147it [01:30,  1.56it/s]Extractor Estimating: 148it [01:31,  1.64it/s]Extractor Estimating: 149it [01:31,  1.75it/s]Extractor Estimating: 150it [01:32,  1.82it/s]Extractor Estimating: 151it [01:32,  1.81it/s]Extractor Estimating: 152it [01:33,  1.92it/s]Extractor Estimating: 153it [01:34,  1.72it/s]Extractor Estimating: 154it [01:34,  1.78it/s]Extractor Estimating: 155it [01:35,  1.80it/s]Extractor Estimating: 156it [01:35,  1.87it/s]Extractor Estimating: 157it [01:36,  1.75it/s]Extractor Estimating: 158it [01:36,  1.83it/s]Extractor Estimating: 159it [01:37,  1.50it/s]Extractor Estimating: 160it [01:38,  1.51it/s]Extractor Estimating: 161it [01:38,  1.63it/s]Extractor Estimating: 162it [01:39,  1.73it/s]Extractor Estimating: 163it [01:39,  1.79it/s]Extractor Estimating: 164it [01:40,  1.58it/s]Extractor Estimating: 165it [01:41,  1.66it/s]Extractor Estimating: 166it [01:41,  1.75it/s]Extractor Estimating: 167it [01:42,  1.76it/s]Extractor Estimating: 168it [01:42,  1.82it/s]Extractor Estimating: 169it [01:43,  1.89it/s]Extractor Estimating: 170it [01:43,  1.77it/s]Extractor Estimating: 171it [01:44,  1.82it/s]Extractor Estimating: 172it [01:45,  1.81it/s]Extractor Estimating: 173it [01:45,  1.85it/s]Extractor Estimating: 174it [01:46,  1.84it/s]Extractor Estimating: 175it [01:46,  1.85it/s]Extractor Estimating: 176it [01:47,  1.83it/s]Extractor Estimating: 177it [01:47,  1.87it/s]Extractor Estimating: 178it [01:48,  1.87it/s]Extractor Estimating: 179it [01:48,  1.82it/s]Extractor Estimating: 180it [01:49,  1.70it/s]Extractor Estimating: 181it [01:50,  1.72it/s]Extractor Estimating: 182it [01:50,  1.65it/s]Extractor Estimating: 183it [01:51,  1.70it/s]Extractor Estimating: 184it [01:51,  1.70it/s]Extractor Estimating: 185it [01:52,  1.74it/s]Extractor Estimating: 186it [01:52,  1.74it/s]Extractor Estimating: 187it [01:53,  1.81it/s]Extractor Estimating: 188it [01:54,  1.74it/s]Extractor Estimating: 189it [01:54,  1.80it/s]Extractor Estimating: 190it [01:55,  1.80it/s]Extractor Estimating: 191it [01:55,  1.88it/s]Extractor Estimating: 192it [01:56,  1.93it/s]Extractor Estimating: 193it [01:56,  1.92it/s]Extractor Estimating: 194it [01:57,  1.90it/s]Extractor Estimating: 195it [01:57,  1.96it/s]Extractor Estimating: 196it [01:58,  1.88it/s]Extractor Estimating: 197it [01:58,  1.94it/s]Extractor Estimating: 198it [01:59,  1.87it/s]Extractor Estimating: 199it [01:59,  1.90it/s]Extractor Estimating: 200it [02:00,  1.92it/s]Extractor Estimating: 201it [02:00,  1.80it/s]Extractor Estimating: 202it [02:01,  1.59it/s]Extractor Estimating: 203it [02:02,  1.56it/s]Extractor Estimating: 204it [02:03,  1.58it/s]Extractor Estimating: 205it [02:03,  1.59it/s]Extractor Estimating: 206it [02:04,  1.62it/s]Extractor Estimating: 207it [02:05,  1.48it/s]Extractor Estimating: 208it [02:05,  1.47it/s]Extractor Estimating: 209it [02:06,  1.51it/s]Extractor Estimating: 210it [02:06,  1.58it/s]Extractor Estimating: 211it [02:07,  1.64it/s]Extractor Estimating: 212it [02:08,  1.59it/s]Extractor Estimating: 213it [02:08,  1.59it/s]Extractor Estimating: 214it [02:09,  1.62it/s]Extractor Estimating: 215it [02:09,  1.63it/s]Extractor Estimating: 216it [02:10,  1.65it/s]Extractor Estimating: 217it [02:11,  1.43it/s]Extractor Estimating: 218it [02:12,  1.49it/s]Extractor Estimating: 219it [02:12,  1.50it/s]Extractor Estimating: 220it [02:13,  1.60it/s]Extractor Estimating: 221it [02:13,  1.59it/s]Extractor Estimating: 222it [02:14,  1.46it/s]Extractor Estimating: 223it [02:15,  1.48it/s]Extractor Estimating: 224it [02:16,  1.46it/s]Extractor Estimating: 225it [02:16,  1.53it/s]Extractor Estimating: 226it [02:17,  1.63it/s]Extractor Estimating: 227it [02:17,  1.51it/s]Extractor Estimating: 228it [02:18,  1.65it/s]Extractor Estimating: 229it [02:18,  1.72it/s]Extractor Estimating: 230it [02:19,  1.75it/s]Extractor Estimating: 231it [02:20,  1.76it/s]Extractor Estimating: 232it [02:20,  1.80it/s]Extractor Estimating: 233it [02:21,  1.71it/s]Extractor Estimating: 234it [02:21,  1.78it/s]Extractor Estimating: 235it [02:22,  1.77it/s]Extractor Estimating: 236it [02:22,  1.83it/s]Extractor Estimating: 237it [02:23,  1.84it/s]Extractor Estimating: 238it [02:23,  1.89it/s]Extractor Estimating: 239it [02:24,  1.52it/s]Extractor Estimating: 240it [02:25,  1.64it/s]Extractor Estimating: 241it [02:26,  1.46it/s]Extractor Estimating: 242it [02:26,  1.52it/s]Extractor Estimating: 243it [02:27,  1.60it/s]Extractor Estimating: 244it [02:27,  1.67it/s]Extractor Estimating: 245it [02:28,  1.78it/s]Extractor Estimating: 246it [02:29,  1.49it/s]Extractor Estimating: 247it [02:29,  1.56it/s]Extractor Estimating: 248it [02:30,  1.69it/s]Extractor Estimating: 249it [02:30,  1.73it/s]Extractor Estimating: 250it [02:31,  1.73it/s]Extractor Estimating: 251it [02:32,  1.49it/s]Extractor Estimating: 252it [02:32,  1.57it/s]Extractor Estimating: 253it [02:33,  1.51it/s]Extractor Estimating: 254it [02:34,  1.60it/s]Extractor Estimating: 255it [02:34,  1.61it/s]Extractor Estimating: 256it [02:35,  1.51it/s]Extractor Estimating: 257it [02:36,  1.56it/s]Extractor Estimating: 258it [02:36,  1.61it/s]Extractor Estimating: 259it [02:37,  1.66it/s]Extractor Estimating: 260it [02:37,  1.65it/s]Extractor Estimating: 261it [02:38,  1.51it/s]Extractor Estimating: 262it [02:39,  1.56it/s]Extractor Estimating: 263it [02:39,  1.57it/s]Extractor Estimating: 264it [02:40,  1.52it/s]Extractor Estimating: 265it [02:41,  1.58it/s]Extractor Estimating: 266it [02:42,  1.39it/s]Extractor Estimating: 267it [02:42,  1.38it/s]Extractor Estimating: 268it [02:43,  1.47it/s]Extractor Estimating: 269it [02:44,  1.51it/s]Extractor Estimating: 270it [02:44,  1.56it/s]Extractor Estimating: 271it [02:45,  1.47it/s]Extractor Estimating: 272it [02:46,  1.51it/s]Extractor Estimating: 273it [02:46,  1.57it/s]Extractor Estimating: 274it [02:47,  1.57it/s]Extractor Estimating: 275it [02:47,  1.61it/s]Extractor Estimating: 276it [02:48,  1.47it/s]Extractor Estimating: 277it [02:49,  1.56it/s]Extractor Estimating: 278it [02:49,  1.61it/s]Extractor Estimating: 279it [02:50,  1.61it/s]Extractor Estimating: 280it [02:50,  1.64it/s]Extractor Estimating: 281it [02:51,  1.69it/s]Extractor Estimating: 282it [02:52,  1.76it/s]Extractor Estimating: 283it [02:52,  1.82it/s]Extractor Estimating: 284it [02:53,  1.85it/s]Extractor Estimating: 285it [02:53,  1.81it/s]Extractor Estimating: 286it [02:54,  1.79it/s]Extractor Estimating: 287it [02:54,  1.59it/s]Extractor Estimating: 288it [02:55,  1.70it/s]Extractor Estimating: 289it [02:56,  1.75it/s]Extractor Estimating: 290it [02:56,  1.80it/s]Extractor Estimating: 291it [02:57,  1.79it/s]Extractor Estimating: 292it [02:57,  1.82it/s]Extractor Estimating: 293it [02:58,  1.87it/s]Extractor Estimating: 294it [02:58,  1.88it/s]Extractor Estimating: 295it [02:59,  1.94it/s]Extractor Estimating: 296it [02:59,  1.70it/s]Extractor Estimating: 297it [03:00,  1.69it/s]Extractor Estimating: 298it [03:01,  1.70it/s]Extractor Estimating: 299it [03:01,  1.79it/s]Extractor Estimating: 300it [03:02,  1.84it/s]Extractor Estimating: 301it [03:02,  1.86it/s]Extractor Estimating: 302it [03:03,  1.77it/s]Extractor Estimating: 303it [03:03,  1.72it/s]Extractor Estimating: 304it [03:04,  1.76it/s]Extractor Estimating: 305it [03:04,  1.78it/s]Extractor Estimating: 306it [03:05,  1.74it/s]Extractor Estimating: 307it [03:06,  1.75it/s]Extractor Estimating: 308it [03:06,  1.68it/s]Extractor Estimating: 309it [03:07,  1.78it/s]Extractor Estimating: 310it [03:07,  1.80it/s]Extractor Estimating: 311it [03:08,  1.80it/s]Extractor Estimating: 312it [03:08,  1.81it/s]Extractor Estimating: 313it [03:09,  1.81it/s]Extractor Estimating: 314it [03:10,  1.61it/s]Extractor Estimating: 315it [03:10,  1.69it/s]Extractor Estimating: 316it [03:11,  1.70it/s]Extractor Estimating: 317it [03:11,  1.82it/s]Extractor Estimating: 318it [03:12,  1.78it/s]Extractor Estimating: 319it [03:12,  1.72it/s]Extractor Estimating: 320it [03:13,  1.67it/s]Extractor Estimating: 321it [03:14,  1.74it/s]Extractor Estimating: 322it [03:14,  1.77it/s]Extractor Estimating: 323it [03:15,  1.78it/s]Extractor Estimating: 324it [03:15,  1.85it/s]Extractor Estimating: 325it [03:16,  1.90it/s]Extractor Estimating: 326it [03:16,  1.71it/s]Extractor Estimating: 327it [03:17,  1.73it/s]Extractor Estimating: 328it [03:18,  1.78it/s]Extractor Estimating: 329it [03:18,  1.84it/s]Extractor Estimating: 330it [03:19,  1.81it/s]Extractor Estimating: 331it [03:19,  1.84it/s]Extractor Estimating: 332it [03:20,  1.50it/s]Extractor Estimating: 333it [03:21,  1.62it/s]Extractor Estimating: 334it [03:21,  1.72it/s]Extractor Estimating: 335it [03:22,  1.72it/s]Extractor Estimating: 336it [03:22,  1.77it/s]Extractor Estimating: 337it [03:23,  1.36it/s]Extractor Estimating: 338it [03:24,  1.44it/s]Extractor Estimating: 339it [03:25,  1.48it/s]Extractor Estimating: 340it [03:25,  1.52it/s]Extractor Estimating: 341it [03:26,  1.58it/s]Extractor Estimating: 342it [03:27,  1.24it/s]Extractor Estimating: 343it [03:27,  1.39it/s]Extractor Estimating: 344it [03:28,  1.44it/s]Extractor Estimating: 345it [03:29,  1.54it/s]Extractor Estimating: 346it [03:29,  1.65it/s]Extractor Estimating: 347it [03:30,  1.51it/s]Extractor Estimating: 348it [03:30,  1.60it/s]Extractor Estimating: 349it [03:31,  1.65it/s]Extractor Estimating: 350it [03:32,  1.70it/s]Extractor Estimating: 351it [03:32,  1.72it/s]Extractor Estimating: 352it [03:33,  1.51it/s]Extractor Estimating: 353it [03:34,  1.54it/s]Extractor Estimating: 354it [03:34,  1.49it/s]Extractor Estimating: 355it [03:35,  1.54it/s]Extractor Estimating: 356it [03:36,  1.55it/s]Extractor Estimating: 357it [03:36,  1.45it/s]Extractor Estimating: 358it [03:37,  1.47it/s]Extractor Estimating: 359it [03:38,  1.52it/s]Extractor Estimating: 360it [03:38,  1.58it/s]Extractor Estimating: 361it [03:39,  1.64it/s]Extractor Estimating: 362it [03:40,  1.30it/s]Extractor Estimating: 363it [03:41,  1.40it/s]Extractor Estimating: 364it [03:41,  1.48it/s]Extractor Estimating: 365it [03:42,  1.48it/s]Extractor Estimating: 366it [03:42,  1.55it/s]Extractor Estimating: 367it [03:43,  1.38it/s]Extractor Estimating: 368it [03:44,  1.47it/s]Extractor Estimating: 369it [03:44,  1.52it/s]Extractor Estimating: 370it [03:45,  1.59it/s]Extractor Estimating: 371it [03:46,  1.55it/s]Extractor Estimating: 372it [03:47,  1.34it/s]Extractor Estimating: 373it [03:47,  1.39it/s]Extractor Estimating: 374it [03:48,  1.49it/s]Extractor Estimating: 375it [03:48,  1.55it/s]Extractor Estimating: 375it [03:48,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:03,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:03,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:03,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:03,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:03,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:02:05,710 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:02:05,712 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:06,176 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:07,505 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:07,618 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:10,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:10,936 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:10,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:10,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:10,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:02:12,442 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:02:12,443 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:13,202 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:13,745 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:13,745 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:14:40,167 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:14:40,224 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4499 mean pseudo reward: 0.9809538619897911
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 14815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14915, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.917, loss:373.4702
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 0.926, loss:370.8647
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 0.936, loss:318.5768
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 0.913, loss:290.6801
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 0.925, loss:281.1460
>> valid entity prec:0.5683, rec:0.5789, f1:0.5735
>> valid relation prec:0.2250, rec:0.1424, f1:0.1744
>> valid relation with NER prec:0.2250, rec:0.1424, f1:0.1744
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 2.179, loss:279.5586
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 0.926, loss:253.6670
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 0.932, loss:254.8319
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 0.919, loss:240.4379
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 0.925, loss:255.7932
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5517, rec:0.5324, f1:0.5419
>> valid relation prec:0.2165, rec:0.1418, f1:0.1713
>> valid relation with NER prec:0.2165, rec:0.1418, f1:0.1713
g_step 1100, step 160, avg_time 2.152, loss:269.8061
g_step 1200, step 72, avg_time 0.911, loss:227.3056
g_step 1300, step 172, avg_time 0.930, loss:239.8289
g_step 1400, step 84, avg_time 0.910, loss:202.6579
g_step 1500, step 184, avg_time 0.936, loss:229.0737
>> valid entity prec:0.5409, rec:0.5010, f1:0.5202
>> valid relation prec:0.1729, rec:0.1137, f1:0.1372
>> valid relation with NER prec:0.1729, rec:0.1137, f1:0.1372
g_step 1600, step 96, avg_time 2.145, loss:201.5279
g_step 1700, step 8, avg_time 0.919, loss:196.7914
g_step 1800, step 108, avg_time 0.922, loss:194.2881
g_step 1900, step 20, avg_time 0.929, loss:194.4891
g_step 2000, step 120, avg_time 0.925, loss:173.5089
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5722, rec:0.5215, f1:0.5457
>> valid relation prec:0.1587, rec:0.1051, f1:0.1264
>> valid relation with NER prec:0.1587, rec:0.1051, f1:0.1264
g_step 2100, step 32, avg_time 2.145, loss:170.3471
g_step 2200, step 132, avg_time 0.924, loss:169.3859
g_step 2300, step 44, avg_time 0.941, loss:155.1950
g_step 2400, step 144, avg_time 0.917, loss:153.1490
g_step 2500, step 56, avg_time 0.914, loss:152.1648
>> valid entity prec:0.5562, rec:0.5230, f1:0.5391
>> valid relation prec:0.1711, rec:0.1171, f1:0.1390
>> valid relation with NER prec:0.1711, rec:0.1171, f1:0.1390
g_step 2600, step 156, avg_time 2.155, loss:155.4079
g_step 2700, step 68, avg_time 0.912, loss:157.5720
g_step 2800, step 168, avg_time 0.922, loss:150.9824
g_step 2900, step 80, avg_time 0.942, loss:143.8817
g_step 3000, step 180, avg_time 0.911, loss:145.2000
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5505, rec:0.5773, f1:0.5636
>> valid relation prec:0.1651, rec:0.1429, f1:0.1532
>> valid relation with NER prec:0.1651, rec:0.1429, f1:0.1532
g_step 3100, step 92, avg_time 2.140, loss:129.6900
g_step 3200, step 4, avg_time 0.926, loss:145.8045
g_step 3300, step 104, avg_time 0.911, loss:117.7639
g_step 3400, step 16, avg_time 0.919, loss:125.8280
g_step 3500, step 116, avg_time 0.923, loss:118.7332
>> valid entity prec:0.5551, rec:0.4879, f1:0.5193
>> valid relation prec:0.1742, rec:0.1177, f1:0.1405
>> valid relation with NER prec:0.1742, rec:0.1177, f1:0.1405
g_step 3600, step 28, avg_time 2.140, loss:120.7272
g_step 3700, step 128, avg_time 0.928, loss:118.3529
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:14:40 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:14:40 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-14-40_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:14:42 - WARNING - datasets.builder -   Using custom data configuration default-a0a01d58e7de29db
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a0a01d58e7de29db/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:14:52,861 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:52,893 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:14:52,894 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:52,895 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:14:53,014 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:53,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:14:54,731 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:14:58,096 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:14:58,426 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a0a01d58e7de29db/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.06ba/s] 40%|████      | 2/5 [00:01<00:01,  2.00ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.79ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.41ba/s]100%|██████████| 5/5 [00:01<00:00,  3.09ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.65ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.34ba/s]100%|██████████| 4/4 [00:01<00:00,  3.31ba/s]100%|██████████| 4/4 [00:01<00:00,  2.35ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:04,  1.24s/ba] 40%|████      | 2/5 [00:01<00:01,  1.73ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.74ba/s]100%|██████████| 5/5 [00:01<00:00,  5.06ba/s]100%|██████████| 5/5 [00:01<00:00,  3.12ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.26s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.70ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71ba/s]100%|██████████| 4/4 [00:01<00:00,  2.61ba/s]
[INFO|trainer.py:414] 2023-08-28 21:15:22,779 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:15:23,110 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:15:23,110 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 21:15:23,110 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:15:23,110 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:15:23,110 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:15:23,110 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:15:23,110 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:02<15:32,  2.67s/it]  1%|          | 2/350 [00:04<13:20,  2.30s/it]  1%|          | 3/350 [00:05<09:40,  1.67s/it]  1%|          | 4/350 [00:05<06:29,  1.13s/it]  1%|▏         | 5/350 [00:06<05:41,  1.01it/s]  2%|▏         | 6/350 [00:07<04:23,  1.31it/s]  2%|▏         | 7/350 [00:07<03:29,  1.64it/s]  2%|▏         | 8/350 [00:07<02:54,  1.96it/s]  3%|▎         | 9/350 [00:07<02:30,  2.27it/s]  3%|▎         | 10/350 [00:08<02:14,  2.53it/s]  3%|▎         | 11/350 [00:08<02:03,  2.75it/s]  3%|▎         | 12/350 [00:08<01:55,  2.93it/s]  4%|▎         | 13/350 [00:09<01:50,  3.06it/s]  4%|▍         | 14/350 [00:09<01:55,  2.91it/s]  4%|▍         | 15/350 [00:09<01:49,  3.05it/s]  5%|▍         | 16/350 [00:10<01:45,  3.15it/s]  5%|▍         | 17/350 [00:10<01:43,  3.23it/s]  5%|▌         | 18/350 [00:10<01:40,  3.29it/s]  5%|▌         | 19/350 [00:10<01:39,  3.33it/s]  6%|▌         | 20/350 [00:11<01:38,  3.36it/s]  6%|▌         | 21/350 [00:11<01:37,  3.38it/s]  6%|▋         | 22/350 [00:11<01:36,  3.39it/s]  7%|▋         | 23/350 [00:12<01:36,  3.40it/s]  7%|▋         | 24/350 [00:12<02:31,  2.16it/s]  7%|▋         | 25/350 [00:13<02:13,  2.43it/s]  7%|▋         | 26/350 [00:13<02:01,  2.66it/s]  8%|▊         | 27/350 [00:13<01:53,  2.85it/s]  8%|▊         | 28/350 [00:14<01:47,  3.00it/s]  8%|▊         | 29/350 [00:14<01:43,  3.12it/s]  9%|▊         | 30/350 [00:14<01:39,  3.20it/s]  9%|▉         | 31/350 [00:14<01:37,  3.27it/s]  9%|▉         | 32/350 [00:15<01:36,  3.31it/s]  9%|▉         | 33/350 [00:15<01:39,  3.18it/s] 10%|▉         | 34/350 [00:15<01:37,  3.25it/s] 10%|█         | 35/350 [00:16<01:35,  3.30it/s] 10%|█         | 36/350 [00:16<01:34,  3.34it/s] 11%|█         | 37/350 [00:16<01:33,  3.36it/s] 11%|█         | 38/350 [00:17<01:32,  3.38it/s] 11%|█         | 39/350 [00:17<01:31,  3.39it/s] 11%|█▏        | 40/350 [00:17<01:31,  3.40it/s] 12%|█▏        | 41/350 [00:17<01:30,  3.41it/s] 12%|█▏        | 42/350 [00:18<01:30,  3.41it/s] 12%|█▏        | 43/350 [00:18<01:29,  3.41it/s] 13%|█▎        | 44/350 [00:19<02:17,  2.23it/s] 13%|█▎        | 45/350 [00:19<02:02,  2.49it/s] 13%|█▎        | 46/350 [00:19<01:52,  2.71it/s] 13%|█▎        | 47/350 [00:20<01:44,  2.89it/s] 14%|█▎        | 48/350 [00:20<01:39,  3.03it/s] 14%|█▍        | 49/350 [00:20<01:35,  3.14it/s] 14%|█▍        | 50/350 [00:21<01:33,  3.22it/s] 15%|█▍        | 51/350 [00:21<01:31,  3.28it/s] 15%|█▍        | 52/350 [00:21<01:29,  3.32it/s] 15%|█▌        | 53/350 [00:22<01:49,  2.72it/s] 15%|█▌        | 54/350 [00:22<02:04,  2.38it/s] 16%|█▌        | 55/350 [00:23<01:52,  2.61it/s] 16%|█▌        | 56/350 [00:23<01:44,  2.81it/s] 16%|█▋        | 57/350 [00:23<01:38,  2.97it/s] 17%|█▋        | 58/350 [00:23<01:34,  3.09it/s] 17%|█▋        | 59/350 [00:24<01:31,  3.18it/s] 17%|█▋        | 60/350 [00:24<01:29,  3.25it/s] 17%|█▋        | 61/350 [00:24<01:27,  3.30it/s] 18%|█▊        | 62/350 [00:25<01:26,  3.33it/s] 18%|█▊        | 63/350 [00:25<01:40,  2.87it/s] 18%|█▊        | 64/350 [00:25<01:34,  3.03it/s] 19%|█▊        | 65/350 [00:26<01:30,  3.15it/s] 19%|█▉        | 66/350 [00:26<01:27,  3.24it/s] 19%|█▉        | 67/350 [00:26<01:25,  3.31it/s] 19%|█▉        | 68/350 [00:26<01:24,  3.35it/s] 20%|█▉        | 69/350 [00:27<01:22,  3.39it/s] 20%|██        | 70/350 [00:27<01:22,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:15:50,709 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:15:50,709 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:15:50,709 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.53it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.09it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.47it/s][A
  5%|▌         | 22/436 [00:01<00:22, 18.29it/s][A
  6%|▌         | 26/436 [00:01<00:25, 16.28it/s][A
  7%|▋         | 31/436 [00:01<00:19, 20.96it/s][A
  8%|▊         | 36/436 [00:01<00:15, 25.43it/s][A
  9%|▉         | 41/436 [00:01<00:13, 29.59it/s][A
 11%|█         | 46/436 [00:01<00:11, 33.20it/s][A
 12%|█▏        | 51/436 [00:02<00:19, 19.89it/s][A
 13%|█▎        | 56/436 [00:02<00:15, 24.09it/s][A
 14%|█▍        | 60/436 [00:02<00:14, 25.19it/s][A
 15%|█▍        | 65/436 [00:02<00:12, 29.28it/s][A
 16%|█▌        | 70/436 [00:02<00:11, 32.87it/s][A
 17%|█▋        | 75/436 [00:02<00:10, 35.85it/s][A
 18%|█▊        | 80/436 [00:02<00:09, 38.30it/s][A
 19%|█▉        | 85/436 [00:02<00:08, 40.19it/s][A
 21%|██        | 90/436 [00:03<00:13, 25.89it/s][A
 22%|██▏       | 95/436 [00:03<00:11, 29.87it/s][A
 23%|██▎       | 100/436 [00:03<00:10, 33.25it/s][A
 24%|██▍       | 105/436 [00:03<00:09, 36.05it/s][A
 25%|██▌       | 110/436 [00:03<00:08, 38.36it/s][A
 26%|██▋       | 115/436 [00:03<00:07, 40.14it/s][A
 28%|██▊       | 120/436 [00:04<00:07, 41.47it/s][A
 29%|██▊       | 125/436 [00:04<00:17, 18.25it/s][A
 30%|██▉       | 130/436 [00:04<00:13, 22.21it/s][A
 31%|███       | 135/436 [00:04<00:11, 26.22it/s][A
 32%|███▏      | 140/436 [00:04<00:09, 30.02it/s][A
 33%|███▎      | 145/436 [00:05<00:08, 33.34it/s][A
 34%|███▍      | 150/436 [00:05<00:07, 36.17it/s][A
 36%|███▌      | 155/436 [00:05<00:07, 38.52it/s][A
 37%|███▋      | 160/436 [00:05<00:06, 40.12it/s][A
 38%|███▊      | 165/436 [00:05<00:06, 41.11it/s][A
 39%|███▉      | 170/436 [00:05<00:06, 41.78it/s][A
 40%|████      | 175/436 [00:06<00:11, 22.71it/s][A
 41%|████▏     | 180/436 [00:06<00:09, 26.79it/s][A
 42%|████▏     | 185/436 [00:06<00:08, 30.53it/s][A
 44%|████▎     | 190/436 [00:06<00:07, 33.81it/s][A
 45%|████▍     | 195/436 [00:06<00:06, 36.59it/s][A
 46%|████▌     | 200/436 [00:06<00:06, 38.87it/s][A
 47%|████▋     | 205/436 [00:06<00:05, 40.60it/s][A
 48%|████▊     | 210/436 [00:06<00:05, 41.89it/s][A
 49%|████▉     | 215/436 [00:06<00:05, 42.54it/s][A
 50%|█████     | 220/436 [00:07<00:05, 42.62it/s][A
 52%|█████▏    | 225/436 [00:07<00:05, 36.97it/s][A
 53%|█████▎    | 230/436 [00:07<00:05, 39.04it/s][A
 54%|█████▍    | 235/436 [00:07<00:04, 40.72it/s][A
 55%|█████▌    | 240/436 [00:07<00:04, 41.87it/s][A
 56%|█████▌    | 245/436 [00:07<00:04, 42.77it/s][A
 57%|█████▋    | 250/436 [00:07<00:04, 43.45it/s][A
 58%|█████▊    | 255/436 [00:07<00:04, 43.92it/s][A
 60%|█████▉    | 260/436 [00:08<00:03, 44.19it/s][A
 61%|██████    | 265/436 [00:08<00:03, 43.87it/s][A
 62%|██████▏   | 270/436 [00:08<00:03, 43.98it/s][A
 63%|██████▎   | 275/436 [00:08<00:03, 44.16it/s][A
 64%|██████▍   | 280/436 [00:08<00:03, 44.40it/s][A
 65%|██████▌   | 285/436 [00:08<00:03, 44.61it/s][A
 67%|██████▋   | 290/436 [00:08<00:03, 44.67it/s][A
 68%|██████▊   | 295/436 [00:08<00:03, 44.76it/s][A
 69%|██████▉   | 300/436 [00:08<00:03, 39.35it/s][A
 70%|██████▉   | 305/436 [00:09<00:03, 41.06it/s][A
 71%|███████   | 310/436 [00:09<00:05, 21.28it/s][A
 72%|███████▏  | 315/436 [00:09<00:04, 25.32it/s][A
 73%|███████▎  | 320/436 [00:09<00:03, 29.19it/s][A
 75%|███████▍  | 325/436 [00:09<00:03, 32.66it/s][A
 76%|███████▌  | 330/436 [00:10<00:02, 35.65it/s][A
 77%|███████▋  | 335/436 [00:10<00:02, 38.10it/s][A
 78%|███████▊  | 340/436 [00:10<00:02, 33.28it/s][A
 79%|███████▉  | 345/436 [00:10<00:02, 36.21it/s][A
 80%|████████  | 350/436 [00:10<00:02, 38.52it/s][A
 81%|████████▏ | 355/436 [00:10<00:02, 40.32it/s][A
 83%|████████▎ | 360/436 [00:10<00:01, 41.68it/s][A
 84%|████████▎ | 365/436 [00:10<00:01, 42.76it/s][A
 85%|████████▍ | 370/436 [00:10<00:01, 43.56it/s][A
 86%|████████▌ | 375/436 [00:11<00:01, 43.90it/s][A
 87%|████████▋ | 380/436 [00:11<00:01, 43.84it/s][A
 88%|████████▊ | 385/436 [00:11<00:01, 43.76it/s][A
 89%|████████▉ | 390/436 [00:11<00:01, 43.89it/s][A
 91%|█████████ | 395/436 [00:11<00:00, 44.28it/s][A
 92%|█████████▏| 400/436 [00:11<00:00, 44.50it/s][A
 93%|█████████▎| 405/436 [00:11<00:00, 44.80it/s][A
 94%|█████████▍| 410/436 [00:11<00:00, 44.98it/s][A
 95%|█████████▌| 415/436 [00:11<00:00, 45.03it/s][A
 96%|█████████▋| 420/436 [00:12<00:00, 44.90it/s][A
 97%|█████████▋| 425/436 [00:12<00:00, 44.56it/s][A
 99%|█████████▊| 430/436 [00:12<00:00, 44.39it/s][A
100%|█████████▉| 435/436 [00:12<00:00, 44.39it/s][A                                                
                                                 [A 20%|██        | 70/350 [00:40<01:22,  3.41it/s]
100%|██████████| 436/436 [00:12<00:00, 44.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:04,833 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 21:16:06,704 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:16:22,160 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:16:23,619 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:16:23,965 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [01:19<1:12:54, 15.68s/it] 21%|██        | 72/350 [01:19<51:35, 11.13s/it]   21%|██        | 73/350 [01:19<36:23,  7.88s/it] 21%|██        | 74/350 [01:20<25:46,  5.60s/it] 21%|██▏       | 75/350 [01:20<18:23,  4.01s/it] 22%|██▏       | 76/350 [01:20<13:13,  2.90s/it] 22%|██▏       | 77/350 [01:21<09:37,  2.12s/it] 22%|██▏       | 78/350 [01:21<07:06,  1.57s/it] 23%|██▎       | 79/350 [01:21<05:21,  1.19s/it] 23%|██▎       | 80/350 [01:22<04:07,  1.09it/s] 23%|██▎       | 81/350 [01:22<03:16,  1.37it/s] 23%|██▎       | 82/350 [01:22<02:52,  1.55it/s] 24%|██▎       | 83/350 [01:23<02:23,  1.85it/s] 24%|██▍       | 84/350 [01:23<02:03,  2.15it/s] 24%|██▍       | 85/350 [01:23<01:49,  2.42it/s] 25%|██▍       | 86/350 [01:23<01:39,  2.65it/s] 25%|██▍       | 87/350 [01:24<01:32,  2.84it/s] 25%|██▌       | 88/350 [01:24<01:27,  2.99it/s] 25%|██▌       | 89/350 [01:24<01:23,  3.11it/s] 26%|██▌       | 90/350 [01:25<01:21,  3.19it/s] 26%|██▌       | 91/350 [01:25<01:19,  3.26it/s] 26%|██▋       | 92/350 [01:25<01:20,  3.19it/s] 27%|██▋       | 93/350 [01:26<01:18,  3.25it/s] 27%|██▋       | 94/350 [01:26<01:17,  3.30it/s] 27%|██▋       | 95/350 [01:26<01:16,  3.33it/s] 27%|██▋       | 96/350 [01:26<01:15,  3.35it/s] 28%|██▊       | 97/350 [01:27<01:14,  3.37it/s] 28%|██▊       | 98/350 [01:27<01:14,  3.38it/s] 28%|██▊       | 99/350 [01:27<01:13,  3.40it/s] 29%|██▊       | 100/350 [01:28<01:13,  3.40it/s] 29%|██▉       | 101/350 [01:28<01:16,  3.24it/s] 29%|██▉       | 102/350 [01:28<01:19,  3.10it/s] 29%|██▉       | 103/350 [01:29<01:17,  3.19it/s] 30%|██▉       | 104/350 [01:29<01:15,  3.26it/s] 30%|███       | 105/350 [01:29<01:14,  3.30it/s] 30%|███       | 106/350 [01:29<01:13,  3.33it/s] 31%|███       | 107/350 [01:30<01:12,  3.36it/s] 31%|███       | 108/350 [01:30<01:11,  3.37it/s] 31%|███       | 109/350 [01:30<01:11,  3.39it/s] 31%|███▏      | 110/350 [01:31<01:10,  3.39it/s] 32%|███▏      | 111/350 [01:31<01:10,  3.39it/s] 32%|███▏      | 112/350 [01:31<01:10,  3.40it/s] 32%|███▏      | 113/350 [01:32<01:17,  3.05it/s] 33%|███▎      | 114/350 [01:32<01:14,  3.15it/s] 33%|███▎      | 115/350 [01:32<01:12,  3.23it/s] 33%|███▎      | 116/350 [01:32<01:11,  3.28it/s] 33%|███▎      | 117/350 [01:33<01:10,  3.32it/s] 34%|███▎      | 118/350 [01:33<01:09,  3.35it/s] 34%|███▍      | 119/350 [01:33<01:08,  3.37it/s] 34%|███▍      | 120/350 [01:34<01:08,  3.38it/s] 35%|███▍      | 121/350 [01:34<01:07,  3.39it/s] 35%|███▍      | 122/350 [01:34<01:07,  3.39it/s] 35%|███▌      | 123/350 [01:35<01:28,  2.57it/s] 35%|███▌      | 124/350 [01:35<01:21,  2.78it/s] 36%|███▌      | 125/350 [01:35<01:16,  2.94it/s] 36%|███▌      | 126/350 [01:36<01:13,  3.06it/s] 36%|███▋      | 127/350 [01:36<01:10,  3.16it/s] 37%|███▋      | 128/350 [01:36<01:08,  3.23it/s] 37%|███▋      | 129/350 [01:37<01:07,  3.28it/s] 37%|███▋      | 130/350 [01:37<01:06,  3.32it/s] 37%|███▋      | 131/350 [01:37<01:05,  3.34it/s] 38%|███▊      | 132/350 [01:37<01:04,  3.36it/s] 38%|███▊      | 133/350 [01:38<01:22,  2.64it/s] 38%|███▊      | 134/350 [01:38<01:16,  2.84it/s] 39%|███▊      | 135/350 [01:39<01:11,  3.00it/s] 39%|███▉      | 136/350 [01:39<01:08,  3.13it/s] 39%|███▉      | 137/350 [01:39<01:06,  3.22it/s] 39%|███▉      | 138/350 [01:39<01:04,  3.29it/s] 40%|███▉      | 139/350 [01:40<01:03,  3.34it/s] 40%|████      | 140/350 [01:40<01:02,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:17:03,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:03,727 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:17:03,727 >>   Batch size = 8
{'eval_loss': 1.0834219455718994, 'eval_runtime': 12.5236, 'eval_samples_per_second': 278.513, 'eval_steps_per_second': 34.814, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.01it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.94it/s][A
  4%|▍         | 17/436 [00:01<00:09, 46.44it/s][A
  5%|▌         | 22/436 [00:01<00:40, 10.19it/s][A
  6%|▌         | 27/436 [00:01<00:29, 13.93it/s][A
  7%|▋         | 32/436 [00:01<00:22, 18.08it/s][A
  8%|▊         | 37/436 [00:01<00:17, 22.37it/s][A
 10%|▉         | 42/436 [00:01<00:14, 26.60it/s][A
 11%|█         | 47/436 [00:02<00:12, 30.48it/s][A
 12%|█▏        | 52/436 [00:02<00:11, 33.88it/s][A
 13%|█▎        | 57/436 [00:02<00:10, 36.69it/s][A
 14%|█▍        | 62/436 [00:02<00:09, 38.46it/s][A
 15%|█▌        | 67/436 [00:02<00:09, 39.84it/s][A
 17%|█▋        | 72/436 [00:02<00:08, 41.13it/s][A
 18%|█▊        | 77/436 [00:02<00:08, 42.21it/s][A
 19%|█▉        | 82/436 [00:02<00:08, 43.02it/s][A
 20%|█▉        | 87/436 [00:02<00:08, 43.59it/s][A
 21%|██        | 92/436 [00:03<00:07, 44.08it/s][A
 22%|██▏       | 97/436 [00:03<00:07, 44.40it/s][A
 23%|██▎       | 102/436 [00:03<00:07, 44.37it/s][A
 25%|██▍       | 107/436 [00:03<00:07, 44.28it/s][A
 26%|██▌       | 112/436 [00:03<00:11, 27.14it/s][A
 27%|██▋       | 117/436 [00:03<00:10, 30.87it/s][A
 28%|██▊       | 122/436 [00:03<00:09, 34.12it/s][A
 29%|██▉       | 127/436 [00:04<00:08, 36.78it/s][A
 30%|███       | 132/436 [00:04<00:07, 39.02it/s][A
 31%|███▏      | 137/436 [00:04<00:07, 40.70it/s][A
 33%|███▎      | 142/436 [00:04<00:06, 42.06it/s][A
 34%|███▎      | 147/436 [00:04<00:06, 42.61it/s][A
 35%|███▍      | 152/436 [00:04<00:06, 42.89it/s][A
 36%|███▌      | 157/436 [00:04<00:06, 43.05it/s][A
 37%|███▋      | 162/436 [00:04<00:06, 43.44it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 43.81it/s][A
 39%|███▉      | 172/436 [00:05<00:05, 44.25it/s][A
 41%|████      | 177/436 [00:05<00:05, 44.61it/s][A
 42%|████▏     | 182/436 [00:05<00:05, 44.78it/s][A
 43%|████▎     | 187/436 [00:05<00:05, 44.86it/s][A
 44%|████▍     | 192/436 [00:05<00:05, 44.63it/s][A
 45%|████▌     | 197/436 [00:05<00:05, 44.37it/s][A
 46%|████▋     | 202/436 [00:05<00:05, 44.23it/s][A
 47%|████▋     | 207/436 [00:05<00:05, 44.25it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 44.26it/s][A
 50%|████▉     | 217/436 [00:06<00:04, 44.51it/s][A
 51%|█████     | 222/436 [00:06<00:04, 44.71it/s][A
 52%|█████▏    | 227/436 [00:06<00:04, 44.84it/s][A
 53%|█████▎    | 232/436 [00:06<00:04, 44.95it/s][A
 54%|█████▍    | 237/436 [00:06<00:05, 39.21it/s][A
 56%|█████▌    | 242/436 [00:06<00:04, 40.52it/s][A
 57%|█████▋    | 247/436 [00:06<00:04, 42.01it/s][A
 58%|█████▊    | 252/436 [00:06<00:04, 42.81it/s][A
 59%|█████▉    | 257/436 [00:07<00:04, 43.36it/s][A
 60%|██████    | 262/436 [00:07<00:03, 43.91it/s][A
 61%|██████    | 267/436 [00:07<00:03, 44.12it/s][A
 62%|██████▏   | 272/436 [00:07<00:03, 44.32it/s][A
 64%|██████▎   | 277/436 [00:07<00:03, 44.13it/s][A
 65%|██████▍   | 282/436 [00:07<00:03, 44.00it/s][A
 66%|██████▌   | 287/436 [00:07<00:03, 44.23it/s][A
 67%|██████▋   | 292/436 [00:07<00:03, 44.36it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 44.59it/s][A
 69%|██████▉   | 302/436 [00:08<00:03, 44.61it/s][A
 70%|███████   | 307/436 [00:08<00:02, 44.66it/s][A
 72%|███████▏  | 312/436 [00:08<00:02, 44.82it/s][A
 73%|███████▎  | 317/436 [00:08<00:02, 44.61it/s][A
 74%|███████▍  | 322/436 [00:08<00:02, 44.38it/s][A
 75%|███████▌  | 327/436 [00:08<00:02, 44.26it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 44.30it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 44.55it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 44.61it/s][A
 80%|███████▉  | 347/436 [00:09<00:01, 44.61it/s][A
 81%|████████  | 352/436 [00:09<00:01, 44.80it/s][A
 82%|████████▏ | 357/436 [00:09<00:01, 44.72it/s][A
 83%|████████▎ | 362/436 [00:09<00:01, 44.58it/s][A
 84%|████████▍ | 367/436 [00:09<00:01, 44.33it/s][A
 85%|████████▌ | 372/436 [00:09<00:02, 28.09it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 31.68it/s][A
 88%|████████▊ | 382/436 [00:10<00:01, 34.79it/s][A
 89%|████████▉ | 387/436 [00:10<00:01, 37.40it/s][A
 90%|████████▉ | 392/436 [00:10<00:01, 39.40it/s][A
 91%|█████████ | 397/436 [00:10<00:00, 40.89it/s][A
 92%|█████████▏| 402/436 [00:10<00:00, 42.09it/s][A
 93%|█████████▎| 407/436 [00:10<00:00, 42.80it/s][A
 94%|█████████▍| 412/436 [00:10<00:00, 43.00it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 43.13it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 43.54it/s][A
 98%|█████████▊| 427/436 [00:11<00:00, 43.96it/s][A
 99%|█████████▉| 432/436 [00:11<00:00, 44.28it/s][A                                                 
                                                 [A 40%|████      | 140/350 [01:52<01:02,  3.37it/s]
100%|██████████| 436/436 [00:11<00:00, 44.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:17:15,725 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 21:17:17,775 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:17:32,429 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:17:33,436 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:17:33,589 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [02:29<51:46, 14.86s/it] 41%|████      | 142/350 [02:29<36:28, 10.52s/it] 41%|████      | 143/350 [02:30<25:42,  7.45s/it] 41%|████      | 144/350 [02:30<18:13,  5.31s/it] 41%|████▏     | 145/350 [02:30<12:59,  3.80s/it] 42%|████▏     | 146/350 [02:30<09:20,  2.75s/it] 42%|████▏     | 147/350 [02:31<06:48,  2.01s/it] 42%|████▏     | 148/350 [02:31<05:02,  1.50s/it] 43%|████▎     | 149/350 [02:31<03:48,  1.13s/it] 43%|████▎     | 150/350 [02:32<02:56,  1.14it/s] 43%|████▎     | 151/350 [02:32<02:19,  1.42it/s] 43%|████▎     | 152/350 [02:32<01:54,  1.73it/s] 44%|████▎     | 153/350 [02:33<01:36,  2.03it/s] 44%|████▍     | 154/350 [02:33<01:40,  1.96it/s] 44%|████▍     | 155/350 [02:33<01:26,  2.25it/s] 45%|████▍     | 156/350 [02:34<01:17,  2.52it/s] 45%|████▍     | 157/350 [02:34<01:10,  2.75it/s] 45%|████▌     | 158/350 [02:34<01:05,  2.93it/s] 45%|████▌     | 159/350 [02:35<01:02,  3.07it/s] 46%|████▌     | 160/350 [02:35<00:59,  3.18it/s] 46%|████▌     | 161/350 [02:35<00:57,  3.26it/s] 46%|████▋     | 162/350 [02:35<00:56,  3.32it/s] 47%|████▋     | 163/350 [02:36<00:55,  3.36it/s] 47%|████▋     | 164/350 [02:36<01:11,  2.61it/s] 47%|████▋     | 165/350 [02:37<01:05,  2.82it/s] 47%|████▋     | 166/350 [02:37<01:01,  2.99it/s] 48%|████▊     | 167/350 [02:37<00:58,  3.12it/s] 48%|████▊     | 168/350 [02:37<00:56,  3.21it/s] 48%|████▊     | 169/350 [02:38<00:55,  3.29it/s] 49%|████▊     | 170/350 [02:38<00:54,  3.33it/s] 49%|████▉     | 171/350 [02:38<00:53,  3.37it/s] 49%|████▉     | 172/350 [02:39<00:52,  3.40it/s] 49%|████▉     | 173/350 [02:39<00:51,  3.42it/s] 50%|████▉     | 174/350 [02:39<01:05,  2.70it/s] 50%|█████     | 175/350 [02:40<01:00,  2.89it/s] 50%|█████     | 176/350 [02:40<00:57,  3.04it/s] 51%|█████     | 177/350 [02:40<00:54,  3.16it/s] 51%|█████     | 178/350 [02:41<00:53,  3.24it/s] 51%|█████     | 179/350 [02:41<00:51,  3.30it/s] 51%|█████▏    | 180/350 [02:41<00:50,  3.35it/s] 52%|█████▏    | 181/350 [02:42<01:08,  2.46it/s] 52%|█████▏    | 182/350 [02:42<01:02,  2.69it/s] 52%|█████▏    | 183/350 [02:43<01:07,  2.49it/s] 53%|█████▎    | 184/350 [02:43<01:00,  2.72it/s] 53%|█████▎    | 185/350 [02:43<00:56,  2.91it/s] 53%|█████▎    | 186/350 [02:43<00:53,  3.05it/s] 53%|█████▎    | 187/350 [02:44<00:51,  3.16it/s] 54%|█████▎    | 188/350 [02:44<00:49,  3.24it/s] 54%|█████▍    | 189/350 [02:44<00:48,  3.30it/s] 54%|█████▍    | 190/350 [02:45<00:47,  3.35it/s] 55%|█████▍    | 191/350 [02:45<00:47,  3.38it/s] 55%|█████▍    | 192/350 [02:45<00:46,  3.41it/s] 55%|█████▌    | 193/350 [02:46<01:09,  2.25it/s] 55%|█████▌    | 194/350 [02:46<01:02,  2.51it/s] 56%|█████▌    | 195/350 [02:47<00:56,  2.74it/s] 56%|█████▌    | 196/350 [02:47<00:52,  2.92it/s] 56%|█████▋    | 197/350 [02:47<00:49,  3.06it/s] 57%|█████▋    | 198/350 [02:47<00:47,  3.17it/s] 57%|█████▋    | 199/350 [02:48<00:46,  3.25it/s] 57%|█████▋    | 200/350 [02:48<00:45,  3.31it/s] 57%|█████▋    | 201/350 [02:48<00:44,  3.35it/s] 58%|█████▊    | 202/350 [02:49<01:14,  1.99it/s] 58%|█████▊    | 203/350 [02:50<01:04,  2.28it/s] 58%|█████▊    | 204/350 [02:50<00:57,  2.54it/s] 59%|█████▊    | 205/350 [02:50<00:52,  2.76it/s] 59%|█████▉    | 206/350 [02:50<00:49,  2.94it/s] 59%|█████▉    | 207/350 [02:51<00:46,  3.08it/s] 59%|█████▉    | 208/350 [02:51<00:44,  3.18it/s] 60%|█████▉    | 209/350 [02:51<00:43,  3.26it/s] 60%|██████    | 210/350 [02:52<00:53,  2.60it/s][INFO|trainer.py:2140] 2023-08-28 21:18:15,492 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:18:15,492 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:18:15,492 >>   Batch size = 8
{'eval_loss': 1.102112054824829, 'eval_runtime': 11.4919, 'eval_samples_per_second': 303.518, 'eval_steps_per_second': 37.94, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.23it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.13it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.54it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.59it/s][A
  6%|▌         | 27/436 [00:00<00:08, 46.21it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.82it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.22it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.69it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.39it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.35it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.57it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.97it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 45.00it/s][A
 18%|█▊        | 77/436 [00:01<00:07, 45.04it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.79it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.44it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.32it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.34it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.45it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/436 [00:02<00:10, 29.65it/s][A
 27%|██▋       | 117/436 [00:02<00:09, 33.05it/s][A
 28%|██▊       | 122/436 [00:02<00:08, 35.90it/s][A
 29%|██▉       | 127/436 [00:03<00:08, 38.25it/s][A
 30%|███       | 132/436 [00:03<00:07, 40.14it/s][A
 31%|███▏      | 137/436 [00:03<00:07, 41.56it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 42.59it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.13it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.23it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.37it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.67it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.97it/s][A
 39%|███▉      | 172/436 [00:04<00:05, 44.28it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.48it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.66it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.91it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.75it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.49it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.19it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.28it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.44it/s][A
 50%|████▉     | 217/436 [00:05<00:04, 44.59it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.73it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.92it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.95it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.78it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 39.69it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 41.44it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 42.47it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.15it/s][A
 60%|██████    | 262/436 [00:06<00:03, 43.75it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.12it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.41it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.37it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.04it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.02it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.19it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.30it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.44it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.65it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.87it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.88it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.72it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.31it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.28it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.46it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.57it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.58it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.52it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.48it/s][A
 84%|████████▍ | 367/436 [00:08<00:03, 19.58it/s][A
 85%|████████▌ | 372/436 [00:09<00:02, 23.60it/s][A
 86%|████████▋ | 377/436 [00:09<00:02, 27.52it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 31.11it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 34.31it/s][A
 90%|████████▉ | 392/436 [00:09<00:01, 37.04it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 39.12it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 40.58it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 41.40it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 42.06it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 42.70it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 43.20it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 43.79it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.16it/s][A                                                 
                                                 [A 60%|██████    | 210/350 [03:02<00:53,  2.60it/s]
100%|██████████| 436/436 [00:10<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:18:26,297 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 21:18:26,618 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:41,705 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:42,897 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:43,378 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [03:47<39:12, 16.92s/it] 61%|██████    | 212/350 [03:48<27:30, 11.96s/it] 61%|██████    | 213/350 [03:48<19:19,  8.46s/it] 61%|██████    | 214/350 [03:48<13:37,  6.01s/it] 61%|██████▏   | 215/350 [03:49<09:39,  4.29s/it] 62%|██████▏   | 216/350 [03:49<06:54,  3.09s/it] 62%|██████▏   | 217/350 [03:49<04:59,  2.25s/it] 62%|██████▏   | 218/350 [03:49<03:39,  1.67s/it] 63%|██████▎   | 219/350 [03:50<02:44,  1.25s/it] 63%|██████▎   | 220/350 [03:50<02:05,  1.04it/s] 63%|██████▎   | 221/350 [03:50<01:38,  1.31it/s] 63%|██████▎   | 222/350 [03:51<01:25,  1.50it/s] 64%|██████▎   | 223/350 [03:51<01:10,  1.81it/s] 64%|██████▍   | 224/350 [03:51<00:59,  2.12it/s] 64%|██████▍   | 225/350 [03:52<00:52,  2.39it/s] 65%|██████▍   | 226/350 [03:52<00:53,  2.31it/s] 65%|██████▍   | 227/350 [03:52<00:47,  2.57it/s] 65%|██████▌   | 228/350 [03:53<00:43,  2.79it/s] 65%|██████▌   | 229/350 [03:53<00:40,  2.96it/s] 66%|██████▌   | 230/350 [03:53<00:38,  3.10it/s] 66%|██████▌   | 231/350 [03:54<00:37,  3.20it/s] 66%|██████▋   | 232/350 [03:54<00:49,  2.38it/s] 67%|██████▋   | 233/350 [03:55<00:44,  2.63it/s] 67%|██████▋   | 234/350 [03:55<00:40,  2.83it/s] 67%|██████▋   | 235/350 [03:55<00:38,  2.99it/s] 67%|██████▋   | 236/350 [03:55<00:36,  3.12it/s] 68%|██████▊   | 237/350 [03:56<00:35,  3.20it/s] 68%|██████▊   | 238/350 [03:56<00:34,  3.28it/s] 68%|██████▊   | 239/350 [03:56<00:33,  3.34it/s] 69%|██████▊   | 240/350 [03:57<00:32,  3.38it/s] 69%|██████▉   | 241/350 [03:57<00:32,  3.40it/s] 69%|██████▉   | 242/350 [03:57<00:32,  3.28it/s] 69%|██████▉   | 243/350 [03:57<00:32,  3.33it/s] 70%|██████▉   | 244/350 [03:58<00:31,  3.37it/s] 70%|███████   | 245/350 [03:58<00:30,  3.39it/s] 70%|███████   | 246/350 [03:58<00:30,  3.42it/s] 71%|███████   | 247/350 [03:59<00:30,  3.42it/s] 71%|███████   | 248/350 [03:59<00:29,  3.44it/s] 71%|███████   | 249/350 [03:59<00:29,  3.44it/s] 71%|███████▏  | 250/350 [03:59<00:29,  3.45it/s] 72%|███████▏  | 251/350 [04:00<00:28,  3.45it/s] 72%|███████▏  | 252/350 [04:00<00:28,  3.45it/s] 72%|███████▏  | 253/350 [04:01<00:45,  2.13it/s] 73%|███████▎  | 254/350 [04:01<00:39,  2.40it/s] 73%|███████▎  | 255/350 [04:02<00:35,  2.65it/s] 73%|███████▎  | 256/350 [04:02<00:33,  2.85it/s] 73%|███████▎  | 257/350 [04:02<00:40,  2.32it/s] 74%|███████▎  | 258/350 [04:03<00:35,  2.58it/s] 74%|███████▍  | 259/350 [04:03<00:32,  2.79it/s] 74%|███████▍  | 260/350 [04:03<00:30,  2.97it/s] 75%|███████▍  | 261/350 [04:04<00:28,  3.10it/s] 75%|███████▍  | 262/350 [04:04<00:27,  3.20it/s] 75%|███████▌  | 263/350 [04:04<00:26,  3.28it/s] 75%|███████▌  | 264/350 [04:04<00:25,  3.33it/s] 76%|███████▌  | 265/350 [04:05<00:25,  3.37it/s] 76%|███████▌  | 266/350 [04:05<00:28,  2.99it/s] 76%|███████▋  | 267/350 [04:05<00:26,  3.11it/s] 77%|███████▋  | 268/350 [04:06<00:25,  3.19it/s] 77%|███████▋  | 269/350 [04:06<00:24,  3.25it/s] 77%|███████▋  | 270/350 [04:06<00:24,  3.30it/s] 77%|███████▋  | 271/350 [04:07<00:23,  3.33it/s] 78%|███████▊  | 272/350 [04:07<00:23,  3.36it/s] 78%|███████▊  | 273/350 [04:07<00:22,  3.38it/s] 78%|███████▊  | 274/350 [04:08<00:22,  3.38it/s] 79%|███████▊  | 275/350 [04:08<00:22,  3.39it/s] 79%|███████▉  | 276/350 [04:08<00:24,  3.06it/s] 79%|███████▉  | 277/350 [04:09<00:23,  3.16it/s] 79%|███████▉  | 278/350 [04:09<00:22,  3.23it/s] 80%|███████▉  | 279/350 [04:09<00:21,  3.27it/s] 80%|████████  | 280/350 [04:09<00:21,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 21:19:33,045 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:33,045 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:19:33,045 >>   Batch size = 8
{'eval_loss': 1.1145073175430298, 'eval_runtime': 10.5145, 'eval_samples_per_second': 331.731, 'eval_steps_per_second': 41.466, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.77it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.08it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.54it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.81it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.26it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.16it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.00it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.90it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.97it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.00it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.91it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.72it/s][A
 15%|█▌        | 67/436 [00:01<00:14, 25.41it/s][A
 17%|█▋        | 72/436 [00:01<00:12, 29.28it/s][A
 18%|█▊        | 77/436 [00:01<00:10, 32.73it/s][A
 19%|█▉        | 82/436 [00:02<00:09, 35.67it/s][A
 20%|█▉        | 87/436 [00:02<00:09, 38.08it/s][A
 21%|██        | 92/436 [00:02<00:08, 39.98it/s][A
 22%|██▏       | 97/436 [00:02<00:08, 41.31it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 42.11it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 42.36it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 42.60it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.05it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.28it/s][A
 29%|██▉       | 127/436 [00:03<00:07, 43.68it/s][A
 30%|███       | 132/436 [00:03<00:06, 43.94it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.36it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.63it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.60it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.35it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.13it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.22it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 44.45it/s][A
 39%|███▉      | 172/436 [00:04<00:05, 44.60it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.80it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.87it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.91it/s][A
 44%|████▍     | 192/436 [00:04<00:08, 30.09it/s][A
 45%|████▌     | 197/436 [00:04<00:07, 33.45it/s][A
 46%|████▋     | 202/436 [00:04<00:06, 36.20it/s][A
 47%|████▋     | 207/436 [00:05<00:05, 38.56it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 40.37it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 41.62it/s][A
 51%|█████     | 222/436 [00:05<00:05, 42.69it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.26it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.32it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.41it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.49it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.08it/s][A
 58%|█████▊    | 252/436 [00:06<00:04, 44.38it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 44.64it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.76it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.87it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.60it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.26it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.20it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.02it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.21it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 44.64it/s][A
 69%|██████▉   | 302/436 [00:07<00:02, 44.80it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.96it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.83it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.53it/s][A
 74%|███████▍  | 322/436 [00:07<00:03, 31.97it/s][A
 75%|███████▌  | 327/436 [00:07<00:03, 35.07it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 37.50it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 39.48it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 41.00it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 42.09it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.01it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.41it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.49it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.63it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.85it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 44.23it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 44.35it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.56it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.66it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.71it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.54it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.26it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.05it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.21it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 44.35it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 44.54it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.73it/s][A                                                 
                                                 [A 80%|████████  | 280/350 [04:20<00:21,  3.31it/s]
100%|██████████| 436/436 [00:10<00:00, 44.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:43,644 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 21:19:44,032 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:59,359 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:20:00,317 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:20:01,390 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [05:13<22:08, 19.25s/it] 81%|████████  | 282/350 [05:14<15:33, 13.73s/it] 81%|████████  | 283/350 [05:14<10:49,  9.70s/it] 81%|████████  | 284/350 [05:14<07:33,  6.88s/it] 81%|████████▏ | 285/350 [05:15<05:18,  4.90s/it] 82%|████████▏ | 286/350 [05:15<03:45,  3.52s/it] 82%|████████▏ | 287/350 [05:15<02:40,  2.55s/it] 82%|████████▏ | 288/350 [05:15<01:56,  1.87s/it] 83%|████████▎ | 289/350 [05:16<01:25,  1.40s/it] 83%|████████▎ | 290/350 [05:16<01:04,  1.07s/it] 83%|████████▎ | 291/350 [05:17<00:53,  1.10it/s] 83%|████████▎ | 292/350 [05:17<00:42,  1.38it/s] 84%|████████▎ | 293/350 [05:17<00:33,  1.68it/s] 84%|████████▍ | 294/350 [05:17<00:28,  1.98it/s] 84%|████████▍ | 295/350 [05:18<00:24,  2.27it/s] 85%|████████▍ | 296/350 [05:18<00:21,  2.52it/s] 85%|████████▍ | 297/350 [05:18<00:19,  2.74it/s] 85%|████████▌ | 298/350 [05:19<00:17,  2.91it/s] 85%|████████▌ | 299/350 [05:19<00:16,  3.04it/s] 86%|████████▌ | 300/350 [05:19<00:15,  3.14it/s] 86%|████████▌ | 301/350 [05:20<00:19,  2.48it/s] 86%|████████▋ | 302/350 [05:20<00:17,  2.70it/s] 87%|████████▋ | 303/350 [05:20<00:16,  2.88it/s] 87%|████████▋ | 304/350 [05:21<00:15,  3.02it/s] 87%|████████▋ | 305/350 [05:21<00:14,  3.13it/s] 87%|████████▋ | 306/350 [05:21<00:13,  3.21it/s] 88%|████████▊ | 307/350 [05:22<00:13,  3.27it/s] 88%|████████▊ | 308/350 [05:22<00:12,  3.31it/s] 88%|████████▊ | 309/350 [05:22<00:12,  3.34it/s] 89%|████████▊ | 310/350 [05:22<00:11,  3.36it/s] 89%|████████▉ | 311/350 [05:23<00:13,  2.98it/s] 89%|████████▉ | 312/350 [05:23<00:12,  3.10it/s] 89%|████████▉ | 313/350 [05:23<00:11,  3.18it/s] 90%|████████▉ | 314/350 [05:24<00:11,  3.25it/s] 90%|█████████ | 315/350 [05:24<00:10,  3.29it/s] 90%|█████████ | 316/350 [05:24<00:10,  3.32it/s] 91%|█████████ | 317/350 [05:25<00:09,  3.35it/s] 91%|█████████ | 318/350 [05:25<00:09,  3.37it/s] 91%|█████████ | 319/350 [05:25<00:09,  3.38it/s] 91%|█████████▏| 320/350 [05:26<00:08,  3.39it/s] 92%|█████████▏| 321/350 [05:26<00:09,  3.19it/s] 92%|█████████▏| 322/350 [05:26<00:08,  3.25it/s] 92%|█████████▏| 323/350 [05:26<00:08,  3.30it/s] 93%|█████████▎| 324/350 [05:27<00:07,  3.33it/s] 93%|█████████▎| 325/350 [05:27<00:07,  3.35it/s] 93%|█████████▎| 326/350 [05:27<00:07,  3.37it/s] 93%|█████████▎| 327/350 [05:28<00:06,  3.38it/s] 94%|█████████▎| 328/350 [05:28<00:06,  3.39it/s] 94%|█████████▍| 329/350 [05:28<00:06,  3.39it/s] 94%|█████████▍| 330/350 [05:29<00:05,  3.40it/s] 95%|█████████▍| 331/350 [05:29<00:05,  3.40it/s] 95%|█████████▍| 332/350 [05:29<00:06,  2.70it/s] 95%|█████████▌| 333/350 [05:30<00:05,  2.88it/s] 95%|█████████▌| 334/350 [05:30<00:05,  3.02it/s] 96%|█████████▌| 335/350 [05:30<00:04,  3.13it/s] 96%|█████████▌| 336/350 [05:31<00:04,  3.20it/s] 96%|█████████▋| 337/350 [05:31<00:03,  3.26it/s] 97%|█████████▋| 338/350 [05:31<00:03,  3.31it/s] 97%|█████████▋| 339/350 [05:31<00:03,  3.34it/s] 97%|█████████▋| 340/350 [05:32<00:02,  3.36it/s] 97%|█████████▋| 341/350 [05:32<00:02,  3.37it/s] 98%|█████████▊| 342/350 [05:33<00:02,  2.81it/s] 98%|█████████▊| 343/350 [05:33<00:02,  2.97it/s] 98%|█████████▊| 344/350 [05:33<00:01,  3.09it/s] 99%|█████████▊| 345/350 [05:33<00:01,  3.18it/s] 99%|█████████▉| 346/350 [05:34<00:01,  3.24it/s] 99%|█████████▉| 347/350 [05:34<00:00,  3.29it/s] 99%|█████████▉| 348/350 [05:35<00:00,  2.60it/s]100%|█████████▉| 349/350 [05:35<00:00,  2.79it/s]100%|██████████| 350/350 [05:35<00:00,  2.95it/s][INFO|trainer.py:2140] 2023-08-28 21:20:58,745 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:20:58,745 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:20:58,745 >>   Batch size = 8
{'eval_loss': 1.1280704736709595, 'eval_runtime': 10.4227, 'eval_samples_per_second': 334.655, 'eval_steps_per_second': 41.832, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.06it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.44it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.98it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.29it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.97it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.70it/s][A
 10%|▉         | 42/436 [00:01<00:10, 36.09it/s][A
 11%|█         | 47/436 [00:01<00:10, 38.56it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 40.44it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 41.79it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.67it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.36it/s][A
 17%|█▋        | 72/436 [00:02<00:08, 43.90it/s][A
 18%|█▊        | 77/436 [00:02<00:18, 19.43it/s][A
 19%|█▉        | 82/436 [00:02<00:15, 23.42it/s][A
 20%|█▉        | 87/436 [00:02<00:12, 27.37it/s][A
 21%|██        | 92/436 [00:02<00:11, 31.05it/s][A
 22%|██▏       | 97/436 [00:02<00:09, 34.22it/s][A
 23%|██▎       | 102/436 [00:02<00:09, 36.93it/s][A
 25%|██▍       | 107/436 [00:02<00:08, 39.05it/s][A
 26%|██▌       | 112/436 [00:03<00:07, 40.51it/s][A
 27%|██▋       | 117/436 [00:03<00:07, 41.33it/s][A
 28%|██▊       | 122/436 [00:03<00:07, 41.93it/s][A
 29%|██▉       | 127/436 [00:03<00:07, 42.57it/s][A
 30%|███       | 132/436 [00:03<00:07, 43.06it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.64it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.03it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.34it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.54it/s][A
 36%|███▌      | 157/436 [00:04<00:06, 44.62it/s][A
 37%|███▋      | 162/436 [00:04<00:06, 44.22it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 44.11it/s][A
 39%|███▉      | 172/436 [00:04<00:05, 44.11it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.18it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.26it/s][A
 43%|████▎     | 187/436 [00:05<00:05, 44.53it/s][A
 44%|████▍     | 192/436 [00:05<00:11, 21.34it/s][A
 45%|████▌     | 197/436 [00:05<00:09, 25.33it/s][A
 46%|████▋     | 202/436 [00:05<00:08, 29.14it/s][A
 47%|████▋     | 207/436 [00:05<00:07, 32.58it/s][A
 49%|████▊     | 212/436 [00:05<00:06, 35.55it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 37.89it/s][A
 51%|█████     | 222/436 [00:05<00:05, 39.86it/s][A
 52%|█████▏    | 227/436 [00:06<00:05, 41.28it/s][A
 53%|█████▎    | 232/436 [00:06<00:04, 41.75it/s][A
 54%|█████▍    | 237/436 [00:06<00:04, 42.46it/s][A
 56%|█████▌    | 242/436 [00:06<00:04, 42.94it/s][A
 57%|█████▋    | 247/436 [00:06<00:08, 22.10it/s][A
 58%|█████▊    | 252/436 [00:06<00:07, 26.11it/s][A
 59%|█████▉    | 257/436 [00:07<00:05, 29.92it/s][A
 60%|██████    | 262/436 [00:07<00:05, 33.33it/s][A
 61%|██████    | 267/436 [00:07<00:04, 36.19it/s][A
 62%|██████▏   | 272/436 [00:09<00:12, 12.69it/s][A
 63%|██████▎   | 276/436 [00:09<00:22,  7.01it/s][A
 64%|██████▍   | 279/436 [00:09<00:19,  8.03it/s][A
 65%|██████▌   | 284/436 [00:09<00:13, 11.14it/s][A
 66%|██████▋   | 289/436 [00:09<00:09, 14.78it/s][A
 67%|██████▋   | 294/436 [00:10<00:07, 18.79it/s][A
 69%|██████▊   | 299/436 [00:10<00:05, 22.99it/s][A
 70%|██████▉   | 304/436 [00:10<00:04, 27.07it/s][A
 71%|███████   | 309/436 [00:10<00:04, 30.91it/s][A
 72%|███████▏  | 314/436 [00:10<00:03, 34.25it/s][A
 73%|███████▎  | 319/436 [00:10<00:03, 36.76it/s][A
 74%|███████▍  | 324/436 [00:10<00:02, 38.61it/s][A
 75%|███████▌  | 329/436 [00:10<00:02, 40.03it/s][A
 77%|███████▋  | 334/436 [00:10<00:02, 41.17it/s][A
 78%|███████▊  | 339/436 [00:11<00:02, 42.40it/s][A
 79%|███████▉  | 344/436 [00:11<00:02, 43.06it/s][A
 80%|████████  | 349/436 [00:11<00:01, 43.79it/s][A
 81%|████████  | 354/436 [00:12<00:04, 17.57it/s][A
 82%|████████▏ | 359/436 [00:12<00:03, 21.50it/s][A
 83%|████████▎ | 364/436 [00:12<00:02, 25.55it/s][A
 85%|████████▍ | 369/436 [00:12<00:02, 29.38it/s][A
 86%|████████▌ | 374/436 [00:12<00:01, 32.81it/s][A
 87%|████████▋ | 379/436 [00:12<00:01, 35.82it/s][A
 88%|████████▊ | 384/436 [00:12<00:01, 38.21it/s][A
 89%|████████▉ | 389/436 [00:12<00:01, 39.97it/s][A
 90%|█████████ | 394/436 [00:12<00:01, 40.91it/s][A
 92%|█████████▏| 399/436 [00:13<00:00, 41.70it/s][A
 93%|█████████▎| 404/436 [00:13<00:00, 42.54it/s][A
 94%|█████████▍| 409/436 [00:13<00:00, 43.25it/s][A
 95%|█████████▍| 414/436 [00:13<00:00, 43.85it/s][A
 96%|█████████▌| 419/436 [00:13<00:00, 44.21it/s][A
 97%|█████████▋| 424/436 [00:13<00:00, 44.39it/s][A
 98%|█████████▊| 429/436 [00:13<00:00, 44.71it/s][A
100%|█████████▉| 434/436 [00:13<00:00, 44.59it/s][A                                                 
                                                 [A100%|██████████| 350/350 [05:49<00:00,  2.95it/s]
100%|██████████| 436/436 [00:13<00:00, 44.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:21:14,217 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 21:21:15,309 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:27,423 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:27,734 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:28,029 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:21:48,331 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:21:48,431 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70 (score: 1.0834219455718994).
                                                 100%|██████████| 350/350 [06:56<00:00,  2.95it/s]100%|██████████| 350/350 [06:56<00:00,  1.19s/it]
[INFO|trainer.py:1894] 2023-08-28 21:22:19,882 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 21:22:20,212 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:22:30,585 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:22:30,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:22:30,965 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:22:32,285 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,285 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,285 >>   train_loss               =        0.4
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,285 >>   train_runtime            = 0:06:55.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,286 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,286 >>   train_samples_per_second =     54.092
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:32,286 >>   train_steps_per_second   =      0.841
{'eval_loss': 1.1348206996917725, 'eval_runtime': 13.8666, 'eval_samples_per_second': 251.54, 'eval_steps_per_second': 31.442, 'epoch': 4.99}
{'train_runtime': 415.9587, 'train_samples_per_second': 54.092, 'train_steps_per_second': 0.841, 'train_loss': 0.40002437046595984, 'epoch': 4.99}
08/28/2023 21:22:32 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:22:32,948 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:22:32,948 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:22:32,948 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.39it/s]  3%|▎         | 12/436 [00:00<00:08, 49.44it/s]  4%|▍         | 17/436 [00:00<00:08, 47.47it/s]  5%|▌         | 22/436 [00:00<00:08, 46.67it/s]  6%|▌         | 27/436 [00:00<00:08, 46.18it/s]  7%|▋         | 32/436 [00:00<00:08, 45.89it/s]  8%|▊         | 37/436 [00:00<00:08, 45.83it/s] 10%|▉         | 42/436 [00:00<00:08, 45.41it/s] 11%|█         | 47/436 [00:01<00:08, 45.04it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.87it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.77it/s] 14%|█▍        | 62/436 [00:01<00:08, 45.01it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.85it/s] 17%|█▋        | 72/436 [00:01<00:08, 45.08it/s] 18%|█▊        | 77/436 [00:01<00:07, 45.02it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.11it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.89it/s] 21%|██        | 92/436 [00:02<00:07, 44.78it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.71it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.70it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.95it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.85it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.98it/s] 28%|██▊       | 122/436 [00:02<00:06, 45.02it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.91it/s] 30%|███       | 132/436 [00:02<00:06, 44.93it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.62it/s] 33%|███▎      | 142/436 [00:03<00:15, 18.70it/s] 34%|███▎      | 147/436 [00:03<00:12, 22.70it/s] 35%|███▍      | 152/436 [00:03<00:10, 26.68it/s] 36%|███▌      | 157/436 [00:03<00:09, 30.48it/s] 37%|███▋      | 162/436 [00:04<00:08, 33.80it/s] 38%|███▊      | 167/436 [00:04<00:07, 36.62it/s] 39%|███▉      | 172/436 [00:04<00:06, 38.92it/s] 41%|████      | 177/436 [00:04<00:06, 40.58it/s] 42%|████▏     | 182/436 [00:04<00:06, 41.50it/s] 43%|████▎     | 187/436 [00:04<00:05, 41.97it/s] 44%|████▍     | 192/436 [00:04<00:05, 42.76it/s] 45%|████▌     | 197/436 [00:04<00:05, 43.40it/s] 46%|████▋     | 202/436 [00:04<00:05, 43.97it/s] 47%|████▋     | 207/436 [00:05<00:05, 44.48it/s] 49%|████▊     | 212/436 [00:05<00:04, 44.81it/s] 50%|████▉     | 217/436 [00:05<00:04, 44.91it/s] 51%|█████     | 222/436 [00:05<00:04, 44.81it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.63it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.32it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.43it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.44it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.70it/s] 58%|█████▊    | 252/436 [00:06<00:04, 44.87it/s] 59%|█████▉    | 257/436 [00:06<00:04, 37.26it/s] 60%|██████    | 262/436 [00:06<00:04, 39.38it/s] 61%|██████    | 267/436 [00:06<00:04, 40.97it/s] 62%|██████▏   | 272/436 [00:06<00:03, 42.26it/s] 64%|██████▎   | 277/436 [00:06<00:03, 43.20it/s] 65%|██████▍   | 282/436 [00:06<00:03, 43.73it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.21it/s] 67%|██████▋   | 292/436 [00:07<00:03, 44.29it/s] 68%|██████▊   | 297/436 [00:07<00:03, 44.11it/s] 69%|██████▉   | 302/436 [00:07<00:03, 44.06it/s] 70%|███████   | 307/436 [00:07<00:02, 44.10it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.43it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.63it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.96it/s] 75%|███████▌  | 327/436 [00:07<00:02, 45.03it/s] 76%|███████▌  | 332/436 [00:07<00:02, 45.08it/s] 77%|███████▋  | 337/436 [00:08<00:02, 44.91it/s] 78%|███████▊  | 342/436 [00:08<00:02, 44.59it/s] 80%|███████▉  | 347/436 [00:08<00:02, 44.38it/s] 81%|████████  | 352/436 [00:08<00:01, 44.29it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.41it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.75it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.88it/s] 85%|████████▌ | 372/436 [00:08<00:01, 45.01it/s] 86%|████████▋ | 377/436 [00:08<00:01, 45.01it/s] 88%|████████▊ | 382/436 [00:09<00:01, 44.92it/s] 89%|████████▉ | 387/436 [00:09<00:01, 44.69it/s] 90%|████████▉ | 392/436 [00:09<00:01, 42.18it/s] 91%|█████████ | 397/436 [00:09<00:00, 42.97it/s] 92%|█████████▏| 402/436 [00:09<00:00, 43.59it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.03it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.39it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.71it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.78it/s] 98%|█████████▊| 427/436 [00:10<00:00, 44.63it/s] 99%|█████████▉| 432/436 [00:10<00:00, 44.34it/s]100%|██████████| 436/436 [00:10<00:00, 42.25it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:22:43,286 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   eval_loss               =     1.0834
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   eval_runtime            = 0:00:10.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   eval_samples_per_second =    337.398
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   eval_steps_per_second   =     42.175
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:43,286 >>   perplexity              =     2.9548
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:04,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:05,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:05,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:05,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:05,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:23:06,017 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:23:06,018 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:06,992 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:08,027 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:08,078 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:13,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:13,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:13,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:13,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:13,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:23:14,350 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:23:14,352 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:15,062 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:15,230 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:15,230 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-350
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-70
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-280
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-140
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-210
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:21,  1.54it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:23,  1.53it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:25,  1.54it/s]Extractor Predicting: 41it [00:25,  1.47it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:27,  1.50it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.48it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.53it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:35,  1.45it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:37,  1.42it/s]Extractor Predicting: 60it [00:38,  1.44it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:39,  1.48it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:41,  1.32it/s]Extractor Predicting: 65it [00:41,  1.40it/s]Extractor Predicting: 66it [00:42,  1.45it/s]Extractor Predicting: 67it [00:43,  1.48it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:45,  1.22it/s]Extractor Predicting: 70it [00:45,  1.30it/s]Extractor Predicting: 71it [00:46,  1.35it/s]Extractor Predicting: 72it [00:46,  1.43it/s]Extractor Predicting: 73it [00:47,  1.32it/s]Extractor Predicting: 74it [00:48,  1.40it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:49,  1.49it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:51,  1.13it/s]Extractor Predicting: 79it [00:52,  1.24it/s]Extractor Predicting: 80it [00:53,  1.31it/s]Extractor Predicting: 81it [00:53,  1.37it/s]Extractor Predicting: 82it [00:54,  1.19it/s]Extractor Predicting: 83it [00:55,  1.31it/s]Extractor Predicting: 84it [00:56,  1.36it/s]Extractor Predicting: 85it [00:56,  1.40it/s]Extractor Predicting: 86it [00:57,  1.19it/s]Extractor Predicting: 87it [00:58,  1.29it/s]Extractor Predicting: 88it [00:59,  1.38it/s]Extractor Predicting: 89it [00:59,  1.44it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.45it/s]Extractor Predicting: 92it [01:01,  1.52it/s]Extractor Predicting: 93it [01:02,  1.53it/s]Extractor Predicting: 94it [01:02,  1.59it/s]Extractor Predicting: 95it [01:03,  1.61it/s]Extractor Predicting: 96it [01:04,  1.53it/s]Extractor Predicting: 97it [01:04,  1.56it/s]Extractor Predicting: 98it [01:05,  1.56it/s]Extractor Predicting: 99it [01:06,  1.55it/s]Extractor Predicting: 100it [01:06,  1.55it/s]Extractor Predicting: 101it [01:07,  1.61it/s]Extractor Predicting: 102it [01:08,  1.53it/s]Extractor Predicting: 103it [01:08,  1.55it/s]Extractor Predicting: 104it [01:09,  1.60it/s]Extractor Predicting: 105it [01:09,  1.60it/s]Extractor Predicting: 106it [01:10,  1.64it/s]Extractor Predicting: 107it [01:11,  1.42it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.52it/s]Extractor Predicting: 110it [01:13,  1.54it/s]Extractor Predicting: 111it [01:13,  1.61it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:15,  1.62it/s]Extractor Predicting: 114it [01:15,  1.63it/s]Extractor Predicting: 115it [01:16,  1.66it/s]Extractor Predicting: 116it [01:16,  1.66it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:18,  1.58it/s]Extractor Predicting: 119it [01:18,  1.56it/s]Extractor Predicting: 120it [01:19,  1.56it/s]Extractor Predicting: 121it [01:20,  1.60it/s]Extractor Predicting: 122it [01:20,  1.57it/s]Extractor Predicting: 123it [01:21,  1.58it/s]Extractor Predicting: 124it [01:21,  1.57it/s]Extractor Predicting: 125it [01:22,  1.56it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.50it/s]Extractor Predicting: 128it [01:24,  1.57it/s]Extractor Predicting: 129it [01:25,  1.55it/s]Extractor Predicting: 130it [01:25,  1.60it/s]Extractor Predicting: 131it [01:26,  1.59it/s]Extractor Predicting: 132it [01:27,  1.42it/s]Extractor Predicting: 133it [01:27,  1.46it/s]Extractor Predicting: 134it [01:28,  1.35it/s]Extractor Predicting: 135it [01:29,  1.41it/s]Extractor Predicting: 136it [01:30,  1.37it/s]Extractor Predicting: 137it [01:30,  1.43it/s]Extractor Predicting: 138it [01:31,  1.47it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.49it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:34,  1.53it/s]Extractor Predicting: 144it [01:35,  1.47it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:19,540 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:19,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:19,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:19,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:19,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:25:19,996 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:25:19,997 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:21,331 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:22,332 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:22,332 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:26,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:27,269 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:27,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:27,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:27,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:25:28,267 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:25:28,268 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:29,671 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:29,848 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:29,848 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2916902204635387,
  "recall": 0.14793577981651376,
  "score": 0.1963096823283241,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.31it/s]Extractor Predicting: 12it [00:08,  1.38it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.49it/s]Extractor Predicting: 16it [00:10,  1.35it/s]Extractor Predicting: 17it [00:11,  1.38it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:15,  1.59it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.55it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:22,  1.66it/s]Extractor Predicting: 35it [00:22,  1.70it/s]Extractor Predicting: 36it [00:23,  1.72it/s]Extractor Predicting: 37it [00:24,  1.46it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.65it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:26,  1.64it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.61it/s]Extractor Predicting: 44it [00:28,  1.69it/s]Extractor Predicting: 45it [00:29,  1.71it/s]Extractor Predicting: 46it [00:29,  1.68it/s]Extractor Predicting: 47it [00:30,  1.67it/s]Extractor Predicting: 48it [00:30,  1.75it/s]Extractor Predicting: 49it [00:31,  1.78it/s]Extractor Predicting: 50it [00:31,  1.78it/s]Extractor Predicting: 51it [00:32,  1.82it/s]Extractor Predicting: 52it [00:33,  1.78it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:35,  1.62it/s]Extractor Predicting: 56it [00:35,  1.68it/s]Extractor Predicting: 57it [00:36,  1.69it/s]Extractor Predicting: 58it [00:37,  1.19it/s]Extractor Predicting: 59it [00:38,  1.29it/s]Extractor Predicting: 60it [00:38,  1.35it/s]Extractor Predicting: 61it [00:39,  1.37it/s]Extractor Predicting: 62it [00:40,  1.35it/s]Extractor Predicting: 63it [00:41,  1.41it/s]Extractor Predicting: 64it [00:41,  1.42it/s]Extractor Predicting: 65it [00:42,  1.48it/s]Extractor Predicting: 66it [00:42,  1.50it/s]Extractor Predicting: 67it [00:43,  1.44it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:45,  1.44it/s]Extractor Predicting: 71it [00:46,  1.43it/s]Extractor Predicting: 72it [00:47,  1.15it/s]Extractor Predicting: 73it [00:48,  1.23it/s]Extractor Predicting: 74it [00:49,  1.31it/s]Extractor Predicting: 75it [00:49,  1.38it/s]Extractor Predicting: 76it [00:50,  1.34it/s]Extractor Predicting: 77it [00:51,  1.38it/s]Extractor Predicting: 78it [00:51,  1.41it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.44it/s]Extractor Predicting: 81it [00:54,  1.21it/s]Extractor Predicting: 82it [00:55,  1.28it/s]Extractor Predicting: 83it [00:55,  1.34it/s]Extractor Predicting: 84it [00:56,  1.41it/s]Extractor Predicting: 85it [00:57,  1.43it/s]Extractor Predicting: 86it [00:57,  1.42it/s]Extractor Predicting: 87it [00:58,  1.37it/s]Extractor Predicting: 88it [00:59,  1.42it/s]Extractor Predicting: 89it [00:59,  1.45it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:01,  1.48it/s]Extractor Predicting: 92it [01:02,  1.26it/s]Extractor Predicting: 93it [01:02,  1.35it/s]Extractor Predicting: 94it [01:03,  1.40it/s]Extractor Predicting: 95it [01:04,  1.47it/s]Extractor Predicting: 96it [01:04,  1.53it/s]Extractor Predicting: 97it [01:05,  1.35it/s]Extractor Predicting: 98it [01:06,  1.44it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:07,  1.52it/s]Extractor Predicting: 101it [01:08,  1.60it/s]Extractor Predicting: 102it [01:08,  1.59it/s]Extractor Predicting: 103it [01:09,  1.58it/s]Extractor Predicting: 104it [01:10,  1.54it/s]Extractor Predicting: 105it [01:10,  1.56it/s]Extractor Predicting: 106it [01:11,  1.55it/s]Extractor Predicting: 107it [01:11,  1.54it/s]Extractor Predicting: 108it [01:12,  1.55it/s]Extractor Predicting: 109it [01:13,  1.54it/s]Extractor Predicting: 110it [01:13,  1.52it/s]Extractor Predicting: 111it [01:14,  1.55it/s]Extractor Predicting: 112it [01:15,  1.37it/s]Extractor Predicting: 113it [01:16,  1.45it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:17,  1.56it/s]Extractor Predicting: 117it [01:18,  1.37it/s]Extractor Predicting: 118it [01:19,  1.48it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:20,  1.60it/s]Extractor Predicting: 121it [01:21,  1.65it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:22,  1.56it/s]Extractor Predicting: 124it [01:23,  1.62it/s]Extractor Predicting: 125it [01:23,  1.67it/s]Extractor Predicting: 126it [01:24,  1.67it/s]Extractor Predicting: 127it [01:24,  1.61it/s]Extractor Predicting: 128it [01:25,  1.69it/s]Extractor Predicting: 129it [01:26,  1.71it/s]Extractor Predicting: 130it [01:26,  1.80it/s]Extractor Predicting: 131it [01:27,  1.83it/s]Extractor Predicting: 132it [01:27,  1.79it/s]Extractor Predicting: 133it [01:28,  1.74it/s]Extractor Predicting: 134it [01:28,  1.70it/s]Extractor Predicting: 135it [01:29,  1.73it/s]Extractor Predicting: 136it [01:29,  1.76it/s]Extractor Predicting: 137it [01:30,  1.77it/s]Extractor Predicting: 138it [01:31,  1.78it/s]Extractor Predicting: 139it [01:31,  1.75it/s]Extractor Predicting: 140it [01:32,  1.40it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:33,  1.59it/s]Extractor Predicting: 143it [01:34,  1.64it/s]Extractor Predicting: 144it [01:34,  1.66it/s]Extractor Predicting: 145it [01:35,  1.64it/s]Extractor Predicting: 146it [01:36,  1.62it/s]Extractor Predicting: 147it [01:36,  1.61it/s]Extractor Predicting: 148it [01:37,  1.63it/s]Extractor Predicting: 149it [01:38,  1.61it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:39,  1.54it/s]Extractor Predicting: 152it [01:40,  1.57it/s]Extractor Predicting: 153it [01:40,  1.55it/s]Extractor Predicting: 154it [01:41,  1.59it/s]Extractor Predicting: 155it [01:42,  1.48it/s]Extractor Predicting: 156it [01:42,  1.49it/s]Extractor Predicting: 157it [01:43,  1.53it/s]Extractor Predicting: 158it [01:44,  1.53it/s]Extractor Predicting: 159it [01:44,  1.53it/s]Extractor Predicting: 160it [01:45,  1.51it/s]Extractor Predicting: 161it [01:46,  1.53it/s]Extractor Predicting: 162it [01:46,  1.53it/s]Extractor Predicting: 163it [01:47,  1.57it/s]Extractor Predicting: 164it [01:47,  1.59it/s]Extractor Predicting: 165it [01:48,  1.55it/s]Extractor Predicting: 166it [01:49,  1.53it/s]Extractor Predicting: 167it [01:49,  1.56it/s]Extractor Predicting: 168it [01:50,  1.56it/s]Extractor Predicting: 169it [01:51,  1.59it/s]Extractor Predicting: 170it [01:51,  1.49it/s]Extractor Predicting: 171it [01:52,  1.52it/s]Extractor Predicting: 172it [01:53,  1.54it/s]Extractor Predicting: 173it [01:53,  1.54it/s]Extractor Predicting: 174it [01:54,  1.55it/s]Extractor Predicting: 175it [01:55,  1.33it/s]Extractor Predicting: 176it [01:56,  1.39it/s]Extractor Predicting: 177it [01:57,  1.16it/s]Extractor Predicting: 178it [01:57,  1.27it/s]Extractor Predicting: 179it [01:58,  1.35it/s]Extractor Predicting: 180it [01:59,  1.40it/s]Extractor Predicting: 181it [02:00,  1.23it/s]Extractor Predicting: 182it [02:00,  1.32it/s]Extractor Predicting: 183it [02:01,  1.38it/s]Extractor Predicting: 184it [02:02,  1.44it/s]Extractor Predicting: 185it [02:02,  1.49it/s]Extractor Predicting: 186it [02:03,  1.45it/s]Extractor Predicting: 187it [02:04,  1.50it/s]Extractor Predicting: 188it [02:04,  1.52it/s]Extractor Predicting: 189it [02:05,  1.56it/s]Extractor Predicting: 190it [02:05,  1.57it/s]Extractor Predicting: 191it [02:07,  1.13it/s]Extractor Predicting: 192it [02:07,  1.25it/s]Extractor Predicting: 193it [02:08,  1.36it/s]Extractor Predicting: 194it [02:09,  1.41it/s]Extractor Predicting: 195it [02:10,  1.12it/s]Extractor Predicting: 196it [02:11,  1.23it/s]Extractor Predicting: 197it [02:11,  1.32it/s]Extractor Predicting: 198it [02:12,  1.37it/s]Extractor Predicting: 199it [02:13,  1.16it/s]Extractor Predicting: 200it [02:14,  1.26it/s]Extractor Predicting: 201it [02:14,  1.36it/s]Extractor Predicting: 202it [02:15,  1.41it/s]Extractor Predicting: 203it [02:16,  1.16it/s]Extractor Predicting: 204it [02:17,  1.26it/s]Extractor Predicting: 205it [02:17,  1.34it/s]Extractor Predicting: 206it [02:18,  1.46it/s]Extractor Predicting: 207it [02:19,  1.39it/s]Extractor Predicting: 208it [02:19,  1.48it/s]Extractor Predicting: 209it [02:20,  1.51it/s]Extractor Predicting: 210it [02:21,  1.54it/s]Extractor Predicting: 211it [02:21,  1.55it/s]Extractor Predicting: 212it [02:22,  1.51it/s]Extractor Predicting: 213it [02:23,  1.53it/s]Extractor Predicting: 214it [02:23,  1.55it/s]Extractor Predicting: 215it [02:24,  1.54it/s]Extractor Predicting: 216it [02:25,  1.54it/s]Extractor Predicting: 217it [02:26,  1.19it/s]Extractor Predicting: 218it [02:26,  1.29it/s]Extractor Predicting: 219it [02:27,  1.36it/s]Extractor Predicting: 220it [02:28,  1.44it/s]Extractor Predicting: 221it [02:28,  1.47it/s]Extractor Predicting: 222it [02:29,  1.49it/s]Extractor Predicting: 223it [02:30,  1.53it/s]Extractor Predicting: 224it [02:30,  1.46it/s]Extractor Predicting: 225it [02:31,  1.48it/s]Extractor Predicting: 226it [02:32,  1.51it/s]Extractor Predicting: 227it [02:32,  1.51it/s]Extractor Predicting: 228it [02:33,  1.57it/s]Extractor Predicting: 229it [02:34,  1.35it/s]Extractor Predicting: 230it [02:34,  1.42it/s]Extractor Predicting: 231it [02:35,  1.43it/s]Extractor Predicting: 232it [02:36,  1.46it/s]Extractor Predicting: 233it [02:36,  1.49it/s]Extractor Predicting: 234it [02:38,  1.19it/s]Extractor Predicting: 235it [02:38,  1.29it/s]Extractor Predicting: 236it [02:39,  1.37it/s]Extractor Predicting: 237it [02:40,  1.37it/s]Extractor Predicting: 238it [02:41,  1.31it/s]Extractor Predicting: 239it [02:41,  1.36it/s]Extractor Predicting: 240it [02:42,  1.42it/s]Extractor Predicting: 241it [02:42,  1.45it/s]Extractor Predicting: 242it [02:43,  1.49it/s]Extractor Predicting: 243it [02:44,  1.32it/s]Extractor Predicting: 244it [02:45,  1.38it/s]Extractor Predicting: 245it [02:45,  1.42it/s]Extractor Predicting: 246it [02:46,  1.44it/s]Extractor Predicting: 247it [02:47,  1.42it/s]Extractor Predicting: 248it [02:48,  1.11it/s]Extractor Predicting: 249it [02:49,  1.24it/s]Extractor Predicting: 250it [02:49,  1.34it/s]Extractor Predicting: 251it [02:50,  1.39it/s]Extractor Predicting: 252it [02:51,  1.37it/s]Extractor Predicting: 253it [02:51,  1.43it/s]Extractor Predicting: 254it [02:52,  1.45it/s]Extractor Predicting: 255it [02:53,  1.47it/s]Extractor Predicting: 256it [02:53,  1.48it/s]Extractor Predicting: 257it [02:54,  1.29it/s]Extractor Predicting: 258it [02:55,  1.35it/s]Extractor Predicting: 259it [02:56,  1.43it/s]Extractor Predicting: 260it [02:56,  1.48it/s]Extractor Predicting: 261it [02:57,  1.53it/s]Extractor Predicting: 262it [02:58,  1.24it/s]Extractor Predicting: 263it [02:59,  1.31it/s]Extractor Predicting: 264it [02:59,  1.40it/s]Extractor Predicting: 265it [03:00,  1.46it/s]Extractor Predicting: 266it [03:01,  1.26it/s]Extractor Predicting: 267it [03:02,  1.36it/s]Extractor Predicting: 268it [03:02,  1.43it/s]Extractor Predicting: 269it [03:03,  1.47it/s]Extractor Predicting: 270it [03:03,  1.49it/s]Extractor Predicting: 271it [03:04,  1.31it/s]Extractor Predicting: 272it [03:05,  1.39it/s]Extractor Predicting: 273it [03:06,  1.47it/s]Extractor Predicting: 274it [03:06,  1.52it/s]Extractor Predicting: 275it [03:07,  1.57it/s]Extractor Predicting: 276it [03:08,  1.15it/s]Extractor Predicting: 277it [03:09,  1.26it/s]Extractor Predicting: 278it [03:09,  1.36it/s]Extractor Predicting: 279it [03:10,  1.43it/s]Extractor Predicting: 280it [03:11,  1.43it/s]Extractor Predicting: 281it [03:11,  1.51it/s]Extractor Predicting: 282it [03:12,  1.53it/s]Extractor Predicting: 283it [03:13,  1.52it/s]Extractor Predicting: 284it [03:13,  1.76it/s]Extractor Predicting: 284it [03:13,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:08,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:09,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:09,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:09,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:09,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:29:09,713 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:29:09,714 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:29:10,261 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:29:11,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:29:11,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:16,707 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:17,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:17,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:17,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:17,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:29:17,740 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:29:17,741 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:29:18,669 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:29:18,843 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:29:18,843 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.28752580359775876,
  "recall": 0.14334019406057041,
  "score": 0.19130776022760718,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:02,  2.00it/s]Extractor Predicting: 5it [00:03,  1.59it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:29:31,566 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:29:31,836 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:29:32,187 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:29:32,188 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:29:32,303 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:30:17,277 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:30:17,346 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:30:17,734 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:30:17,735 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:30:17,835 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:17,873 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3409090909090909,
  "recall": 0.07575757575757576,
  "score": 0.12396694214876033,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:30:18,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:18,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:19,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:19,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:20,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:21,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:21,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:22,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:22,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:23,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:23,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:24,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:26,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:26,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:27,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:28,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:28,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:29,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:29,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:30,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:31,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:31,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:20, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:32,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:32,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:33,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:34,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:35,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:35,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:36,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:36,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:37,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:39,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:39,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:40,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:40,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:43,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:43,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:49, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:44,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:45,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:46,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:46,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:47,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:48,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:49,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:49,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:50,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:51,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:52,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:54,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:55,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:55,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:56,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:56,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:57,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:58,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:59,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:59,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:51, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:00,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:01,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:01,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:02,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:02,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:03,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:04,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:04,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:05,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:05,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:06,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:06,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:07,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:08,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:09,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:09,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:10,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:10,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:11,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:11,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:12,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:13,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:33, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:13,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:14,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:14,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:15,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:15,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:18,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:21,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:23,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:23,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:24,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:09, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:24,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:26,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:27,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:27,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:29,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:30,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:30,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:31,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:31,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:32,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:34,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:34,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:19<01:54, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:37,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:37,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:38,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:38,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:39,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:41,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:41,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:42,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:43,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:45,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:46,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:46,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:47,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:47,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:48,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:48,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:49,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:49,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:51,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:33<01:46, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:51,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:52,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:52,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:54,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:55,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:55,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:56,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:57,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:57,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:59,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:00,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:01,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:01,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:02,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:02,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:04,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:05,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:05,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:37, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:07,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:07,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:08,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:08,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:09,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:11,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:12,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:12,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:13,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:13,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:14,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:15,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:16,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:17,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:17,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:18,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:19,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:19,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:20,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:21,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:21,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:04<01:26, 14.34s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:22,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:22,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:24,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:25,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:25,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:26,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:26,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:29,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:29,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:30,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:31,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:31,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:32,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:32,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:33,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:34,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:34,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:35,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:35,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:18<01:11, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:36,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:36,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:37,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:37,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:40,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:42,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:42,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:43,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:43,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:44,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:45,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:45,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:47,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:30<00:54, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:48,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:48,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:50,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:52,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:53,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:54,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:54,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:55,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:55,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:56,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:56,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:57,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:58,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:58,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:59,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:59,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:00,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:42<00:39, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:00,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:01,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:02,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:02,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:03,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:03,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:04,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:04,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:05,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:06,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:06,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:07,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:07,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:08,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:08,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:09,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:10,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:10,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:11,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:11,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:12,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:12,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:55<00:26, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:13,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:13,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:14,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:15,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:15,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:16,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:16,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:17,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:17,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:18,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:18,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:19,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:19,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:20,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:20,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:21,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:21,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:22,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:22,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:23,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:24,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:06<00:12, 12.54s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:24,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:25,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:25,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:26,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:27,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:27,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:28,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:29,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:30,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:30,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:31,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:31,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:32,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:32,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:33,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:34,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:34,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:35,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:35,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:36,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:18<00:00, 12.39s/it]Generating: 100%|██████████| 15/15 [03:18<00:00, 13.25s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:49,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:49,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:49,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:49,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:49,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:33:50,283 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:33:50,284 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:50,871 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:51,942 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:51,942 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:33:57,220 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:33:57,221 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:58,609 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:58,782 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:58,782 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : genre . Context : Following his initial film debut , " The Great Escape " ( 1982 ) , he starred in the 1985 Academy Award for Best Foreign Language Film , directed by Alfred Hitchcock . Head Entity : The Great Escape , Tail Entity : foreign language .\n']
['Relation : genre . Context : Following his initial film debut , " The Great Escape " ( 1982 ) , he starred in the 1985 Academy Award for Best Foreign Language Film , directed by Alfred Hitchcock . Head Entity : The Great Escape , Tail Entity : foreign language .\n', 'Relation : genre . Context : " The Adventures of Catelyn Mankiewicz " was published by Warner Brothers in 2001 and adapted into seven episodes . Head Entity : The Adventures of Catelyn Mankiewicz , Tail Entity : horror .\n']
['Relation : genre . Context : Following his initial film debut , " The Great Escape " ( 1982 ) , he starred in the 1985 Academy Award for Best Foreign Language Film , directed by Alfred Hitchcock . Head Entity : The Great Escape , Tail Entity : foreign language .\n', 'Relation : genre . Context : " The Adventures of Catelyn Mankiewicz " was published by Warner Brothers in 2001 and adapted into seven episodes . Head Entity : The Adventures of Catelyn Mankiewicz , Tail Entity : horror .\n', 'Relation : genre . Context : This film explores the social and political significance of the New York city in the wake of the war and the rise of the Ku Klux Klan . Head Entity : New York City , Tail Entity : New York .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : licensed to broadcast to . Context : Following his promotion with the New York Times , he won the Academy Award for Best Original Score in 1985 for " The Great Gatsby " . Head Entity : The Great Gatsby , Tail Entity : The New York Times .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : located in the administrative territorial entity . Context : On 31 March 2014 , the municipality announced that it would be expanding its administrative boundaries at the end of 2016 , bringing it to a total of 26 municipalities . Head Entity : Municipality , Tail Entity : Municipality .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : On 31 March 1859 , he founded the company " The Company of Engineers " at White Rock , Arkansas , in which he made several construction projects . Head Entity : The Company of Engineers , Tail Entity : The United States .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9421875, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 7939
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8039, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.64it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:04,  1.61it/s]Extractor Estimating: 9it [00:05,  1.69it/s]Extractor Estimating: 10it [00:06,  1.77it/s]Extractor Estimating: 11it [00:06,  1.82it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.61it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:08,  1.73it/s]Extractor Estimating: 16it [00:09,  1.77it/s]Extractor Estimating: 17it [00:10,  1.79it/s]Extractor Estimating: 18it [00:10,  1.81it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:13,  1.73it/s]Extractor Estimating: 23it [00:13,  1.75it/s]Extractor Estimating: 24it [00:14,  1.34it/s]Extractor Estimating: 25it [00:15,  1.48it/s]Extractor Estimating: 26it [00:15,  1.66it/s]Extractor Estimating: 27it [00:16,  1.79it/s]Extractor Estimating: 28it [00:16,  1.85it/s]Extractor Estimating: 29it [00:17,  1.53it/s]Extractor Estimating: 30it [00:18,  1.68it/s]Extractor Estimating: 31it [00:19,  1.36it/s]Extractor Estimating: 32it [00:19,  1.58it/s]Extractor Estimating: 33it [00:19,  1.77it/s]Extractor Estimating: 34it [00:20,  1.87it/s]Extractor Estimating: 35it [00:20,  1.94it/s]Extractor Estimating: 36it [00:21,  1.94it/s]Extractor Estimating: 37it [00:22,  1.46it/s]Extractor Estimating: 38it [00:22,  1.65it/s]Extractor Estimating: 39it [00:23,  1.78it/s]Extractor Estimating: 40it [00:23,  1.89it/s]Extractor Estimating: 41it [00:24,  2.01it/s]Extractor Estimating: 42it [00:24,  2.05it/s]Extractor Estimating: 43it [00:25,  1.85it/s]Extractor Estimating: 44it [00:25,  2.01it/s]Extractor Estimating: 45it [00:26,  2.09it/s]Extractor Estimating: 46it [00:26,  2.16it/s]Extractor Estimating: 47it [00:27,  2.19it/s]Extractor Estimating: 48it [00:27,  2.17it/s]Extractor Estimating: 49it [00:27,  2.17it/s]Extractor Estimating: 50it [00:28,  1.59it/s]Extractor Estimating: 51it [00:29,  1.64it/s]Extractor Estimating: 52it [00:30,  1.65it/s]Extractor Estimating: 53it [00:30,  1.76it/s]Extractor Estimating: 54it [00:31,  1.76it/s]Extractor Estimating: 55it [00:31,  1.58it/s]Extractor Estimating: 56it [00:32,  1.42it/s]Extractor Estimating: 57it [00:33,  1.51it/s]Extractor Estimating: 58it [00:34,  1.40it/s]Extractor Estimating: 59it [00:34,  1.45it/s]Extractor Estimating: 60it [00:35,  1.57it/s]Extractor Estimating: 61it [00:36,  1.49it/s]Extractor Estimating: 62it [00:36,  1.56it/s]Extractor Estimating: 63it [00:37,  1.59it/s]Extractor Estimating: 64it [00:37,  1.56it/s]Extractor Estimating: 65it [00:38,  1.69it/s]Extractor Estimating: 66it [00:38,  1.76it/s]Extractor Estimating: 67it [00:39,  1.77it/s]Extractor Estimating: 68it [00:40,  1.82it/s]Extractor Estimating: 69it [00:40,  1.88it/s]Extractor Estimating: 70it [00:41,  1.75it/s]Extractor Estimating: 71it [00:41,  1.82it/s]Extractor Estimating: 72it [00:42,  1.90it/s]Extractor Estimating: 73it [00:42,  1.91it/s]Extractor Estimating: 74it [00:43,  1.89it/s]Extractor Estimating: 75it [00:43,  1.86it/s]Extractor Estimating: 76it [00:44,  1.71it/s]Extractor Estimating: 77it [00:45,  1.63it/s]Extractor Estimating: 78it [00:45,  1.67it/s]Extractor Estimating: 79it [00:46,  1.70it/s]Extractor Estimating: 80it [00:46,  1.68it/s]Extractor Estimating: 81it [00:47,  1.61it/s]Extractor Estimating: 82it [00:48,  1.67it/s]Extractor Estimating: 83it [00:48,  1.73it/s]Extractor Estimating: 84it [00:49,  1.75it/s]Extractor Estimating: 85it [00:49,  1.80it/s]Extractor Estimating: 86it [00:50,  1.74it/s]Extractor Estimating: 87it [00:50,  1.75it/s]Extractor Estimating: 88it [00:51,  1.81it/s]Extractor Estimating: 89it [00:51,  1.79it/s]Extractor Estimating: 90it [00:52,  1.84it/s]Extractor Estimating: 91it [00:53,  1.44it/s]Extractor Estimating: 92it [00:54,  1.54it/s]Extractor Estimating: 93it [00:54,  1.58it/s]Extractor Estimating: 94it [00:55,  1.64it/s]Extractor Estimating: 95it [00:55,  1.71it/s]Extractor Estimating: 96it [00:56,  1.70it/s]Extractor Estimating: 97it [00:56,  1.72it/s]Extractor Estimating: 98it [00:57,  1.74it/s]Extractor Estimating: 99it [00:58,  1.76it/s]Extractor Estimating: 100it [00:58,  1.76it/s]Extractor Estimating: 101it [00:59,  1.83it/s]Extractor Estimating: 102it [00:59,  1.80it/s]Extractor Estimating: 103it [01:00,  1.88it/s]Extractor Estimating: 104it [01:00,  1.91it/s]Extractor Estimating: 105it [01:01,  1.97it/s]Extractor Estimating: 106it [01:01,  2.02it/s]Extractor Estimating: 107it [01:02,  2.05it/s]Extractor Estimating: 108it [01:02,  1.97it/s]Extractor Estimating: 109it [01:03,  1.95it/s]Extractor Estimating: 110it [01:03,  2.00it/s]Extractor Estimating: 111it [01:04,  2.05it/s]Extractor Estimating: 112it [01:04,  2.03it/s]Extractor Estimating: 113it [01:05,  2.06it/s]Extractor Estimating: 114it [01:05,  2.13it/s]Extractor Estimating: 115it [01:05,  2.22it/s]Extractor Estimating: 116it [01:06,  1.82it/s]Extractor Estimating: 117it [01:07,  1.95it/s]Extractor Estimating: 118it [01:07,  1.96it/s]Extractor Estimating: 119it [01:08,  1.94it/s]Extractor Estimating: 120it [01:08,  1.94it/s]Extractor Estimating: 121it [01:09,  1.95it/s]Extractor Estimating: 122it [01:09,  1.89it/s]Extractor Estimating: 123it [01:10,  1.95it/s]Extractor Estimating: 124it [01:10,  1.96it/s]Extractor Estimating: 125it [01:11,  1.98it/s]Extractor Estimating: 126it [01:11,  2.04it/s]Extractor Estimating: 127it [01:12,  2.03it/s]Extractor Estimating: 128it [01:12,  2.14it/s]Extractor Estimating: 129it [01:13,  2.05it/s]Extractor Estimating: 130it [01:13,  2.12it/s]Extractor Estimating: 131it [01:13,  2.19it/s]Extractor Estimating: 132it [01:14,  2.19it/s]Extractor Estimating: 133it [01:14,  2.24it/s]Extractor Estimating: 134it [01:15,  2.29it/s]Extractor Estimating: 135it [01:15,  2.21it/s]Extractor Estimating: 136it [01:16,  1.99it/s]Extractor Estimating: 137it [01:16,  2.06it/s]Extractor Estimating: 138it [01:17,  2.12it/s]Extractor Estimating: 139it [01:17,  2.16it/s]Extractor Estimating: 140it [01:18,  2.05it/s]Extractor Estimating: 141it [01:18,  2.07it/s]Extractor Estimating: 142it [01:19,  1.98it/s]Extractor Estimating: 143it [01:19,  1.94it/s]Extractor Estimating: 144it [01:20,  2.06it/s]Extractor Estimating: 145it [01:20,  2.14it/s]Extractor Estimating: 146it [01:21,  1.72it/s]Extractor Estimating: 147it [01:21,  1.81it/s]Extractor Estimating: 148it [01:22,  1.92it/s]Extractor Estimating: 149it [01:22,  2.06it/s]Extractor Estimating: 150it [01:23,  2.14it/s]Extractor Estimating: 151it [01:23,  2.21it/s]Extractor Estimating: 152it [01:24,  2.04it/s]Extractor Estimating: 153it [01:24,  2.10it/s]Extractor Estimating: 154it [01:25,  2.10it/s]Extractor Estimating: 155it [01:25,  2.18it/s]Extractor Estimating: 156it [01:25,  2.28it/s]Extractor Estimating: 157it [01:26,  2.14it/s]Extractor Estimating: 158it [01:26,  2.19it/s]Extractor Estimating: 159it [01:27,  1.94it/s]Extractor Estimating: 160it [01:27,  2.06it/s]Extractor Estimating: 161it [01:28,  2.09it/s]Extractor Estimating: 162it [01:28,  2.13it/s]Extractor Estimating: 163it [01:29,  2.13it/s]Extractor Estimating: 164it [01:29,  2.25it/s]Extractor Estimating: 165it [01:30,  2.30it/s]Extractor Estimating: 166it [01:31,  1.50it/s]Extractor Estimating: 167it [01:32,  1.53it/s]Extractor Estimating: 168it [01:32,  1.64it/s]Extractor Estimating: 169it [01:32,  1.75it/s]Extractor Estimating: 170it [01:33,  1.84it/s]Extractor Estimating: 171it [01:33,  1.86it/s]Extractor Estimating: 172it [01:34,  1.92it/s]Extractor Estimating: 173it [01:34,  1.95it/s]Extractor Estimating: 174it [01:35,  2.01it/s]Extractor Estimating: 175it [01:35,  2.07it/s]Extractor Estimating: 176it [01:36,  2.05it/s]Extractor Estimating: 177it [01:36,  2.08it/s]Extractor Estimating: 178it [01:37,  1.94it/s]Extractor Estimating: 179it [01:37,  1.97it/s]Extractor Estimating: 180it [01:38,  1.99it/s]Extractor Estimating: 181it [01:38,  2.05it/s]Extractor Estimating: 182it [01:39,  2.09it/s]Extractor Estimating: 183it [01:39,  2.14it/s]Extractor Estimating: 184it [01:40,  2.06it/s]Extractor Estimating: 185it [01:41,  1.68it/s]Extractor Estimating: 186it [01:41,  1.77it/s]Extractor Estimating: 187it [01:42,  1.84it/s]Extractor Estimating: 188it [01:42,  1.80it/s]Extractor Estimating: 189it [01:43,  1.84it/s]Extractor Estimating: 190it [01:43,  1.96it/s]Extractor Estimating: 191it [01:44,  1.80it/s]Extractor Estimating: 192it [01:44,  1.87it/s]Extractor Estimating: 193it [01:45,  1.88it/s]Extractor Estimating: 194it [01:45,  1.98it/s]Extractor Estimating: 195it [01:46,  1.87it/s]Extractor Estimating: 196it [01:47,  1.75it/s]Extractor Estimating: 197it [01:47,  1.50it/s]Extractor Estimating: 198it [01:48,  1.63it/s]Extractor Estimating: 199it [01:48,  1.74it/s]Extractor Estimating: 200it [01:49,  1.80it/s]Extractor Estimating: 201it [01:49,  1.82it/s]Extractor Estimating: 202it [01:50,  1.80it/s]Extractor Estimating: 203it [01:51,  1.80it/s]Extractor Estimating: 204it [01:51,  1.84it/s]Extractor Estimating: 205it [01:52,  1.43it/s]Extractor Estimating: 206it [01:53,  1.54it/s]Extractor Estimating: 207it [01:53,  1.62it/s]Extractor Estimating: 208it [01:54,  1.60it/s]Extractor Estimating: 209it [01:54,  1.63it/s]Extractor Estimating: 210it [01:55,  1.59it/s]Extractor Estimating: 211it [01:56,  1.66it/s]Extractor Estimating: 212it [01:56,  1.68it/s]Extractor Estimating: 213it [01:57,  1.81it/s]Extractor Estimating: 214it [01:57,  1.78it/s]Extractor Estimating: 215it [01:58,  1.76it/s]Extractor Estimating: 216it [01:58,  1.71it/s]Extractor Estimating: 217it [01:59,  1.71it/s]Extractor Estimating: 218it [02:00,  1.69it/s]Extractor Estimating: 219it [02:00,  1.73it/s]Extractor Estimating: 220it [02:01,  1.65it/s]Extractor Estimating: 221it [02:01,  1.69it/s]Extractor Estimating: 222it [02:02,  1.68it/s]Extractor Estimating: 223it [02:03,  1.72it/s]Extractor Estimating: 224it [02:03,  1.70it/s]Extractor Estimating: 225it [02:04,  1.82it/s]Extractor Estimating: 226it [02:04,  1.88it/s]Extractor Estimating: 227it [02:05,  1.62it/s]Extractor Estimating: 228it [02:05,  1.73it/s]Extractor Estimating: 229it [02:06,  1.77it/s]Extractor Estimating: 230it [02:07,  1.82it/s]Extractor Estimating: 231it [02:07,  1.84it/s]Extractor Estimating: 232it [02:08,  1.91it/s]Extractor Estimating: 233it [02:08,  1.72it/s]Extractor Estimating: 234it [02:09,  1.81it/s]Extractor Estimating: 235it [02:09,  1.87it/s]Extractor Estimating: 236it [02:10,  1.85it/s]Extractor Estimating: 237it [02:10,  1.85it/s]Extractor Estimating: 238it [02:11,  1.90it/s]Extractor Estimating: 239it [02:12,  1.70it/s]Extractor Estimating: 240it [02:12,  1.79it/s]Extractor Estimating: 241it [02:13,  1.85it/s]Extractor Estimating: 242it [02:13,  1.91it/s]Extractor Estimating: 243it [02:13,  1.94it/s]Extractor Estimating: 244it [02:14,  1.92it/s]Extractor Estimating: 245it [02:15,  1.82it/s]Extractor Estimating: 246it [02:15,  1.90it/s]Extractor Estimating: 247it [02:16,  1.89it/s]Extractor Estimating: 248it [02:16,  1.92it/s]Extractor Estimating: 249it [02:17,  1.82it/s]Extractor Estimating: 250it [02:17,  1.91it/s]Extractor Estimating: 251it [02:18,  1.80it/s]Extractor Estimating: 252it [02:18,  1.87it/s]Extractor Estimating: 253it [02:19,  1.88it/s]Extractor Estimating: 254it [02:19,  1.94it/s]Extractor Estimating: 255it [02:20,  1.87it/s]Extractor Estimating: 256it [02:20,  1.84it/s]Extractor Estimating: 257it [02:21,  1.73it/s]Extractor Estimating: 258it [02:22,  1.75it/s]Extractor Estimating: 259it [02:22,  1.79it/s]Extractor Estimating: 260it [02:23,  1.84it/s]Extractor Estimating: 261it [02:23,  1.82it/s]Extractor Estimating: 262it [02:24,  1.82it/s]Extractor Estimating: 263it [02:24,  1.82it/s]Extractor Estimating: 264it [02:25,  1.86it/s]Extractor Estimating: 265it [02:26,  1.65it/s]Extractor Estimating: 266it [02:26,  1.71it/s]Extractor Estimating: 267it [02:27,  1.57it/s]Extractor Estimating: 268it [02:28,  1.59it/s]Extractor Estimating: 269it [02:28,  1.63it/s]Extractor Estimating: 270it [02:29,  1.42it/s]Extractor Estimating: 271it [02:30,  1.47it/s]Extractor Estimating: 272it [02:30,  1.58it/s]Extractor Estimating: 273it [02:31,  1.64it/s]Extractor Estimating: 274it [02:31,  1.69it/s]Extractor Estimating: 275it [02:32,  1.57it/s]Extractor Estimating: 276it [02:33,  1.69it/s]Extractor Estimating: 277it [02:33,  1.80it/s]Extractor Estimating: 278it [02:33,  1.89it/s]Extractor Estimating: 279it [02:34,  1.94it/s]Extractor Estimating: 280it [02:34,  1.99it/s]Extractor Estimating: 281it [02:35,  1.97it/s]Extractor Estimating: 282it [02:36,  1.94it/s]Extractor Estimating: 283it [02:36,  1.97it/s]Extractor Estimating: 284it [02:36,  2.01it/s]Extractor Estimating: 285it [02:37,  2.02it/s]Extractor Estimating: 286it [02:37,  2.08it/s]Extractor Estimating: 287it [02:38,  2.11it/s]Extractor Estimating: 288it [02:39,  1.63it/s]Extractor Estimating: 289it [02:39,  1.76it/s]Extractor Estimating: 290it [02:40,  1.76it/s]Extractor Estimating: 291it [02:40,  1.83it/s]Extractor Estimating: 292it [02:41,  1.99it/s]Extractor Estimating: 293it [02:41,  2.12it/s]Extractor Estimating: 294it [02:42,  1.96it/s]Extractor Estimating: 295it [02:42,  1.93it/s]Extractor Estimating: 296it [02:43,  1.97it/s]Extractor Estimating: 297it [02:43,  1.95it/s]Extractor Estimating: 298it [02:44,  1.93it/s]Extractor Estimating: 299it [02:44,  1.96it/s]Extractor Estimating: 300it [02:45,  1.92it/s]Extractor Estimating: 301it [02:45,  1.95it/s]Extractor Estimating: 302it [02:46,  1.92it/s]Extractor Estimating: 303it [02:46,  1.93it/s]Extractor Estimating: 304it [02:47,  1.89it/s]Extractor Estimating: 305it [02:47,  2.00it/s]Extractor Estimating: 306it [02:48,  1.96it/s]Extractor Estimating: 307it [02:48,  1.97it/s]Extractor Estimating: 308it [02:49,  1.95it/s]Extractor Estimating: 309it [02:49,  2.00it/s]Extractor Estimating: 310it [02:50,  1.97it/s]Extractor Estimating: 311it [02:50,  1.99it/s]Extractor Estimating: 312it [02:51,  1.81it/s]Extractor Estimating: 313it [02:52,  1.86it/s]Extractor Estimating: 314it [02:52,  1.97it/s]Extractor Estimating: 315it [02:52,  2.05it/s]Extractor Estimating: 316it [02:53,  2.02it/s]Extractor Estimating: 317it [02:53,  2.06it/s]Extractor Estimating: 318it [02:54,  2.10it/s]Extractor Estimating: 319it [02:54,  2.09it/s]Extractor Estimating: 320it [02:55,  2.12it/s]Extractor Estimating: 321it [02:55,  1.96it/s]Extractor Estimating: 322it [02:56,  1.92it/s]Extractor Estimating: 323it [02:56,  1.96it/s]Extractor Estimating: 324it [02:57,  1.95it/s]Extractor Estimating: 325it [02:57,  1.96it/s]Extractor Estimating: 326it [02:58,  1.94it/s]Extractor Estimating: 327it [02:59,  1.96it/s]Extractor Estimating: 328it [02:59,  1.77it/s]Extractor Estimating: 329it [03:00,  1.83it/s]Extractor Estimating: 330it [03:00,  1.91it/s]Extractor Estimating: 331it [03:01,  1.94it/s]Extractor Estimating: 332it [03:01,  2.01it/s]Extractor Estimating: 333it [03:02,  2.03it/s]Extractor Estimating: 334it [03:02,  1.76it/s]Extractor Estimating: 335it [03:03,  1.83it/s]Extractor Estimating: 336it [03:03,  1.93it/s]Extractor Estimating: 337it [03:04,  1.96it/s]Extractor Estimating: 338it [03:04,  2.02it/s]Extractor Estimating: 339it [03:05,  2.03it/s]Extractor Estimating: 340it [03:05,  1.89it/s]Extractor Estimating: 341it [03:06,  1.98it/s]Extractor Estimating: 342it [03:06,  2.08it/s]Extractor Estimating: 343it [03:07,  2.08it/s]Extractor Estimating: 344it [03:07,  2.07it/s]Extractor Estimating: 345it [03:08,  2.14it/s]Extractor Estimating: 346it [03:08,  2.13it/s]Extractor Estimating: 347it [03:09,  1.97it/s]Extractor Estimating: 348it [03:09,  2.06it/s]Extractor Estimating: 349it [03:10,  2.09it/s]Extractor Estimating: 350it [03:10,  2.09it/s]Extractor Estimating: 351it [03:11,  1.94it/s]Extractor Estimating: 352it [03:11,  1.90it/s]Extractor Estimating: 353it [03:12,  1.80it/s]Extractor Estimating: 354it [03:13,  1.67it/s]Extractor Estimating: 355it [03:13,  1.62it/s]Extractor Estimating: 356it [03:14,  1.65it/s]Extractor Estimating: 357it [03:14,  1.69it/s]Extractor Estimating: 358it [03:15,  1.66it/s]Extractor Estimating: 359it [03:16,  1.73it/s]Extractor Estimating: 360it [03:16,  1.70it/s]Extractor Estimating: 361it [03:17,  1.73it/s]Extractor Estimating: 362it [03:17,  1.74it/s]Extractor Estimating: 363it [03:18,  1.75it/s]Extractor Estimating: 364it [03:19,  1.61it/s]Extractor Estimating: 365it [03:19,  1.72it/s]Extractor Estimating: 366it [03:20,  1.72it/s]Extractor Estimating: 367it [03:20,  1.74it/s]Extractor Estimating: 368it [03:21,  1.73it/s]Extractor Estimating: 369it [03:21,  1.75it/s]Extractor Estimating: 370it [03:22,  1.39it/s]Extractor Estimating: 371it [03:23,  1.51it/s]Extractor Estimating: 372it [03:24,  1.48it/s]Extractor Estimating: 373it [03:24,  1.55it/s]Extractor Estimating: 374it [03:25,  1.65it/s]Extractor Estimating: 375it [03:25,  1.74it/s]Extractor Estimating: 375it [03:25,  1.82it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:54,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:55,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:55,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:55,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:55,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:37:55,554 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:37:55,555 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:55,973 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:57,065 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:57,066 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:59,331 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:59,333 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:59,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:59,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:59,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:59,860 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:59,861 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:38:00,612 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:38:00,917 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:38:00,918 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:12:27,284 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:12:27,709 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5996 mean pseudo reward: 0.965934996104163
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 14984
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15084, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15084, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.884, loss:378.4545
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.879, loss:343.3011
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.880, loss:316.1715
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 0.865, loss:313.6435
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 0.886, loss:335.8968
>> valid entity prec:0.5569, rec:0.5569, f1:0.5569
>> valid relation prec:0.2093, rec:0.1272, f1:0.1582
>> valid relation with NER prec:0.2093, rec:0.1272, f1:0.1582
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.893, loss:282.7411
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 0.879, loss:302.0503
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.883, loss:304.2308
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 0.870, loss:284.8587
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 0.878, loss:321.1897
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5246, rec:0.5406, f1:0.5325
>> valid relation prec:0.1993, rec:0.1389, f1:0.1637
>> valid relation with NER prec:0.1993, rec:0.1389, f1:0.1637
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 100, avg_time 0.898, loss:256.6207
g_step 1200, step 200, avg_time 0.873, loss:290.4556
g_step 1300, step 50, avg_time 0.861, loss:284.0988
g_step 1400, step 150, avg_time 0.846, loss:265.0809
g_step 1500, step 250, avg_time 0.921, loss:269.8484
>> valid entity prec:0.5634, rec:0.4759, f1:0.5160
>> valid relation prec:0.1820, rec:0.1053, f1:0.1335
>> valid relation with NER prec:0.1820, rec:0.1053, f1:0.1335
g_step 1600, step 100, avg_time 0.853, loss:227.4575
g_step 1700, step 200, avg_time 0.911, loss:262.0172
g_step 1800, step 50, avg_time 0.888, loss:236.1279
g_step 1900, step 150, avg_time 0.861, loss:213.2461
g_step 2000, step 250, avg_time 0.897, loss:243.9856
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5691, rec:0.5183, f1:0.5425
>> valid relation prec:0.1754, rec:0.1114, f1:0.1362
>> valid relation with NER prec:0.1754, rec:0.1114, f1:0.1362
g_step 2100, step 100, avg_time 0.891, loss:211.5551
g_step 2200, step 200, avg_time 0.881, loss:222.7340
g_step 2300, step 50, avg_time 0.881, loss:210.2000
g_step 2400, step 150, avg_time 0.894, loss:205.0539
g_step 2500, step 250, avg_time 0.868, loss:230.8561
>> valid entity prec:0.5406, rec:0.5347, f1:0.5376
>> valid relation prec:0.1464, rec:0.1125, f1:0.1273
>> valid relation with NER prec:0.1464, rec:0.1125, f1:0.1273
g_step 2600, step 100, avg_time 0.890, loss:197.4990
g_step 2700, step 200, avg_time 0.858, loss:185.5619
g_step 2800, step 50, avg_time 0.883, loss:193.6977
g_step 2900, step 150, avg_time 0.909, loss:186.5554
g_step 3000, step 250, avg_time 0.874, loss:187.8012
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5641, rec:0.5261, f1:0.5445
>> valid relation prec:0.1652, rec:0.1111, f1:0.1328
>> valid relation with NER prec:0.1652, rec:0.1111, f1:0.1328
g_step 3100, step 100, avg_time 0.880, loss:155.0188
g_step 3200, step 200, avg_time 0.899, loss:191.1228
g_step 3300, step 50, avg_time 0.862, loss:168.9077
g_step 3400, step 150, avg_time 0.871, loss:155.6470
g_step 3500, step 250, avg_time 0.910, loss:180.7556
>> valid entity prec:0.5289, rec:0.5461, f1:0.5373
>> valid relation prec:0.1601, rec:0.1323, f1:0.1449
>> valid relation with NER prec:0.1601, rec:0.1323, f1:0.1449
g_step 3600, step 100, avg_time 0.881, loss:145.7238
g_step 3700, step 200, avg_time 0.871, loss:163.8660
g_step 3800, step 50, avg_time 0.897, loss:148.0915
g_step 3900, step 150, avg_time 0.897, loss:151.7041
g_step 4000, step 250, avg_time 0.887, loss:156.5586
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5419, rec:0.5321, f1:0.5369
>> valid relation prec:0.1705, rec:0.1214, f1:0.1418
>> valid relation with NER prec:0.1705, rec:0.1214, f1:0.1418
g_step 4100, step 100, avg_time 0.899, loss:134.2344
g_step 4200, step 200, avg_time 0.878, loss:152.5311
g_step 4300, step 50, avg_time 0.887, loss:128.4370
g_step 4400, step 150, avg_time 0.881, loss:137.4672
g_step 4500, step 250, avg_time 0.921, loss:151.4001
>> valid entity prec:0.5521, rec:0.5077, f1:0.5289
>> valid relation prec:0.1569, rec:0.1105, f1:0.1297
>> valid relation with NER prec:0.1569, rec:0.1105, f1:0.1297
g_step 4600, step 100, avg_time 0.914, loss:119.3884
g_step 4700, step 200, avg_time 0.876, loss:141.3187
g_step 4800, step 50, avg_time 0.894, loss:131.5645
g_step 4900, step 150, avg_time 0.896, loss:119.8054
g_step 5000, step 250, avg_time 0.865, loss:132.9413
learning rate was adjusted to 0.0008
>> valid entity prec:0.5515, rec:0.5131, f1:0.5316
>> valid relation prec:0.1683, rec:0.1246, f1:0.1432
>> valid relation with NER prec:0.1683, rec:0.1246, f1:0.1432
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:12:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:12:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-12-27_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:12:29 - WARNING - datasets.builder -   Using custom data configuration default-9a6cc33205d4eb65
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9a6cc33205d4eb65/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:12:32,697 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:12:32,698 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:12:32,699 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:12:32,700 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:12:32,947 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:12:33,021 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:12:33,454 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:12:36,803 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:12:36,867 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9a6cc33205d4eb65/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.94ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.09ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.84ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.30ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.65ba/s]100%|██████████| 6/6 [00:01<00:00,  4.88ba/s]100%|██████████| 6/6 [00:01<00:00,  4.14ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.50ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.50ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.16ba/s]100%|██████████| 4/4 [00:01<00:00,  4.27ba/s]100%|██████████| 4/4 [00:01<00:00,  3.32ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.13ba/s] 50%|█████     | 3/6 [00:00<00:00,  6.57ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  6.09ba/s]100%|██████████| 6/6 [00:00<00:00,  7.77ba/s]100%|██████████| 6/6 [00:00<00:00,  6.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.25ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.52ba/s]100%|██████████| 4/4 [00:01<00:00,  5.22ba/s]100%|██████████| 4/4 [00:01<00:00,  3.80ba/s]
[INFO|trainer.py:414] 2023-08-28 23:12:48,283 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:12:48,840 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:12:48,841 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-28 23:12:48,841 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:12:48,841 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:12:48,841 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:12:48,841 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:12:48,841 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:01<12:32,  1.60s/it]  0%|          | 2/470 [00:02<08:54,  1.14s/it]  1%|          | 3/470 [00:03<07:33,  1.03it/s]  1%|          | 4/470 [00:03<06:50,  1.13it/s]  1%|          | 5/470 [00:04<05:11,  1.49it/s]  1%|▏         | 6/470 [00:04<04:27,  1.74it/s]  1%|▏         | 7/470 [00:05<04:00,  1.93it/s]  2%|▏         | 8/470 [00:05<03:59,  1.93it/s]  2%|▏         | 9/470 [00:05<03:26,  2.23it/s]  2%|▏         | 10/470 [00:06<03:04,  2.50it/s]  2%|▏         | 11/470 [00:06<02:48,  2.72it/s]  3%|▎         | 12/470 [00:07<03:31,  2.17it/s]  3%|▎         | 13/470 [00:07<03:15,  2.33it/s]  3%|▎         | 14/470 [00:07<02:56,  2.58it/s]  3%|▎         | 15/470 [00:08<02:43,  2.79it/s]  3%|▎         | 16/470 [00:08<02:33,  2.95it/s]  4%|▎         | 17/470 [00:08<02:27,  3.07it/s]  4%|▍         | 18/470 [00:08<02:22,  3.17it/s]  4%|▍         | 19/470 [00:09<02:26,  3.08it/s]  4%|▍         | 20/470 [00:09<02:21,  3.18it/s]  4%|▍         | 21/470 [00:09<02:18,  3.24it/s]  5%|▍         | 22/470 [00:10<02:16,  3.29it/s]  5%|▍         | 23/470 [00:10<02:14,  3.33it/s]  5%|▌         | 24/470 [00:10<02:12,  3.36it/s]  5%|▌         | 25/470 [00:11<02:11,  3.38it/s]  6%|▌         | 26/470 [00:11<02:10,  3.39it/s]  6%|▌         | 27/470 [00:11<02:10,  3.40it/s]  6%|▌         | 28/470 [00:11<02:09,  3.41it/s]  6%|▌         | 29/470 [00:12<02:32,  2.89it/s]  6%|▋         | 30/470 [00:12<02:25,  3.03it/s]  7%|▋         | 31/470 [00:12<02:19,  3.14it/s]  7%|▋         | 32/470 [00:13<02:45,  2.64it/s]  7%|▋         | 33/470 [00:13<02:33,  2.84it/s]  7%|▋         | 34/470 [00:14<02:25,  2.99it/s]  7%|▋         | 35/470 [00:14<02:20,  3.11it/s]  8%|▊         | 36/470 [00:14<02:15,  3.19it/s]  8%|▊         | 37/470 [00:14<02:12,  3.26it/s]  8%|▊         | 38/470 [00:15<02:21,  3.05it/s]  8%|▊         | 39/470 [00:15<02:16,  3.16it/s]  9%|▊         | 40/470 [00:15<02:13,  3.23it/s]  9%|▊         | 41/470 [00:16<02:10,  3.28it/s]  9%|▉         | 42/470 [00:16<02:08,  3.32it/s]  9%|▉         | 43/470 [00:16<02:07,  3.35it/s]  9%|▉         | 44/470 [00:17<02:06,  3.37it/s] 10%|▉         | 45/470 [00:17<02:05,  3.38it/s] 10%|▉         | 46/470 [00:17<02:04,  3.39it/s] 10%|█         | 47/470 [00:17<02:04,  3.40it/s] 10%|█         | 48/470 [00:18<02:26,  2.88it/s] 10%|█         | 49/470 [00:18<02:19,  3.02it/s] 11%|█         | 50/470 [00:18<02:14,  3.13it/s] 11%|█         | 51/470 [00:19<02:10,  3.21it/s] 11%|█         | 52/470 [00:19<02:07,  3.27it/s] 11%|█▏        | 53/470 [00:19<02:05,  3.31it/s] 11%|█▏        | 54/470 [00:20<02:04,  3.34it/s] 12%|█▏        | 55/470 [00:20<02:03,  3.36it/s] 12%|█▏        | 56/470 [00:20<02:02,  3.38it/s] 12%|█▏        | 57/470 [00:21<02:02,  3.38it/s] 12%|█▏        | 58/470 [00:21<02:06,  3.25it/s] 13%|█▎        | 59/470 [00:21<02:04,  3.30it/s] 13%|█▎        | 60/470 [00:21<02:03,  3.33it/s] 13%|█▎        | 61/470 [00:22<02:01,  3.35it/s] 13%|█▎        | 62/470 [00:22<02:00,  3.38it/s] 13%|█▎        | 63/470 [00:22<02:00,  3.39it/s] 14%|█▎        | 64/470 [00:23<01:59,  3.40it/s] 14%|█▍        | 65/470 [00:23<01:58,  3.41it/s] 14%|█▍        | 66/470 [00:23<01:58,  3.40it/s] 14%|█▍        | 67/470 [00:24<01:58,  3.41it/s] 14%|█▍        | 68/470 [00:24<01:57,  3.41it/s] 15%|█▍        | 69/470 [00:24<02:05,  3.19it/s] 15%|█▍        | 70/470 [00:24<02:02,  3.25it/s] 15%|█▌        | 71/470 [00:25<02:00,  3.30it/s] 15%|█▌        | 72/470 [00:25<01:59,  3.33it/s] 16%|█▌        | 73/470 [00:25<01:58,  3.35it/s] 16%|█▌        | 74/470 [00:26<01:57,  3.37it/s] 16%|█▌        | 75/470 [00:26<01:56,  3.38it/s] 16%|█▌        | 76/470 [00:26<01:56,  3.39it/s] 16%|█▋        | 77/470 [00:27<01:55,  3.40it/s] 17%|█▋        | 78/470 [00:27<01:55,  3.40it/s] 17%|█▋        | 79/470 [00:27<02:01,  3.22it/s] 17%|█▋        | 80/470 [00:27<01:59,  3.28it/s] 17%|█▋        | 81/470 [00:28<01:57,  3.31it/s] 17%|█▋        | 82/470 [00:28<01:56,  3.34it/s] 18%|█▊        | 83/470 [00:28<01:55,  3.36it/s] 18%|█▊        | 84/470 [00:29<01:54,  3.38it/s] 18%|█▊        | 85/470 [00:29<01:53,  3.39it/s] 18%|█▊        | 86/470 [00:29<01:53,  3.40it/s] 19%|█▊        | 87/470 [00:29<01:52,  3.40it/s] 19%|█▊        | 88/470 [00:30<01:52,  3.40it/s] 19%|█▉        | 89/470 [00:30<01:51,  3.40it/s] 19%|█▉        | 90/470 [00:30<02:01,  3.14it/s] 19%|█▉        | 91/470 [00:31<01:58,  3.21it/s] 20%|█▉        | 92/470 [00:31<01:55,  3.27it/s] 20%|█▉        | 93/470 [00:31<01:53,  3.31it/s] 20%|██        | 94/470 [00:32<01:45,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 23:13:20,922 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:13:20,922 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:13:20,922 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.18it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.67it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.99it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.69it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.33it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.99it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.74it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.85it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 39.57it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 41.24it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.35it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.26it/s][A
 17%|█▋        | 72/436 [00:01<00:10, 35.79it/s][A
 18%|█▊        | 77/436 [00:01<00:09, 38.19it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 40.10it/s][A
 20%|█▉        | 87/436 [00:02<00:08, 41.51it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.51it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.32it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.88it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.17it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.86it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.81it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.06it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.38it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.64it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.77it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.93it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 45.01it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.75it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.40it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.28it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.21it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.58it/s][A
 41%|████      | 177/436 [00:04<00:06, 38.12it/s][A
 42%|████▏     | 182/436 [00:04<00:06, 40.10it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 41.60it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 42.64it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.37it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.59it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.37it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.81it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 43.72it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.71it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.00it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.34it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.56it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.64it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.70it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.82it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.75it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.52it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.38it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.38it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.50it/s][A
 65%|██████▍   | 282/436 [00:06<00:08, 18.94it/s][A
 66%|██████▌   | 287/436 [00:07<00:06, 22.96it/s][A
 67%|██████▋   | 292/436 [00:07<00:05, 26.94it/s][A
 68%|██████▊   | 297/436 [00:07<00:04, 30.65it/s][A
 69%|██████▉   | 302/436 [00:07<00:04, 32.65it/s][A
 70%|███████   | 307/436 [00:07<00:04, 29.19it/s][A
 72%|███████▏  | 312/436 [00:07<00:03, 32.68it/s][A
 73%|███████▎  | 317/436 [00:07<00:03, 35.65it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 38.02it/s][A
 75%|███████▌  | 327/436 [00:08<00:02, 40.00it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 41.38it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 42.51it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 43.11it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 43.26it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.36it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.64it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.04it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.41it/s][A
 85%|████████▌ | 372/436 [00:09<00:01, 44.59it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 44.81it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 44.90it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.74it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.53it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.23it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.36it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.45it/s][A
 94%|█████████▍| 412/436 [00:10<00:00, 44.63it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 44.79it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 44.70it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 45.03it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.84it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:10<00:00, 44.84it/s][A 20%|██        | 94/470 [00:42<01:45,  3.56it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:13:32,060 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 23:13:32,346 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:13:37,991 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:13:38,439 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:13:38,777 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [01:13<1:18:05, 12.50s/it] 20%|██        | 96/470 [01:13<55:04,  8.83s/it]   21%|██        | 97/470 [01:13<39:09,  6.30s/it] 21%|██        | 98/470 [01:14<27:52,  4.50s/it] 21%|██        | 99/470 [01:14<20:00,  3.24s/it] 21%|██▏       | 100/470 [01:14<14:30,  2.35s/it] 21%|██▏       | 101/470 [01:14<10:40,  1.73s/it] 22%|██▏       | 102/470 [01:15<07:59,  1.30s/it] 22%|██▏       | 103/470 [01:15<06:06,  1.00it/s] 22%|██▏       | 104/470 [01:15<04:47,  1.27it/s] 22%|██▏       | 105/470 [01:16<03:53,  1.57it/s] 23%|██▎       | 106/470 [01:16<03:14,  1.87it/s] 23%|██▎       | 107/470 [01:16<03:00,  2.01it/s] 23%|██▎       | 108/470 [01:17<02:37,  2.29it/s] 23%|██▎       | 109/470 [01:17<02:21,  2.55it/s] 23%|██▎       | 110/470 [01:17<02:10,  2.76it/s] 24%|██▎       | 111/470 [01:17<02:02,  2.93it/s] 24%|██▍       | 112/470 [01:18<01:56,  3.06it/s] 24%|██▍       | 113/470 [01:18<01:52,  3.16it/s] 24%|██▍       | 114/470 [01:18<01:50,  3.23it/s] 24%|██▍       | 115/470 [01:19<01:48,  3.29it/s] 25%|██▍       | 116/470 [01:19<01:46,  3.33it/s] 25%|██▍       | 117/470 [01:19<01:49,  3.22it/s] 25%|██▌       | 118/470 [01:20<01:47,  3.28it/s] 25%|██▌       | 119/470 [01:20<01:45,  3.32it/s] 26%|██▌       | 120/470 [01:20<01:44,  3.35it/s] 26%|██▌       | 121/470 [01:20<01:43,  3.37it/s] 26%|██▌       | 122/470 [01:21<01:42,  3.38it/s] 26%|██▌       | 123/470 [01:21<01:42,  3.39it/s] 26%|██▋       | 124/470 [01:21<01:41,  3.40it/s] 27%|██▋       | 125/470 [01:22<01:41,  3.41it/s] 27%|██▋       | 126/470 [01:22<01:40,  3.41it/s] 27%|██▋       | 127/470 [01:22<01:40,  3.41it/s] 27%|██▋       | 128/470 [01:22<01:42,  3.33it/s] 27%|██▋       | 129/470 [01:23<01:41,  3.36it/s] 28%|██▊       | 130/470 [01:23<01:40,  3.37it/s] 28%|██▊       | 131/470 [01:23<01:40,  3.38it/s] 28%|██▊       | 132/470 [01:24<01:39,  3.39it/s] 28%|██▊       | 133/470 [01:24<01:39,  3.40it/s] 29%|██▊       | 134/470 [01:24<01:38,  3.40it/s] 29%|██▊       | 135/470 [01:25<01:38,  3.41it/s] 29%|██▉       | 136/470 [01:25<01:38,  3.41it/s] 29%|██▉       | 137/470 [01:25<01:37,  3.41it/s] 29%|██▉       | 138/470 [01:25<01:37,  3.41it/s] 30%|██▉       | 139/470 [01:26<01:39,  3.32it/s] 30%|██▉       | 140/470 [01:26<01:38,  3.34it/s] 30%|███       | 141/470 [01:26<01:37,  3.37it/s] 30%|███       | 142/470 [01:27<01:37,  3.38it/s] 30%|███       | 143/470 [01:27<01:36,  3.39it/s] 31%|███       | 144/470 [01:27<01:36,  3.39it/s] 31%|███       | 145/470 [01:28<01:35,  3.40it/s] 31%|███       | 146/470 [01:28<01:35,  3.40it/s] 31%|███▏      | 147/470 [01:28<01:34,  3.41it/s] 31%|███▏      | 148/470 [01:28<01:34,  3.41it/s] 32%|███▏      | 149/470 [01:29<01:34,  3.40it/s] 32%|███▏      | 150/470 [01:29<01:44,  3.06it/s] 32%|███▏      | 151/470 [01:29<01:41,  3.15it/s] 32%|███▏      | 152/470 [01:30<01:38,  3.23it/s] 33%|███▎      | 153/470 [01:30<01:36,  3.28it/s] 33%|███▎      | 154/470 [01:30<01:35,  3.32it/s] 33%|███▎      | 155/470 [01:31<01:34,  3.34it/s] 33%|███▎      | 156/470 [01:31<01:33,  3.37it/s] 33%|███▎      | 157/470 [01:31<01:32,  3.38it/s] 34%|███▎      | 158/470 [01:31<01:32,  3.39it/s] 34%|███▍      | 159/470 [01:32<01:31,  3.40it/s] 34%|███▍      | 160/470 [01:32<01:40,  3.07it/s] 34%|███▍      | 161/470 [01:32<01:37,  3.16it/s] 34%|███▍      | 162/470 [01:33<01:35,  3.24it/s] 35%|███▍      | 163/470 [01:33<01:33,  3.29it/s] 35%|███▍      | 164/470 [01:33<01:32,  3.32it/s] 35%|███▌      | 165/470 [01:34<01:31,  3.34it/s] 35%|███▌      | 166/470 [01:34<01:30,  3.36it/s] 36%|███▌      | 167/470 [01:34<01:29,  3.37it/s] 36%|███▌      | 168/470 [01:34<01:29,  3.38it/s] 36%|███▌      | 169/470 [01:35<01:28,  3.39it/s] 36%|███▌      | 170/470 [01:35<01:36,  3.10it/s] 36%|███▋      | 171/470 [01:35<01:33,  3.19it/s] 37%|███▋      | 172/470 [01:36<01:34,  3.14it/s] 37%|███▋      | 173/470 [01:36<01:32,  3.22it/s] 37%|███▋      | 174/470 [01:36<01:30,  3.27it/s] 37%|███▋      | 175/470 [01:37<01:29,  3.31it/s] 37%|███▋      | 176/470 [01:37<01:28,  3.34it/s] 38%|███▊      | 177/470 [01:37<01:27,  3.36it/s] 38%|███▊      | 178/470 [01:38<01:26,  3.37it/s] 38%|███▊      | 179/470 [01:38<01:25,  3.38it/s] 38%|███▊      | 180/470 [01:39<02:45,  1.75it/s] 39%|███▊      | 181/470 [01:39<02:20,  2.05it/s] 39%|███▊      | 182/470 [01:40<02:03,  2.33it/s] 39%|███▉      | 183/470 [01:40<01:51,  2.57it/s] 39%|███▉      | 184/470 [01:40<01:43,  2.78it/s] 39%|███▉      | 185/470 [01:41<01:36,  2.94it/s] 40%|███▉      | 186/470 [01:41<01:39,  2.85it/s] 40%|███▉      | 187/470 [01:41<01:34,  3.00it/s] 40%|████      | 188/470 [01:41<01:25,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 23:14:30,758 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:14:30,758 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:14:30,758 >>   Batch size = 8
{'eval_loss': 1.1205992698669434, 'eval_runtime': 10.6129, 'eval_samples_per_second': 328.655, 'eval_steps_per_second': 41.082, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.95it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.78it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.30it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.16it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.50it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.02it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.80it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.63it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.57it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.80it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.87it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.84it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.44it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.46it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.39it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.32it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.46it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 38.79it/s][A
 25%|██▍       | 107/436 [00:02<00:08, 40.52it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 41.66it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 42.74it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.46it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.99it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.24it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.34it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.05it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.88it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.86it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.15it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.43it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.66it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.79it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.96it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.80it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.40it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.18it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.15it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.20it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.44it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.57it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.59it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.83it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.60it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.31it/s][A
 54%|█████▍    | 237/436 [00:05<00:05, 39.70it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 41.20it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 42.29it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.15it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.80it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.14it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.27it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.16it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.88it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.74it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.93it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.25it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.43it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.70it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.83it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.85it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.61it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.22it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.10it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.30it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.50it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.66it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.78it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.88it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.57it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.37it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.18it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.67it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.87it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.21it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.45it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.55it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.40it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.30it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.11it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.17it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.27it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.52it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.59it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.59it/s][A 40%|████      | 188/470 [01:51<01:25,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:14:41,725 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 23:14:42,701 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:14:50,251 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:14:50,941 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:14:51,465 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [02:20<55:34, 11.87s/it] 40%|████      | 190/470 [02:21<39:22,  8.44s/it] 41%|████      | 191/470 [02:21<27:52,  6.00s/it] 41%|████      | 192/470 [02:21<19:51,  4.28s/it] 41%|████      | 193/470 [02:22<14:15,  3.09s/it] 41%|████▏     | 194/470 [02:22<10:20,  2.25s/it] 41%|████▏     | 195/470 [02:22<07:37,  1.66s/it] 42%|████▏     | 196/470 [02:22<05:43,  1.25s/it] 42%|████▏     | 197/470 [02:23<04:23,  1.04it/s] 42%|████▏     | 198/470 [02:23<03:27,  1.31it/s] 42%|████▏     | 199/470 [02:23<02:48,  1.61it/s] 43%|████▎     | 200/470 [02:24<02:26,  1.84it/s] 43%|████▎     | 201/470 [02:24<02:05,  2.14it/s] 43%|████▎     | 202/470 [02:24<01:50,  2.42it/s] 43%|████▎     | 203/470 [02:25<01:40,  2.66it/s] 43%|████▎     | 204/470 [02:25<01:33,  2.85it/s] 44%|████▎     | 205/470 [02:25<01:28,  3.01it/s] 44%|████▍     | 206/470 [02:25<01:24,  3.13it/s] 44%|████▍     | 207/470 [02:26<01:21,  3.22it/s] 44%|████▍     | 208/470 [02:26<01:19,  3.28it/s] 44%|████▍     | 209/470 [02:26<01:18,  3.33it/s] 45%|████▍     | 210/470 [02:27<01:17,  3.36it/s] 45%|████▍     | 211/470 [02:27<01:23,  3.11it/s] 45%|████▌     | 212/470 [02:27<01:20,  3.20it/s] 45%|████▌     | 213/470 [02:28<01:18,  3.27it/s] 46%|████▌     | 214/470 [02:28<01:17,  3.32it/s] 46%|████▌     | 215/470 [02:28<01:15,  3.36it/s] 46%|████▌     | 216/470 [02:28<01:14,  3.39it/s] 46%|████▌     | 217/470 [02:29<01:14,  3.41it/s] 46%|████▋     | 218/470 [02:29<01:13,  3.42it/s] 47%|████▋     | 219/470 [02:29<01:13,  3.43it/s] 47%|████▋     | 220/470 [02:30<01:12,  3.44it/s] 47%|████▋     | 221/470 [02:30<01:12,  3.44it/s] 47%|████▋     | 222/470 [02:30<01:23,  2.99it/s] 47%|████▋     | 223/470 [02:31<01:19,  3.11it/s] 48%|████▊     | 224/470 [02:31<01:16,  3.21it/s] 48%|████▊     | 225/470 [02:31<01:14,  3.28it/s] 48%|████▊     | 226/470 [02:31<01:13,  3.33it/s] 48%|████▊     | 227/470 [02:32<01:12,  3.37it/s] 49%|████▊     | 228/470 [02:32<01:11,  3.39it/s] 49%|████▊     | 229/470 [02:32<01:10,  3.41it/s] 49%|████▉     | 230/470 [02:33<01:10,  3.42it/s] 49%|████▉     | 231/470 [02:33<01:09,  3.43it/s] 49%|████▉     | 232/470 [02:33<01:12,  3.30it/s] 50%|████▉     | 233/470 [02:34<01:10,  3.35it/s] 50%|████▉     | 234/470 [02:34<01:09,  3.38it/s] 50%|█████     | 235/470 [02:34<01:09,  3.40it/s] 50%|█████     | 236/470 [02:34<01:08,  3.41it/s] 50%|█████     | 237/470 [02:35<01:08,  3.42it/s] 51%|█████     | 238/470 [02:35<01:07,  3.44it/s] 51%|█████     | 239/470 [02:35<01:07,  3.44it/s] 51%|█████     | 240/470 [02:36<01:06,  3.44it/s] 51%|█████▏    | 241/470 [02:36<01:09,  3.29it/s] 51%|█████▏    | 242/470 [02:36<01:08,  3.33it/s] 52%|█████▏    | 243/470 [02:37<01:09,  3.26it/s] 52%|█████▏    | 244/470 [02:37<01:08,  3.32it/s] 52%|█████▏    | 245/470 [02:37<01:07,  3.35it/s] 52%|█████▏    | 246/470 [02:37<01:06,  3.38it/s] 53%|█████▎    | 247/470 [02:38<01:05,  3.40it/s] 53%|█████▎    | 248/470 [02:38<01:04,  3.42it/s] 53%|█████▎    | 249/470 [02:38<01:04,  3.43it/s] 53%|█████▎    | 250/470 [02:39<01:52,  1.95it/s] 53%|█████▎    | 251/470 [02:40<01:40,  2.18it/s] 54%|█████▎    | 252/470 [02:40<01:29,  2.45it/s] 54%|█████▍    | 253/470 [02:40<01:20,  2.68it/s] 54%|█████▍    | 254/470 [02:40<01:15,  2.87it/s] 54%|█████▍    | 255/470 [02:41<01:11,  3.02it/s] 54%|█████▍    | 256/470 [02:41<01:08,  3.14it/s] 55%|█████▍    | 257/470 [02:41<01:05,  3.23it/s] 55%|█████▍    | 258/470 [02:42<01:04,  3.29it/s] 55%|█████▌    | 259/470 [02:42<01:03,  3.34it/s] 55%|█████▌    | 260/470 [02:42<01:05,  3.19it/s] 56%|█████▌    | 261/470 [02:43<01:04,  3.26it/s] 56%|█████▌    | 262/470 [02:43<01:02,  3.32it/s] 56%|█████▌    | 263/470 [02:43<01:01,  3.36it/s] 56%|█████▌    | 264/470 [02:43<01:00,  3.38it/s] 56%|█████▋    | 265/470 [02:44<01:00,  3.40it/s] 57%|█████▋    | 266/470 [02:44<00:59,  3.41it/s] 57%|█████▋    | 267/470 [02:44<00:59,  3.42it/s] 57%|█████▋    | 268/470 [02:45<00:58,  3.43it/s] 57%|█████▋    | 269/470 [02:45<00:58,  3.43it/s] 57%|█████▋    | 270/470 [02:45<00:58,  3.42it/s] 58%|█████▊    | 271/470 [02:46<01:20,  2.48it/s] 58%|█████▊    | 272/470 [02:46<01:13,  2.70it/s] 58%|█████▊    | 273/470 [02:46<01:08,  2.88it/s] 58%|█████▊    | 274/470 [02:47<01:05,  3.01it/s] 59%|█████▊    | 275/470 [02:47<01:02,  3.12it/s] 59%|█████▊    | 276/470 [02:47<01:00,  3.20it/s] 59%|█████▉    | 277/470 [02:48<00:59,  3.26it/s] 59%|█████▉    | 278/470 [02:48<00:58,  3.29it/s] 59%|█████▉    | 279/470 [02:48<00:57,  3.32it/s] 60%|█████▉    | 280/470 [02:49<01:04,  2.96it/s] 60%|█████▉    | 281/470 [02:49<01:01,  3.08it/s] 60%|██████    | 282/470 [02:49<00:55,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 23:15:38,507 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:15:38,507 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:15:38,508 >>   Batch size = 8
{'eval_loss': 1.1293046474456787, 'eval_runtime': 9.8954, 'eval_samples_per_second': 352.486, 'eval_steps_per_second': 44.061, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.18it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.30it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.74it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.72it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.14it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.74it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.61it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.70it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.82it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.64it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.27it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.52it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.53it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.48it/s][A
 22%|██▏       | 97/436 [00:02<00:08, 40.39it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 41.71it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 42.69it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.26it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.64it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.89it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.20it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.18it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.97it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.00it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.32it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.52it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.65it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.72it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.70it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.61it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.46it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.33it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.37it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.60it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.68it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.70it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.70it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.60it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.37it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.09it/s][A
 53%|█████▎    | 232/436 [00:05<00:05, 38.30it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 40.25it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 41.60it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 42.63it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.33it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.96it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.22it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.25it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.93it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.76it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.88it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.19it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.39it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.63it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.72it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.93it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.81it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.42it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.22it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.28it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.42it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.56it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.60it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.70it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.64it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.49it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.27it/s][A
 84%|████████▍ | 367/436 [00:08<00:02, 34.41it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 37.00it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 39.07it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 40.74it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 41.90it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 42.85it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.40it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.79it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.72it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.58it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.73it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.96it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.27it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.47it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.47it/s][A 60%|██████    | 282/470 [02:59<00:55,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:15:49,103 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-28 23:15:49,328 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:15:55,021 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:15:55,385 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:15:55,670 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [03:20<29:28,  9.46s/it] 60%|██████    | 284/470 [03:20<20:51,  6.73s/it] 61%|██████    | 285/470 [03:21<14:47,  4.80s/it] 61%|██████    | 286/470 [03:21<10:33,  3.45s/it] 61%|██████    | 287/470 [03:21<07:37,  2.50s/it] 61%|██████▏   | 288/470 [03:22<05:34,  1.84s/it] 61%|██████▏   | 289/470 [03:22<04:08,  1.37s/it] 62%|██████▏   | 290/470 [03:22<03:09,  1.05s/it] 62%|██████▏   | 291/470 [03:22<02:27,  1.21it/s] 62%|██████▏   | 292/470 [03:23<01:58,  1.51it/s] 62%|██████▏   | 293/470 [03:23<01:37,  1.81it/s] 63%|██████▎   | 294/470 [03:23<01:25,  2.06it/s] 63%|██████▎   | 295/470 [03:24<01:14,  2.34it/s] 63%|██████▎   | 296/470 [03:24<01:07,  2.58it/s] 63%|██████▎   | 297/470 [03:24<01:02,  2.78it/s] 63%|██████▎   | 298/470 [03:24<00:58,  2.94it/s] 64%|██████▎   | 299/470 [03:25<00:55,  3.06it/s] 64%|██████▍   | 300/470 [03:25<00:53,  3.16it/s] 64%|██████▍   | 301/470 [03:25<00:52,  3.23it/s] 64%|██████▍   | 302/470 [03:26<00:51,  3.28it/s] 64%|██████▍   | 303/470 [03:26<00:50,  3.31it/s] 65%|██████▍   | 304/470 [03:26<00:49,  3.34it/s] 65%|██████▍   | 305/470 [03:27<00:50,  3.26it/s] 65%|██████▌   | 306/470 [03:27<00:49,  3.30it/s] 65%|██████▌   | 307/470 [03:27<00:48,  3.33it/s] 66%|██████▌   | 308/470 [03:27<00:48,  3.35it/s] 66%|██████▌   | 309/470 [03:28<00:47,  3.36it/s] 66%|██████▌   | 310/470 [03:28<00:47,  3.37it/s] 66%|██████▌   | 311/470 [03:28<00:47,  3.38it/s] 66%|██████▋   | 312/470 [03:29<00:46,  3.39it/s] 67%|██████▋   | 313/470 [03:29<00:46,  3.39it/s] 67%|██████▋   | 314/470 [03:29<00:45,  3.39it/s] 67%|██████▋   | 315/470 [03:30<00:45,  3.39it/s] 67%|██████▋   | 316/470 [03:30<00:46,  3.30it/s] 67%|██████▋   | 317/470 [03:30<00:46,  3.32it/s] 68%|██████▊   | 318/470 [03:30<00:45,  3.34it/s] 68%|██████▊   | 319/470 [03:31<00:44,  3.36it/s] 68%|██████▊   | 320/470 [03:31<00:44,  3.37it/s] 68%|██████▊   | 321/470 [03:31<00:44,  3.38it/s] 69%|██████▊   | 322/470 [03:32<00:43,  3.39it/s] 69%|██████▊   | 323/470 [03:32<00:43,  3.39it/s] 69%|██████▉   | 324/470 [03:32<00:43,  3.39it/s] 69%|██████▉   | 325/470 [03:32<00:42,  3.39it/s] 69%|██████▉   | 326/470 [03:33<00:42,  3.40it/s] 70%|██████▉   | 327/470 [03:33<00:46,  3.05it/s] 70%|██████▉   | 328/470 [03:33<00:45,  3.15it/s] 70%|███████   | 329/470 [03:34<00:43,  3.21it/s] 70%|███████   | 330/470 [03:34<00:42,  3.26it/s] 70%|███████   | 331/470 [03:34<00:42,  3.30it/s] 71%|███████   | 332/470 [03:35<00:41,  3.33it/s] 71%|███████   | 333/470 [03:35<00:40,  3.35it/s] 71%|███████   | 334/470 [03:35<00:40,  3.36it/s] 71%|███████▏  | 335/470 [03:36<00:40,  3.37it/s] 71%|███████▏  | 336/470 [03:36<00:39,  3.38it/s] 72%|███████▏  | 337/470 [03:36<00:41,  3.18it/s] 72%|███████▏  | 338/470 [03:36<00:40,  3.25it/s] 72%|███████▏  | 339/470 [03:37<00:39,  3.29it/s] 72%|███████▏  | 340/470 [03:37<00:39,  3.33it/s] 73%|███████▎  | 341/470 [03:37<00:38,  3.37it/s] 73%|███████▎  | 342/470 [03:38<00:37,  3.38it/s] 73%|███████▎  | 343/470 [03:38<00:37,  3.40it/s] 73%|███████▎  | 344/470 [03:38<00:36,  3.42it/s] 73%|███████▎  | 345/470 [03:39<00:36,  3.43it/s] 74%|███████▎  | 346/470 [03:39<00:50,  2.46it/s] 74%|███████▍  | 347/470 [03:40<00:51,  2.41it/s] 74%|███████▍  | 348/470 [03:40<00:46,  2.65it/s] 74%|███████▍  | 349/470 [03:40<00:42,  2.84it/s] 74%|███████▍  | 350/470 [03:41<00:39,  3.00it/s] 75%|███████▍  | 351/470 [03:41<00:38,  3.12it/s] 75%|███████▍  | 352/470 [03:41<00:36,  3.21it/s] 75%|███████▌  | 353/470 [03:41<00:35,  3.28it/s] 75%|███████▌  | 354/470 [03:42<00:34,  3.33it/s] 76%|███████▌  | 355/470 [03:42<00:34,  3.36it/s] 76%|███████▌  | 356/470 [03:42<00:33,  3.38it/s] 76%|███████▌  | 357/470 [03:43<00:34,  3.28it/s] 76%|███████▌  | 358/470 [03:43<00:33,  3.33it/s] 76%|███████▋  | 359/470 [03:43<00:33,  3.36it/s] 77%|███████▋  | 360/470 [03:43<00:32,  3.39it/s] 77%|███████▋  | 361/470 [03:44<00:32,  3.40it/s] 77%|███████▋  | 362/470 [03:44<00:31,  3.41it/s] 77%|███████▋  | 363/470 [03:44<00:31,  3.42it/s] 77%|███████▋  | 364/470 [03:45<00:30,  3.43it/s] 78%|███████▊  | 365/470 [03:45<00:30,  3.44it/s] 78%|███████▊  | 366/470 [03:45<00:30,  3.43it/s] 78%|███████▊  | 367/470 [03:45<00:29,  3.44it/s] 78%|███████▊  | 368/470 [03:46<00:29,  3.44it/s] 79%|███████▊  | 369/470 [03:46<00:29,  3.44it/s] 79%|███████▊  | 370/470 [03:46<00:29,  3.44it/s] 79%|███████▉  | 371/470 [03:47<00:29,  3.33it/s] 79%|███████▉  | 372/470 [03:47<00:29,  3.36it/s] 79%|███████▉  | 373/470 [03:47<00:28,  3.39it/s] 80%|███████▉  | 374/470 [03:48<00:28,  3.40it/s] 80%|███████▉  | 375/470 [03:48<00:27,  3.42it/s] 80%|████████  | 376/470 [03:48<00:25,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 23:16:37,418 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:16:37,418 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:16:37,418 >>   Batch size = 8
{'eval_loss': 1.1486834287643433, 'eval_runtime': 9.994, 'eval_samples_per_second': 349.008, 'eval_steps_per_second': 43.626, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.94it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.16it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.43it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.50it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.21it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.83it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.68it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.58it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.55it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.69it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.70it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.56it/s][A
 15%|█▌        | 67/436 [00:01<00:09, 40.84it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 42.00it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 42.79it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.39it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.73it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.92it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.19it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.22it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.01it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.04it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.08it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.29it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.39it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.55it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.52it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.55it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.52it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.28it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.09it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.18it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.42it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.46it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.51it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.48it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.48it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.48it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.28it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 39.02it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 40.68it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 41.81it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 42.64it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.37it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.81it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.15it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.25it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.87it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.78it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.90it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.09it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.32it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.53it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.67it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.64it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.48it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.22it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.11it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.12it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.26it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.43it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.58it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.66it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.70it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.40it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.25it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 36.68it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 38.89it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 40.46it/s][A
 81%|████████  | 352/436 [00:08<00:02, 41.76it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 42.64it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.29it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.84it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.90it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.74it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.58it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.72it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.93it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.16it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.25it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.43it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.62it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.48it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.16it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.00it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.98it/s][A 80%|████████  | 376/470 [03:58<00:25,  3.66it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:16:47,695 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-28 23:16:48,219 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:16:53,959 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:16:54,712 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:16:55,017 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [04:19<14:43,  9.50s/it] 80%|████████  | 378/470 [04:19<10:20,  6.75s/it] 81%|████████  | 379/470 [04:20<07:17,  4.81s/it] 81%|████████  | 380/470 [04:20<05:10,  3.46s/it] 81%|████████  | 381/470 [04:20<03:43,  2.51s/it] 81%|████████▏ | 382/470 [04:21<02:42,  1.84s/it] 81%|████████▏ | 383/470 [04:21<01:59,  1.38s/it] 82%|████████▏ | 384/470 [04:21<01:30,  1.05s/it] 82%|████████▏ | 385/470 [04:21<01:10,  1.21it/s] 82%|████████▏ | 386/470 [04:22<00:55,  1.50it/s] 82%|████████▏ | 387/470 [04:22<00:46,  1.80it/s] 83%|████████▎ | 388/470 [04:22<00:39,  2.10it/s] 83%|████████▎ | 389/470 [04:23<00:35,  2.26it/s] 83%|████████▎ | 390/470 [04:23<00:31,  2.52it/s] 83%|████████▎ | 391/470 [04:23<00:28,  2.73it/s] 83%|████████▎ | 392/470 [04:24<00:26,  2.90it/s] 84%|████████▎ | 393/470 [04:24<00:25,  3.04it/s] 84%|████████▍ | 394/470 [04:24<00:24,  3.14it/s] 84%|████████▍ | 395/470 [04:24<00:23,  3.21it/s] 84%|████████▍ | 396/470 [04:25<00:22,  3.27it/s] 84%|████████▍ | 397/470 [04:25<00:22,  3.31it/s] 85%|████████▍ | 398/470 [04:25<00:21,  3.33it/s] 85%|████████▍ | 399/470 [04:26<00:23,  3.03it/s] 85%|████████▌ | 400/470 [04:26<00:22,  3.14it/s] 85%|████████▌ | 401/470 [04:26<00:21,  3.21it/s] 86%|████████▌ | 402/470 [04:27<00:20,  3.27it/s] 86%|████████▌ | 403/470 [04:27<00:20,  3.31it/s] 86%|████████▌ | 404/470 [04:27<00:19,  3.34it/s] 86%|████████▌ | 405/470 [04:28<00:19,  3.35it/s] 86%|████████▋ | 406/470 [04:28<00:19,  3.37it/s] 87%|████████▋ | 407/470 [04:28<00:18,  3.38it/s] 87%|████████▋ | 408/470 [04:28<00:18,  3.38it/s] 87%|████████▋ | 409/470 [04:29<00:19,  3.17it/s] 87%|████████▋ | 410/470 [04:29<00:18,  3.24it/s] 87%|████████▋ | 411/470 [04:29<00:17,  3.29it/s] 88%|████████▊ | 412/470 [04:30<00:17,  3.32it/s] 88%|████████▊ | 413/470 [04:30<00:17,  3.34it/s] 88%|████████▊ | 414/470 [04:30<00:16,  3.36it/s] 88%|████████▊ | 415/470 [04:31<00:16,  3.37it/s] 89%|████████▊ | 416/470 [04:31<00:15,  3.38it/s] 89%|████████▊ | 417/470 [04:31<00:15,  3.39it/s] 89%|████████▉ | 418/470 [04:31<00:15,  3.39it/s] 89%|████████▉ | 419/470 [04:32<00:15,  3.32it/s] 89%|████████▉ | 420/470 [04:32<00:14,  3.34it/s] 90%|████████▉ | 421/470 [04:32<00:14,  3.36it/s] 90%|████████▉ | 422/470 [04:33<00:14,  3.37it/s] 90%|█████████ | 423/470 [04:33<00:13,  3.38it/s] 90%|█████████ | 424/470 [04:33<00:13,  3.39it/s] 90%|█████████ | 425/470 [04:33<00:13,  3.39it/s] 91%|█████████ | 426/470 [04:34<00:12,  3.40it/s] 91%|█████████ | 427/470 [04:34<00:12,  3.40it/s] 91%|█████████ | 428/470 [04:34<00:12,  3.40it/s] 91%|█████████▏| 429/470 [04:35<00:12,  3.40it/s] 91%|█████████▏| 430/470 [04:35<00:12,  3.33it/s] 92%|█████████▏| 431/470 [04:35<00:11,  3.35it/s] 92%|█████████▏| 432/470 [04:36<00:11,  3.37it/s] 92%|█████████▏| 433/470 [04:36<00:10,  3.38it/s] 92%|█████████▏| 434/470 [04:36<00:10,  3.34it/s] 93%|█████████▎| 435/470 [04:36<00:10,  3.35it/s] 93%|█████████▎| 436/470 [04:37<00:10,  3.37it/s] 93%|█████████▎| 437/470 [04:37<00:09,  3.37it/s] 93%|█████████▎| 438/470 [04:37<00:09,  3.38it/s] 93%|█████████▎| 439/470 [04:38<00:09,  3.39it/s] 94%|█████████▎| 440/470 [04:38<00:08,  3.39it/s] 94%|█████████▍| 441/470 [04:38<00:09,  3.09it/s] 94%|█████████▍| 442/470 [04:39<00:08,  3.18it/s] 94%|█████████▍| 443/470 [04:39<00:12,  2.13it/s] 94%|█████████▍| 444/470 [04:40<00:11,  2.32it/s] 95%|█████████▍| 445/470 [04:40<00:09,  2.57it/s] 95%|█████████▍| 446/470 [04:40<00:08,  2.77it/s] 95%|█████████▌| 447/470 [04:41<00:07,  2.93it/s] 95%|█████████▌| 448/470 [04:41<00:07,  3.06it/s] 96%|█████████▌| 449/470 [04:41<00:06,  3.04it/s] 96%|█████████▌| 450/470 [04:42<00:06,  3.14it/s] 96%|█████████▌| 451/470 [04:42<00:05,  3.21it/s] 96%|█████████▌| 452/470 [04:42<00:05,  3.27it/s] 96%|█████████▋| 453/470 [04:42<00:05,  3.30it/s] 97%|█████████▋| 454/470 [04:43<00:04,  3.33it/s] 97%|█████████▋| 455/470 [04:43<00:04,  3.35it/s] 97%|█████████▋| 456/470 [04:43<00:04,  3.37it/s] 97%|█████████▋| 457/470 [04:44<00:03,  3.38it/s] 97%|█████████▋| 458/470 [04:44<00:03,  3.38it/s] 98%|█████████▊| 459/470 [04:44<00:03,  3.39it/s] 98%|█████████▊| 460/470 [04:45<00:03,  3.20it/s] 98%|█████████▊| 461/470 [04:45<00:02,  3.25it/s] 98%|█████████▊| 462/470 [04:45<00:02,  3.08it/s] 99%|█████████▊| 463/470 [04:46<00:02,  3.17it/s] 99%|█████████▊| 464/470 [04:46<00:01,  3.24it/s] 99%|█████████▉| 465/470 [04:46<00:01,  3.28it/s] 99%|█████████▉| 466/470 [04:46<00:01,  3.32it/s] 99%|█████████▉| 467/470 [04:47<00:00,  3.34it/s]100%|█████████▉| 468/470 [04:47<00:00,  3.36it/s]100%|█████████▉| 469/470 [04:47<00:00,  3.37it/s]100%|██████████| 470/470 [04:48<00:00,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 23:17:36,899 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:17:36,900 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:17:36,900 >>   Batch size = 8
{'eval_loss': 1.1582170724868774, 'eval_runtime': 9.9757, 'eval_samples_per_second': 349.648, 'eval_steps_per_second': 43.706, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.58it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.39it/s][A
  4%|▍         | 17/436 [00:00<00:09, 44.70it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.82it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.77it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.63it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.50it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.58it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.73it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.60it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.50it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.50it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.52it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.57it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.45it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.63it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.61it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.60it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.56it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.52it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.44it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.50it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.49it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.57it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.40it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.47it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.61it/s][A
 35%|███▍      | 152/436 [00:03<00:07, 38.22it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 40.15it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 41.49it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 42.60it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.32it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.85it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.94it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.70it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.93it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.32it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.53it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.75it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.84it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.81it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.66it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.15it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.07it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.29it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.35it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.58it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.72it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.87it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.91it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.51it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 40.93it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 42.23it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.11it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.64it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.11it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.38it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.29it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.24it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.90it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.97it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.44it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.72it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.82it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.83it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.65it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.38it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.21it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.11it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.22it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.41it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.64it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.80it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.77it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.67it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.42it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 35.54it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 37.81it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 39.77it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 39.77it/s][A100%|██████████| 470/470 [04:58<00:00,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:17:47,188 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-28 23:17:48,070 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:17:54,660 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:17:55,655 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:17:56,073 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:18:09,158 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:18:09,220 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94 (score: 1.1205992698669434).
                                                 100%|██████████| 470/470 [05:37<00:00,  3.61it/s]100%|██████████| 470/470 [05:37<00:00,  1.39it/s]
[INFO|trainer.py:1894] 2023-08-28 23:18:26,067 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 23:18:26,331 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:18:31,268 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:18:31,520 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:18:31,792 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:18:33,310 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,310 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,311 >>   train_loss               =     0.3462
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,311 >>   train_runtime            = 0:05:37.13
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,311 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,311 >>   train_samples_per_second =     88.986
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:33,311 >>   train_steps_per_second   =      1.394
{'eval_loss': 1.1667579412460327, 'eval_runtime': 9.9802, 'eval_samples_per_second': 349.493, 'eval_steps_per_second': 43.687, 'epoch': 5.0}
{'train_runtime': 337.1314, 'train_samples_per_second': 88.986, 'train_steps_per_second': 1.394, 'train_loss': 0.3461729333755818, 'epoch': 5.0}
08/28/2023 23:18:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:18:33,914 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:18:33,914 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:18:33,914 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.42it/s]  3%|▎         | 12/436 [00:00<00:08, 49.23it/s]  4%|▍         | 17/436 [00:00<00:08, 47.52it/s]  5%|▌         | 22/436 [00:00<00:09, 42.87it/s]  6%|▌         | 27/436 [00:00<00:09, 43.72it/s]  7%|▋         | 32/436 [00:00<00:09, 44.24it/s]  8%|▊         | 37/436 [00:00<00:08, 44.55it/s] 10%|▉         | 42/436 [00:00<00:08, 44.68it/s] 11%|█         | 47/436 [00:01<00:08, 44.55it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.87it/s] 13%|█▎        | 57/436 [00:01<00:08, 45.03it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.83it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.57it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.81it/s] 18%|█▊        | 77/436 [00:01<00:07, 44.99it/s] 19%|█▉        | 82/436 [00:01<00:08, 44.08it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.32it/s] 21%|██        | 92/436 [00:02<00:07, 44.43it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.58it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.81it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.73it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.76it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.74it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.72it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.76it/s] 30%|███       | 132/436 [00:02<00:06, 44.83it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.90it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.78it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.92it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.87it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.80it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.84it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.72it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.91it/s] 41%|████      | 177/436 [00:03<00:05, 44.97it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.76it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.84it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.83it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.86it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.75it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.70it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.65it/s] 50%|████▉     | 217/436 [00:04<00:05, 39.30it/s] 51%|█████     | 222/436 [00:05<00:05, 40.85it/s] 52%|█████▏    | 227/436 [00:05<00:04, 42.20it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.13it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.78it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.29it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.45it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.54it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.06it/s] 60%|██████    | 262/436 [00:05<00:03, 44.04it/s] 61%|██████    | 267/436 [00:06<00:03, 44.21it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.48it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.73it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.93it/s] 66%|██████▌   | 287/436 [00:06<00:03, 45.08it/s] 67%|██████▋   | 292/436 [00:06<00:03, 45.12it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.86it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.52it/s] 70%|███████   | 307/436 [00:06<00:02, 44.36it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.51it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.61it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.86it/s] 75%|███████▌  | 327/436 [00:07<00:02, 45.05it/s] 76%|███████▌  | 332/436 [00:07<00:02, 45.18it/s] 77%|███████▋  | 337/436 [00:07<00:02, 45.02it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.83it/s] 80%|███████▉  | 347/436 [00:07<00:01, 44.62it/s] 81%|████████  | 352/436 [00:07<00:02, 35.56it/s] 82%|████████▏ | 357/436 [00:08<00:02, 38.00it/s] 83%|████████▎ | 362/436 [00:08<00:01, 39.86it/s] 84%|████████▍ | 367/436 [00:08<00:01, 41.34it/s] 85%|████████▌ | 372/436 [00:08<00:01, 42.31it/s] 86%|████████▋ | 377/436 [00:08<00:01, 43.17it/s] 88%|████████▊ | 382/436 [00:08<00:01, 43.78it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.19it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.02it/s] 91%|█████████ | 397/436 [00:09<00:00, 44.02it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.27it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.52it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.64it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.80it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.89it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.94it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.79it/s]100%|██████████| 436/436 [00:09<00:00, 44.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:18:43,817 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   eval_loss               =     1.1206
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   eval_runtime            = 0:00:09.90
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   eval_samples_per_second =    352.229
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   eval_steps_per_second   =     44.029
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:18:43,817 >>   perplexity              =     3.0667
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:02,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:02,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:02,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:02,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:02,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:19:03,587 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:19:03,588 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:19:03,938 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:19:05,124 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:19:05,124 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:08,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:08,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:08,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:08,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:19:08,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:19:09,775 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:19:09,804 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:19:10,150 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:19:10,396 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:19:10,396 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-94
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-470
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:05,  1.66it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.47it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:19,  1.48it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.47it/s]Extractor Predicting: 45it [00:29,  1.47it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:36,  1.56it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.39it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:45,  1.51it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.51it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.34it/s]Extractor Predicting: 75it [00:49,  1.39it/s]Extractor Predicting: 76it [00:50,  1.44it/s]Extractor Predicting: 77it [00:50,  1.49it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.44it/s]Extractor Predicting: 80it [00:52,  1.46it/s]Extractor Predicting: 81it [00:53,  1.47it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:56,  1.41it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.53it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.56it/s]Extractor Predicting: 91it [00:59,  1.61it/s]Extractor Predicting: 92it [01:00,  1.62it/s]Extractor Predicting: 93it [01:01,  1.60it/s]Extractor Predicting: 94it [01:01,  1.63it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:03,  1.58it/s]Extractor Predicting: 98it [01:04,  1.57it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:06,  1.58it/s]Extractor Predicting: 102it [01:06,  1.64it/s]Extractor Predicting: 103it [01:07,  1.62it/s]Extractor Predicting: 104it [01:08,  1.64it/s]Extractor Predicting: 105it [01:08,  1.57it/s]Extractor Predicting: 106it [01:09,  1.60it/s]Extractor Predicting: 107it [01:09,  1.59it/s]Extractor Predicting: 108it [01:10,  1.59it/s]Extractor Predicting: 109it [01:11,  1.60it/s]Extractor Predicting: 110it [01:11,  1.57it/s]Extractor Predicting: 111it [01:12,  1.62it/s]Extractor Predicting: 112it [01:13,  1.48it/s]Extractor Predicting: 113it [01:13,  1.57it/s]Extractor Predicting: 114it [01:14,  1.58it/s]Extractor Predicting: 115it [01:15,  1.56it/s]Extractor Predicting: 116it [01:15,  1.58it/s]Extractor Predicting: 117it [01:16,  1.57it/s]Extractor Predicting: 118it [01:16,  1.56it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.50it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:19,  1.56it/s]Extractor Predicting: 123it [01:20,  1.55it/s]Extractor Predicting: 124it [01:20,  1.53it/s]Extractor Predicting: 125it [01:21,  1.45it/s]Extractor Predicting: 126it [01:22,  1.45it/s]Extractor Predicting: 127it [01:23,  1.48it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:24,  1.55it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:26,  1.48it/s]Extractor Predicting: 133it [01:26,  1.49it/s]Extractor Predicting: 134it [01:27,  1.48it/s]Extractor Predicting: 135it [01:28,  1.50it/s]Extractor Predicting: 136it [01:28,  1.53it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.54it/s]Extractor Predicting: 141it [01:32,  1.53it/s]Extractor Predicting: 142it [01:32,  1.46it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 144it [01:33,  1.83it/s]Extractor Predicting: 144it [01:33,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:09,460 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:09,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:09,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:09,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:09,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:21:10,525 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:21:10,526 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:21:11,071 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:21:12,468 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:21:12,555 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:14,886 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:14,889 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:14,889 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:14,889 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:14,889 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:21:16,401 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:21:16,461 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:21:16,878 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:21:17,220 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:21:17,220 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2858926342072409,
  "recall": 0.1313073394495413,
  "score": 0.17996070726915522,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:15,  1.60it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.65it/s]Extractor Predicting: 34it [00:22,  1.59it/s]Extractor Predicting: 35it [00:22,  1.65it/s]Extractor Predicting: 36it [00:23,  1.67it/s]Extractor Predicting: 37it [00:23,  1.69it/s]Extractor Predicting: 38it [00:24,  1.73it/s]Extractor Predicting: 39it [00:24,  1.77it/s]Extractor Predicting: 40it [00:25,  1.75it/s]Extractor Predicting: 41it [00:26,  1.75it/s]Extractor Predicting: 42it [00:26,  1.78it/s]Extractor Predicting: 43it [00:27,  1.75it/s]Extractor Predicting: 44it [00:27,  1.80it/s]Extractor Predicting: 45it [00:28,  1.79it/s]Extractor Predicting: 46it [00:29,  1.68it/s]Extractor Predicting: 47it [00:29,  1.73it/s]Extractor Predicting: 48it [00:30,  1.78it/s]Extractor Predicting: 49it [00:30,  1.81it/s]Extractor Predicting: 50it [00:31,  1.80it/s]Extractor Predicting: 51it [00:31,  1.83it/s]Extractor Predicting: 52it [00:32,  1.67it/s]Extractor Predicting: 53it [00:32,  1.70it/s]Extractor Predicting: 54it [00:33,  1.71it/s]Extractor Predicting: 55it [00:34,  1.70it/s]Extractor Predicting: 56it [00:34,  1.74it/s]Extractor Predicting: 57it [00:35,  1.65it/s]Extractor Predicting: 58it [00:36,  1.64it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:38,  1.42it/s]Extractor Predicting: 63it [00:39,  1.46it/s]Extractor Predicting: 64it [00:40,  1.45it/s]Extractor Predicting: 65it [00:40,  1.50it/s]Extractor Predicting: 66it [00:41,  1.51it/s]Extractor Predicting: 67it [00:42,  1.47it/s]Extractor Predicting: 68it [00:42,  1.46it/s]Extractor Predicting: 69it [00:43,  1.38it/s]Extractor Predicting: 70it [00:44,  1.39it/s]Extractor Predicting: 71it [00:45,  1.39it/s]Extractor Predicting: 72it [00:45,  1.44it/s]Extractor Predicting: 73it [00:46,  1.43it/s]Extractor Predicting: 74it [00:47,  1.46it/s]Extractor Predicting: 75it [00:47,  1.49it/s]Extractor Predicting: 76it [00:48,  1.50it/s]Extractor Predicting: 77it [00:49,  1.49it/s]Extractor Predicting: 78it [00:49,  1.41it/s]Extractor Predicting: 79it [00:50,  1.41it/s]Extractor Predicting: 80it [00:51,  1.44it/s]Extractor Predicting: 81it [00:51,  1.45it/s]Extractor Predicting: 82it [00:52,  1.45it/s]Extractor Predicting: 83it [00:53,  1.39it/s]Extractor Predicting: 84it [00:54,  1.31it/s]Extractor Predicting: 85it [00:54,  1.35it/s]Extractor Predicting: 86it [00:55,  1.36it/s]Extractor Predicting: 87it [00:56,  1.37it/s]Extractor Predicting: 88it [00:57,  1.41it/s]Extractor Predicting: 89it [00:57,  1.43it/s]Extractor Predicting: 90it [00:58,  1.48it/s]Extractor Predicting: 91it [00:59,  1.46it/s]Extractor Predicting: 92it [00:59,  1.40it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.47it/s]Extractor Predicting: 95it [01:01,  1.51it/s]Extractor Predicting: 96it [01:02,  1.55it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.57it/s]Extractor Predicting: 99it [01:04,  1.56it/s]Extractor Predicting: 100it [01:04,  1.57it/s]Extractor Predicting: 101it [01:05,  1.62it/s]Extractor Predicting: 102it [01:06,  1.61it/s]Extractor Predicting: 103it [01:06,  1.57it/s]Extractor Predicting: 104it [01:07,  1.52it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.53it/s]Extractor Predicting: 109it [01:10,  1.52it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:12,  1.52it/s]Extractor Predicting: 112it [01:12,  1.49it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:14,  1.49it/s]Extractor Predicting: 116it [01:15,  1.56it/s]Extractor Predicting: 117it [01:16,  1.56it/s]Extractor Predicting: 118it [01:16,  1.62it/s]Extractor Predicting: 119it [01:17,  1.56it/s]Extractor Predicting: 120it [01:17,  1.59it/s]Extractor Predicting: 121it [01:18,  1.64it/s]Extractor Predicting: 122it [01:19,  1.64it/s]Extractor Predicting: 123it [01:19,  1.67it/s]Extractor Predicting: 124it [01:20,  1.67it/s]Extractor Predicting: 125it [01:20,  1.68it/s]Extractor Predicting: 126it [01:21,  1.67it/s]Extractor Predicting: 127it [01:22,  1.67it/s]Extractor Predicting: 128it [01:22,  1.72it/s]Extractor Predicting: 129it [01:23,  1.70it/s]Extractor Predicting: 130it [01:23,  1.73it/s]Extractor Predicting: 131it [01:24,  1.78it/s]Extractor Predicting: 132it [01:24,  1.74it/s]Extractor Predicting: 133it [01:25,  1.70it/s]Extractor Predicting: 134it [01:26,  1.69it/s]Extractor Predicting: 135it [01:26,  1.72it/s]Extractor Predicting: 136it [01:27,  1.71it/s]Extractor Predicting: 137it [01:27,  1.72it/s]Extractor Predicting: 138it [01:28,  1.74it/s]Extractor Predicting: 139it [01:28,  1.70it/s]Extractor Predicting: 140it [01:29,  1.73it/s]Extractor Predicting: 141it [01:30,  1.71it/s]Extractor Predicting: 142it [01:30,  1.71it/s]Extractor Predicting: 143it [01:31,  1.71it/s]Extractor Predicting: 144it [01:31,  1.71it/s]Extractor Predicting: 145it [01:32,  1.70it/s]Extractor Predicting: 146it [01:33,  1.66it/s]Extractor Predicting: 147it [01:33,  1.64it/s]Extractor Predicting: 148it [01:34,  1.59it/s]Extractor Predicting: 149it [01:35,  1.58it/s]Extractor Predicting: 150it [01:35,  1.58it/s]Extractor Predicting: 151it [01:36,  1.59it/s]Extractor Predicting: 152it [01:36,  1.59it/s]Extractor Predicting: 153it [01:37,  1.50it/s]Extractor Predicting: 154it [01:38,  1.55it/s]Extractor Predicting: 155it [01:38,  1.59it/s]Extractor Predicting: 156it [01:39,  1.56it/s]Extractor Predicting: 157it [01:40,  1.58it/s]Extractor Predicting: 158it [01:40,  1.55it/s]Extractor Predicting: 159it [01:41,  1.55it/s]Extractor Predicting: 160it [01:42,  1.55it/s]Extractor Predicting: 161it [01:42,  1.55it/s]Extractor Predicting: 162it [01:43,  1.55it/s]Extractor Predicting: 163it [01:44,  1.53it/s]Extractor Predicting: 164it [01:44,  1.55it/s]Extractor Predicting: 165it [01:45,  1.54it/s]Extractor Predicting: 166it [01:46,  1.52it/s]Extractor Predicting: 167it [01:46,  1.55it/s]Extractor Predicting: 168it [01:47,  1.55it/s]Extractor Predicting: 169it [01:48,  1.49it/s]Extractor Predicting: 170it [01:48,  1.50it/s]Extractor Predicting: 171it [01:49,  1.52it/s]Extractor Predicting: 172it [01:49,  1.55it/s]Extractor Predicting: 173it [01:50,  1.53it/s]Extractor Predicting: 174it [01:51,  1.54it/s]Extractor Predicting: 175it [01:51,  1.56it/s]Extractor Predicting: 176it [01:52,  1.56it/s]Extractor Predicting: 177it [01:53,  1.54it/s]Extractor Predicting: 178it [01:53,  1.55it/s]Extractor Predicting: 179it [01:54,  1.55it/s]Extractor Predicting: 180it [01:55,  1.54it/s]Extractor Predicting: 181it [01:55,  1.54it/s]Extractor Predicting: 182it [01:56,  1.54it/s]Extractor Predicting: 183it [01:57,  1.38it/s]Extractor Predicting: 184it [01:57,  1.42it/s]Extractor Predicting: 185it [01:58,  1.46it/s]Extractor Predicting: 186it [01:59,  1.46it/s]Extractor Predicting: 187it [01:59,  1.50it/s]Extractor Predicting: 188it [02:00,  1.51it/s]Extractor Predicting: 189it [02:01,  1.54it/s]Extractor Predicting: 190it [02:01,  1.53it/s]Extractor Predicting: 191it [02:02,  1.56it/s]Extractor Predicting: 192it [02:03,  1.56it/s]Extractor Predicting: 193it [02:03,  1.60it/s]Extractor Predicting: 194it [02:04,  1.56it/s]Extractor Predicting: 195it [02:05,  1.56it/s]Extractor Predicting: 196it [02:05,  1.56it/s]Extractor Predicting: 197it [02:06,  1.56it/s]Extractor Predicting: 198it [02:07,  1.52it/s]Extractor Predicting: 199it [02:07,  1.52it/s]Extractor Predicting: 200it [02:08,  1.53it/s]Extractor Predicting: 201it [02:08,  1.57it/s]Extractor Predicting: 202it [02:09,  1.54it/s]Extractor Predicting: 203it [02:10,  1.57it/s]Extractor Predicting: 204it [02:10,  1.55it/s]Extractor Predicting: 205it [02:11,  1.54it/s]Extractor Predicting: 206it [02:12,  1.60it/s]Extractor Predicting: 207it [02:12,  1.59it/s]Extractor Predicting: 208it [02:13,  1.61it/s]Extractor Predicting: 209it [02:13,  1.58it/s]Extractor Predicting: 210it [02:14,  1.57it/s]Extractor Predicting: 211it [02:15,  1.56it/s]Extractor Predicting: 212it [02:15,  1.51it/s]Extractor Predicting: 213it [02:16,  1.52it/s]Extractor Predicting: 214it [02:17,  1.52it/s]Extractor Predicting: 215it [02:17,  1.51it/s]Extractor Predicting: 216it [02:18,  1.50it/s]Extractor Predicting: 217it [02:19,  1.46it/s]Extractor Predicting: 218it [02:20,  1.49it/s]Extractor Predicting: 219it [02:20,  1.50it/s]Extractor Predicting: 220it [02:21,  1.54it/s]Extractor Predicting: 221it [02:21,  1.50it/s]Extractor Predicting: 222it [02:22,  1.50it/s]Extractor Predicting: 223it [02:23,  1.53it/s]Extractor Predicting: 224it [02:23,  1.52it/s]Extractor Predicting: 225it [02:24,  1.52it/s]Extractor Predicting: 226it [02:25,  1.49it/s]Extractor Predicting: 227it [02:25,  1.49it/s]Extractor Predicting: 228it [02:26,  1.55it/s]Extractor Predicting: 229it [02:27,  1.51it/s]Extractor Predicting: 230it [02:27,  1.53it/s]Extractor Predicting: 231it [02:28,  1.34it/s]Extractor Predicting: 232it [02:29,  1.39it/s]Extractor Predicting: 233it [02:30,  1.43it/s]Extractor Predicting: 234it [02:30,  1.47it/s]Extractor Predicting: 235it [02:31,  1.50it/s]Extractor Predicting: 236it [02:32,  1.44it/s]Extractor Predicting: 237it [02:32,  1.42it/s]Extractor Predicting: 238it [02:33,  1.43it/s]Extractor Predicting: 239it [02:34,  1.44it/s]Extractor Predicting: 240it [02:34,  1.47it/s]Extractor Predicting: 241it [02:35,  1.41it/s]Extractor Predicting: 242it [02:36,  1.46it/s]Extractor Predicting: 243it [02:36,  1.48it/s]Extractor Predicting: 244it [02:37,  1.49it/s]Extractor Predicting: 245it [02:38,  1.50it/s]Extractor Predicting: 246it [02:39,  1.40it/s]Extractor Predicting: 247it [02:39,  1.39it/s]Extractor Predicting: 248it [02:40,  1.43it/s]Extractor Predicting: 249it [02:41,  1.50it/s]Extractor Predicting: 250it [02:41,  1.53it/s]Extractor Predicting: 251it [02:42,  1.45it/s]Extractor Predicting: 252it [02:43,  1.50it/s]Extractor Predicting: 253it [02:43,  1.52it/s]Extractor Predicting: 254it [02:44,  1.51it/s]Extractor Predicting: 255it [02:45,  1.51it/s]Extractor Predicting: 256it [02:45,  1.43it/s]Extractor Predicting: 257it [02:46,  1.47it/s]Extractor Predicting: 258it [02:47,  1.48it/s]Extractor Predicting: 259it [02:47,  1.52it/s]Extractor Predicting: 260it [02:48,  1.55it/s]Extractor Predicting: 261it [02:49,  1.58it/s]Extractor Predicting: 262it [02:49,  1.48it/s]Extractor Predicting: 263it [02:50,  1.50it/s]Extractor Predicting: 264it [02:51,  1.54it/s]Extractor Predicting: 265it [02:51,  1.56it/s]Extractor Predicting: 266it [02:52,  1.56it/s]Extractor Predicting: 267it [02:52,  1.56it/s]Extractor Predicting: 268it [02:53,  1.57it/s]Extractor Predicting: 269it [02:54,  1.57it/s]Extractor Predicting: 270it [02:54,  1.55it/s]Extractor Predicting: 271it [02:55,  1.59it/s]Extractor Predicting: 272it [02:56,  1.53it/s]Extractor Predicting: 273it [02:56,  1.57it/s]Extractor Predicting: 274it [02:57,  1.59it/s]Extractor Predicting: 275it [02:57,  1.61it/s]Extractor Predicting: 276it [02:58,  1.59it/s]Extractor Predicting: 277it [02:59,  1.54it/s]Extractor Predicting: 278it [02:59,  1.57it/s]Extractor Predicting: 279it [03:00,  1.58it/s]Extractor Predicting: 280it [03:01,  1.57it/s]Extractor Predicting: 281it [03:01,  1.61it/s]Extractor Predicting: 282it [03:02,  1.57it/s]Extractor Predicting: 283it [03:03,  1.54it/s]Extractor Predicting: 284it [03:03,  1.77it/s]Extractor Predicting: 284it [03:03,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:43,462 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:43,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:43,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:43,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:43,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:24:45,520 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:24:45,521 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:24:46,579 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:24:47,778 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:24:47,874 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:52,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:52,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:52,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:52,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:52,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:24:54,489 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:24:54,490 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:24:55,861 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:24:56,277 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:24:56,277 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.303743961352657,
  "recall": 0.1478976771537783,
  "score": 0.19893217322523235,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.44it/s]Extractor Predicting: 5it [00:02,  1.96it/s]Extractor Predicting: 5it [00:03,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:25:05,029 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:25:05,030 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:25:05,105 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:25:05,106 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:25:05,175 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:25:27,317 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:25:27,349 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:25:27,556 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:25:27,557 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:25:27,708 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,752 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,752 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,752 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,753 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,753 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:25:27,753 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3225806451612903,
  "recall": 0.050505050505050504,
  "score": 0.08733624454148471,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:25:28,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:28,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:29,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:30,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:30,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:31,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:31,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:32,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:32,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:33,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:33,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:34,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:35,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:36,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:36,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:37,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:37,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:38,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:38,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:39,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:53, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 23:25:40,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:41,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:41,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:42,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:42,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:43,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:43,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:44,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:44,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:45,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:45,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:46,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:46,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:47,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:47,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:47,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:48,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:48,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:49,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:50,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:22<02:21, 10.88s/it][WARNING|generation_utils.py:914] 2023-08-28 23:25:50,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:51,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:52,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:53,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:53,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:54,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:54,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:55,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:56,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:56,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:57,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:58,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:59,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:00,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:00,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:01,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:02,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:02,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:03,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:04,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:05,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:37<02:36, 13.07s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:06,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:06,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:07,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:08,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:09,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:09,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:10,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:10,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:11,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:12,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:12,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:13,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:14,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:14,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:15,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:15,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:16,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:17,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:17,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:18,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:22, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:18,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:19,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:19,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:20,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:20,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:21,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:21,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:22,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:22,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:23,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:24,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:25,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:25,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:26,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:26,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:27,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:27,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:28,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:29,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:29,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:01<02:03, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:30,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:30,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:31,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:31,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:32,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:32,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:33,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:33,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:34,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:34,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:35,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:35,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:36,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:36,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:37,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:37,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:38,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:38,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:39,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:40,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:40,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:12<01:46, 11.86s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:41,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:41,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:42,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:42,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:43,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:43,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:44,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:44,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:45,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:46,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:46,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:47,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:47,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:47,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:48,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:48,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:49,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:49,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:50,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:50,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:51,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:23<01:32, 11.50s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:51,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:52,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:53,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:53,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:54,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:54,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:55,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:55,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:56,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:57,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:57,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:58,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:58,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:59,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:59,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:00,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:00,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:01,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:02,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:03,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:04,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:05,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:37<01:25, 12.21s/it][WARNING|generation_utils.py:914] 2023-08-28 23:27:05,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:06,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:06,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:07,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:08,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:08,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:09,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:09,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:10,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:11,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:11,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:12,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:12,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:13,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:14,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:14,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:15,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:16,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:17,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:17,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:18,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:19,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:19,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:51<01:17, 12.96s/it][WARNING|generation_utils.py:914] 2023-08-28 23:27:20,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:20,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:21,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:22,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:22,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:23,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:23,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:24,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:24,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:25,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:26,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:26,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:27,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:27,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:28,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:29,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:30,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:30,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:31,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:03<01:02, 12.52s/it][WARNING|generation_utils.py:914] 2023-08-28 23:27:31,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:32,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:32,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:33,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:33,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:34,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:35,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:35,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:36,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:37,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:37,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:38,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:39,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:39,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:40,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:40,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:41,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:41,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:42,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:42,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:15<00:49, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-28 23:27:43,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:44,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:44,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:45,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:46,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:46,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:47,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:48,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:49,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:49,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:50,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:51,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:51,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:52,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:52,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:53,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:54,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:54,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:55,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:55,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:28<00:37, 12.49s/it][WARNING|generation_utils.py:914] 2023-08-28 23:27:56,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:57,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:57,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:58,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:58,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:59,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:59,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:00,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:01,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:01,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:02,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:02,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:03,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:04,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:04,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:05,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:06,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:06,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:07,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:07,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:08,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:40<00:25, 12.50s/it][WARNING|generation_utils.py:914] 2023-08-28 23:28:09,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:09,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:10,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:10,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:11,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:11,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:12,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:12,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:13,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:14,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:14,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:15,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:16,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:16,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:17,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:17,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:18,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:19,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:19,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:20,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:20,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:52<00:12, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 23:28:21,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:21,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:22,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:22,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:23,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:24,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:24,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:25,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:25,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:26,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:26,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:27,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:27,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:28,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:28,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:29,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:30,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:30,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:31,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:28:31,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:03<00:00, 12.02s/it]Generating: 100%|██████████| 15/15 [03:04<00:00, 12.27s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:40,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:40,931 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:40,931 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:40,932 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:40,932 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:28:41,704 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:28:41,705 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:42,394 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:43,482 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:43,482 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:45,997 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:46,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:46,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:46,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:46,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:28:46,399 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:28:46,400 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:47,144 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:47,315 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:47,315 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8622159090909091, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 630, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8948863636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : He began his studies at the University of New Brunswick , where he received teaching degrees at the University of Manitoba , where he also earned a Bachelor of Science degree in mathematics . Head Entity : University of New Brunswick , Tail Entity : Canadian .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9453125, 'errors': {''}}
['Relation : located in the administrative territorial entity . Context : On 31 March 2014 , the municipality announced that it would be expanding its municipal government at a cost of €300 million . Head Entity : City , Tail Entity : city .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9765625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 7002
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7102, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.53it/s]Extractor Estimating: 3it [00:01,  1.68it/s]Extractor Estimating: 4it [00:02,  1.72it/s]Extractor Estimating: 5it [00:02,  1.70it/s]Extractor Estimating: 6it [00:03,  1.79it/s]Extractor Estimating: 7it [00:04,  1.81it/s]Extractor Estimating: 8it [00:04,  1.88it/s]Extractor Estimating: 9it [00:05,  1.91it/s]Extractor Estimating: 10it [00:05,  1.89it/s]Extractor Estimating: 11it [00:06,  1.81it/s]Extractor Estimating: 12it [00:06,  1.79it/s]Extractor Estimating: 13it [00:07,  1.81it/s]Extractor Estimating: 14it [00:07,  1.81it/s]Extractor Estimating: 15it [00:08,  1.78it/s]Extractor Estimating: 16it [00:09,  1.75it/s]Extractor Estimating: 17it [00:09,  1.73it/s]Extractor Estimating: 18it [00:10,  1.77it/s]Extractor Estimating: 19it [00:10,  1.78it/s]Extractor Estimating: 20it [00:11,  1.85it/s]Extractor Estimating: 21it [00:11,  1.85it/s]Extractor Estimating: 22it [00:12,  1.77it/s]Extractor Estimating: 23it [00:12,  1.72it/s]Extractor Estimating: 24it [00:13,  1.76it/s]Extractor Estimating: 25it [00:14,  1.74it/s]Extractor Estimating: 26it [00:14,  1.92it/s]Extractor Estimating: 27it [00:14,  1.97it/s]Extractor Estimating: 28it [00:15,  2.07it/s]Extractor Estimating: 29it [00:15,  2.07it/s]Extractor Estimating: 30it [00:16,  2.12it/s]Extractor Estimating: 31it [00:16,  2.18it/s]Extractor Estimating: 32it [00:17,  2.25it/s]Extractor Estimating: 33it [00:17,  2.21it/s]Extractor Estimating: 34it [00:18,  2.25it/s]Extractor Estimating: 35it [00:18,  2.24it/s]Extractor Estimating: 36it [00:19,  1.99it/s]Extractor Estimating: 37it [00:19,  2.08it/s]Extractor Estimating: 38it [00:20,  2.17it/s]Extractor Estimating: 39it [00:20,  2.20it/s]Extractor Estimating: 40it [00:20,  2.23it/s]Extractor Estimating: 41it [00:21,  2.21it/s]Extractor Estimating: 42it [00:21,  2.24it/s]Extractor Estimating: 43it [00:22,  2.14it/s]Extractor Estimating: 44it [00:22,  2.17it/s]Extractor Estimating: 45it [00:23,  2.22it/s]Extractor Estimating: 46it [00:23,  2.26it/s]Extractor Estimating: 47it [00:24,  2.19it/s]Extractor Estimating: 48it [00:24,  2.27it/s]Extractor Estimating: 49it [00:24,  2.26it/s]Extractor Estimating: 50it [00:25,  1.76it/s]Extractor Estimating: 51it [00:26,  1.77it/s]Extractor Estimating: 52it [00:26,  1.78it/s]Extractor Estimating: 53it [00:27,  1.84it/s]Extractor Estimating: 54it [00:27,  1.88it/s]Extractor Estimating: 55it [00:28,  1.94it/s]Extractor Estimating: 56it [00:28,  1.88it/s]Extractor Estimating: 57it [00:29,  1.92it/s]Extractor Estimating: 58it [00:29,  1.91it/s]Extractor Estimating: 59it [00:30,  1.74it/s]Extractor Estimating: 60it [00:31,  1.88it/s]Extractor Estimating: 61it [00:31,  1.91it/s]Extractor Estimating: 62it [00:32,  1.93it/s]Extractor Estimating: 63it [00:32,  1.93it/s]Extractor Estimating: 64it [00:33,  1.94it/s]Extractor Estimating: 65it [00:33,  1.85it/s]Extractor Estimating: 66it [00:34,  1.93it/s]Extractor Estimating: 67it [00:34,  1.90it/s]Extractor Estimating: 68it [00:35,  1.84it/s]Extractor Estimating: 69it [00:35,  1.87it/s]Extractor Estimating: 70it [00:36,  1.93it/s]Extractor Estimating: 71it [00:36,  1.93it/s]Extractor Estimating: 72it [00:37,  1.96it/s]Extractor Estimating: 73it [00:37,  1.97it/s]Extractor Estimating: 74it [00:38,  1.85it/s]Extractor Estimating: 75it [00:38,  1.92it/s]Extractor Estimating: 76it [00:39,  1.82it/s]Extractor Estimating: 77it [00:40,  1.80it/s]Extractor Estimating: 78it [00:40,  1.86it/s]Extractor Estimating: 79it [00:41,  1.79it/s]Extractor Estimating: 80it [00:41,  1.73it/s]Extractor Estimating: 81it [00:42,  1.65it/s]Extractor Estimating: 82it [00:43,  1.70it/s]Extractor Estimating: 83it [00:43,  1.71it/s]Extractor Estimating: 84it [00:44,  1.63it/s]Extractor Estimating: 85it [00:44,  1.59it/s]Extractor Estimating: 86it [00:45,  1.58it/s]Extractor Estimating: 87it [00:46,  1.58it/s]Extractor Estimating: 88it [00:46,  1.59it/s]Extractor Estimating: 89it [00:47,  1.63it/s]Extractor Estimating: 90it [00:48,  1.65it/s]Extractor Estimating: 91it [00:48,  1.66it/s]Extractor Estimating: 92it [00:49,  1.63it/s]Extractor Estimating: 93it [00:49,  1.69it/s]Extractor Estimating: 94it [00:50,  1.69it/s]Extractor Estimating: 95it [00:50,  1.75it/s]Extractor Estimating: 96it [00:51,  1.74it/s]Extractor Estimating: 97it [00:52,  1.75it/s]Extractor Estimating: 98it [00:52,  1.73it/s]Extractor Estimating: 99it [00:53,  1.65it/s]Extractor Estimating: 100it [00:53,  1.72it/s]Extractor Estimating: 101it [00:54,  1.83it/s]Extractor Estimating: 102it [00:54,  1.93it/s]Extractor Estimating: 103it [00:55,  1.96it/s]Extractor Estimating: 104it [00:55,  2.01it/s]Extractor Estimating: 105it [00:56,  1.95it/s]Extractor Estimating: 106it [00:56,  2.06it/s]Extractor Estimating: 107it [00:57,  2.08it/s]Extractor Estimating: 108it [00:57,  2.14it/s]Extractor Estimating: 109it [00:58,  2.16it/s]Extractor Estimating: 110it [00:58,  2.05it/s]Extractor Estimating: 111it [00:59,  2.03it/s]Extractor Estimating: 112it [00:59,  1.89it/s]Extractor Estimating: 113it [01:00,  1.95it/s]Extractor Estimating: 114it [01:00,  1.91it/s]Extractor Estimating: 115it [01:01,  1.90it/s]Extractor Estimating: 116it [01:01,  1.98it/s]Extractor Estimating: 117it [01:02,  1.99it/s]Extractor Estimating: 118it [01:02,  2.05it/s]Extractor Estimating: 119it [01:03,  2.08it/s]Extractor Estimating: 120it [01:03,  2.13it/s]Extractor Estimating: 121it [01:04,  2.20it/s]Extractor Estimating: 122it [01:04,  2.06it/s]Extractor Estimating: 123it [01:05,  2.04it/s]Extractor Estimating: 124it [01:05,  2.03it/s]Extractor Estimating: 125it [01:06,  2.05it/s]Extractor Estimating: 126it [01:06,  2.11it/s]Extractor Estimating: 127it [01:06,  2.13it/s]Extractor Estimating: 128it [01:07,  2.19it/s]Extractor Estimating: 129it [01:07,  2.22it/s]Extractor Estimating: 130it [01:08,  2.30it/s]Extractor Estimating: 131it [01:08,  2.04it/s]Extractor Estimating: 132it [01:09,  2.16it/s]Extractor Estimating: 133it [01:09,  2.27it/s]Extractor Estimating: 134it [01:10,  2.35it/s]Extractor Estimating: 135it [01:10,  2.36it/s]Extractor Estimating: 136it [01:10,  2.37it/s]Extractor Estimating: 137it [01:11,  2.40it/s]Extractor Estimating: 138it [01:11,  2.44it/s]Extractor Estimating: 139it [01:12,  2.46it/s]Extractor Estimating: 140it [01:12,  2.49it/s]Extractor Estimating: 141it [01:12,  2.53it/s]Extractor Estimating: 142it [01:13,  2.46it/s]Extractor Estimating: 143it [01:13,  2.36it/s]Extractor Estimating: 144it [01:14,  2.13it/s]Extractor Estimating: 145it [01:14,  2.24it/s]Extractor Estimating: 146it [01:15,  2.19it/s]Extractor Estimating: 147it [01:15,  2.22it/s]Extractor Estimating: 148it [01:15,  2.33it/s]Extractor Estimating: 149it [01:16,  2.37it/s]Extractor Estimating: 150it [01:16,  2.22it/s]Extractor Estimating: 151it [01:17,  2.10it/s]Extractor Estimating: 152it [01:17,  2.12it/s]Extractor Estimating: 153it [01:18,  2.26it/s]Extractor Estimating: 154it [01:18,  2.29it/s]Extractor Estimating: 155it [01:19,  2.27it/s]Extractor Estimating: 156it [01:19,  2.33it/s]Extractor Estimating: 157it [01:19,  2.41it/s]Extractor Estimating: 158it [01:20,  2.10it/s]Extractor Estimating: 159it [01:20,  2.14it/s]Extractor Estimating: 160it [01:21,  2.24it/s]Extractor Estimating: 161it [01:21,  2.30it/s]Extractor Estimating: 162it [01:22,  2.34it/s]Extractor Estimating: 163it [01:22,  2.43it/s]Extractor Estimating: 164it [01:23,  2.35it/s]Extractor Estimating: 165it [01:23,  2.25it/s]Extractor Estimating: 166it [01:23,  2.32it/s]Extractor Estimating: 167it [01:24,  2.25it/s]Extractor Estimating: 168it [01:25,  1.99it/s]Extractor Estimating: 169it [01:25,  2.12it/s]Extractor Estimating: 170it [01:25,  2.10it/s]Extractor Estimating: 171it [01:26,  2.16it/s]Extractor Estimating: 172it [01:26,  2.21it/s]Extractor Estimating: 173it [01:27,  2.19it/s]Extractor Estimating: 174it [01:27,  2.28it/s]Extractor Estimating: 175it [01:28,  2.21it/s]Extractor Estimating: 176it [01:28,  2.17it/s]Extractor Estimating: 177it [01:29,  2.24it/s]Extractor Estimating: 178it [01:29,  2.21it/s]Extractor Estimating: 179it [01:30,  2.08it/s]Extractor Estimating: 180it [01:30,  2.08it/s]Extractor Estimating: 181it [01:31,  2.08it/s]Extractor Estimating: 182it [01:31,  2.14it/s]Extractor Estimating: 183it [01:31,  2.08it/s]Extractor Estimating: 184it [01:32,  2.06it/s]Extractor Estimating: 185it [01:32,  2.01it/s]Extractor Estimating: 186it [01:33,  2.00it/s]Extractor Estimating: 187it [01:34,  1.96it/s]Extractor Estimating: 188it [01:34,  2.01it/s]Extractor Estimating: 189it [01:35,  2.00it/s]Extractor Estimating: 190it [01:35,  2.07it/s]Extractor Estimating: 191it [01:35,  2.14it/s]Extractor Estimating: 192it [01:36,  2.08it/s]Extractor Estimating: 193it [01:36,  2.20it/s]Extractor Estimating: 194it [01:37,  2.15it/s]Extractor Estimating: 195it [01:37,  2.03it/s]Extractor Estimating: 196it [01:38,  2.06it/s]Extractor Estimating: 197it [01:38,  2.18it/s]Extractor Estimating: 198it [01:39,  2.20it/s]Extractor Estimating: 199it [01:39,  2.13it/s]Extractor Estimating: 200it [01:40,  2.17it/s]Extractor Estimating: 201it [01:40,  2.03it/s]Extractor Estimating: 202it [01:41,  1.99it/s]Extractor Estimating: 203it [01:41,  1.96it/s]Extractor Estimating: 204it [01:42,  1.90it/s]Extractor Estimating: 205it [01:42,  1.91it/s]Extractor Estimating: 206it [01:43,  1.90it/s]Extractor Estimating: 207it [01:43,  1.90it/s]Extractor Estimating: 208it [01:44,  1.85it/s]Extractor Estimating: 209it [01:45,  1.72it/s]Extractor Estimating: 210it [01:45,  1.68it/s]Extractor Estimating: 211it [01:46,  1.76it/s]Extractor Estimating: 212it [01:46,  1.84it/s]Extractor Estimating: 213it [01:47,  1.88it/s]Extractor Estimating: 214it [01:47,  1.87it/s]Extractor Estimating: 215it [01:48,  1.73it/s]Extractor Estimating: 216it [01:49,  1.72it/s]Extractor Estimating: 217it [01:49,  1.81it/s]Extractor Estimating: 218it [01:50,  1.65it/s]Extractor Estimating: 219it [01:50,  1.72it/s]Extractor Estimating: 220it [01:51,  1.66it/s]Extractor Estimating: 221it [01:51,  1.76it/s]Extractor Estimating: 222it [01:52,  1.76it/s]Extractor Estimating: 223it [01:53,  1.72it/s]Extractor Estimating: 224it [01:53,  1.74it/s]Extractor Estimating: 225it [01:54,  1.81it/s]Extractor Estimating: 226it [01:54,  1.72it/s]Extractor Estimating: 227it [01:55,  1.80it/s]Extractor Estimating: 228it [01:55,  1.80it/s]Extractor Estimating: 229it [01:56,  1.91it/s]Extractor Estimating: 230it [01:56,  1.95it/s]Extractor Estimating: 231it [01:57,  2.00it/s]Extractor Estimating: 232it [01:57,  1.91it/s]Extractor Estimating: 233it [01:58,  1.91it/s]Extractor Estimating: 234it [01:58,  1.93it/s]Extractor Estimating: 235it [01:59,  1.92it/s]Extractor Estimating: 236it [01:59,  1.90it/s]Extractor Estimating: 237it [02:00,  1.94it/s]Extractor Estimating: 238it [02:00,  1.90it/s]Extractor Estimating: 239it [02:01,  1.97it/s]Extractor Estimating: 240it [02:01,  1.98it/s]Extractor Estimating: 241it [02:02,  1.98it/s]Extractor Estimating: 242it [02:02,  1.97it/s]Extractor Estimating: 243it [02:03,  2.02it/s]Extractor Estimating: 244it [02:03,  1.95it/s]Extractor Estimating: 245it [02:04,  1.92it/s]Extractor Estimating: 246it [02:05,  1.91it/s]Extractor Estimating: 247it [02:05,  1.88it/s]Extractor Estimating: 248it [02:06,  1.88it/s]Extractor Estimating: 249it [02:06,  1.85it/s]Extractor Estimating: 250it [02:07,  1.90it/s]Extractor Estimating: 251it [02:07,  1.91it/s]Extractor Estimating: 252it [02:08,  1.89it/s]Extractor Estimating: 253it [02:08,  1.88it/s]Extractor Estimating: 254it [02:09,  1.82it/s]Extractor Estimating: 255it [02:09,  1.85it/s]Extractor Estimating: 256it [02:10,  1.87it/s]Extractor Estimating: 257it [02:11,  1.76it/s]Extractor Estimating: 258it [02:11,  1.70it/s]Extractor Estimating: 259it [02:12,  1.78it/s]Extractor Estimating: 260it [02:12,  1.68it/s]Extractor Estimating: 261it [02:13,  1.76it/s]Extractor Estimating: 262it [02:13,  1.77it/s]Extractor Estimating: 263it [02:14,  1.87it/s]Extractor Estimating: 264it [02:14,  1.85it/s]Extractor Estimating: 265it [02:15,  1.75it/s]Extractor Estimating: 266it [02:16,  1.68it/s]Extractor Estimating: 267it [02:16,  1.76it/s]Extractor Estimating: 268it [02:17,  1.80it/s]Extractor Estimating: 269it [02:17,  1.81it/s]Extractor Estimating: 270it [02:18,  1.87it/s]Extractor Estimating: 271it [02:18,  1.86it/s]Extractor Estimating: 272it [02:19,  1.76it/s]Extractor Estimating: 273it [02:20,  1.62it/s]Extractor Estimating: 274it [02:20,  1.70it/s]Extractor Estimating: 275it [02:21,  1.75it/s]Extractor Estimating: 276it [02:21,  1.81it/s]Extractor Estimating: 277it [02:22,  1.86it/s]Extractor Estimating: 278it [02:22,  1.84it/s]Extractor Estimating: 279it [02:23,  1.88it/s]Extractor Estimating: 280it [02:23,  1.91it/s]Extractor Estimating: 281it [02:24,  1.96it/s]Extractor Estimating: 282it [02:24,  1.96it/s]Extractor Estimating: 283it [02:25,  2.03it/s]Extractor Estimating: 284it [02:25,  1.86it/s]Extractor Estimating: 285it [02:26,  1.98it/s]Extractor Estimating: 286it [02:26,  2.07it/s]Extractor Estimating: 287it [02:27,  2.09it/s]Extractor Estimating: 288it [02:27,  2.15it/s]Extractor Estimating: 289it [02:28,  2.15it/s]Extractor Estimating: 290it [02:28,  2.17it/s]Extractor Estimating: 291it [02:29,  2.09it/s]Extractor Estimating: 292it [02:29,  2.11it/s]Extractor Estimating: 293it [02:30,  2.15it/s]Extractor Estimating: 294it [02:30,  2.17it/s]Extractor Estimating: 295it [02:30,  2.21it/s]Extractor Estimating: 296it [02:31,  2.26it/s]Extractor Estimating: 297it [02:31,  2.22it/s]Extractor Estimating: 298it [02:32,  2.09it/s]Extractor Estimating: 299it [02:32,  2.08it/s]Extractor Estimating: 300it [02:33,  1.92it/s]Extractor Estimating: 301it [02:33,  1.97it/s]Extractor Estimating: 302it [02:34,  2.00it/s]Extractor Estimating: 303it [02:34,  1.99it/s]Extractor Estimating: 304it [02:35,  1.98it/s]Extractor Estimating: 305it [02:35,  1.96it/s]Extractor Estimating: 306it [02:36,  1.97it/s]Extractor Estimating: 307it [02:36,  2.04it/s]Extractor Estimating: 308it [02:37,  2.02it/s]Extractor Estimating: 309it [02:37,  2.06it/s]Extractor Estimating: 310it [02:38,  2.06it/s]Extractor Estimating: 311it [02:38,  2.06it/s]Extractor Estimating: 312it [02:39,  2.09it/s]Extractor Estimating: 313it [02:39,  1.94it/s]Extractor Estimating: 314it [02:40,  1.98it/s]Extractor Estimating: 315it [02:40,  1.97it/s]Extractor Estimating: 316it [02:41,  2.07it/s]Extractor Estimating: 317it [02:41,  1.97it/s]Extractor Estimating: 318it [02:42,  2.03it/s]Extractor Estimating: 319it [02:42,  2.03it/s]Extractor Estimating: 320it [02:43,  2.04it/s]Extractor Estimating: 321it [02:43,  1.99it/s]Extractor Estimating: 322it [02:44,  1.93it/s]Extractor Estimating: 323it [02:44,  1.90it/s]Extractor Estimating: 324it [02:45,  1.89it/s]Extractor Estimating: 325it [02:46,  1.88it/s]Extractor Estimating: 326it [02:46,  1.95it/s]Extractor Estimating: 327it [02:46,  2.00it/s]Extractor Estimating: 328it [02:47,  1.98it/s]Extractor Estimating: 329it [02:47,  2.03it/s]Extractor Estimating: 330it [02:48,  2.06it/s]Extractor Estimating: 331it [02:48,  2.08it/s]Extractor Estimating: 332it [02:49,  2.07it/s]Extractor Estimating: 333it [02:49,  2.18it/s]Extractor Estimating: 334it [02:50,  2.23it/s]Extractor Estimating: 335it [02:50,  2.21it/s]Extractor Estimating: 336it [02:51,  2.18it/s]Extractor Estimating: 337it [02:51,  2.13it/s]Extractor Estimating: 338it [02:52,  2.11it/s]Extractor Estimating: 339it [02:52,  2.14it/s]Extractor Estimating: 340it [02:53,  2.08it/s]Extractor Estimating: 341it [02:53,  2.12it/s]Extractor Estimating: 342it [02:54,  2.09it/s]Extractor Estimating: 343it [02:54,  2.01it/s]Extractor Estimating: 344it [02:55,  2.05it/s]Extractor Estimating: 345it [02:55,  2.10it/s]Extractor Estimating: 346it [02:56,  2.06it/s]Extractor Estimating: 347it [02:56,  2.15it/s]Extractor Estimating: 348it [02:56,  2.12it/s]Extractor Estimating: 349it [02:57,  2.16it/s]Extractor Estimating: 350it [02:57,  2.05it/s]Extractor Estimating: 351it [02:58,  1.98it/s]Extractor Estimating: 352it [02:59,  1.85it/s]Extractor Estimating: 353it [02:59,  1.76it/s]Extractor Estimating: 354it [03:00,  1.79it/s]Extractor Estimating: 355it [03:00,  1.80it/s]Extractor Estimating: 356it [03:01,  1.80it/s]Extractor Estimating: 357it [03:01,  1.81it/s]Extractor Estimating: 358it [03:02,  1.85it/s]Extractor Estimating: 359it [03:02,  1.84it/s]Extractor Estimating: 360it [03:03,  1.84it/s]Extractor Estimating: 361it [03:04,  1.83it/s]Extractor Estimating: 362it [03:04,  1.81it/s]Extractor Estimating: 363it [03:05,  1.86it/s]Extractor Estimating: 364it [03:05,  1.89it/s]Extractor Estimating: 365it [03:06,  1.84it/s]Extractor Estimating: 366it [03:06,  1.78it/s]Extractor Estimating: 367it [03:07,  1.72it/s]Extractor Estimating: 368it [03:07,  1.80it/s]Extractor Estimating: 369it [03:08,  1.78it/s]Extractor Estimating: 370it [03:09,  1.82it/s]Extractor Estimating: 371it [03:09,  1.83it/s]Extractor Estimating: 372it [03:10,  1.85it/s]Extractor Estimating: 373it [03:10,  1.78it/s]Extractor Estimating: 374it [03:11,  1.82it/s]Extractor Estimating: 375it [03:11,  2.18it/s]Extractor Estimating: 375it [03:11,  1.96it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:26,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:26,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:26,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:26,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:26,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:32:27,781 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:32:27,782 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:28,397 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:29,604 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:29,604 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:32,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:32,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:32,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:32,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:32,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:32:33,820 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:32:33,821 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:34,532 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:34,755 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:34,756 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:28:17,414 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:28:17,655 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7485 mean pseudo reward: 0.9556493048633861
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 15044
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15144, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15144, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.873, loss:356.5127
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.869, loss:339.6532
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.869, loss:338.3580
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.875, loss:309.6345
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.869, loss:327.1294
>> valid entity prec:0.5398, rec:0.4837, f1:0.5102
>> valid relation prec:0.1918, rec:0.1013, f1:0.1326
>> valid relation with NER prec:0.1918, rec:0.1013, f1:0.1326
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.137, loss:345.1521
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.862, loss:293.1239
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.877, loss:310.5402
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.853, loss:324.2800
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.861, loss:314.4971
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5133, rec:0.6090, f1:0.5571
>> valid relation prec:0.1459, rec:0.1254, f1:0.1349
>> valid relation with NER prec:0.1459, rec:0.1254, f1:0.1349
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.129, loss:302.2887
g_step 1200, step 264, avg_time 0.870, loss:323.6380
g_step 1300, step 52, avg_time 0.870, loss:299.0417
g_step 1400, step 152, avg_time 0.872, loss:282.1380
g_step 1500, step 252, avg_time 0.863, loss:274.3196
>> valid entity prec:0.5384, rec:0.5071, f1:0.5223
>> valid relation prec:0.1583, rec:0.0873, f1:0.1125
>> valid relation with NER prec:0.1583, rec:0.0873, f1:0.1125
g_step 1600, step 40, avg_time 2.112, loss:270.9507
g_step 1700, step 140, avg_time 0.865, loss:272.2872
g_step 1800, step 240, avg_time 0.867, loss:276.3656
g_step 1900, step 28, avg_time 0.858, loss:263.4069
g_step 2000, step 128, avg_time 0.866, loss:264.6014
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5247, rec:0.5916, f1:0.5561
>> valid relation prec:0.1710, rec:0.1197, f1:0.1408
>> valid relation with NER prec:0.1710, rec:0.1197, f1:0.1408
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.140, loss:263.7023
g_step 2200, step 16, avg_time 0.862, loss:242.1420
g_step 2300, step 116, avg_time 0.859, loss:232.8346
g_step 2400, step 216, avg_time 0.866, loss:242.4689
g_step 2500, step 4, avg_time 0.853, loss:238.6252
>> valid entity prec:0.5350, rec:0.4847, f1:0.5086
>> valid relation prec:0.1654, rec:0.0962, f1:0.1216
>> valid relation with NER prec:0.1654, rec:0.0962, f1:0.1216
g_step 2600, step 104, avg_time 2.122, loss:195.3156
g_step 2700, step 204, avg_time 0.866, loss:230.2958
g_step 2800, step 304, avg_time 0.870, loss:248.9585
g_step 2900, step 92, avg_time 0.858, loss:213.9650
g_step 3000, step 192, avg_time 0.868, loss:207.2358
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5587, rec:0.4996, f1:0.5275
>> valid relation prec:0.1740, rec:0.1053, f1:0.1312
>> valid relation with NER prec:0.1740, rec:0.1053, f1:0.1312
g_step 3100, step 292, avg_time 2.140, loss:224.9937
g_step 3200, step 80, avg_time 0.869, loss:196.6924
g_step 3300, step 180, avg_time 0.880, loss:216.6300
g_step 3400, step 280, avg_time 0.878, loss:204.8319
g_step 3500, step 68, avg_time 0.870, loss:197.5787
>> valid entity prec:0.5340, rec:0.5087, f1:0.5210
>> valid relation prec:0.1606, rec:0.1010, f1:0.1240
>> valid relation with NER prec:0.1606, rec:0.1010, f1:0.1240
g_step 3600, step 168, avg_time 2.142, loss:193.3941
g_step 3700, step 268, avg_time 0.866, loss:204.9910
g_step 3800, step 56, avg_time 0.872, loss:202.2094
g_step 3900, step 156, avg_time 0.880, loss:178.5706
g_step 4000, step 256, avg_time 0.867, loss:196.8174
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5154, rec:0.5230, f1:0.5191
>> valid relation prec:0.1597, rec:0.1094, f1:0.1298
>> valid relation with NER prec:0.1597, rec:0.1094, f1:0.1298
g_step 4100, step 44, avg_time 2.134, loss:184.6466
g_step 4200, step 144, avg_time 0.883, loss:168.6262
g_step 4300, step 244, avg_time 0.872, loss:203.1464
g_step 4400, step 32, avg_time 0.884, loss:192.1950
g_step 4500, step 132, avg_time 0.880, loss:172.6473
>> valid entity prec:0.5390, rec:0.5068, f1:0.5224
>> valid relation prec:0.1476, rec:0.1028, f1:0.1212
>> valid relation with NER prec:0.1476, rec:0.1028, f1:0.1212
g_step 4600, step 232, avg_time 2.130, loss:173.0520
g_step 4700, step 20, avg_time 0.860, loss:182.0600
g_step 4800, step 120, avg_time 0.862, loss:159.7306
g_step 4900, step 220, avg_time 0.879, loss:186.0715
g_step 5000, step 8, avg_time 0.869, loss:169.4051
learning rate was adjusted to 0.0008
>> valid entity prec:0.5305, rec:0.5113, f1:0.5207
>> valid relation prec:0.1517, rec:0.1048, f1:0.1239
>> valid relation with NER prec:0.1517, rec:0.1048, f1:0.1239
g_step 5100, step 108, avg_time 2.121, loss:154.4608
g_step 5200, step 208, avg_time 0.868, loss:170.9130
g_step 5300, step 308, avg_time 0.884, loss:169.0069
g_step 5400, step 96, avg_time 0.874, loss:142.6823
g_step 5500, step 196, avg_time 0.877, loss:159.0024
>> valid entity prec:0.5343, rec:0.4952, f1:0.5140
>> valid relation prec:0.1645, rec:0.1094, f1:0.1314
>> valid relation with NER prec:0.1645, rec:0.1094, f1:0.1314
g_step 5600, step 296, avg_time 2.124, loss:160.1473
g_step 5700, step 84, avg_time 0.863, loss:146.1667
g_step 5800, step 184, avg_time 0.877, loss:149.5727
g_step 5900, step 284, avg_time 0.872, loss:154.1896
g_step 6000, step 72, avg_time 0.856, loss:147.0798
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5304, rec:0.4899, f1:0.5093
>> valid relation prec:0.1423, rec:0.0918, f1:0.1117
>> valid relation with NER prec:0.1423, rec:0.0918, f1:0.1117
g_step 6100, step 172, avg_time 2.135, loss:150.0585
g_step 6200, step 272, avg_time 0.865, loss:153.7139
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:28:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:28:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-28-17_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:28:18 - WARNING - datasets.builder -   Using custom data configuration default-8f0c6ca6e9184978
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8f0c6ca6e9184978/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:28:20,360 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:28:20,361 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:28:20,361 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:28:20,362 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:28:20,478 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:28:20,535 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:28:20,920 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:28:24,099 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:28:24,131 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8f0c6ca6e9184978/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.14ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.13ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.86ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.34ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.67ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.89ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  5.06ba/s]100%|██████████| 8/8 [00:01<00:00,  4.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.27ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.90ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18ba/s]100%|██████████| 4/4 [00:00<00:00,  5.28ba/s]100%|██████████| 4/4 [00:00<00:00,  4.65ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.91ba/s] 25%|██▌       | 2/8 [00:00<00:00,  7.63ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.37ba/s] 75%|███████▌  | 6/8 [00:00<00:00, 10.04ba/s]100%|██████████| 8/8 [00:00<00:00, 11.50ba/s]100%|██████████| 8/8 [00:00<00:00, 10.28ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.28ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.12ba/s]100%|██████████| 4/4 [00:00<00:00, 10.19ba/s]
[INFO|trainer.py:414] 2023-08-29 01:28:28,770 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:28:28,840 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:28:28,840 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:28:28,840 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:28:28,840 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:28:28,840 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:28:28,840 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:28:28,840 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:48,  3.44it/s]  1%|          | 6/585 [00:01<02:54,  3.32it/s]  1%|          | 7/585 [00:02<02:52,  3.34it/s]  1%|▏         | 8/585 [00:02<02:51,  3.36it/s]  2%|▏         | 9/585 [00:02<02:51,  3.36it/s]  2%|▏         | 10/585 [00:02<02:50,  3.38it/s]  2%|▏         | 11/585 [00:03<02:49,  3.38it/s]  2%|▏         | 12/585 [00:03<02:49,  3.39it/s]  2%|▏         | 13/585 [00:03<02:48,  3.39it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:48,  3.39it/s]  3%|▎         | 16/585 [00:04<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:51,  3.32it/s]  3%|▎         | 18/585 [00:05<02:49,  3.34it/s]  3%|▎         | 19/585 [00:05<02:48,  3.36it/s]  3%|▎         | 20/585 [00:05<02:47,  3.37it/s]  4%|▎         | 21/585 [00:06<02:46,  3.38it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:44,  3.39it/s]  4%|▍         | 26/585 [00:07<02:44,  3.40it/s]  5%|▍         | 27/585 [00:07<02:44,  3.39it/s]  5%|▍         | 28/585 [00:08<02:49,  3.28it/s]  5%|▍         | 29/585 [00:08<02:47,  3.31it/s]  5%|▌         | 30/585 [00:08<02:46,  3.33it/s]  5%|▌         | 31/585 [00:09<02:45,  3.35it/s]  5%|▌         | 32/585 [00:09<02:44,  3.36it/s]  6%|▌         | 33/585 [00:09<02:43,  3.37it/s]  6%|▌         | 34/585 [00:10<02:43,  3.38it/s]  6%|▌         | 35/585 [00:10<02:42,  3.38it/s]  6%|▌         | 36/585 [00:10<02:42,  3.39it/s]  6%|▋         | 37/585 [00:10<02:41,  3.39it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:44,  3.31it/s]  7%|▋         | 40/585 [00:11<02:42,  3.35it/s]  7%|▋         | 41/585 [00:12<02:41,  3.38it/s]  7%|▋         | 42/585 [00:12<02:39,  3.40it/s]  7%|▋         | 43/585 [00:12<02:38,  3.41it/s]  8%|▊         | 44/585 [00:13<02:38,  3.42it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.43it/s]  8%|▊         | 48/585 [00:14<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:35,  3.44it/s]  9%|▊         | 50/585 [00:14<02:37,  3.39it/s]  9%|▊         | 51/585 [00:15<02:36,  3.41it/s]  9%|▉         | 52/585 [00:15<02:35,  3.42it/s]  9%|▉         | 53/585 [00:15<02:35,  3.43it/s]  9%|▉         | 54/585 [00:15<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:17<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:32,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:18<02:36,  3.35it/s] 11%|█         | 62/585 [00:18<02:34,  3.38it/s] 11%|█         | 63/585 [00:18<02:33,  3.40it/s] 11%|█         | 64/585 [00:18<02:32,  3.41it/s] 11%|█         | 65/585 [00:19<02:32,  3.42it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:20<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:22<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:36,  3.25it/s] 13%|█▎        | 78/585 [00:23<02:33,  3.30it/s] 14%|█▎        | 79/585 [00:23<02:31,  3.34it/s] 14%|█▎        | 80/585 [00:23<02:29,  3.37it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.39it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.42it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.43it/s] 15%|█▍        | 85/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:30,  3.31it/s] 15%|█▌        | 89/585 [00:26<02:28,  3.35it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.39it/s] 16%|█▌        | 92/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.42it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.42it/s] 16%|█▌        | 95/585 [00:27<02:23,  3.43it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 99/585 [00:29<02:32,  3.20it/s] 17%|█▋        | 100/585 [00:29<02:28,  3.27it/s] 17%|█▋        | 101/585 [00:29<02:26,  3.31it/s] 17%|█▋        | 102/585 [00:30<02:24,  3.35it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 105/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.42it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.42it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.42it/s] 19%|█▊        | 109/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:26,  3.24it/s] 19%|█▉        | 111/585 [00:32<02:23,  3.30it/s] 19%|█▉        | 112/585 [00:33<02:21,  3.34it/s] 19%|█▉        | 113/585 [00:33<02:20,  3.36it/s] 19%|█▉        | 114/585 [00:33<02:19,  3.38it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.41it/s] 20%|██        | 117/585 [00:34<02:17,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 01:29:03,401 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:29:03,401 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:29:03,401 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.24it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.78it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.23it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.56it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.07it/s][A
  7%|▋         | 32/436 [00:00<00:09, 42.21it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.00it/s][A
 10%|▉         | 42/436 [00:00<00:09, 43.31it/s][A
 11%|█         | 47/436 [00:01<00:08, 43.80it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.04it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.12it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.20it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.16it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.07it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.25it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.35it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.38it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.44it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.52it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.46it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.33it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.27it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.22it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.30it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.29it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.47it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.58it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.51it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.28it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.33it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.11it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.42it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.80it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.24it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.13it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.15it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.05it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.11it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.31it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.44it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.51it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.47it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.43it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.40it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.27it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.22it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.30it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.38it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.47it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.35it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.50it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.40it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.19it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.20it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.30it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.17it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.69it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.98it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.18it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.28it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.26it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.21it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.00it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.13it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.33it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.51it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.54it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.53it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.36it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.33it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.19it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.22it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.21it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.31it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.47it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.62it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.49it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.46it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.36it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.26it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.19it/s][A 20%|██        | 117/585 [00:44<02:17,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:29:13,425 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:29:13,618 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:29:17,045 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:29:17,501 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:29:17,628 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:56<52:29,  6.74s/it] 20%|██        | 119/585 [00:56<37:22,  4.81s/it] 21%|██        | 120/585 [00:56<26:47,  3.46s/it] 21%|██        | 121/585 [00:57<19:23,  2.51s/it] 21%|██        | 122/585 [00:57<14:13,  1.84s/it] 21%|██        | 123/585 [00:57<10:37,  1.38s/it] 21%|██        | 124/585 [00:58<08:05,  1.05s/it] 21%|██▏       | 125/585 [00:58<06:19,  1.21it/s] 22%|██▏       | 126/585 [00:58<05:05,  1.50it/s] 22%|██▏       | 127/585 [00:58<04:14,  1.80it/s] 22%|██▏       | 128/585 [00:59<03:37,  2.10it/s] 22%|██▏       | 129/585 [00:59<03:12,  2.37it/s] 22%|██▏       | 130/585 [00:59<02:57,  2.57it/s] 22%|██▏       | 131/585 [01:00<02:43,  2.77it/s] 23%|██▎       | 132/585 [01:00<02:34,  2.93it/s] 23%|██▎       | 133/585 [01:00<02:27,  3.06it/s] 23%|██▎       | 134/585 [01:01<02:22,  3.15it/s] 23%|██▎       | 135/585 [01:01<02:19,  3.22it/s] 23%|██▎       | 136/585 [01:01<02:17,  3.28it/s] 23%|██▎       | 137/585 [01:01<02:15,  3.31it/s] 24%|██▎       | 138/585 [01:02<02:14,  3.33it/s] 24%|██▍       | 139/585 [01:02<02:13,  3.35it/s] 24%|██▍       | 140/585 [01:02<02:12,  3.37it/s] 24%|██▍       | 141/585 [01:03<02:14,  3.31it/s] 24%|██▍       | 142/585 [01:03<02:12,  3.33it/s] 24%|██▍       | 143/585 [01:03<02:11,  3.35it/s] 25%|██▍       | 144/585 [01:04<02:10,  3.37it/s] 25%|██▍       | 145/585 [01:04<02:10,  3.37it/s] 25%|██▍       | 146/585 [01:04<02:09,  3.38it/s] 25%|██▌       | 147/585 [01:04<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:05<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:05<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:05<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:06<02:07,  3.39it/s] 26%|██▌       | 152/585 [01:06<02:10,  3.33it/s] 26%|██▌       | 153/585 [01:06<02:09,  3.35it/s] 26%|██▋       | 154/585 [01:06<02:08,  3.36it/s] 26%|██▋       | 155/585 [01:07<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:07<02:07,  3.37it/s] 27%|██▋       | 157/585 [01:07<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:08<02:06,  3.39it/s] 27%|██▋       | 159/585 [01:08<02:05,  3.39it/s] 27%|██▋       | 160/585 [01:08<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:09<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:09<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:09<02:07,  3.32it/s] 28%|██▊       | 164/585 [01:09<02:06,  3.34it/s] 28%|██▊       | 165/585 [01:10<02:05,  3.35it/s] 28%|██▊       | 166/585 [01:10<02:04,  3.37it/s] 29%|██▊       | 167/585 [01:10<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:11<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:11<02:03,  3.38it/s] 29%|██▉       | 170/585 [01:11<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:12<02:02,  3.39it/s] 29%|██▉       | 172/585 [01:12<02:01,  3.39it/s] 30%|██▉       | 173/585 [01:12<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:12<02:03,  3.34it/s] 30%|██▉       | 175/585 [01:13<02:02,  3.35it/s] 30%|███       | 176/585 [01:13<02:01,  3.37it/s] 30%|███       | 177/585 [01:13<02:00,  3.38it/s] 30%|███       | 178/585 [01:14<02:00,  3.38it/s] 31%|███       | 179/585 [01:14<01:59,  3.38it/s] 31%|███       | 180/585 [01:14<01:59,  3.39it/s] 31%|███       | 181/585 [01:14<01:59,  3.39it/s] 31%|███       | 182/585 [01:15<01:58,  3.39it/s] 31%|███▏      | 183/585 [01:15<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:15<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:16<02:01,  3.28it/s] 32%|███▏      | 186/585 [01:16<02:00,  3.31it/s] 32%|███▏      | 187/585 [01:16<01:59,  3.34it/s] 32%|███▏      | 188/585 [01:17<01:58,  3.36it/s] 32%|███▏      | 189/585 [01:17<01:57,  3.37it/s] 32%|███▏      | 190/585 [01:17<01:57,  3.37it/s] 33%|███▎      | 191/585 [01:17<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:18<01:56,  3.39it/s] 33%|███▎      | 193/585 [01:18<01:55,  3.39it/s] 33%|███▎      | 194/585 [01:18<01:55,  3.39it/s] 33%|███▎      | 195/585 [01:19<01:54,  3.39it/s] 34%|███▎      | 196/585 [01:19<01:58,  3.29it/s] 34%|███▎      | 197/585 [01:19<01:56,  3.33it/s] 34%|███▍      | 198/585 [01:20<01:55,  3.34it/s] 34%|███▍      | 199/585 [01:20<01:55,  3.36it/s] 34%|███▍      | 200/585 [01:20<01:54,  3.37it/s] 34%|███▍      | 201/585 [01:20<01:53,  3.38it/s] 35%|███▍      | 202/585 [01:21<01:53,  3.38it/s] 35%|███▍      | 203/585 [01:21<01:52,  3.39it/s] 35%|███▍      | 204/585 [01:21<01:52,  3.39it/s] 35%|███▌      | 205/585 [01:22<01:52,  3.39it/s] 35%|███▌      | 206/585 [01:22<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:22<01:51,  3.39it/s] 36%|███▌      | 208/585 [01:22<01:51,  3.39it/s] 36%|███▌      | 209/585 [01:23<01:50,  3.39it/s] 36%|███▌      | 210/585 [01:23<01:50,  3.39it/s] 36%|███▌      | 211/585 [01:23<01:54,  3.27it/s] 36%|███▌      | 212/585 [01:24<01:52,  3.31it/s] 36%|███▋      | 213/585 [01:24<01:51,  3.33it/s] 37%|███▋      | 214/585 [01:24<01:50,  3.35it/s] 37%|███▋      | 215/585 [01:25<01:49,  3.37it/s] 37%|███▋      | 216/585 [01:25<01:49,  3.38it/s] 37%|███▋      | 217/585 [01:25<01:48,  3.38it/s] 37%|███▋      | 218/585 [01:25<01:48,  3.38it/s] 37%|███▋      | 219/585 [01:26<01:47,  3.39it/s] 38%|███▊      | 220/585 [01:26<01:47,  3.39it/s] 38%|███▊      | 221/585 [01:26<01:47,  3.39it/s] 38%|███▊      | 222/585 [01:27<01:51,  3.25it/s] 38%|███▊      | 223/585 [01:27<01:49,  3.30it/s] 38%|███▊      | 224/585 [01:27<01:48,  3.32it/s] 38%|███▊      | 225/585 [01:28<01:47,  3.34it/s] 39%|███▊      | 226/585 [01:28<01:46,  3.36it/s] 39%|███▉      | 227/585 [01:28<01:46,  3.37it/s] 39%|███▉      | 228/585 [01:28<01:45,  3.38it/s] 39%|███▉      | 229/585 [01:29<01:45,  3.39it/s] 39%|███▉      | 230/585 [01:29<01:44,  3.39it/s] 39%|███▉      | 231/585 [01:29<01:44,  3.39it/s] 40%|███▉      | 232/585 [01:30<01:44,  3.39it/s] 40%|███▉      | 233/585 [01:30<01:51,  3.16it/s] 40%|████      | 234/585 [01:30<01:48,  3.23it/s][INFO|trainer.py:2140] 2023-08-29 01:29:59,690 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:29:59,690 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:29:59,691 >>   Batch size = 8
{'eval_loss': 1.143255352973938, 'eval_runtime': 9.9027, 'eval_samples_per_second': 352.228, 'eval_steps_per_second': 44.029, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.72it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.49it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.29it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.43it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.85it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.73it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.68it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.58it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.75it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.86it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.88it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.61it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.46it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.33it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.36it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.46it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.59it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.73it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.75it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.57it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.42it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.62it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.16it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.38it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.49it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.37it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.21it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.23it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.36it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.49it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.57it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.68it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.75it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.54it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.42it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.27it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.29it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.48it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.55it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.69it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.74it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.73it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.54it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.44it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 42.45it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.19it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.67it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.02it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.26it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.43it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.45it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.04it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.14it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.30it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.48it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.60it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.74it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.79it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.58it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.32it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.28it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.38it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.47it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.61it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.70it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.69it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.56it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.39it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.35it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 42.25it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.02it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.62it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.02it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.19it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.36it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.29it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.32it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.14it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.24it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.45it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.45it/s][A 40%|████      | 234/585 [01:40<01:48,  3.23it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:30:09,676 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:30:09,868 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:30:13,226 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:30:13,348 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:30:13,432 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:52<39:35,  6.79s/it] 40%|████      | 236/585 [01:53<28:09,  4.84s/it] 41%|████      | 237/585 [01:53<20:09,  3.48s/it] 41%|████      | 238/585 [01:53<14:35,  2.52s/it] 41%|████      | 239/585 [01:53<10:41,  1.85s/it] 41%|████      | 240/585 [01:54<07:58,  1.39s/it] 41%|████      | 241/585 [01:54<06:04,  1.06s/it] 41%|████▏     | 242/585 [01:54<04:45,  1.20it/s] 42%|████▏     | 243/585 [01:55<03:49,  1.49it/s] 42%|████▏     | 244/585 [01:55<03:10,  1.79it/s] 42%|████▏     | 245/585 [01:55<02:42,  2.09it/s] 42%|████▏     | 246/585 [01:55<02:23,  2.36it/s] 42%|████▏     | 247/585 [01:56<02:10,  2.60it/s] 42%|████▏     | 248/585 [01:56<02:00,  2.80it/s] 43%|████▎     | 249/585 [01:56<01:53,  2.96it/s] 43%|████▎     | 250/585 [01:57<01:48,  3.07it/s] 43%|████▎     | 251/585 [01:57<01:45,  3.16it/s] 43%|████▎     | 252/585 [01:57<01:42,  3.23it/s] 43%|████▎     | 253/585 [01:58<01:43,  3.21it/s] 43%|████▎     | 254/585 [01:58<01:41,  3.26it/s] 44%|████▎     | 255/585 [01:58<01:39,  3.30it/s] 44%|████▍     | 256/585 [01:58<01:38,  3.33it/s] 44%|████▍     | 257/585 [01:59<01:37,  3.35it/s] 44%|████▍     | 258/585 [01:59<01:37,  3.37it/s] 44%|████▍     | 259/585 [01:59<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:00<01:35,  3.39it/s] 45%|████▍     | 261/585 [02:00<01:35,  3.39it/s] 45%|████▍     | 262/585 [02:00<01:35,  3.39it/s] 45%|████▍     | 263/585 [02:00<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:01<01:38,  3.26it/s] 45%|████▌     | 265/585 [02:01<01:37,  3.29it/s] 45%|████▌     | 266/585 [02:01<01:36,  3.32it/s] 46%|████▌     | 267/585 [02:02<01:34,  3.35it/s] 46%|████▌     | 268/585 [02:02<01:34,  3.36it/s] 46%|████▌     | 269/585 [02:02<01:33,  3.37it/s] 46%|████▌     | 270/585 [02:03<01:33,  3.38it/s] 46%|████▋     | 271/585 [02:03<01:32,  3.39it/s] 46%|████▋     | 272/585 [02:03<01:32,  3.39it/s] 47%|████▋     | 273/585 [02:03<01:31,  3.39it/s] 47%|████▋     | 274/585 [02:04<01:31,  3.40it/s] 47%|████▋     | 275/585 [02:04<01:33,  3.31it/s] 47%|████▋     | 276/585 [02:04<01:32,  3.34it/s] 47%|████▋     | 277/585 [02:05<01:31,  3.36it/s] 48%|████▊     | 278/585 [02:05<01:31,  3.37it/s] 48%|████▊     | 279/585 [02:05<01:30,  3.38it/s] 48%|████▊     | 280/585 [02:06<01:30,  3.38it/s] 48%|████▊     | 281/585 [02:06<01:29,  3.39it/s] 48%|████▊     | 282/585 [02:06<01:29,  3.39it/s] 48%|████▊     | 283/585 [02:06<01:29,  3.38it/s] 49%|████▊     | 284/585 [02:07<01:28,  3.39it/s] 49%|████▊     | 285/585 [02:07<01:28,  3.39it/s] 49%|████▉     | 286/585 [02:07<01:29,  3.33it/s] 49%|████▉     | 287/585 [02:08<01:29,  3.35it/s] 49%|████▉     | 288/585 [02:08<01:28,  3.36it/s] 49%|████▉     | 289/585 [02:08<01:27,  3.38it/s] 50%|████▉     | 290/585 [02:09<01:27,  3.38it/s] 50%|████▉     | 291/585 [02:09<01:26,  3.38it/s] 50%|████▉     | 292/585 [02:09<01:26,  3.39it/s] 50%|█████     | 293/585 [02:09<01:26,  3.39it/s] 50%|█████     | 294/585 [02:10<01:25,  3.39it/s] 50%|█████     | 295/585 [02:10<01:25,  3.39it/s] 51%|█████     | 296/585 [02:10<01:25,  3.40it/s] 51%|█████     | 297/585 [02:11<01:26,  3.33it/s] 51%|█████     | 298/585 [02:11<01:25,  3.35it/s] 51%|█████     | 299/585 [02:11<01:25,  3.36it/s] 51%|█████▏    | 300/585 [02:11<01:24,  3.37it/s] 51%|█████▏    | 301/585 [02:12<01:24,  3.38it/s] 52%|█████▏    | 302/585 [02:12<01:23,  3.38it/s] 52%|█████▏    | 303/585 [02:12<01:23,  3.39it/s] 52%|█████▏    | 304/585 [02:13<01:22,  3.39it/s] 52%|█████▏    | 305/585 [02:13<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:13<01:22,  3.39it/s] 52%|█████▏    | 307/585 [02:14<01:21,  3.40it/s] 53%|█████▎    | 308/585 [02:14<01:23,  3.32it/s] 53%|█████▎    | 309/585 [02:14<01:22,  3.34it/s] 53%|█████▎    | 310/585 [02:14<01:21,  3.36it/s] 53%|█████▎    | 311/585 [02:15<01:21,  3.37it/s] 53%|█████▎    | 312/585 [02:15<01:20,  3.38it/s] 54%|█████▎    | 313/585 [02:15<01:20,  3.38it/s] 54%|█████▎    | 314/585 [02:16<01:20,  3.39it/s] 54%|█████▍    | 315/585 [02:16<01:19,  3.39it/s] 54%|█████▍    | 316/585 [02:16<01:19,  3.39it/s] 54%|█████▍    | 317/585 [02:17<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:17<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:17<01:20,  3.31it/s] 55%|█████▍    | 320/585 [02:17<01:19,  3.34it/s] 55%|█████▍    | 321/585 [02:18<01:18,  3.35it/s] 55%|█████▌    | 322/585 [02:18<01:18,  3.36it/s] 55%|█████▌    | 323/585 [02:18<01:17,  3.38it/s] 55%|█████▌    | 324/585 [02:19<01:17,  3.38it/s] 56%|█████▌    | 325/585 [02:19<01:16,  3.38it/s] 56%|█████▌    | 326/585 [02:19<01:16,  3.40it/s] 56%|█████▌    | 327/585 [02:19<01:15,  3.41it/s] 56%|█████▌    | 328/585 [02:20<01:15,  3.42it/s] 56%|█████▌    | 329/585 [02:20<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:20<01:16,  3.35it/s] 57%|█████▋    | 331/585 [02:21<01:15,  3.38it/s] 57%|█████▋    | 332/585 [02:21<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:21<01:13,  3.41it/s] 57%|█████▋    | 334/585 [02:22<01:13,  3.42it/s] 57%|█████▋    | 335/585 [02:22<01:15,  3.32it/s] 57%|█████▋    | 336/585 [02:22<01:14,  3.35it/s] 58%|█████▊    | 337/585 [02:22<01:13,  3.38it/s] 58%|█████▊    | 338/585 [02:23<01:12,  3.40it/s] 58%|█████▊    | 339/585 [02:23<01:12,  3.41it/s] 58%|█████▊    | 340/585 [02:23<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:24<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:24<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:24<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:24<01:10,  3.44it/s] 59%|█████▉    | 345/585 [02:25<01:09,  3.44it/s] 59%|█████▉    | 346/585 [02:25<01:12,  3.27it/s] 59%|█████▉    | 347/585 [02:25<01:11,  3.32it/s] 59%|█████▉    | 348/585 [02:26<01:10,  3.36it/s] 60%|█████▉    | 349/585 [02:26<01:09,  3.38it/s] 60%|█████▉    | 350/585 [02:26<01:09,  3.40it/s] 60%|██████    | 351/585 [02:27<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 01:30:55,926 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:30:55,927 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:30:55,927 >>   Batch size = 8
{'eval_loss': 1.1585907936096191, 'eval_runtime': 9.8528, 'eval_samples_per_second': 354.01, 'eval_steps_per_second': 44.251, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.38it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.91it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.31it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.26it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.11it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.73it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.54it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.79it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.71it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 39.66it/s][A
 14%|█▍        | 62/436 [00:01<00:09, 41.25it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 42.40it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.21it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.71it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.25it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.29it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.01it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.99it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.22it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.54it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.72it/s][A
 28%|██▊       | 122/436 [00:02<00:06, 44.90it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.79it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.86it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.56it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.31it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.38it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.44it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.55it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.70it/s][A
 38%|███▊      | 167/436 [00:03<00:05, 44.85it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.90it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.84it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.52it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.42it/s][A
 44%|████▍     | 192/436 [00:04<00:06, 36.58it/s][A
 45%|████▌     | 197/436 [00:04<00:06, 38.84it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 40.55it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 41.88it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 42.81it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.50it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.94it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.95it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.88it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.01it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.38it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.61it/s][A
 59%|█████▉    | 257/436 [00:05<00:03, 44.75it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.84it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.90it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.77it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.45it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.27it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.35it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.46it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.62it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.73it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.94it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.86it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.71it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.55it/s][A
 75%|███████▌  | 327/436 [00:07<00:03, 34.37it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 37.00it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 39.16it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 40.74it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 42.03it/s][A
 81%|████████  | 352/436 [00:08<00:01, 42.87it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.59it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.87it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.71it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.65it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.85it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.21it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.47it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.58it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.73it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.78it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.73it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.58it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.38it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.45it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.52it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.63it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.63it/s][A 60%|██████    | 351/585 [02:37<01:08,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:31:06,073 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:31:06,411 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:31:10,105 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:31:10,321 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:31:10,411 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:49<26:34,  6.84s/it] 60%|██████    | 353/585 [02:49<18:53,  4.88s/it] 61%|██████    | 354/585 [02:49<13:29,  3.51s/it] 61%|██████    | 355/585 [02:50<09:44,  2.54s/it] 61%|██████    | 356/585 [02:50<07:07,  1.87s/it] 61%|██████    | 357/585 [02:50<05:17,  1.39s/it] 61%|██████    | 358/585 [02:50<04:01,  1.06s/it] 61%|██████▏   | 359/585 [02:51<03:07,  1.20it/s] 62%|██████▏   | 360/585 [02:51<02:30,  1.50it/s] 62%|██████▏   | 361/585 [02:51<02:05,  1.78it/s] 62%|██████▏   | 362/585 [02:52<01:59,  1.86it/s] 62%|██████▏   | 363/585 [02:52<01:44,  2.12it/s] 62%|██████▏   | 364/585 [02:52<01:32,  2.40it/s] 62%|██████▏   | 365/585 [02:53<01:23,  2.64it/s] 63%|██████▎   | 366/585 [02:53<01:17,  2.84it/s] 63%|██████▎   | 367/585 [02:53<01:12,  3.00it/s] 63%|██████▎   | 368/585 [02:54<01:09,  3.12it/s] 63%|██████▎   | 369/585 [02:54<01:07,  3.21it/s] 63%|██████▎   | 370/585 [02:54<01:05,  3.28it/s] 63%|██████▎   | 371/585 [02:54<01:04,  3.33it/s] 64%|██████▎   | 372/585 [02:55<01:03,  3.37it/s] 64%|██████▍   | 373/585 [02:55<01:02,  3.39it/s] 64%|██████▍   | 374/585 [02:55<01:01,  3.41it/s] 64%|██████▍   | 375/585 [02:56<01:03,  3.33it/s] 64%|██████▍   | 376/585 [02:56<01:02,  3.37it/s] 64%|██████▍   | 377/585 [02:56<01:01,  3.39it/s] 65%|██████▍   | 378/585 [02:57<01:00,  3.41it/s] 65%|██████▍   | 379/585 [02:57<01:00,  3.42it/s] 65%|██████▍   | 380/585 [02:57<00:59,  3.43it/s] 65%|██████▌   | 381/585 [02:57<00:59,  3.44it/s] 65%|██████▌   | 382/585 [02:58<00:58,  3.44it/s] 65%|██████▌   | 383/585 [02:58<00:58,  3.44it/s] 66%|██████▌   | 384/585 [02:58<00:58,  3.45it/s] 66%|██████▌   | 385/585 [02:59<00:58,  3.45it/s] 66%|██████▌   | 386/585 [02:59<00:59,  3.32it/s] 66%|██████▌   | 387/585 [02:59<00:59,  3.35it/s] 66%|██████▋   | 388/585 [02:59<00:58,  3.39it/s] 66%|██████▋   | 389/585 [03:00<00:57,  3.40it/s] 67%|██████▋   | 390/585 [03:00<00:57,  3.42it/s] 67%|██████▋   | 391/585 [03:00<00:56,  3.42it/s] 67%|██████▋   | 392/585 [03:01<00:56,  3.43it/s] 67%|██████▋   | 393/585 [03:01<00:55,  3.43it/s] 67%|██████▋   | 394/585 [03:01<00:55,  3.44it/s] 68%|██████▊   | 395/585 [03:01<00:55,  3.44it/s] 68%|██████▊   | 396/585 [03:02<00:54,  3.45it/s] 68%|██████▊   | 397/585 [03:02<00:55,  3.39it/s] 68%|██████▊   | 398/585 [03:02<00:54,  3.41it/s] 68%|██████▊   | 399/585 [03:03<00:54,  3.42it/s] 68%|██████▊   | 400/585 [03:03<00:53,  3.43it/s] 69%|██████▊   | 401/585 [03:03<00:53,  3.43it/s] 69%|██████▊   | 402/585 [03:04<00:53,  3.44it/s] 69%|██████▉   | 403/585 [03:04<00:52,  3.44it/s] 69%|██████▉   | 404/585 [03:04<00:52,  3.44it/s] 69%|██████▉   | 405/585 [03:04<00:52,  3.44it/s] 69%|██████▉   | 406/585 [03:05<00:51,  3.44it/s] 70%|██████▉   | 407/585 [03:05<00:51,  3.45it/s] 70%|██████▉   | 408/585 [03:05<00:52,  3.35it/s] 70%|██████▉   | 409/585 [03:06<00:52,  3.38it/s] 70%|███████   | 410/585 [03:06<00:51,  3.40it/s] 70%|███████   | 411/585 [03:06<00:50,  3.41it/s] 70%|███████   | 412/585 [03:06<00:50,  3.43it/s] 71%|███████   | 413/585 [03:07<00:50,  3.43it/s] 71%|███████   | 414/585 [03:07<00:49,  3.43it/s] 71%|███████   | 415/585 [03:07<00:49,  3.44it/s] 71%|███████   | 416/585 [03:08<00:49,  3.44it/s] 71%|███████▏  | 417/585 [03:08<00:48,  3.44it/s] 71%|███████▏  | 418/585 [03:08<00:48,  3.44it/s] 72%|███████▏  | 419/585 [03:09<00:49,  3.35it/s] 72%|███████▏  | 420/585 [03:09<00:48,  3.38it/s] 72%|███████▏  | 421/585 [03:09<00:48,  3.40it/s] 72%|███████▏  | 422/585 [03:09<00:47,  3.41it/s] 72%|███████▏  | 423/585 [03:10<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:10<00:46,  3.43it/s] 73%|███████▎  | 425/585 [03:10<00:46,  3.44it/s] 73%|███████▎  | 426/585 [03:11<00:46,  3.44it/s] 73%|███████▎  | 427/585 [03:11<00:45,  3.44it/s] 73%|███████▎  | 428/585 [03:11<00:45,  3.44it/s] 73%|███████▎  | 429/585 [03:11<00:45,  3.44it/s] 74%|███████▎  | 430/585 [03:12<00:46,  3.33it/s] 74%|███████▎  | 431/585 [03:12<00:45,  3.37it/s] 74%|███████▍  | 432/585 [03:12<00:45,  3.39it/s] 74%|███████▍  | 433/585 [03:13<00:44,  3.41it/s] 74%|███████▍  | 434/585 [03:13<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:13<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:13<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:14<00:43,  3.44it/s] 75%|███████▍  | 438/585 [03:14<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:14<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:15<00:42,  3.44it/s] 75%|███████▌  | 441/585 [03:15<00:42,  3.37it/s] 76%|███████▌  | 442/585 [03:15<00:42,  3.39it/s] 76%|███████▌  | 443/585 [03:16<00:41,  3.41it/s] 76%|███████▌  | 444/585 [03:16<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:16<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:16<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:17<00:40,  3.44it/s] 77%|███████▋  | 448/585 [03:17<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:17<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:18<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:18<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:18<00:40,  3.29it/s] 77%|███████▋  | 453/585 [03:18<00:39,  3.33it/s] 78%|███████▊  | 454/585 [03:19<00:38,  3.37it/s] 78%|███████▊  | 455/585 [03:19<00:38,  3.39it/s] 78%|███████▊  | 456/585 [03:19<00:37,  3.40it/s] 78%|███████▊  | 457/585 [03:20<00:37,  3.42it/s] 78%|███████▊  | 458/585 [03:20<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:20<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:21<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:21<00:36,  3.44it/s] 79%|███████▉  | 462/585 [03:21<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:21<00:36,  3.33it/s] 79%|███████▉  | 464/585 [03:22<00:35,  3.37it/s] 79%|███████▉  | 465/585 [03:22<00:35,  3.39it/s] 80%|███████▉  | 466/585 [03:22<00:34,  3.41it/s] 80%|███████▉  | 467/585 [03:23<00:34,  3.42it/s] 80%|████████  | 468/585 [03:23<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:31:52,240 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:31:52,240 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:31:52,241 >>   Batch size = 8
{'eval_loss': 1.1738723516464233, 'eval_runtime': 10.0166, 'eval_samples_per_second': 348.223, 'eval_steps_per_second': 43.528, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  2%|▏         | 7/436 [00:00<00:07, 57.83it/s][A
  3%|▎         | 13/436 [00:00<00:08, 50.01it/s][A
  4%|▍         | 19/436 [00:00<00:08, 47.70it/s][A
  6%|▌         | 24/436 [00:00<00:08, 46.87it/s][A
  7%|▋         | 29/436 [00:00<00:08, 46.23it/s][A
  8%|▊         | 34/436 [00:00<00:08, 45.87it/s][A
  9%|▉         | 39/436 [00:00<00:08, 45.58it/s][A
 10%|█         | 44/436 [00:00<00:08, 44.95it/s][A
 11%|█         | 49/436 [00:01<00:08, 44.67it/s][A
 12%|█▏        | 54/436 [00:01<00:08, 44.55it/s][A
 14%|█▎        | 59/436 [00:01<00:08, 44.78it/s][A
 15%|█▍        | 64/436 [00:01<00:08, 44.89it/s][A
 16%|█▌        | 69/436 [00:01<00:08, 44.93it/s][A
 17%|█▋        | 74/436 [00:01<00:08, 44.94it/s][A
 18%|█▊        | 79/436 [00:01<00:07, 44.88it/s][A
 19%|█▉        | 84/436 [00:01<00:07, 44.77it/s][A
 20%|██        | 89/436 [00:01<00:07, 44.46it/s][A
 22%|██▏       | 94/436 [00:02<00:07, 44.35it/s][A
 23%|██▎       | 99/436 [00:02<00:07, 44.41it/s][A
 24%|██▍       | 104/436 [00:02<00:07, 44.54it/s][A
 25%|██▌       | 109/436 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 114/436 [00:02<00:07, 44.64it/s][A
 27%|██▋       | 119/436 [00:02<00:07, 44.75it/s][A
 28%|██▊       | 124/436 [00:02<00:06, 44.80it/s][A
 30%|██▉       | 129/436 [00:02<00:06, 44.53it/s][A
 31%|███       | 134/436 [00:02<00:06, 44.41it/s][A
 32%|███▏      | 139/436 [00:03<00:07, 42.32it/s][A
 33%|███▎      | 144/436 [00:03<00:06, 43.10it/s][A
 34%|███▍      | 149/436 [00:03<00:06, 43.65it/s][A
 35%|███▌      | 154/436 [00:03<00:06, 44.05it/s][A
 36%|███▋      | 159/436 [00:03<00:06, 44.39it/s][A
 38%|███▊      | 164/436 [00:03<00:06, 44.48it/s][A
 39%|███▉      | 169/436 [00:03<00:05, 44.51it/s][A
 40%|███▉      | 174/436 [00:03<00:05, 44.27it/s][A
 41%|████      | 179/436 [00:03<00:05, 44.14it/s][A
 42%|████▏     | 184/436 [00:04<00:05, 44.24it/s][A
 43%|████▎     | 189/436 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 194/436 [00:04<00:05, 44.67it/s][A
 46%|████▌     | 199/436 [00:04<00:05, 44.76it/s][A
 47%|████▋     | 204/436 [00:04<00:05, 44.84it/s][A
 48%|████▊     | 209/436 [00:04<00:05, 44.80it/s][A
 49%|████▉     | 214/436 [00:04<00:04, 44.63it/s][A
 50%|█████     | 219/436 [00:04<00:04, 44.32it/s][A
 51%|█████▏    | 224/436 [00:04<00:04, 44.38it/s][A
 53%|█████▎    | 229/436 [00:05<00:04, 44.40it/s][A
 54%|█████▎    | 234/436 [00:05<00:04, 44.56it/s][A
 55%|█████▍    | 239/436 [00:05<00:04, 44.75it/s][A
 56%|█████▌    | 244/436 [00:05<00:04, 44.84it/s][A
 57%|█████▋    | 249/436 [00:05<00:04, 44.90it/s][A
 58%|█████▊    | 254/436 [00:05<00:04, 44.77it/s][A
 59%|█████▉    | 259/436 [00:05<00:03, 44.50it/s][A
 61%|██████    | 264/436 [00:05<00:03, 44.40it/s][A
 62%|██████▏   | 269/436 [00:06<00:03, 44.36it/s][A
 63%|██████▎   | 274/436 [00:06<00:04, 39.48it/s][A
 64%|██████▍   | 279/436 [00:06<00:03, 41.04it/s][A
 65%|██████▌   | 284/436 [00:06<00:03, 42.14it/s][A
 66%|██████▋   | 289/436 [00:06<00:03, 42.98it/s][A
 67%|██████▋   | 294/436 [00:06<00:03, 43.53it/s][A
 69%|██████▊   | 299/436 [00:06<00:03, 44.07it/s][A
 70%|██████▉   | 304/436 [00:06<00:02, 44.37it/s][A
 71%|███████   | 309/436 [00:06<00:02, 44.23it/s][A
 72%|███████▏  | 314/436 [00:07<00:02, 43.97it/s][A
 73%|███████▎  | 319/436 [00:07<00:02, 43.86it/s][A
 74%|███████▍  | 324/436 [00:07<00:02, 44.16it/s][A
 75%|███████▌  | 329/436 [00:07<00:02, 44.39it/s][A
 77%|███████▋  | 334/436 [00:07<00:02, 44.65it/s][A
 78%|███████▊  | 339/436 [00:07<00:02, 44.86it/s][A
 79%|███████▉  | 344/436 [00:07<00:02, 44.95it/s][A
 80%|████████  | 349/436 [00:07<00:01, 44.80it/s][A
 81%|████████  | 354/436 [00:07<00:01, 44.63it/s][A
 82%|████████▏ | 359/436 [00:08<00:01, 44.36it/s][A
 83%|████████▎ | 364/436 [00:08<00:01, 44.14it/s][A
 85%|████████▍ | 369/436 [00:08<00:01, 44.25it/s][A
 86%|████████▌ | 374/436 [00:08<00:01, 44.53it/s][A
 87%|████████▋ | 379/436 [00:08<00:01, 44.77it/s][A
 88%|████████▊ | 384/436 [00:08<00:01, 44.91it/s][A
 89%|████████▉ | 389/436 [00:08<00:01, 44.78it/s][A
 90%|█████████ | 394/436 [00:08<00:00, 44.81it/s][A
 92%|█████████▏| 399/436 [00:08<00:00, 44.61it/s][A
 93%|█████████▎| 404/436 [00:09<00:00, 44.37it/s][A
 94%|█████████▍| 409/436 [00:09<00:00, 36.27it/s][A
 95%|█████████▍| 414/436 [00:09<00:00, 38.58it/s][A
 96%|█████████▌| 419/436 [00:09<00:00, 40.37it/s][A
 97%|█████████▋| 424/436 [00:09<00:00, 41.70it/s][A
 98%|█████████▊| 429/436 [00:09<00:00, 42.56it/s][A
100%|█████████▉| 434/436 [00:09<00:00, 43.40it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.40it/s][A 80%|████████  | 468/585 [03:33<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:32:02,880 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:32:03,580 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:32:06,989 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:32:07,137 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:32:07,206 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:44<12:48,  6.62s/it] 80%|████████  | 470/585 [03:45<09:03,  4.73s/it] 81%|████████  | 471/585 [03:45<06:27,  3.40s/it] 81%|████████  | 472/585 [03:45<04:38,  2.46s/it] 81%|████████  | 473/585 [03:45<03:22,  1.81s/it] 81%|████████  | 474/585 [03:46<02:30,  1.36s/it] 81%|████████  | 475/585 [03:46<01:53,  1.04s/it] 81%|████████▏ | 476/585 [03:46<01:28,  1.23it/s] 82%|████████▏ | 477/585 [03:47<01:10,  1.53it/s] 82%|████████▏ | 478/585 [03:47<00:58,  1.83it/s] 82%|████████▏ | 479/585 [03:47<00:49,  2.13it/s] 82%|████████▏ | 480/585 [03:47<00:43,  2.41it/s] 82%|████████▏ | 481/585 [03:48<00:39,  2.60it/s] 82%|████████▏ | 482/585 [03:48<00:36,  2.81it/s] 83%|████████▎ | 483/585 [03:48<00:34,  2.96it/s] 83%|████████▎ | 484/585 [03:49<00:32,  3.08it/s] 83%|████████▎ | 485/585 [03:49<00:31,  3.17it/s] 83%|████████▎ | 486/585 [03:49<00:30,  3.24it/s] 83%|████████▎ | 487/585 [03:50<00:29,  3.29it/s] 83%|████████▎ | 488/585 [03:50<00:29,  3.32it/s] 84%|████████▎ | 489/585 [03:50<00:28,  3.34it/s] 84%|████████▍ | 490/585 [03:50<00:28,  3.36it/s] 84%|████████▍ | 491/585 [03:51<00:27,  3.37it/s] 84%|████████▍ | 492/585 [03:51<00:28,  3.30it/s] 84%|████████▍ | 493/585 [03:51<00:28,  3.28it/s] 84%|████████▍ | 494/585 [03:52<00:33,  2.70it/s] 85%|████████▍ | 495/585 [03:52<00:31,  2.88it/s] 85%|████████▍ | 496/585 [03:52<00:29,  3.02it/s] 85%|████████▍ | 497/585 [03:53<00:28,  3.12it/s] 85%|████████▌ | 498/585 [03:53<00:27,  3.20it/s] 85%|████████▌ | 499/585 [03:53<00:26,  3.26it/s] 85%|████████▌ | 500/585 [03:54<00:25,  3.30it/s]                                                  85%|████████▌ | 500/585 [03:54<00:25,  3.30it/s] 86%|████████▌ | 501/585 [03:54<00:25,  3.33it/s] 86%|████████▌ | 502/585 [03:54<00:24,  3.35it/s] 86%|████████▌ | 503/585 [03:55<00:24,  3.36it/s] 86%|████████▌ | 504/585 [03:55<00:23,  3.38it/s] 86%|████████▋ | 505/585 [03:55<00:23,  3.39it/s] 86%|████████▋ | 506/585 [03:55<00:23,  3.39it/s] 87%|████████▋ | 507/585 [03:56<00:22,  3.39it/s] 87%|████████▋ | 508/585 [03:56<00:22,  3.40it/s] 87%|████████▋ | 509/585 [03:56<00:22,  3.40it/s] 87%|████████▋ | 510/585 [03:57<00:22,  3.40it/s] 87%|████████▋ | 511/585 [03:57<00:21,  3.40it/s] 88%|████████▊ | 512/585 [03:57<00:21,  3.34it/s] 88%|████████▊ | 513/585 [03:57<00:21,  3.36it/s] 88%|████████▊ | 514/585 [03:58<00:21,  3.37it/s] 88%|████████▊ | 515/585 [03:58<00:20,  3.38it/s] 88%|████████▊ | 516/585 [03:58<00:20,  3.38it/s] 88%|████████▊ | 517/585 [03:59<00:20,  3.39it/s] 89%|████████▊ | 518/585 [03:59<00:19,  3.40it/s] 89%|████████▊ | 519/585 [03:59<00:19,  3.40it/s] 89%|████████▉ | 520/585 [04:00<00:19,  3.40it/s] 89%|████████▉ | 521/585 [04:00<00:18,  3.40it/s] 89%|████████▉ | 522/585 [04:00<00:18,  3.40it/s] 89%|████████▉ | 523/585 [04:00<00:18,  3.32it/s] 90%|████████▉ | 524/585 [04:01<00:18,  3.34it/s] 90%|████████▉ | 525/585 [04:01<00:17,  3.36it/s] 90%|████████▉ | 526/585 [04:01<00:17,  3.37it/s] 90%|█████████ | 527/585 [04:02<00:17,  3.38it/s] 90%|█████████ | 528/585 [04:02<00:16,  3.38it/s] 90%|█████████ | 529/585 [04:02<00:16,  3.39it/s] 91%|█████████ | 530/585 [04:02<00:16,  3.39it/s] 91%|█████████ | 531/585 [04:03<00:15,  3.39it/s] 91%|█████████ | 532/585 [04:03<00:15,  3.39it/s] 91%|█████████ | 533/585 [04:03<00:15,  3.40it/s] 91%|█████████▏| 534/585 [04:04<00:15,  3.35it/s] 91%|█████████▏| 535/585 [04:04<00:14,  3.37it/s] 92%|█████████▏| 536/585 [04:04<00:14,  3.37it/s] 92%|█████████▏| 537/585 [04:05<00:14,  3.38it/s] 92%|█████████▏| 538/585 [04:05<00:13,  3.38it/s] 92%|█████████▏| 539/585 [04:05<00:13,  3.39it/s] 92%|█████████▏| 540/585 [04:05<00:13,  3.39it/s] 92%|█████████▏| 541/585 [04:06<00:12,  3.39it/s] 93%|█████████▎| 542/585 [04:06<00:12,  3.39it/s] 93%|█████████▎| 543/585 [04:06<00:12,  3.40it/s] 93%|█████████▎| 544/585 [04:07<00:12,  3.39it/s] 93%|█████████▎| 545/585 [04:07<00:12,  3.26it/s] 93%|█████████▎| 546/585 [04:07<00:11,  3.30it/s] 94%|█████████▎| 547/585 [04:08<00:11,  3.33it/s] 94%|█████████▎| 548/585 [04:08<00:11,  3.35it/s] 94%|█████████▍| 549/585 [04:08<00:10,  3.36it/s] 94%|█████████▍| 550/585 [04:08<00:10,  3.38it/s] 94%|█████████▍| 551/585 [04:09<00:10,  3.38it/s] 94%|█████████▍| 552/585 [04:09<00:09,  3.38it/s] 95%|█████████▍| 553/585 [04:09<00:09,  3.39it/s] 95%|█████████▍| 554/585 [04:10<00:09,  3.39it/s] 95%|█████████▍| 555/585 [04:10<00:08,  3.39it/s] 95%|█████████▌| 556/585 [04:10<00:08,  3.30it/s] 95%|█████████▌| 557/585 [04:11<00:08,  3.32it/s] 95%|█████████▌| 558/585 [04:11<00:08,  3.35it/s] 96%|█████████▌| 559/585 [04:11<00:07,  3.36it/s] 96%|█████████▌| 560/585 [04:11<00:07,  3.37it/s] 96%|█████████▌| 561/585 [04:12<00:07,  3.38it/s] 96%|█████████▌| 562/585 [04:12<00:06,  3.40it/s] 96%|█████████▌| 563/585 [04:12<00:06,  3.41it/s] 96%|█████████▋| 564/585 [04:13<00:06,  3.42it/s] 97%|█████████▋| 565/585 [04:13<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:13<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:13<00:05,  3.31it/s] 97%|█████████▋| 568/585 [04:14<00:05,  3.35it/s] 97%|█████████▋| 569/585 [04:14<00:04,  3.38it/s] 97%|█████████▋| 570/585 [04:14<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:15<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:15<00:03,  3.42it/s] 98%|█████████▊| 573/585 [04:15<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:16<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:16<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:16<00:02,  3.44it/s] 99%|█████████▊| 577/585 [04:16<00:02,  3.44it/s] 99%|█████████▉| 578/585 [04:17<00:02,  3.33it/s] 99%|█████████▉| 579/585 [04:17<00:01,  3.36it/s] 99%|█████████▉| 580/585 [04:17<00:01,  3.39it/s] 99%|█████████▉| 581/585 [04:18<00:01,  3.40it/s] 99%|█████████▉| 582/585 [04:18<00:00,  3.42it/s]100%|█████████▉| 583/585 [04:18<00:00,  3.42it/s]100%|█████████▉| 584/585 [04:18<00:00,  3.43it/s]100%|██████████| 585/585 [04:19<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:32:48,074 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:32:48,074 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:32:48,074 >>   Batch size = 8
{'eval_loss': 1.1901423931121826, 'eval_runtime': 10.0025, 'eval_samples_per_second': 348.714, 'eval_steps_per_second': 43.589, 'epoch': 4.0}
{'loss': 0.3183, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.66it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.12it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.42it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.85it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.28it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.97it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.91it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.08it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.45it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.53it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.52it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.63it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.53it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.34it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.45it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.57it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.64it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.73it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.77it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.69it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.52it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.35it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.41it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.52it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.56it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.75it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.76it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.57it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.52it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.58it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 42.63it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.35it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.84it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.42it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.35it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.32it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.38it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.19it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.29it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.49it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.73it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.64it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.78it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.60it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.55it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.46it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.50it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.92it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.23it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.63it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.63it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.61it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.23it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.23it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.42it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.63it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.68it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.76it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.80it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.76it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.57it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.31it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.46it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.52it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.59it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.62it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.67it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.76it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.57it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.33it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.40it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 40.66it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 41.94it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 42.80it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.39it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.90it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.33it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.52it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.49it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.13it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.13it/s][A100%|██████████| 585/585 [04:29<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:32:58,095 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:32:58,516 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:33:01,837 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:33:02,083 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:33:02,236 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:33:10,745 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:33:10,762 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117 (score: 1.143255352973938).
                                                 100%|██████████| 585/585 [04:50<00:00,  3.43it/s]100%|██████████| 585/585 [04:50<00:00,  2.01it/s]
[INFO|trainer.py:1894] 2023-08-29 01:33:19,492 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:33:19,641 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:33:22,515 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:33:22,676 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:33:22,744 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:33:23,429 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   train_loss               =      0.316
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   train_runtime            = 0:04:50.56
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   train_samples_per_second =     129.06
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:23,429 >>   train_steps_per_second   =      2.013
{'eval_loss': 1.1954236030578613, 'eval_runtime': 9.8447, 'eval_samples_per_second': 354.304, 'eval_steps_per_second': 44.288, 'epoch': 5.0}
{'train_runtime': 290.5624, 'train_samples_per_second': 129.06, 'train_steps_per_second': 2.013, 'train_loss': 0.3159792940840762, 'epoch': 5.0}
08/29/2023 01:33:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:33:23,646 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:33:23,646 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 01:33:23,646 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.24it/s]  3%|▎         | 12/436 [00:00<00:08, 48.98it/s]  4%|▍         | 17/436 [00:00<00:08, 47.35it/s]  5%|▌         | 22/436 [00:00<00:08, 46.54it/s]  6%|▌         | 27/436 [00:00<00:08, 46.00it/s]  7%|▋         | 32/436 [00:00<00:08, 45.64it/s]  8%|▊         | 37/436 [00:00<00:08, 45.55it/s] 10%|▉         | 42/436 [00:00<00:08, 45.20it/s] 11%|█         | 47/436 [00:01<00:08, 44.70it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.62it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.72it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.89it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.96it/s] 17%|█▋        | 72/436 [00:01<00:08, 45.00it/s] 18%|█▊        | 77/436 [00:01<00:07, 44.96it/s] 19%|█▉        | 82/436 [00:01<00:07, 44.90it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.59it/s] 21%|██        | 92/436 [00:02<00:07, 43.59it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.98it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.30it/s] 25%|██▍       | 107/436 [00:02<00:07, 41.91it/s] 26%|██▌       | 112/436 [00:02<00:07, 42.99it/s] 27%|██▋       | 117/436 [00:02<00:07, 43.58it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.09it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.20it/s] 30%|███       | 132/436 [00:02<00:06, 44.16it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.29it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.43it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.30it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.42it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.50it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.84it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.82it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.84it/s] 41%|████      | 177/436 [00:03<00:05, 44.71it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.63it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.55it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.39it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.52it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.63it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.84it/s] 49%|████▊     | 212/436 [00:04<00:04, 44.82it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.77it/s] 51%|█████     | 222/436 [00:04<00:04, 44.76it/s] 52%|█████▏    | 227/436 [00:05<00:04, 42.81it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.36it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.74it/s] 56%|█████▌    | 242/436 [00:05<00:04, 43.97it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.20it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.36it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.57it/s] 60%|██████    | 262/436 [00:05<00:03, 44.58it/s] 61%|██████    | 267/436 [00:06<00:04, 41.21it/s] 62%|██████▏   | 272/436 [00:06<00:03, 42.31it/s] 64%|██████▎   | 277/436 [00:06<00:03, 43.04it/s] 65%|██████▍   | 282/436 [00:06<00:03, 43.48it/s] 66%|██████▌   | 287/436 [00:06<00:03, 43.65it/s] 67%|██████▋   | 292/436 [00:06<00:03, 43.94it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.14it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.46it/s] 70%|███████   | 307/436 [00:06<00:02, 44.22it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.37it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.62it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.69it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.64it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.51it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.66it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.73it/s] 80%|███████▉  | 347/436 [00:07<00:01, 44.58it/s] 81%|████████  | 352/436 [00:07<00:01, 44.55it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.61it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.68it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.84it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.63it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.50it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.56it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.62it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.60it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.51it/s] 92%|█████████▏| 402/436 [00:09<00:00, 43.65it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.17it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.37it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.28it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.36it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.53it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.63it/s]100%|██████████| 436/436 [00:09<00:00, 44.38it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:33:33,487 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   eval_loss               =     1.1433
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   eval_samples_per_second =    354.433
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   eval_steps_per_second   =     44.304
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:33:33,487 >>   perplexity              =      3.137
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:42,353 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:42,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:42,385 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:42,385 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:42,385 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:33:42,990 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:33:42,992 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:33:43,286 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:33:44,404 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:33:44,404 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:47,022 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:47,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:47,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:47,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:47,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:33:47,886 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:33:47,887 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:33:48,595 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:33:48,795 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:33:48,795 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.65it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.45it/s]Extractor Predicting: 34it [00:21,  1.47it/s]Extractor Predicting: 35it [00:22,  1.45it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:23,  1.47it/s]Extractor Predicting: 38it [00:24,  1.48it/s]Extractor Predicting: 39it [00:25,  1.49it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:27,  1.47it/s]Extractor Predicting: 44it [00:28,  1.45it/s]Extractor Predicting: 45it [00:29,  1.44it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.51it/s]Extractor Predicting: 70it [00:45,  1.51it/s]Extractor Predicting: 71it [00:46,  1.47it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:48,  1.49it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.51it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:51,  1.51it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:57,  1.55it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.58it/s]Extractor Predicting: 91it [00:59,  1.61it/s]Extractor Predicting: 92it [01:00,  1.62it/s]Extractor Predicting: 93it [01:00,  1.60it/s]Extractor Predicting: 94it [01:01,  1.62it/s]Extractor Predicting: 95it [01:02,  1.62it/s]Extractor Predicting: 96it [01:02,  1.61it/s]Extractor Predicting: 97it [01:03,  1.60it/s]Extractor Predicting: 98it [01:04,  1.57it/s]Extractor Predicting: 99it [01:04,  1.54it/s]Extractor Predicting: 100it [01:05,  1.54it/s]Extractor Predicting: 101it [01:06,  1.56it/s]Extractor Predicting: 102it [01:06,  1.62it/s]Extractor Predicting: 103it [01:07,  1.60it/s]Extractor Predicting: 104it [01:07,  1.62it/s]Extractor Predicting: 105it [01:08,  1.61it/s]Extractor Predicting: 106it [01:09,  1.60it/s]Extractor Predicting: 107it [01:09,  1.59it/s]Extractor Predicting: 108it [01:10,  1.45it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:12,  1.56it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:13,  1.63it/s]Extractor Predicting: 114it [01:14,  1.58it/s]Extractor Predicting: 115it [01:14,  1.61it/s]Extractor Predicting: 116it [01:15,  1.61it/s]Extractor Predicting: 117it [01:16,  1.58it/s]Extractor Predicting: 118it [01:16,  1.56it/s]Extractor Predicting: 119it [01:17,  1.51it/s]Extractor Predicting: 120it [01:18,  1.50it/s]Extractor Predicting: 121it [01:18,  1.53it/s]Extractor Predicting: 122it [01:19,  1.53it/s]Extractor Predicting: 123it [01:20,  1.53it/s]Extractor Predicting: 124it [01:20,  1.47it/s]Extractor Predicting: 125it [01:21,  1.47it/s]Extractor Predicting: 126it [01:22,  1.46it/s]Extractor Predicting: 127it [01:22,  1.48it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:24,  1.53it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 134it [01:27,  1.46it/s]Extractor Predicting: 135it [01:28,  1.47it/s]Extractor Predicting: 136it [01:28,  1.50it/s]Extractor Predicting: 137it [01:29,  1.51it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:30,  1.48it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:32,  1.50it/s]Extractor Predicting: 142it [01:32,  1.47it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 144it [01:33,  1.79it/s]Extractor Predicting: 144it [01:33,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:34,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:34,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:34,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:34,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:34,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:35:34,629 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:35:34,630 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:35:35,345 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:35:36,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:35:36,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:38,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:38,728 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:38,728 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:38,728 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:35:38,728 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:35:39,287 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:35:39,288 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:35:39,614 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:35:39,845 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:35:39,845 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.24203133441383037,
  "recall": 0.12844036697247707,
  "score": 0.16782168945495413,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.54it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.51it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:16,  1.53it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:21,  1.61it/s]Extractor Predicting: 33it [00:21,  1.62it/s]Extractor Predicting: 34it [00:22,  1.65it/s]Extractor Predicting: 35it [00:22,  1.68it/s]Extractor Predicting: 36it [00:23,  1.68it/s]Extractor Predicting: 37it [00:24,  1.69it/s]Extractor Predicting: 38it [00:24,  1.72it/s]Extractor Predicting: 39it [00:25,  1.72it/s]Extractor Predicting: 40it [00:25,  1.75it/s]Extractor Predicting: 41it [00:26,  1.74it/s]Extractor Predicting: 42it [00:26,  1.76it/s]Extractor Predicting: 43it [00:27,  1.74it/s]Extractor Predicting: 44it [00:28,  1.78it/s]Extractor Predicting: 45it [00:28,  1.73it/s]Extractor Predicting: 46it [00:29,  1.67it/s]Extractor Predicting: 47it [00:29,  1.72it/s]Extractor Predicting: 48it [00:30,  1.77it/s]Extractor Predicting: 49it [00:30,  1.78it/s]Extractor Predicting: 50it [00:31,  1.77it/s]Extractor Predicting: 51it [00:32,  1.78it/s]Extractor Predicting: 52it [00:32,  1.76it/s]Extractor Predicting: 53it [00:33,  1.75it/s]Extractor Predicting: 54it [00:33,  1.74it/s]Extractor Predicting: 55it [00:34,  1.71it/s]Extractor Predicting: 56it [00:35,  1.73it/s]Extractor Predicting: 57it [00:35,  1.69it/s]Extractor Predicting: 58it [00:36,  1.67it/s]Extractor Predicting: 59it [00:36,  1.63it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.52it/s]Extractor Predicting: 62it [00:39,  1.45it/s]Extractor Predicting: 63it [00:39,  1.47it/s]Extractor Predicting: 64it [00:40,  1.45it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:41,  1.49it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.46it/s]Extractor Predicting: 69it [00:43,  1.45it/s]Extractor Predicting: 70it [00:44,  1.44it/s]Extractor Predicting: 71it [00:45,  1.41it/s]Extractor Predicting: 72it [00:45,  1.44it/s]Extractor Predicting: 73it [00:46,  1.44it/s]Extractor Predicting: 74it [00:47,  1.47it/s]Extractor Predicting: 75it [00:47,  1.48it/s]Extractor Predicting: 76it [00:48,  1.48it/s]Extractor Predicting: 77it [00:49,  1.47it/s]Extractor Predicting: 78it [00:50,  1.44it/s]Extractor Predicting: 79it [00:50,  1.43it/s]Extractor Predicting: 80it [00:51,  1.44it/s]Extractor Predicting: 81it [00:52,  1.43it/s]Extractor Predicting: 82it [00:52,  1.43it/s]Extractor Predicting: 83it [00:53,  1.29it/s]Extractor Predicting: 84it [00:54,  1.36it/s]Extractor Predicting: 85it [00:55,  1.37it/s]Extractor Predicting: 86it [00:55,  1.37it/s]Extractor Predicting: 87it [00:56,  1.38it/s]Extractor Predicting: 88it [00:57,  1.41it/s]Extractor Predicting: 89it [00:57,  1.42it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.41it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.46it/s]Extractor Predicting: 95it [01:01,  1.50it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:03,  1.55it/s]Extractor Predicting: 99it [01:04,  1.54it/s]Extractor Predicting: 100it [01:05,  1.54it/s]Extractor Predicting: 101it [01:05,  1.59it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:07,  1.49it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.50it/s]Extractor Predicting: 107it [01:09,  1.50it/s]Extractor Predicting: 108it [01:10,  1.50it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:11,  1.47it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.45it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:14,  1.46it/s]Extractor Predicting: 115it [01:15,  1.47it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.52it/s]Extractor Predicting: 118it [01:17,  1.58it/s]Extractor Predicting: 119it [01:17,  1.61it/s]Extractor Predicting: 120it [01:18,  1.62it/s]Extractor Predicting: 121it [01:18,  1.65it/s]Extractor Predicting: 122it [01:19,  1.64it/s]Extractor Predicting: 123it [01:20,  1.61it/s]Extractor Predicting: 124it [01:20,  1.64it/s]Extractor Predicting: 125it [01:21,  1.66it/s]Extractor Predicting: 126it [01:21,  1.65it/s]Extractor Predicting: 127it [01:22,  1.65it/s]Extractor Predicting: 128it [01:23,  1.68it/s]Extractor Predicting: 129it [01:23,  1.67it/s]Extractor Predicting: 130it [01:24,  1.76it/s]Extractor Predicting: 131it [01:24,  1.78it/s]Extractor Predicting: 132it [01:25,  1.74it/s]Extractor Predicting: 133it [01:25,  1.69it/s]Extractor Predicting: 134it [01:26,  1.65it/s]Extractor Predicting: 135it [01:27,  1.67it/s]Extractor Predicting: 136it [01:27,  1.71it/s]Extractor Predicting: 137it [01:28,  1.72it/s]Extractor Predicting: 138it [01:28,  1.72it/s]Extractor Predicting: 139it [01:29,  1.69it/s]Extractor Predicting: 140it [01:30,  1.64it/s]Extractor Predicting: 141it [01:30,  1.65it/s]Extractor Predicting: 142it [01:31,  1.72it/s]Extractor Predicting: 143it [01:31,  1.72it/s]Extractor Predicting: 144it [01:32,  1.70it/s]Extractor Predicting: 145it [01:33,  1.69it/s]Extractor Predicting: 146it [01:33,  1.62it/s]Extractor Predicting: 147it [01:34,  1.61it/s]Extractor Predicting: 148it [01:34,  1.61it/s]Extractor Predicting: 149it [01:35,  1.59it/s]Extractor Predicting: 150it [01:36,  1.58it/s]Extractor Predicting: 151it [01:36,  1.58it/s]Extractor Predicting: 152it [01:37,  1.59it/s]Extractor Predicting: 153it [01:38,  1.55it/s]Extractor Predicting: 154it [01:38,  1.58it/s]Extractor Predicting: 155it [01:39,  1.61it/s]Extractor Predicting: 156it [01:40,  1.56it/s]Extractor Predicting: 157it [01:40,  1.58it/s]Extractor Predicting: 158it [01:41,  1.56it/s]Extractor Predicting: 159it [01:42,  1.54it/s]Extractor Predicting: 160it [01:42,  1.53it/s]Extractor Predicting: 161it [01:43,  1.51it/s]Extractor Predicting: 162it [01:44,  1.51it/s]Extractor Predicting: 163it [01:44,  1.55it/s]Extractor Predicting: 164it [01:45,  1.55it/s]Extractor Predicting: 165it [01:45,  1.53it/s]Extractor Predicting: 166it [01:46,  1.48it/s]Extractor Predicting: 167it [01:47,  1.52it/s]Extractor Predicting: 168it [01:47,  1.51it/s]Extractor Predicting: 169it [01:48,  1.54it/s]Extractor Predicting: 170it [01:49,  1.53it/s]Extractor Predicting: 171it [01:49,  1.54it/s]Extractor Predicting: 172it [01:50,  1.56it/s]Extractor Predicting: 173it [01:51,  1.52it/s]Extractor Predicting: 174it [01:51,  1.52it/s]Extractor Predicting: 175it [01:52,  1.55it/s]Extractor Predicting: 176it [01:53,  1.55it/s]Extractor Predicting: 177it [01:53,  1.53it/s]Extractor Predicting: 178it [01:54,  1.53it/s]Extractor Predicting: 179it [01:55,  1.53it/s]Extractor Predicting: 180it [01:55,  1.53it/s]Extractor Predicting: 181it [01:56,  1.52it/s]Extractor Predicting: 182it [01:57,  1.53it/s]Extractor Predicting: 183it [01:57,  1.51it/s]Extractor Predicting: 184it [01:58,  1.52it/s]Extractor Predicting: 185it [01:59,  1.53it/s]Extractor Predicting: 186it [01:59,  1.52it/s]Extractor Predicting: 187it [02:00,  1.54it/s]Extractor Predicting: 188it [02:01,  1.53it/s]Extractor Predicting: 189it [02:01,  1.55it/s]Extractor Predicting: 190it [02:02,  1.55it/s]Extractor Predicting: 191it [02:02,  1.57it/s]Extractor Predicting: 192it [02:03,  1.57it/s]Extractor Predicting: 193it [02:04,  1.59it/s]Extractor Predicting: 194it [02:04,  1.56it/s]Extractor Predicting: 195it [02:05,  1.57it/s]Extractor Predicting: 196it [02:06,  1.57it/s]Extractor Predicting: 197it [02:06,  1.40it/s]Extractor Predicting: 198it [02:07,  1.38it/s]Extractor Predicting: 199it [02:08,  1.42it/s]Extractor Predicting: 200it [02:09,  1.44it/s]Extractor Predicting: 201it [02:09,  1.50it/s]Extractor Predicting: 202it [02:10,  1.49it/s]Extractor Predicting: 203it [02:10,  1.51it/s]Extractor Predicting: 204it [02:11,  1.49it/s]Extractor Predicting: 205it [02:12,  1.49it/s]Extractor Predicting: 206it [02:12,  1.56it/s]Extractor Predicting: 207it [02:13,  1.55it/s]Extractor Predicting: 208it [02:14,  1.57it/s]Extractor Predicting: 209it [02:14,  1.55it/s]Extractor Predicting: 210it [02:15,  1.55it/s]Extractor Predicting: 211it [02:16,  1.53it/s]Extractor Predicting: 212it [02:16,  1.50it/s]Extractor Predicting: 213it [02:17,  1.49it/s]Extractor Predicting: 214it [02:18,  1.50it/s]Extractor Predicting: 215it [02:18,  1.48it/s]Extractor Predicting: 216it [02:19,  1.49it/s]Extractor Predicting: 217it [02:20,  1.45it/s]Extractor Predicting: 218it [02:20,  1.48it/s]Extractor Predicting: 219it [02:21,  1.45it/s]Extractor Predicting: 220it [02:22,  1.49it/s]Extractor Predicting: 221it [02:22,  1.49it/s]Extractor Predicting: 222it [02:23,  1.49it/s]Extractor Predicting: 223it [02:24,  1.51it/s]Extractor Predicting: 224it [02:25,  1.47it/s]Extractor Predicting: 225it [02:25,  1.47it/s]Extractor Predicting: 226it [02:26,  1.49it/s]Extractor Predicting: 227it [02:27,  1.48it/s]Extractor Predicting: 228it [02:27,  1.53it/s]Extractor Predicting: 229it [02:28,  1.45it/s]Extractor Predicting: 230it [02:29,  1.48it/s]Extractor Predicting: 231it [02:29,  1.46it/s]Extractor Predicting: 232it [02:30,  1.46it/s]Extractor Predicting: 233it [02:31,  1.48it/s]Extractor Predicting: 234it [02:31,  1.45it/s]Extractor Predicting: 235it [02:32,  1.48it/s]Extractor Predicting: 236it [02:33,  1.51it/s]Extractor Predicting: 237it [02:33,  1.45it/s]Extractor Predicting: 238it [02:34,  1.44it/s]Extractor Predicting: 239it [02:35,  1.37it/s]Extractor Predicting: 240it [02:36,  1.42it/s]Extractor Predicting: 241it [02:36,  1.43it/s]Extractor Predicting: 242it [02:37,  1.47it/s]Extractor Predicting: 243it [02:38,  1.47it/s]Extractor Predicting: 244it [02:38,  1.47it/s]Extractor Predicting: 245it [02:39,  1.48it/s]Extractor Predicting: 246it [02:40,  1.46it/s]Extractor Predicting: 247it [02:40,  1.42it/s]Extractor Predicting: 248it [02:41,  1.44it/s]Extractor Predicting: 249it [02:42,  1.48it/s]Extractor Predicting: 250it [02:42,  1.51it/s]Extractor Predicting: 251it [02:43,  1.50it/s]Extractor Predicting: 252it [02:44,  1.53it/s]Extractor Predicting: 253it [02:44,  1.54it/s]Extractor Predicting: 254it [02:45,  1.49it/s]Extractor Predicting: 255it [02:46,  1.50it/s]Extractor Predicting: 256it [02:46,  1.48it/s]Extractor Predicting: 257it [02:47,  1.50it/s]Extractor Predicting: 258it [02:48,  1.49it/s]Extractor Predicting: 259it [02:48,  1.51it/s]Extractor Predicting: 260it [02:49,  1.53it/s]Extractor Predicting: 261it [02:49,  1.56it/s]Extractor Predicting: 262it [02:50,  1.55it/s]Extractor Predicting: 263it [02:51,  1.54it/s]Extractor Predicting: 264it [02:51,  1.56it/s]Extractor Predicting: 265it [02:52,  1.56it/s]Extractor Predicting: 266it [02:53,  1.55it/s]Extractor Predicting: 267it [02:53,  1.58it/s]Extractor Predicting: 268it [02:54,  1.58it/s]Extractor Predicting: 269it [02:55,  1.57it/s]Extractor Predicting: 270it [02:55,  1.53it/s]Extractor Predicting: 271it [02:56,  1.57it/s]Extractor Predicting: 272it [02:57,  1.57it/s]Extractor Predicting: 273it [02:57,  1.59it/s]Extractor Predicting: 274it [02:58,  1.60it/s]Extractor Predicting: 275it [02:58,  1.59it/s]Extractor Predicting: 276it [02:59,  1.56it/s]Extractor Predicting: 277it [03:00,  1.57it/s]Extractor Predicting: 278it [03:01,  1.41it/s]Extractor Predicting: 279it [03:01,  1.45it/s]Extractor Predicting: 280it [03:02,  1.45it/s]Extractor Predicting: 281it [03:03,  1.51it/s]Extractor Predicting: 282it [03:03,  1.51it/s]Extractor Predicting: 283it [03:04,  1.47it/s]Extractor Predicting: 284it [03:04,  1.69it/s]Extractor Predicting: 284it [03:04,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:38:55,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:38:55,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:38:55,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:38:55,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:38:55,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:38:56,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:38:56,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:38:57,369 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:38:58,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:38:58,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:39:01,463 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:39:01,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:39:01,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:39:01,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:39:01,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:39:02,340 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:39:02,428 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:39:03,077 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:39:03,333 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:39:03,334 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.26162310866574967,
  "recall": 0.1398118200529256,
  "score": 0.18223627479160678,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.30it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.89it/s]Extractor Predicting: 5it [00:03,  1.61it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2682926829268293,
  "recall": 0.05555555555555555,
  "score": 0.09205020920502091,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
