Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:18<04:16, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:38, 16.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<02:53, 14.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:01<02:44, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:18<02:37, 15.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:33<02:19, 15.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:49<02:05, 15.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:03<01:46, 15.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:20<01:34, 15.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:35<01:17, 15.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:48<00:59, 14.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:48, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:24<00:32, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:38<00:15, 15.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:54<00:00, 15.75s/it]Generating: 100%|██████████| 15/15 [03:54<00:00, 15.64s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.75s/it]Extractor Estimating: 2it [00:19,  8.07s/it]Extractor Estimating: 3it [00:19,  4.66s/it]Extractor Estimating: 4it [00:20,  3.04s/it]Extractor Estimating: 5it [00:20,  2.15s/it]Extractor Estimating: 6it [00:21,  1.63s/it]Extractor Estimating: 7it [00:22,  1.30s/it]Extractor Estimating: 8it [00:22,  1.08s/it]Extractor Estimating: 9it [00:23,  1.12s/it]Extractor Estimating: 10it [00:24,  1.03it/s]Extractor Estimating: 11it [00:25,  1.14it/s]Extractor Estimating: 12it [00:25,  1.26it/s]Extractor Estimating: 13it [00:26,  1.30it/s]Extractor Estimating: 14it [00:27,  1.40it/s]Extractor Estimating: 15it [00:27,  1.43it/s]Extractor Estimating: 16it [00:28,  1.50it/s]Extractor Estimating: 17it [00:28,  1.52it/s]Extractor Estimating: 18it [00:29,  1.55it/s]Extractor Estimating: 19it [00:30,  1.56it/s]Extractor Estimating: 20it [00:30,  1.53it/s]Extractor Estimating: 21it [00:31,  1.61it/s]Extractor Estimating: 22it [00:31,  1.66it/s]Extractor Estimating: 23it [00:32,  1.67it/s]Extractor Estimating: 24it [00:33,  1.63it/s]Extractor Estimating: 25it [00:34,  1.45it/s]Extractor Estimating: 26it [00:34,  1.56it/s]Extractor Estimating: 27it [00:35,  1.57it/s]Extractor Estimating: 28it [00:35,  1.64it/s]Extractor Estimating: 29it [00:36,  1.66it/s]Extractor Estimating: 30it [00:38,  1.05it/s]Extractor Estimating: 31it [00:38,  1.16it/s]Extractor Estimating: 32it [00:39,  1.26it/s]Extractor Estimating: 33it [00:40,  1.35it/s]Extractor Estimating: 34it [00:40,  1.44it/s]Extractor Estimating: 35it [00:41,  1.48it/s]Extractor Estimating: 36it [00:41,  1.52it/s]Extractor Estimating: 37it [00:42,  1.57it/s]Extractor Estimating: 38it [00:43,  1.56it/s]Extractor Estimating: 39it [00:43,  1.61it/s]Extractor Estimating: 40it [00:44,  1.65it/s]Extractor Estimating: 41it [00:44,  1.59it/s]Extractor Estimating: 42it [00:45,  1.65it/s]Extractor Estimating: 43it [00:46,  1.69it/s]Extractor Estimating: 44it [00:46,  1.68it/s]Extractor Estimating: 45it [00:47,  1.72it/s]Extractor Estimating: 46it [00:47,  1.71it/s]Extractor Estimating: 47it [00:48,  1.72it/s]Extractor Estimating: 48it [00:48,  1.71it/s]Extractor Estimating: 49it [00:49,  1.61it/s]Extractor Estimating: 50it [00:50,  1.69it/s]Extractor Estimating: 51it [00:50,  1.73it/s]Extractor Estimating: 52it [00:51,  1.78it/s]Extractor Estimating: 53it [00:51,  1.82it/s]Extractor Estimating: 54it [00:52,  1.86it/s]Extractor Estimating: 55it [00:52,  1.88it/s]Extractor Estimating: 56it [00:53,  1.96it/s]Extractor Estimating: 57it [00:53,  1.94it/s]Extractor Estimating: 58it [00:54,  1.96it/s]Extractor Estimating: 59it [00:55,  1.58it/s]Extractor Estimating: 60it [00:55,  1.64it/s]Extractor Estimating: 61it [00:56,  1.69it/s]Extractor Estimating: 62it [00:56,  1.79it/s]Extractor Estimating: 63it [00:57,  1.88it/s]Extractor Estimating: 64it [00:57,  1.86it/s]Extractor Estimating: 65it [00:58,  1.84it/s]Extractor Estimating: 66it [00:58,  1.93it/s]Extractor Estimating: 67it [00:59,  1.94it/s]Extractor Estimating: 68it [01:00,  1.79it/s]Extractor Estimating: 69it [01:00,  1.87it/s]Extractor Estimating: 70it [01:01,  1.88it/s]Extractor Estimating: 71it [01:01,  1.92it/s]Extractor Estimating: 72it [01:02,  1.92it/s]Extractor Estimating: 73it [01:02,  1.95it/s]Extractor Estimating: 74it [01:03,  1.96it/s]Extractor Estimating: 75it [01:03,  1.94it/s]Extractor Estimating: 76it [01:04,  1.87it/s]Extractor Estimating: 77it [01:04,  1.73it/s]Extractor Estimating: 78it [01:05,  1.57it/s]Extractor Estimating: 79it [01:06,  1.58it/s]Extractor Estimating: 80it [01:06,  1.52it/s]Extractor Estimating: 81it [01:07,  1.53it/s]Extractor Estimating: 82it [01:08,  1.51it/s]Extractor Estimating: 83it [01:08,  1.53it/s]Extractor Estimating: 84it [01:09,  1.51it/s]Extractor Estimating: 85it [01:10,  1.31it/s]Extractor Estimating: 86it [01:11,  1.39it/s]Extractor Estimating: 87it [01:11,  1.41it/s]Extractor Estimating: 88it [01:12,  1.47it/s]Extractor Estimating: 89it [01:13,  1.54it/s]Extractor Estimating: 90it [01:13,  1.59it/s]Extractor Estimating: 91it [01:14,  1.59it/s]Extractor Estimating: 92it [01:14,  1.62it/s]Extractor Estimating: 93it [01:15,  1.45it/s]Extractor Estimating: 94it [01:16,  1.50it/s]Extractor Estimating: 95it [01:16,  1.51it/s]Extractor Estimating: 96it [01:17,  1.56it/s]Extractor Estimating: 97it [01:18,  1.56it/s]Extractor Estimating: 98it [01:18,  1.58it/s]Extractor Estimating: 99it [01:19,  1.60it/s]Extractor Estimating: 100it [01:20,  1.58it/s]Extractor Estimating: 101it [01:20,  1.55it/s]Extractor Estimating: 102it [01:21,  1.57it/s]Extractor Estimating: 103it [01:21,  1.60it/s]Extractor Estimating: 104it [01:22,  1.55it/s]Extractor Estimating: 105it [01:23,  1.58it/s]Extractor Estimating: 106it [01:23,  1.58it/s]Extractor Estimating: 107it [01:24,  1.58it/s]Extractor Estimating: 108it [01:25,  1.60it/s]Extractor Estimating: 109it [01:26,  1.44it/s]Extractor Estimating: 110it [01:26,  1.48it/s]Extractor Estimating: 111it [01:27,  1.48it/s]Extractor Estimating: 112it [01:28,  1.35it/s]Extractor Estimating: 113it [01:28,  1.40it/s]Extractor Estimating: 114it [01:29,  1.43it/s]Extractor Estimating: 115it [01:30,  1.44it/s]Extractor Estimating: 116it [01:30,  1.38it/s]Extractor Estimating: 117it [01:31,  1.42it/s]Extractor Estimating: 118it [01:32,  1.50it/s]Extractor Estimating: 119it [01:32,  1.53it/s]Extractor Estimating: 120it [01:33,  1.51it/s]Extractor Estimating: 121it [01:34,  1.54it/s]Extractor Estimating: 122it [01:34,  1.54it/s]Extractor Estimating: 123it [01:35,  1.54it/s]Extractor Estimating: 124it [01:36,  1.42it/s]Extractor Estimating: 125it [01:36,  1.50it/s]Extractor Estimating: 126it [01:37,  1.57it/s]Extractor Estimating: 127it [01:38,  1.54it/s]Extractor Estimating: 128it [01:39,  1.32it/s]Extractor Estimating: 129it [01:39,  1.40it/s]Extractor Estimating: 130it [01:40,  1.47it/s]Extractor Estimating: 131it [01:40,  1.49it/s]Extractor Estimating: 132it [01:41,  1.34it/s]Extractor Estimating: 133it [01:42,  1.38it/s]Extractor Estimating: 134it [01:43,  1.44it/s]Extractor Estimating: 135it [01:43,  1.42it/s]Extractor Estimating: 136it [01:44,  1.45it/s]Extractor Estimating: 137it [01:46,  1.02it/s]Extractor Estimating: 138it [01:47,  1.08it/s]Extractor Estimating: 139it [01:47,  1.18it/s]Extractor Estimating: 140it [01:48,  1.28it/s]Extractor Estimating: 141it [01:49,  1.20it/s]Extractor Estimating: 142it [01:49,  1.31it/s]Extractor Estimating: 143it [01:50,  1.42it/s]Extractor Estimating: 144it [01:51,  1.47it/s]Extractor Estimating: 145it [01:51,  1.54it/s]Extractor Estimating: 146it [01:52,  1.43it/s]Extractor Estimating: 147it [01:53,  1.48it/s]Extractor Estimating: 148it [01:53,  1.49it/s]Extractor Estimating: 149it [01:54,  1.39it/s]Extractor Estimating: 150it [01:55,  1.44it/s]Extractor Estimating: 151it [01:55,  1.49it/s]Extractor Estimating: 152it [01:56,  1.56it/s]Extractor Estimating: 153it [01:56,  1.61it/s]Extractor Estimating: 154it [01:57,  1.48it/s]Extractor Estimating: 155it [01:58,  1.54it/s]Extractor Estimating: 156it [01:59,  1.55it/s]Extractor Estimating: 157it [01:59,  1.57it/s]Extractor Estimating: 158it [02:00,  1.58it/s]Extractor Estimating: 159it [02:00,  1.60it/s]Extractor Estimating: 160it [02:01,  1.66it/s]Extractor Estimating: 161it [02:02,  1.55it/s]Extractor Estimating: 162it [02:03,  1.28it/s]Extractor Estimating: 163it [02:03,  1.40it/s]Extractor Estimating: 164it [02:04,  1.48it/s]Extractor Estimating: 165it [02:05,  1.19it/s]Extractor Estimating: 166it [02:06,  1.31it/s]Extractor Estimating: 167it [02:06,  1.32it/s]Extractor Estimating: 168it [02:07,  1.44it/s]Extractor Estimating: 169it [02:08,  1.40it/s]Extractor Estimating: 170it [02:08,  1.43it/s]Extractor Estimating: 171it [02:09,  1.49it/s]Extractor Estimating: 172it [02:10,  1.57it/s]Extractor Estimating: 173it [02:10,  1.62it/s]Extractor Estimating: 174it [02:11,  1.60it/s]Extractor Estimating: 175it [02:11,  1.57it/s]Extractor Estimating: 176it [02:12,  1.57it/s]Extractor Estimating: 177it [02:13,  1.60it/s]Extractor Estimating: 178it [02:13,  1.49it/s]Extractor Estimating: 179it [02:14,  1.53it/s]Extractor Estimating: 180it [02:15,  1.53it/s]Extractor Estimating: 181it [02:15,  1.51it/s]Extractor Estimating: 182it [02:16,  1.53it/s]Extractor Estimating: 183it [02:17,  1.56it/s]Extractor Estimating: 184it [02:17,  1.59it/s]Extractor Estimating: 185it [02:18,  1.58it/s]Extractor Estimating: 186it [02:19,  1.49it/s]Extractor Estimating: 187it [02:19,  1.56it/s]Extractor Estimating: 188it [02:20,  1.57it/s]Extractor Estimating: 189it [02:20,  1.58it/s]Extractor Estimating: 190it [02:21,  1.55it/s]Extractor Estimating: 191it [02:22,  1.46it/s]Extractor Estimating: 192it [02:23,  1.50it/s]Extractor Estimating: 193it [02:23,  1.49it/s]Extractor Estimating: 194it [02:24,  1.54it/s]Extractor Estimating: 195it [02:24,  1.57it/s]Extractor Estimating: 196it [02:25,  1.44it/s]Extractor Estimating: 197it [02:26,  1.48it/s]Extractor Estimating: 198it [02:27,  1.50it/s]Extractor Estimating: 199it [02:27,  1.56it/s]Extractor Estimating: 200it [02:28,  1.56it/s]Extractor Estimating: 201it [02:29,  1.41it/s]Extractor Estimating: 202it [02:29,  1.49it/s]Extractor Estimating: 203it [02:30,  1.49it/s]Extractor Estimating: 204it [02:31,  1.53it/s]Extractor Estimating: 205it [02:31,  1.49it/s]Extractor Estimating: 206it [02:33,  1.11it/s]Extractor Estimating: 207it [02:33,  1.24it/s]Extractor Estimating: 208it [02:34,  1.33it/s]Extractor Estimating: 209it [02:34,  1.42it/s]Extractor Estimating: 210it [02:35,  1.31it/s]Extractor Estimating: 211it [02:36,  1.24it/s]Extractor Estimating: 212it [02:37,  1.34it/s]Extractor Estimating: 213it [02:37,  1.43it/s]Extractor Estimating: 214it [02:38,  1.47it/s]Extractor Estimating: 215it [02:39,  1.50it/s]Extractor Estimating: 216it [02:40,  1.39it/s]Extractor Estimating: 217it [02:40,  1.46it/s]Extractor Estimating: 218it [02:41,  1.50it/s]Extractor Estimating: 219it [02:42,  1.42it/s]Extractor Estimating: 220it [02:42,  1.48it/s]Extractor Estimating: 221it [02:43,  1.51it/s]Extractor Estimating: 222it [02:43,  1.54it/s]Extractor Estimating: 223it [02:44,  1.59it/s]Extractor Estimating: 224it [02:45,  1.49it/s]Extractor Estimating: 225it [02:45,  1.52it/s]Extractor Estimating: 226it [02:46,  1.51it/s]Extractor Estimating: 227it [02:47,  1.48it/s]Extractor Estimating: 228it [02:47,  1.49it/s]Extractor Estimating: 229it [02:48,  1.52it/s]Extractor Estimating: 230it [02:49,  1.53it/s]Extractor Estimating: 231it [02:49,  1.56it/s]Extractor Estimating: 232it [02:50,  1.51it/s]Extractor Estimating: 233it [02:51,  1.56it/s]Extractor Estimating: 234it [02:51,  1.54it/s]Extractor Estimating: 235it [02:53,  1.17it/s]Extractor Estimating: 236it [02:53,  1.26it/s]Extractor Estimating: 237it [02:54,  1.33it/s]Extractor Estimating: 238it [02:55,  1.42it/s]Extractor Estimating: 239it [02:55,  1.45it/s]Extractor Estimating: 240it [02:56,  1.48it/s]Extractor Estimating: 241it [02:57,  1.49it/s]Extractor Estimating: 242it [02:57,  1.47it/s]Extractor Estimating: 243it [02:58,  1.49it/s]Extractor Estimating: 244it [02:59,  1.52it/s]Extractor Estimating: 245it [02:59,  1.52it/s]Extractor Estimating: 246it [03:00,  1.39it/s]Extractor Estimating: 247it [03:01,  1.12it/s]Extractor Estimating: 248it [03:02,  1.24it/s]Extractor Estimating: 249it [03:03,  1.34it/s]Extractor Estimating: 250it [03:03,  1.37it/s]Extractor Estimating: 251it [03:04,  1.38it/s]Extractor Estimating: 252it [03:05,  1.41it/s]Extractor Estimating: 253it [03:05,  1.45it/s]Extractor Estimating: 254it [03:06,  1.50it/s]Extractor Estimating: 255it [03:07,  1.54it/s]Extractor Estimating: 256it [03:07,  1.53it/s]Extractor Estimating: 257it [03:08,  1.58it/s]Extractor Estimating: 258it [03:08,  1.63it/s]Extractor Estimating: 259it [03:09,  1.60it/s]Extractor Estimating: 260it [03:10,  1.65it/s]Extractor Estimating: 261it [03:10,  1.47it/s]Extractor Estimating: 262it [03:11,  1.50it/s]Extractor Estimating: 263it [03:12,  1.54it/s]Extractor Estimating: 264it [03:12,  1.59it/s]Extractor Estimating: 265it [03:13,  1.60it/s]Extractor Estimating: 266it [03:14,  1.53it/s]Extractor Estimating: 267it [03:14,  1.56it/s]Extractor Estimating: 268it [03:15,  1.53it/s]Extractor Estimating: 269it [03:15,  1.57it/s]Extractor Estimating: 270it [03:16,  1.57it/s]Extractor Estimating: 271it [03:17,  1.46it/s]Extractor Estimating: 272it [03:18,  1.46it/s]Extractor Estimating: 273it [03:18,  1.53it/s]Extractor Estimating: 274it [03:19,  1.56it/s]Extractor Estimating: 275it [03:19,  1.58it/s]Extractor Estimating: 276it [03:20,  1.55it/s]Extractor Estimating: 277it [03:21,  1.58it/s]Extractor Estimating: 278it [03:21,  1.58it/s]Extractor Estimating: 279it [03:22,  1.57it/s]Extractor Estimating: 280it [03:23,  1.58it/s]Extractor Estimating: 281it [03:24,  1.36it/s]Extractor Estimating: 282it [03:24,  1.44it/s]Extractor Estimating: 283it [03:25,  1.45it/s]Extractor Estimating: 284it [03:25,  1.48it/s]Extractor Estimating: 285it [03:26,  1.50it/s]Extractor Estimating: 286it [03:27,  1.47it/s]Extractor Estimating: 287it [03:27,  1.47it/s]Extractor Estimating: 288it [03:28,  1.53it/s]Extractor Estimating: 289it [03:29,  1.53it/s]Extractor Estimating: 290it [03:29,  1.59it/s]Extractor Estimating: 291it [03:30,  1.60it/s]Extractor Estimating: 292it [03:31,  1.56it/s]Extractor Estimating: 293it [03:31,  1.54it/s]Extractor Estimating: 294it [03:32,  1.50it/s]Extractor Estimating: 295it [03:33,  1.56it/s]Extractor Estimating: 296it [03:33,  1.57it/s]Extractor Estimating: 297it [03:34,  1.24it/s]Extractor Estimating: 298it [03:35,  1.37it/s]Extractor Estimating: 299it [03:36,  1.40it/s]Extractor Estimating: 300it [03:36,  1.46it/s]Extractor Estimating: 301it [03:37,  1.37it/s]Extractor Estimating: 302it [03:38,  1.39it/s]Extractor Estimating: 303it [03:38,  1.43it/s]Extractor Estimating: 304it [03:39,  1.45it/s]Extractor Estimating: 305it [03:40,  1.47it/s]Extractor Estimating: 306it [03:40,  1.47it/s]Extractor Estimating: 307it [03:41,  1.53it/s]Extractor Estimating: 308it [03:42,  1.53it/s]Extractor Estimating: 309it [03:43,  1.29it/s]Extractor Estimating: 310it [03:43,  1.36it/s]Extractor Estimating: 311it [03:44,  1.40it/s]Extractor Estimating: 312it [03:45,  1.40it/s]Extractor Estimating: 313it [03:45,  1.45it/s]Extractor Estimating: 314it [03:46,  1.48it/s]Extractor Estimating: 315it [03:47,  1.50it/s]Extractor Estimating: 316it [03:47,  1.51it/s]Extractor Estimating: 317it [03:48,  1.44it/s]Extractor Estimating: 318it [03:49,  1.33it/s]Extractor Estimating: 319it [03:50,  1.39it/s]Extractor Estimating: 320it [03:50,  1.43it/s]Extractor Estimating: 321it [03:51,  1.44it/s]Extractor Estimating: 322it [03:52,  1.47it/s]Extractor Estimating: 323it [03:52,  1.50it/s]Extractor Estimating: 324it [03:53,  1.51it/s]Extractor Estimating: 325it [03:54,  1.53it/s]Extractor Estimating: 326it [03:54,  1.54it/s]Extractor Estimating: 327it [03:55,  1.58it/s]Extractor Estimating: 328it [03:55,  1.58it/s]Extractor Estimating: 329it [03:56,  1.62it/s]Extractor Estimating: 330it [03:57,  1.58it/s]Extractor Estimating: 331it [03:57,  1.59it/s]Extractor Estimating: 332it [03:58,  1.54it/s]Extractor Estimating: 333it [03:59,  1.57it/s]Extractor Estimating: 334it [03:59,  1.58it/s]Extractor Estimating: 335it [04:00,  1.56it/s]Extractor Estimating: 336it [04:00,  1.60it/s]Extractor Estimating: 337it [04:01,  1.51it/s]Extractor Estimating: 338it [04:02,  1.56it/s]Extractor Estimating: 339it [04:02,  1.53it/s]Extractor Estimating: 340it [04:03,  1.58it/s]Extractor Estimating: 341it [04:04,  1.55it/s]Extractor Estimating: 342it [04:04,  1.55it/s]Extractor Estimating: 343it [04:05,  1.59it/s]Extractor Estimating: 344it [04:06,  1.41it/s]Extractor Estimating: 345it [04:07,  1.43it/s]Extractor Estimating: 346it [04:07,  1.47it/s]Extractor Estimating: 347it [04:08,  1.52it/s]Extractor Estimating: 348it [04:08,  1.49it/s]Extractor Estimating: 349it [04:09,  1.54it/s]Extractor Estimating: 350it [04:10,  1.54it/s]Extractor Estimating: 351it [04:10,  1.59it/s]Extractor Estimating: 352it [04:11,  1.52it/s]Extractor Estimating: 353it [04:12,  1.57it/s]Extractor Estimating: 354it [04:12,  1.58it/s]Extractor Estimating: 355it [04:13,  1.53it/s]Extractor Estimating: 356it [04:14,  1.51it/s]Extractor Estimating: 357it [04:15,  1.37it/s]Extractor Estimating: 358it [04:15,  1.48it/s]Extractor Estimating: 359it [04:16,  1.51it/s]Extractor Estimating: 360it [04:16,  1.43it/s]Extractor Estimating: 361it [04:17,  1.50it/s]Extractor Estimating: 362it [04:18,  1.50it/s]Extractor Estimating: 363it [04:18,  1.51it/s]Extractor Estimating: 364it [04:19,  1.42it/s]Extractor Estimating: 365it [04:20,  1.47it/s]Extractor Estimating: 366it [04:20,  1.56it/s]Extractor Estimating: 367it [04:21,  1.55it/s]Extractor Estimating: 368it [04:22,  1.55it/s]Extractor Estimating: 369it [04:22,  1.57it/s]Extractor Estimating: 370it [04:23,  1.59it/s]Extractor Estimating: 371it [04:24,  1.56it/s]Extractor Estimating: 372it [04:24,  1.60it/s]Extractor Estimating: 373it [04:25,  1.58it/s]Extractor Estimating: 374it [04:25,  1.56it/s]Extractor Estimating: 375it [04:27,  1.24it/s]Extractor Estimating: 375it [04:27,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1572 mean pseudo reward: 0.9490521987925571
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 14933
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15033, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15033, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 34, avg_time 1.260, loss:314.7338
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 2, avg_time 0.958, loss:223.5600
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 36, avg_time 0.962, loss:194.3597
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 4, avg_time 0.961, loss:180.1960
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 38, avg_time 0.972, loss:167.5092
>> valid entity prec:0.6057, rec:0.5485, f1:0.5756
>> valid relation prec:0.2109, rec:0.1045, f1:0.1398
>> valid relation with NER prec:0.2109, rec:0.1045, f1:0.1398
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 6, avg_time 2.567, loss:152.3020
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 0.966, loss:141.2370
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 8, avg_time 0.960, loss:139.9518
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 0.969, loss:135.9158
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 10, avg_time 0.962, loss:145.6855
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5893, rec:0.5561, f1:0.5722
>> valid relation prec:0.2191, rec:0.1111, f1:0.1474
>> valid relation with NER prec:0.2191, rec:0.1111, f1:0.1474
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 44, avg_time 2.587, loss:138.9679
g_step 1200, step 12, avg_time 0.958, loss:129.6180
g_step 1300, step 46, avg_time 0.966, loss:113.9047
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:34:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:34:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-34-48_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:34:52 - WARNING - datasets.builder -   Using custom data configuration default-a69154dd8790755f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a69154dd8790755f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:35:01,577 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:01,578 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:35:01,578 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:01,579 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:35:02,635 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,232 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,232 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,232 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,233 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,233 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:03,233 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:35:05,138 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:35:08,462 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:35:08,462 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a69154dd8790755f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 19:35:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14b74fb0e170> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:02<00:02,  2.36s/ba]100%|██████████| 2/2 [00:02<00:00,  1.05s/ba]100%|██████████| 2/2 [00:02<00:00,  1.25s/ba]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.00ba/s] 40%|████      | 2/5 [00:00<00:00,  3.85ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.21ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.38ba/s]100%|██████████| 5/5 [00:01<00:00,  4.66ba/s]100%|██████████| 5/5 [00:01<00:00,  4.32ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.11ba/s]100%|██████████| 2/2 [00:00<00:00,  2.06ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.96ba/s] 40%|████      | 2/5 [00:00<00:00,  4.80ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.27ba/s]100%|██████████| 5/5 [00:00<00:00,  8.30ba/s]100%|██████████| 5/5 [00:00<00:00,  6.77ba/s]
[INFO|trainer.py:414] 2023-08-28 19:35:19,858 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:35:20,117 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:35:20,117 >>   Num examples = 1574
[INFO|trainer.py:1149] 2023-08-28 19:35:20,118 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:35:20,118 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:35:20,118 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:35:20,118 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:35:20,118 >>   Total optimization steps = 125
  0%|          | 0/125 [00:00<?, ?it/s]  1%|          | 1/125 [00:00<00:37,  3.26it/s]  2%|▏         | 2/125 [00:00<00:36,  3.35it/s]  2%|▏         | 3/125 [00:00<00:36,  3.38it/s]  3%|▎         | 4/125 [00:01<00:35,  3.40it/s]  4%|▍         | 5/125 [00:01<00:35,  3.40it/s]  5%|▍         | 6/125 [00:01<00:34,  3.40it/s]  6%|▌         | 7/125 [00:02<00:34,  3.41it/s]  6%|▋         | 8/125 [00:02<00:34,  3.41it/s]  7%|▋         | 9/125 [00:02<00:33,  3.41it/s]  8%|▊         | 10/125 [00:02<00:33,  3.41it/s]  9%|▉         | 11/125 [00:03<00:33,  3.41it/s] 10%|▉         | 12/125 [00:03<00:33,  3.41it/s] 10%|█         | 13/125 [00:03<00:32,  3.41it/s] 11%|█         | 14/125 [00:04<00:32,  3.41it/s] 12%|█▏        | 15/125 [00:04<00:32,  3.41it/s] 13%|█▎        | 16/125 [00:05<00:50,  2.16it/s] 14%|█▎        | 17/125 [00:05<00:44,  2.43it/s] 14%|█▍        | 18/125 [00:05<00:40,  2.65it/s] 15%|█▌        | 19/125 [00:06<00:37,  2.84it/s] 16%|█▌        | 20/125 [00:06<00:35,  2.99it/s] 17%|█▋        | 21/125 [00:06<00:33,  3.10it/s] 18%|█▊        | 22/125 [00:07<00:32,  3.18it/s] 18%|█▊        | 23/125 [00:07<00:31,  3.25it/s] 19%|█▉        | 24/125 [00:07<00:30,  3.29it/s] 20%|██        | 25/125 [00:07<00:27,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 19:35:27,939 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:35:27,939 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:35:27,939 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.86it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.60it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.12it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.46it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.89it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.34it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.77it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.38it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.48it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.59it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.69it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.85it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.89it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.89it/s][A
 13%|█▎        | 77/611 [00:01<00:11, 44.51it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.32it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.18it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.25it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.32it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.59it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.70it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.85it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.68it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.50it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.37it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.24it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.26it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.31it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.47it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.72it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.77it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.75it/s][A
 27%|██▋       | 167/611 [00:03<00:09, 44.50it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.37it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.32it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.37it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.40it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.52it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.66it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.70it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.66it/s][A
 35%|███▍      | 212/611 [00:04<00:08, 44.44it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.25it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.26it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.23it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.34it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.48it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.68it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.68it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.52it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.33it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.34it/s][A
 44%|████▎     | 267/611 [00:05<00:07, 44.22it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.27it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.29it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.45it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.65it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.60it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.46it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.40it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.22it/s][A
 51%|█████     | 312/611 [00:06<00:06, 44.29it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.31it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.42it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.58it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.57it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.53it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.39it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.25it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.24it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.29it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.44it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.51it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.55it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.46it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.34it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.26it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.30it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.30it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.36it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.44it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.42it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.47it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.41it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.32it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.35it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.27it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.20it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.38it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.49it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.49it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.39it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.37it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.38it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.30it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.20it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.27it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 38.17it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 40.12it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 41.38it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 42.34it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.12it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.71it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.96it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.76it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.71it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.94it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.22it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.42it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.56it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.67it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.62it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.33it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.09it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.89it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.06it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.23it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.48it/s][A                                                
                                                 [A 20%|██        | 25/125 [00:21<00:27,  3.68it/s]
100%|██████████| 611/611 [00:13<00:00, 44.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:35:41,980 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25
[INFO|configuration_utils.py:351] 2023-08-28 19:35:43,032 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:35:51,156 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:35:52,564 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:35:53,518 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25/special_tokens_map.json
 21%|██        | 26/125 [01:03<27:37, 16.75s/it] 22%|██▏       | 27/125 [01:03<19:30, 11.95s/it] 22%|██▏       | 28/125 [01:04<13:39,  8.45s/it] 23%|██▎       | 29/125 [01:04<09:36,  6.00s/it] 24%|██▍       | 30/125 [01:04<06:47,  4.29s/it] 25%|██▍       | 31/125 [01:04<04:50,  3.09s/it] 26%|██▌       | 32/125 [01:05<03:29,  2.25s/it] 26%|██▋       | 33/125 [01:05<02:32,  1.66s/it] 27%|██▋       | 34/125 [01:05<01:53,  1.25s/it] 28%|██▊       | 35/125 [01:06<01:38,  1.09s/it] 29%|██▉       | 36/125 [01:06<01:15,  1.17it/s] 30%|██▉       | 37/125 [01:07<01:00,  1.46it/s] 30%|███       | 38/125 [01:07<00:49,  1.77it/s] 31%|███       | 39/125 [01:07<00:41,  2.07it/s] 32%|███▏      | 40/125 [01:07<00:36,  2.35it/s] 33%|███▎      | 41/125 [01:08<00:32,  2.60it/s] 34%|███▎      | 42/125 [01:08<00:29,  2.81it/s] 34%|███▍      | 43/125 [01:08<00:27,  2.98it/s] 35%|███▌      | 44/125 [01:09<00:30,  2.67it/s] 36%|███▌      | 45/125 [01:09<00:27,  2.86it/s] 37%|███▋      | 46/125 [01:09<00:26,  3.02it/s] 38%|███▊      | 47/125 [01:10<00:24,  3.14it/s] 38%|███▊      | 48/125 [01:10<00:23,  3.22it/s] 39%|███▉      | 49/125 [01:10<00:23,  3.29it/s] 40%|████      | 50/125 [01:10<00:20,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 19:36:31,052 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:36:31,052 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:36:31,052 >>   Batch size = 8
{'eval_loss': 0.9333938360214233, 'eval_runtime': 13.7946, 'eval_samples_per_second': 353.907, 'eval_steps_per_second': 44.293, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.06it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.88it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.13it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.11it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.57it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.13it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.68it/s][A
  7%|▋         | 42/611 [00:01<00:12, 44.41it/s][A
  8%|▊         | 47/611 [00:01<00:19, 29.16it/s][A
  9%|▊         | 52/611 [00:01<00:17, 32.66it/s][A
  9%|▉         | 57/611 [00:01<00:15, 35.51it/s][A
 10%|█         | 62/611 [00:01<00:14, 38.00it/s][A
 11%|█         | 67/611 [00:01<00:13, 39.93it/s][A
 12%|█▏        | 72/611 [00:01<00:13, 41.42it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 42.49it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.08it/s][A
 14%|█▍        | 87/611 [00:02<00:12, 43.18it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.28it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 43.40it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 43.67it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.09it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.46it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.64it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.77it/s][A
 21%|██        | 127/611 [00:03<00:10, 44.60it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.34it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.07it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.15it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.29it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.49it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.67it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.79it/s][A
 27%|██▋       | 167/611 [00:03<00:09, 44.82it/s][A
 28%|██▊       | 172/611 [00:04<00:09, 44.60it/s][A
 29%|██▉       | 177/611 [00:04<00:20, 21.44it/s][A
 30%|██▉       | 182/611 [00:04<00:16, 25.43it/s][A
 31%|███       | 187/611 [00:04<00:14, 29.21it/s][A
 31%|███▏      | 192/611 [00:04<00:12, 32.65it/s][A
 32%|███▏      | 197/611 [00:04<00:11, 35.63it/s][A
 33%|███▎      | 202/611 [00:05<00:10, 37.94it/s][A
 34%|███▍      | 207/611 [00:05<00:10, 39.79it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 41.03it/s][A
 36%|███▌      | 217/611 [00:05<00:09, 41.71it/s][A
 36%|███▋      | 222/611 [00:05<00:09, 42.28it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 42.76it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.29it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.76it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.13it/s][A
 40%|████      | 247/611 [00:06<00:08, 44.36it/s][A
 41%|████      | 252/611 [00:06<00:08, 44.50it/s][A
 42%|████▏     | 257/611 [00:06<00:07, 44.41it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 44.12it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.00it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.07it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.32it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.48it/s][A
 47%|████▋     | 287/611 [00:07<00:07, 44.64it/s][A
 48%|████▊     | 292/611 [00:07<00:07, 44.67it/s][A
 49%|████▊     | 297/611 [00:07<00:12, 26.11it/s][A
 49%|████▉     | 302/611 [00:07<00:10, 29.84it/s][A
 50%|█████     | 307/611 [00:07<00:09, 33.12it/s][A
 51%|█████     | 312/611 [00:07<00:08, 35.95it/s][A
 52%|█████▏    | 317/611 [00:07<00:07, 38.29it/s][A
 53%|█████▎    | 322/611 [00:08<00:07, 36.71it/s][A
 54%|█████▎    | 328/611 [00:08<00:07, 40.16it/s][A
 55%|█████▍    | 333/611 [00:08<00:06, 41.17it/s][A
 55%|█████▌    | 338/611 [00:08<00:06, 41.89it/s][A
 56%|█████▌    | 343/611 [00:08<00:06, 42.61it/s][A
 57%|█████▋    | 348/611 [00:08<00:06, 43.19it/s][A
 58%|█████▊    | 353/611 [00:08<00:05, 43.63it/s][A
 59%|█████▊    | 358/611 [00:08<00:05, 44.04it/s][A
 59%|█████▉    | 363/611 [00:09<00:05, 44.14it/s][A
 60%|██████    | 368/611 [00:09<00:05, 44.05it/s][A
 61%|██████    | 373/611 [00:09<00:05, 44.18it/s][A
 62%|██████▏   | 378/611 [00:09<00:05, 44.09it/s][A
 63%|██████▎   | 383/611 [00:09<00:05, 44.12it/s][A
 64%|██████▎   | 388/611 [00:09<00:05, 44.19it/s][A
 64%|██████▍   | 393/611 [00:09<00:04, 44.24it/s][A
 65%|██████▌   | 398/611 [00:09<00:04, 44.36it/s][A
 66%|██████▌   | 403/611 [00:09<00:04, 44.50it/s][A
 67%|██████▋   | 408/611 [00:10<00:04, 44.26it/s][A
 68%|██████▊   | 413/611 [00:10<00:04, 44.20it/s][A
 68%|██████▊   | 418/611 [00:10<00:08, 21.50it/s][A
 69%|██████▉   | 423/611 [00:10<00:07, 25.46it/s][A
 70%|███████   | 428/611 [00:10<00:06, 29.24it/s][A
 71%|███████   | 433/611 [00:10<00:05, 32.57it/s][A
 72%|███████▏  | 438/611 [00:11<00:04, 35.54it/s][A
 73%|███████▎  | 443/611 [00:11<00:04, 37.95it/s][A
 73%|███████▎  | 448/611 [00:11<00:04, 39.73it/s][A
 74%|███████▍  | 453/611 [00:11<00:03, 41.02it/s][A
 75%|███████▍  | 458/611 [00:11<00:03, 41.66it/s][A
 76%|███████▌  | 463/611 [00:11<00:03, 42.18it/s][A
 77%|███████▋  | 468/611 [00:11<00:03, 42.71it/s][A
 77%|███████▋  | 473/611 [00:11<00:03, 43.28it/s][A
 78%|███████▊  | 478/611 [00:11<00:03, 43.61it/s][A
 79%|███████▉  | 483/611 [00:12<00:02, 44.05it/s][A
 80%|███████▉  | 488/611 [00:12<00:02, 44.26it/s][A
 81%|████████  | 493/611 [00:12<00:02, 44.49it/s][A
 82%|████████▏ | 498/611 [00:12<00:02, 44.36it/s][A
 82%|████████▏ | 503/611 [00:12<00:02, 44.13it/s][A
 83%|████████▎ | 508/611 [00:12<00:02, 44.03it/s][A
 84%|████████▍ | 513/611 [00:12<00:02, 44.07it/s][A
 85%|████████▍ | 518/611 [00:12<00:02, 44.10it/s][A
 86%|████████▌ | 523/611 [00:13<00:01, 44.13it/s][A
 86%|████████▋ | 528/611 [00:13<00:01, 44.31it/s][A
 87%|████████▋ | 533/611 [00:13<00:01, 44.50it/s][A
 88%|████████▊ | 538/611 [00:13<00:02, 34.00it/s][A
 89%|████████▉ | 543/611 [00:13<00:01, 36.62it/s][A
 90%|████████▉ | 548/611 [00:13<00:01, 38.73it/s][A
 91%|█████████ | 553/611 [00:13<00:01, 40.41it/s][A
 91%|█████████▏| 558/611 [00:13<00:01, 41.64it/s][A
 92%|█████████▏| 563/611 [00:14<00:01, 42.54it/s][A
 93%|█████████▎| 568/611 [00:14<00:00, 43.22it/s][A
 94%|█████████▍| 573/611 [00:14<00:00, 43.55it/s][A
 95%|█████████▍| 578/611 [00:14<00:00, 36.09it/s][A
 96%|█████████▌| 584/611 [00:14<00:00, 39.74it/s][A
 96%|█████████▋| 589/611 [00:14<00:00, 41.09it/s][A
 97%|█████████▋| 594/611 [00:14<00:00, 42.20it/s][A
 98%|█████████▊| 599/611 [00:14<00:00, 42.87it/s][A
 99%|█████████▉| 604/611 [00:15<00:00, 43.47it/s][A
100%|█████████▉| 609/611 [00:15<00:00, 43.68it/s][A                                                
                                                 [A 40%|████      | 50/125 [01:26<00:20,  3.70it/s]
100%|██████████| 611/611 [00:15<00:00, 43.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:36:46,282 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50
[INFO|configuration_utils.py:351] 2023-08-28 19:36:46,491 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:36:55,866 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:36:57,755 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:36:58,535 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50/special_tokens_map.json
 41%|████      | 51/125 [02:11<22:49, 18.50s/it] 42%|████▏     | 52/125 [02:12<15:54, 13.07s/it] 42%|████▏     | 53/125 [02:12<11:05,  9.24s/it] 43%|████▎     | 54/125 [02:12<07:45,  6.56s/it] 44%|████▍     | 55/125 [02:13<05:27,  4.68s/it] 45%|████▍     | 56/125 [02:13<03:51,  3.36s/it] 46%|████▌     | 57/125 [02:13<02:45,  2.44s/it] 46%|████▋     | 58/125 [02:14<02:00,  1.80s/it] 47%|████▋     | 59/125 [02:14<01:28,  1.35s/it] 48%|████▊     | 60/125 [02:14<01:06,  1.03s/it] 49%|████▉     | 61/125 [02:15<00:51,  1.24it/s] 50%|████▉     | 62/125 [02:15<00:49,  1.29it/s] 50%|█████     | 63/125 [02:16<00:39,  1.58it/s] 51%|█████     | 64/125 [02:16<00:32,  1.88it/s] 52%|█████▏    | 65/125 [02:16<00:27,  2.18it/s] 53%|█████▎    | 66/125 [02:16<00:24,  2.44it/s] 54%|█████▎    | 67/125 [02:17<00:21,  2.67it/s] 54%|█████▍    | 68/125 [02:17<00:19,  2.86it/s] 55%|█████▌    | 69/125 [02:17<00:18,  3.00it/s] 56%|█████▌    | 70/125 [02:18<00:17,  3.11it/s] 57%|█████▋    | 71/125 [02:18<00:17,  3.04it/s] 58%|█████▊    | 72/125 [02:18<00:16,  3.14it/s] 58%|█████▊    | 73/125 [02:19<00:16,  3.21it/s] 59%|█████▉    | 74/125 [02:19<00:20,  2.52it/s] 60%|██████    | 75/125 [02:19<00:16,  2.97it/s][INFO|trainer.py:2140] 2023-08-28 19:37:39,918 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:37:39,919 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:37:39,919 >>   Batch size = 8
{'eval_loss': 0.9286497235298157, 'eval_runtime': 15.1805, 'eval_samples_per_second': 321.597, 'eval_steps_per_second': 40.249, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.29it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.71it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.26it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.56it/s][A
  4%|▍         | 27/611 [00:00<00:12, 46.11it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.55it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.79it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.44it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.48it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.63it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.63it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.83it/s][A
 11%|█         | 67/611 [00:01<00:27, 19.47it/s][A
 12%|█▏        | 72/611 [00:02<00:22, 23.48it/s][A
 13%|█▎        | 77/611 [00:02<00:19, 27.38it/s][A
 13%|█▎        | 82/611 [00:02<00:17, 31.09it/s][A
 14%|█▍        | 87/611 [00:02<00:15, 34.33it/s][A
 15%|█▌        | 92/611 [00:02<00:14, 37.02it/s][A
 16%|█▌        | 97/611 [00:02<00:13, 39.08it/s][A
 17%|█▋        | 102/611 [00:02<00:12, 40.62it/s][A
 18%|█▊        | 107/611 [00:02<00:12, 41.43it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 42.07it/s][A
 19%|█▉        | 117/611 [00:03<00:11, 42.57it/s][A
 20%|█▉        | 122/611 [00:03<00:11, 43.28it/s][A
 21%|██        | 127/611 [00:03<00:11, 43.78it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.26it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.45it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.58it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.56it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.29it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.03it/s][A
 27%|██▋       | 162/611 [00:04<00:10, 44.14it/s][A
 27%|██▋       | 167/611 [00:04<00:10, 44.32it/s][A
 28%|██▊       | 172/611 [00:04<00:09, 44.56it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.71it/s][A
 30%|██▉       | 182/611 [00:04<00:16, 26.26it/s][A
 31%|███       | 187/611 [00:04<00:14, 29.96it/s][A
 31%|███▏      | 192/611 [00:05<00:12, 33.32it/s][A
 32%|███▏      | 197/611 [00:05<00:11, 36.09it/s][A
 33%|███▎      | 202/611 [00:05<00:10, 38.48it/s][A
 34%|███▍      | 207/611 [00:05<00:10, 40.30it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 41.68it/s][A
 36%|███▌      | 217/611 [00:06<00:30, 13.11it/s][A
 36%|███▋      | 222/611 [00:06<00:23, 16.65it/s][A
 37%|███▋      | 227/611 [00:06<00:18, 20.51it/s][A
 38%|███▊      | 232/611 [00:06<00:15, 24.55it/s][A
 39%|███▉      | 237/611 [00:06<00:13, 28.45it/s][A
 40%|███▉      | 242/611 [00:07<00:11, 32.03it/s][A
 40%|████      | 247/611 [00:07<00:10, 35.12it/s][A
 41%|████      | 252/611 [00:07<00:09, 37.64it/s][A
 42%|████▏     | 257/611 [00:07<00:09, 39.22it/s][A
 43%|████▎     | 262/611 [00:07<00:08, 40.34it/s][A
 44%|████▎     | 267/611 [00:07<00:09, 34.51it/s][A
 45%|████▍     | 272/611 [00:07<00:09, 37.16it/s][A
 45%|████▌     | 277/611 [00:07<00:08, 39.16it/s][A
 46%|████▌     | 282/611 [00:07<00:08, 40.75it/s][A
 47%|████▋     | 287/611 [00:08<00:07, 41.93it/s][A
 48%|████▊     | 292/611 [00:08<00:07, 42.92it/s][A
 49%|████▊     | 297/611 [00:08<00:07, 43.56it/s][A
 49%|████▉     | 302/611 [00:08<00:07, 43.90it/s][A
 50%|█████     | 307/611 [00:08<00:06, 43.62it/s][A
 51%|█████     | 312/611 [00:08<00:06, 43.57it/s][A
 52%|█████▏    | 317/611 [00:08<00:06, 43.65it/s][A
 53%|█████▎    | 322/611 [00:08<00:06, 43.98it/s][A
 54%|█████▎    | 327/611 [00:08<00:06, 44.21it/s][A
 54%|█████▍    | 332/611 [00:09<00:18, 14.82it/s][A
 55%|█████▌    | 337/611 [00:09<00:14, 18.56it/s][A
 56%|█████▌    | 342/611 [00:10<00:11, 22.52it/s][A
 57%|█████▋    | 347/611 [00:10<00:09, 26.50it/s][A
 58%|█████▊    | 352/611 [00:10<00:08, 30.30it/s][A
 58%|█████▊    | 357/611 [00:10<00:07, 33.56it/s][A
 59%|█████▉    | 362/611 [00:10<00:07, 31.40it/s][A
 60%|█████▉    | 366/611 [00:12<00:40,  6.09it/s][A
 61%|██████    | 371/611 [00:12<00:28,  8.37it/s][A
 62%|██████▏   | 376/611 [00:13<00:21, 11.18it/s][A
 62%|██████▏   | 381/611 [00:13<00:15, 14.52it/s][A
 63%|██████▎   | 386/611 [00:13<00:12, 18.27it/s][A
 64%|██████▍   | 391/611 [00:13<00:09, 22.30it/s][A
 65%|██████▍   | 396/611 [00:13<00:08, 26.33it/s][A
 66%|██████▌   | 401/611 [00:13<00:06, 30.11it/s][A
 66%|██████▋   | 406/611 [00:13<00:06, 33.22it/s][A
 67%|██████▋   | 411/611 [00:13<00:07, 25.98it/s][A
 68%|██████▊   | 416/611 [00:14<00:06, 29.75it/s][A
 69%|██████▉   | 421/611 [00:14<00:05, 33.08it/s][A
 70%|██████▉   | 426/611 [00:14<00:05, 35.88it/s][A
 71%|███████   | 431/611 [00:14<00:04, 38.25it/s][A
 71%|███████▏  | 436/611 [00:14<00:04, 40.10it/s][A
 72%|███████▏  | 441/611 [00:14<00:04, 41.43it/s][A
 73%|███████▎  | 446/611 [00:14<00:03, 42.32it/s][A
 74%|███████▍  | 451/611 [00:14<00:03, 42.59it/s][A
 75%|███████▍  | 456/611 [00:14<00:03, 42.88it/s][A
 75%|███████▌  | 461/611 [00:15<00:03, 43.13it/s][A
 76%|███████▋  | 466/611 [00:15<00:03, 43.57it/s][A
 77%|███████▋  | 471/611 [00:15<00:03, 43.95it/s][A
 78%|███████▊  | 476/611 [00:15<00:03, 44.22it/s][A
 79%|███████▊  | 481/611 [00:15<00:02, 44.47it/s][A
 80%|███████▉  | 486/611 [00:15<00:02, 44.68it/s][A
 80%|████████  | 491/611 [00:15<00:02, 44.53it/s][A
 81%|████████  | 496/611 [00:15<00:02, 44.31it/s][A
 82%|████████▏ | 501/611 [00:15<00:02, 44.09it/s][A
 83%|████████▎ | 506/611 [00:16<00:02, 44.04it/s][A
 84%|████████▎ | 511/611 [00:16<00:02, 44.18it/s][A
 84%|████████▍ | 516/611 [00:16<00:02, 44.37it/s][A
 85%|████████▌ | 521/611 [00:16<00:02, 44.42it/s][A
 86%|████████▌ | 526/611 [00:16<00:01, 44.61it/s][A
 87%|████████▋ | 531/611 [00:16<00:01, 44.74it/s][A
 88%|████████▊ | 536/611 [00:16<00:01, 44.68it/s][A
 89%|████████▊ | 541/611 [00:17<00:02, 29.20it/s][A
 89%|████████▉ | 546/611 [00:17<00:01, 32.65it/s][A
 90%|█████████ | 551/611 [00:17<00:01, 35.55it/s][A
 91%|█████████ | 556/611 [00:17<00:01, 37.89it/s][A
 92%|█████████▏| 561/611 [00:17<00:01, 39.85it/s][A
 93%|█████████▎| 566/611 [00:17<00:01, 41.27it/s][A
 93%|█████████▎| 571/611 [00:17<00:00, 42.35it/s][A
 94%|█████████▍| 576/611 [00:17<00:00, 42.84it/s][A
 95%|█████████▌| 581/611 [00:18<00:00, 43.02it/s][A
 96%|█████████▌| 586/611 [00:18<00:01, 20.68it/s][A
 97%|█████████▋| 591/611 [00:18<00:00, 24.65it/s][A
 98%|█████████▊| 596/611 [00:18<00:00, 28.49it/s][A
 98%|█████████▊| 601/611 [00:18<00:00, 31.93it/s][A
 99%|█████████▉| 606/611 [00:18<00:00, 34.88it/s][A
100%|██████████| 611/611 [00:19<00:00, 37.42it/s][A                                                
                                                 [A 60%|██████    | 75/125 [02:38<00:16,  2.97it/s]
100%|██████████| 611/611 [00:19<00:00, 37.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:37:59,433 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75
[INFO|configuration_utils.py:351] 2023-08-28 19:38:00,583 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:38:22,290 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:38:24,005 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:38:24,257 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75/special_tokens_map.json
 61%|██████    | 76/125 [03:37<19:09, 23.46s/it] 62%|██████▏   | 77/125 [03:37<13:13, 16.54s/it] 62%|██████▏   | 78/125 [03:37<09:08, 11.67s/it] 63%|██████▎   | 79/125 [03:38<06:19,  8.25s/it] 64%|██████▍   | 80/125 [03:38<04:24,  5.87s/it] 65%|██████▍   | 81/125 [03:38<03:04,  4.20s/it] 66%|██████▌   | 82/125 [03:39<02:10,  3.03s/it] 66%|██████▋   | 83/125 [03:39<01:32,  2.21s/it] 67%|██████▋   | 84/125 [03:39<01:09,  1.70s/it] 68%|██████▊   | 85/125 [03:40<00:51,  1.28s/it] 69%|██████▉   | 86/125 [03:40<00:38,  1.02it/s] 70%|██████▉   | 87/125 [03:40<00:29,  1.29it/s] 70%|███████   | 88/125 [03:41<00:23,  1.58it/s] 71%|███████   | 89/125 [03:41<00:19,  1.88it/s] 72%|███████▏  | 90/125 [03:41<00:16,  2.18it/s] 73%|███████▎  | 91/125 [03:41<00:13,  2.44it/s] 74%|███████▎  | 92/125 [03:42<00:12,  2.67it/s] 74%|███████▍  | 93/125 [03:42<00:15,  2.09it/s] 75%|███████▌  | 94/125 [03:43<00:13,  2.36it/s] 76%|███████▌  | 95/125 [03:43<00:11,  2.60it/s] 77%|███████▋  | 96/125 [03:43<00:10,  2.80it/s] 78%|███████▊  | 97/125 [03:44<00:09,  2.96it/s] 78%|███████▊  | 98/125 [03:44<00:08,  3.08it/s] 79%|███████▉  | 99/125 [03:44<00:08,  3.17it/s] 80%|████████  | 100/125 [03:44<00:06,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 19:39:05,052 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:39:05,052 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:39:05,052 >>   Batch size = 8
{'eval_loss': 0.9357678890228271, 'eval_runtime': 19.0913, 'eval_samples_per_second': 255.719, 'eval_steps_per_second': 32.004, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.36it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.88it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.17it/s][A
  4%|▎         | 22/611 [00:00<00:25, 22.83it/s][A
  4%|▍         | 27/611 [00:00<00:21, 27.53it/s][A
  5%|▌         | 32/611 [00:00<00:18, 31.60it/s][A
  6%|▌         | 37/611 [00:01<00:16, 34.94it/s][A
  7%|▋         | 42/611 [00:01<00:15, 37.63it/s][A
  8%|▊         | 47/611 [00:01<00:14, 39.66it/s][A
  9%|▊         | 52/611 [00:01<00:13, 41.24it/s][A
  9%|▉         | 57/611 [00:01<00:13, 42.18it/s][A
 10%|█         | 62/611 [00:01<00:12, 42.47it/s][A
 11%|█         | 67/611 [00:01<00:12, 42.65it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.15it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.51it/s][A
 13%|█▎        | 82/611 [00:02<00:12, 43.84it/s][A
 14%|█▍        | 87/611 [00:02<00:11, 44.15it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.40it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.62it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.59it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.22it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.08it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.17it/s][A
 20%|█▉        | 122/611 [00:03<00:11, 44.31it/s][A
 21%|██        | 127/611 [00:03<00:10, 44.50it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.59it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.81it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.85it/s][A
 24%|██▍       | 147/611 [00:03<00:14, 32.51it/s][A
 25%|██▍       | 152/611 [00:03<00:12, 35.53it/s][A
 26%|██▌       | 157/611 [00:03<00:11, 37.89it/s][A
 27%|██▋       | 162/611 [00:04<00:11, 39.83it/s][A
 27%|██▋       | 167/611 [00:04<00:10, 41.20it/s][A
 28%|██▊       | 172/611 [00:04<00:10, 42.30it/s][A
 29%|██▉       | 177/611 [00:04<00:10, 43.15it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.49it/s][A
 31%|███       | 187/611 [00:04<00:09, 43.48it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.44it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.67it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.93it/s][A
 34%|███▍      | 207/611 [00:05<00:09, 44.21it/s][A
 35%|███▍      | 212/611 [00:05<00:08, 44.46it/s][A
 36%|███▌      | 217/611 [00:05<00:08, 44.71it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.77it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.68it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.30it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.17it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.28it/s][A
 41%|████      | 252/611 [00:06<00:08, 44.39it/s][A
 42%|████▏     | 257/611 [00:06<00:07, 44.52it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 44.75it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.57it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.71it/s][A
 45%|████▌     | 277/611 [00:07<00:16, 20.43it/s][A
 46%|████▌     | 282/611 [00:07<00:13, 24.44it/s][A
 47%|████▋     | 287/611 [00:07<00:11, 28.29it/s][A
 48%|████▊     | 292/611 [00:07<00:10, 31.86it/s][A
 49%|████▊     | 297/611 [00:07<00:08, 34.95it/s][A
 49%|████▉     | 302/611 [00:07<00:08, 37.44it/s][A
 50%|█████     | 307/611 [00:07<00:07, 39.49it/s][A
 51%|█████     | 312/611 [00:07<00:07, 40.92it/s][A
 52%|█████▏    | 317/611 [00:07<00:07, 41.57it/s][A
 53%|█████▎    | 322/611 [00:08<00:06, 42.03it/s][A
 54%|█████▎    | 327/611 [00:08<00:06, 42.69it/s][A
 54%|█████▍    | 332/611 [00:08<00:06, 43.33it/s][A
 55%|█████▌    | 337/611 [00:08<00:06, 43.72it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 44.14it/s][A
 57%|█████▋    | 347/611 [00:08<00:05, 44.42it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.64it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.54it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.14it/s][A
 60%|██████    | 367/611 [00:09<00:05, 44.09it/s][A
 61%|██████    | 372/611 [00:09<00:05, 44.14it/s][A
 62%|██████▏   | 377/611 [00:09<00:05, 44.23it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 44.26it/s][A
 63%|██████▎   | 387/611 [00:10<00:05, 44.44it/s][A
 64%|██████▍   | 392/611 [00:10<00:14, 15.58it/s][A
 65%|██████▍   | 397/611 [00:10<00:11, 19.36it/s][A
 66%|██████▌   | 402/611 [00:10<00:08, 23.35it/s][A
 67%|██████▋   | 407/611 [00:10<00:07, 27.29it/s][A
 67%|██████▋   | 412/611 [00:10<00:06, 30.97it/s][A
 68%|██████▊   | 417/611 [00:10<00:05, 34.18it/s][A
 69%|██████▉   | 422/611 [00:11<00:05, 36.88it/s][A
 70%|██████▉   | 427/611 [00:11<00:04, 38.82it/s][A
 71%|███████   | 432/611 [00:11<00:04, 40.13it/s][A
 72%|███████▏  | 437/611 [00:11<00:04, 40.97it/s][A
 72%|███████▏  | 442/611 [00:11<00:04, 41.84it/s][A
 73%|███████▎  | 447/611 [00:11<00:03, 42.66it/s][A
 74%|███████▍  | 452/611 [00:11<00:03, 43.34it/s][A
 75%|███████▍  | 457/611 [00:11<00:03, 43.83it/s][A
 76%|███████▌  | 462/611 [00:11<00:03, 44.14it/s][A
 76%|███████▋  | 467/611 [00:12<00:03, 44.38it/s][A
 77%|███████▋  | 472/611 [00:12<00:03, 44.38it/s][A
 78%|███████▊  | 477/611 [00:12<00:03, 44.22it/s][A
 79%|███████▉  | 482/611 [00:12<00:02, 43.99it/s][A
 80%|███████▉  | 487/611 [00:12<00:02, 44.01it/s][A
 81%|████████  | 492/611 [00:12<00:02, 44.17it/s][A
 81%|████████▏ | 497/611 [00:12<00:04, 25.79it/s][A
 82%|████████▏ | 502/611 [00:13<00:03, 29.56it/s][A
 83%|████████▎ | 507/611 [00:13<00:03, 32.94it/s][A
 84%|████████▍ | 512/611 [00:13<00:02, 35.78it/s][A
 85%|████████▍ | 517/611 [00:13<00:02, 38.16it/s][A
 85%|████████▌ | 522/611 [00:13<00:02, 39.93it/s][A
 86%|████████▋ | 527/611 [00:13<00:02, 41.37it/s][A
 87%|████████▋ | 532/611 [00:13<00:01, 42.24it/s][A
 88%|████████▊ | 537/611 [00:13<00:01, 42.52it/s][A
 89%|████████▊ | 542/611 [00:13<00:01, 42.73it/s][A
 90%|████████▉ | 547/611 [00:14<00:01, 43.13it/s][A
 90%|█████████ | 552/611 [00:14<00:01, 43.62it/s][A
 91%|█████████ | 557/611 [00:14<00:01, 44.02it/s][A
 92%|█████████▏| 562/611 [00:14<00:01, 44.24it/s][A
 93%|█████████▎| 567/611 [00:14<00:00, 44.36it/s][A
 94%|█████████▎| 572/611 [00:14<00:00, 44.63it/s][A
 94%|█████████▍| 577/611 [00:14<00:00, 44.55it/s][A
 95%|█████████▌| 582/611 [00:14<00:00, 44.24it/s][A
 96%|█████████▌| 587/611 [00:14<00:00, 44.06it/s][A
 97%|█████████▋| 592/611 [00:15<00:00, 44.07it/s][A
 98%|█████████▊| 597/611 [00:15<00:00, 44.43it/s][A
 99%|█████████▊| 602/611 [00:15<00:00, 44.68it/s][A
 99%|█████████▉| 607/611 [00:15<00:00, 44.82it/s][A                                                 
                                                 [A 80%|████████  | 100/125 [04:00<00:06,  3.58it/s]
100%|██████████| 611/611 [00:15<00:00, 44.82it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:39:20,883 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100
[INFO|configuration_utils.py:351] 2023-08-28 19:39:21,801 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:39:47,206 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:39:49,269 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:39:49,447 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100/special_tokens_map.json
 81%|████████  | 101/125 [05:10<10:18, 25.78s/it] 82%|████████▏ | 102/125 [05:11<07:01, 18.33s/it] 82%|████████▏ | 103/125 [05:11<04:44, 12.92s/it] 83%|████████▎ | 104/125 [05:12<03:14,  9.26s/it] 84%|████████▍ | 105/125 [05:12<02:11,  6.57s/it] 85%|████████▍ | 106/125 [05:12<01:29,  4.69s/it] 86%|████████▌ | 107/125 [05:13<01:00,  3.37s/it] 86%|████████▋ | 108/125 [05:13<00:41,  2.45s/it] 87%|████████▋ | 109/125 [05:13<00:28,  1.80s/it] 88%|████████▊ | 110/125 [05:13<00:20,  1.35s/it] 89%|████████▉ | 111/125 [05:14<00:14,  1.03s/it] 90%|████████▉ | 112/125 [05:14<00:10,  1.23it/s] 90%|█████████ | 113/125 [05:15<00:09,  1.33it/s] 91%|█████████ | 114/125 [05:15<00:06,  1.63it/s] 92%|█████████▏| 115/125 [05:15<00:05,  1.93it/s] 93%|█████████▎| 116/125 [05:16<00:04,  2.21it/s] 94%|█████████▎| 117/125 [05:16<00:03,  2.47it/s] 94%|█████████▍| 118/125 [05:16<00:02,  2.69it/s] 95%|█████████▌| 119/125 [05:16<00:02,  2.87it/s] 96%|█████████▌| 120/125 [05:17<00:01,  3.01it/s] 97%|█████████▋| 121/125 [05:17<00:01,  3.12it/s] 98%|█████████▊| 122/125 [05:17<00:00,  3.20it/s] 98%|█████████▊| 123/125 [05:18<00:00,  3.04it/s] 99%|█████████▉| 124/125 [05:18<00:00,  3.14it/s]100%|██████████| 125/125 [05:18<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 19:40:38,771 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:40:38,771 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:40:38,771 >>   Batch size = 8
{'eval_loss': 0.9359360337257385, 'eval_runtime': 15.5443, 'eval_samples_per_second': 314.071, 'eval_steps_per_second': 39.307, 'epoch': 4.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.14it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.69it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.16it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.27it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.70it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.30it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.74it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.42it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.66it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.76it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.81it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.87it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.94it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.82it/s][A
 13%|█▎        | 77/611 [00:01<00:11, 44.55it/s][A
 13%|█▎        | 82/611 [00:02<00:19, 26.74it/s][A
 14%|█▍        | 87/611 [00:02<00:17, 30.63it/s][A
 15%|█▍        | 91/611 [00:02<00:27, 18.80it/s][A
 16%|█▌        | 96/611 [00:02<00:22, 23.03it/s][A
 17%|█▋        | 101/611 [00:02<00:18, 27.13it/s][A
 17%|█▋        | 106/611 [00:02<00:16, 30.94it/s][A
 18%|█▊        | 111/611 [00:03<00:14, 34.19it/s][A
 19%|█▉        | 116/611 [00:03<00:13, 36.94it/s][A
 20%|█▉        | 121/611 [00:03<00:12, 39.06it/s][A
 21%|██        | 126/611 [00:03<00:11, 40.64it/s][A
 21%|██▏       | 131/611 [00:03<00:11, 41.46it/s][A
 22%|██▏       | 136/611 [00:03<00:11, 41.94it/s][A
 23%|██▎       | 141/611 [00:03<00:11, 42.54it/s][A
 24%|██▍       | 146/611 [00:03<00:10, 43.24it/s][A
 25%|██▍       | 151/611 [00:03<00:10, 43.70it/s][A
 26%|██▌       | 156/611 [00:04<00:10, 44.16it/s][A
 26%|██▋       | 161/611 [00:04<00:10, 44.37it/s][A
 27%|██▋       | 166/611 [00:04<00:09, 44.70it/s][A
 28%|██▊       | 171/611 [00:04<00:09, 44.68it/s][A
 29%|██▉       | 176/611 [00:04<00:09, 44.37it/s][A
 30%|██▉       | 181/611 [00:04<00:09, 44.09it/s][A
 30%|███       | 186/611 [00:04<00:09, 44.06it/s][A
 31%|███▏      | 191/611 [00:04<00:09, 44.16it/s][A
 32%|███▏      | 196/611 [00:04<00:09, 44.42it/s][A
 33%|███▎      | 201/611 [00:05<00:09, 44.50it/s][A
 34%|███▎      | 206/611 [00:05<00:09, 44.74it/s][A
 35%|███▍      | 211/611 [00:05<00:11, 33.85it/s][A
 35%|███▌      | 216/611 [00:05<00:10, 36.54it/s][A
 36%|███▌      | 221/611 [00:05<00:10, 38.73it/s][A
 37%|███▋      | 226/611 [00:05<00:09, 40.43it/s][A
 38%|███▊      | 231/611 [00:05<00:09, 41.75it/s][A
 39%|███▊      | 236/611 [00:05<00:08, 42.70it/s][A
 39%|███▉      | 241/611 [00:06<00:08, 43.42it/s][A
 40%|████      | 246/611 [00:06<00:08, 43.75it/s][A
 41%|████      | 251/611 [00:06<00:08, 43.58it/s][A
 42%|████▏     | 256/611 [00:06<00:08, 43.51it/s][A
 43%|████▎     | 261/611 [00:06<00:08, 43.68it/s][A
 44%|████▎     | 266/611 [00:06<00:07, 44.00it/s][A
 44%|████▍     | 271/611 [00:06<00:07, 44.30it/s][A
 45%|████▌     | 276/611 [00:06<00:07, 44.57it/s][A
 46%|████▌     | 281/611 [00:06<00:07, 44.74it/s][A
 47%|████▋     | 286/611 [00:07<00:07, 44.86it/s][A
 48%|████▊     | 291/611 [00:07<00:07, 44.62it/s][A
 48%|████▊     | 296/611 [00:07<00:07, 44.26it/s][A
 49%|████▉     | 301/611 [00:07<00:07, 44.14it/s][A
 50%|█████     | 306/611 [00:07<00:06, 44.07it/s][A
 51%|█████     | 311/611 [00:07<00:06, 44.22it/s][A
 52%|█████▏    | 316/611 [00:07<00:06, 44.32it/s][A
 53%|█████▎    | 321/611 [00:07<00:06, 44.41it/s][A
 53%|█████▎    | 326/611 [00:08<00:06, 44.55it/s][A
 54%|█████▍    | 331/611 [00:08<00:06, 44.61it/s][A
 55%|█████▍    | 336/611 [00:08<00:06, 44.45it/s][A
 56%|█████▌    | 341/611 [00:08<00:12, 20.97it/s][A
 57%|█████▋    | 346/611 [00:08<00:10, 25.03it/s][A
 57%|█████▋    | 351/611 [00:08<00:09, 28.86it/s][A
 58%|█████▊    | 356/611 [00:09<00:07, 32.34it/s][A
 59%|█████▉    | 361/611 [00:09<00:07, 35.37it/s][A
 60%|█████▉    | 366/611 [00:09<00:06, 37.71it/s][A
 61%|██████    | 371/611 [00:09<00:06, 39.76it/s][A
 62%|██████▏   | 376/611 [00:09<00:05, 41.19it/s][A
 62%|██████▏   | 381/611 [00:09<00:05, 41.86it/s][A
 63%|██████▎   | 386/611 [00:09<00:05, 42.19it/s][A
 64%|██████▍   | 391/611 [00:09<00:05, 42.56it/s][A
 65%|██████▍   | 396/611 [00:09<00:04, 43.15it/s][A
 66%|██████▌   | 401/611 [00:10<00:04, 43.74it/s][A
 66%|██████▋   | 406/611 [00:10<00:04, 44.16it/s][A
 67%|██████▋   | 411/611 [00:10<00:04, 44.49it/s][A
 68%|██████▊   | 416/611 [00:10<00:04, 44.62it/s][A
 69%|██████▉   | 421/611 [00:10<00:04, 44.55it/s][A
 70%|██████▉   | 426/611 [00:10<00:04, 44.27it/s][A
 71%|███████   | 431/611 [00:10<00:04, 43.96it/s][A
 71%|███████▏  | 436/611 [00:10<00:03, 44.00it/s][A
 72%|███████▏  | 441/611 [00:11<00:03, 44.13it/s][A
 73%|███████▎  | 446/611 [00:11<00:03, 44.35it/s][A
 74%|███████▍  | 451/611 [00:11<00:03, 44.48it/s][A
 75%|███████▍  | 456/611 [00:12<00:10, 15.23it/s][A
 75%|███████▌  | 461/611 [00:12<00:07, 18.98it/s][A
 76%|███████▋  | 466/611 [00:12<00:06, 22.98it/s][A
 77%|███████▋  | 471/611 [00:12<00:05, 26.90it/s][A
 78%|███████▊  | 476/611 [00:12<00:04, 30.65it/s][A
 79%|███████▊  | 481/611 [00:12<00:03, 33.89it/s][A
 80%|███████▉  | 486/611 [00:12<00:03, 36.63it/s][A
 80%|████████  | 491/611 [00:12<00:03, 38.65it/s][A
 81%|████████  | 496/611 [00:12<00:02, 39.88it/s][A
 82%|████████▏ | 501/611 [00:13<00:02, 40.85it/s][A
 83%|████████▎ | 506/611 [00:13<00:02, 41.77it/s][A
 84%|████████▎ | 511/611 [00:13<00:02, 42.63it/s][A
 84%|████████▍ | 516/611 [00:13<00:02, 43.33it/s][A
 85%|████████▌ | 521/611 [00:13<00:02, 43.83it/s][A
 86%|████████▌ | 526/611 [00:13<00:01, 44.17it/s][A
 87%|████████▋ | 531/611 [00:13<00:01, 44.41it/s][A
 88%|████████▊ | 536/611 [00:13<00:01, 44.34it/s][A
 89%|████████▊ | 541/611 [00:13<00:01, 44.12it/s][A
 89%|████████▉ | 546/611 [00:14<00:01, 43.89it/s][A
 90%|█████████ | 551/611 [00:14<00:01, 43.98it/s][A
 91%|█████████ | 556/611 [00:14<00:01, 44.22it/s][A
 92%|█████████▏| 561/611 [00:14<00:01, 31.90it/s][A
 93%|█████████▎| 566/611 [00:14<00:01, 34.94it/s][A
 93%|█████████▎| 571/611 [00:14<00:01, 37.38it/s][A
 94%|█████████▍| 576/611 [00:14<00:00, 39.43it/s][A
 95%|█████████▌| 581/611 [00:18<00:06,  4.78it/s][A
 96%|█████████▌| 584/611 [00:18<00:04,  5.69it/s][A
 96%|█████████▋| 589/611 [00:18<00:02,  7.98it/s][A
 97%|█████████▋| 594/611 [00:18<00:01, 10.82it/s][A
 98%|█████████▊| 599/611 [00:18<00:00, 14.19it/s][A
 99%|█████████▉| 604/611 [00:18<00:00, 17.99it/s][A
100%|█████████▉| 609/611 [00:18<00:00, 22.06it/s][A                                                 
                                                 [A100%|██████████| 125/125 [05:37<00:00,  3.55it/s]
100%|██████████| 611/611 [00:18<00:00, 22.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:40:58,679 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125
[INFO|configuration_utils.py:351] 2023-08-28 19:41:00,190 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:41:23,057 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:41:23,311 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:41:23,517 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:41:58,867 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:41:58,992 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50 (score: 0.9286497235298157).
                                                 100%|██████████| 125/125 [07:17<00:00,  3.55it/s]100%|██████████| 125/125 [07:17<00:00,  3.50s/it]
[INFO|trainer.py:1894] 2023-08-28 19:42:37,502 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 19:42:37,828 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:42:56,276 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:42:56,821 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:57,010 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:42:59,377 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,417 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,417 >>   train_loss               =      0.466
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,417 >>   train_runtime            = 0:07:17.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,418 >>   train_samples            =       1574
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,418 >>   train_samples_per_second =     17.999
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,418 >>   train_steps_per_second   =      0.286
{'eval_loss': 0.9377200603485107, 'eval_runtime': 18.8465, 'eval_samples_per_second': 259.041, 'eval_steps_per_second': 32.42, 'epoch': 5.0}
{'train_runtime': 437.2412, 'train_samples_per_second': 17.999, 'train_steps_per_second': 0.286, 'train_loss': 0.46596710205078123, 'epoch': 5.0}
08/28/2023 19:43:00 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:43:00,587 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:43:00,587 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:43:00,587 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:11, 54.53it/s]  2%|▏         | 12/611 [00:00<00:12, 48.70it/s]  3%|▎         | 17/611 [00:00<00:12, 47.08it/s]  4%|▎         | 22/611 [00:00<00:12, 46.47it/s]  4%|▍         | 27/611 [00:00<00:12, 46.09it/s]  5%|▌         | 32/611 [00:00<00:12, 45.72it/s]  6%|▌         | 37/611 [00:00<00:12, 45.51it/s]  7%|▋         | 42/611 [00:00<00:12, 45.06it/s]  8%|▊         | 47/611 [00:01<00:12, 44.51it/s]  9%|▊         | 52/611 [00:01<00:12, 44.29it/s]  9%|▉         | 57/611 [00:01<00:12, 44.33it/s] 10%|█         | 62/611 [00:01<00:12, 44.51it/s] 11%|█         | 67/611 [00:01<00:12, 44.75it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.87it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.96it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.89it/s] 14%|█▍        | 87/611 [00:02<00:14, 36.05it/s] 15%|█▌        | 92/611 [00:02<00:13, 38.31it/s] 16%|█▌        | 97/611 [00:02<00:12, 40.02it/s] 17%|█▋        | 102/611 [00:02<00:12, 41.34it/s] 18%|█▊        | 107/611 [00:02<00:11, 42.31it/s] 18%|█▊        | 112/611 [00:02<00:11, 43.09it/s] 19%|█▉        | 117/611 [00:02<00:11, 43.59it/s] 20%|█▉        | 122/611 [00:02<00:11, 43.85it/s] 21%|██        | 127/611 [00:02<00:11, 43.74it/s] 22%|██▏       | 132/611 [00:03<00:10, 43.73it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.85it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.14it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.41it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.68it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.81it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.84it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.61it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.42it/s] 29%|██▉       | 177/611 [00:04<00:09, 44.29it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.18it/s] 31%|███       | 187/611 [00:04<00:09, 44.31it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.51it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.73it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.82it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.79it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.71it/s] 36%|███▌      | 217/611 [00:05<00:24, 16.29it/s] 36%|███▋      | 222/611 [00:05<00:19, 20.14it/s] 37%|███▋      | 227/611 [00:05<00:15, 24.11it/s] 38%|███▊      | 232/611 [00:05<00:13, 28.01it/s] 39%|███▉      | 237/611 [00:06<00:11, 31.64it/s] 40%|███▉      | 242/611 [00:06<00:10, 34.83it/s] 40%|████      | 247/611 [00:06<00:09, 37.47it/s] 41%|████      | 252/611 [00:06<00:09, 39.32it/s] 42%|████▏     | 257/611 [00:06<00:08, 40.47it/s] 43%|████▎     | 262/611 [00:06<00:08, 41.14it/s] 44%|████▎     | 267/611 [00:06<00:08, 41.84it/s] 45%|████▍     | 272/611 [00:06<00:07, 42.54it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.15it/s] 46%|████▌     | 282/611 [00:07<00:07, 43.48it/s] 47%|████▋     | 287/611 [00:07<00:07, 44.19it/s] 48%|████▊     | 292/611 [00:07<00:07, 44.42it/s] 49%|████▊     | 297/611 [00:07<00:07, 44.67it/s] 49%|████▉     | 302/611 [00:07<00:06, 44.56it/s] 50%|█████     | 307/611 [00:07<00:06, 44.22it/s] 51%|█████     | 312/611 [00:07<00:06, 44.19it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.14it/s] 53%|█████▎    | 322/611 [00:08<00:10, 26.36it/s] 54%|█████▎    | 327/611 [00:08<00:09, 30.06it/s] 54%|█████▍    | 332/611 [00:08<00:08, 33.38it/s] 55%|█████▌    | 337/611 [00:08<00:07, 36.20it/s] 56%|█████▌    | 342/611 [00:08<00:07, 38.39it/s] 57%|█████▋    | 347/611 [00:08<00:06, 40.28it/s] 58%|█████▊    | 352/611 [00:08<00:06, 41.55it/s] 58%|█████▊    | 357/611 [00:08<00:05, 42.43it/s] 59%|█████▉    | 362/611 [00:09<00:05, 42.70it/s] 60%|██████    | 367/611 [00:09<00:05, 43.04it/s] 61%|██████    | 372/611 [00:09<00:05, 43.51it/s] 62%|██████▏   | 377/611 [00:09<00:05, 43.98it/s] 63%|██████▎   | 382/611 [00:09<00:05, 44.24it/s] 63%|██████▎   | 387/611 [00:09<00:05, 44.62it/s] 64%|██████▍   | 392/611 [00:10<00:09, 21.98it/s] 65%|██████▍   | 397/611 [00:10<00:08, 26.00it/s] 66%|██████▌   | 402/611 [00:10<00:07, 29.76it/s] 67%|██████▋   | 407/611 [00:10<00:06, 33.17it/s] 67%|██████▋   | 412/611 [00:10<00:05, 36.06it/s] 68%|██████▊   | 417/611 [00:10<00:05, 38.42it/s] 69%|██████▉   | 422/611 [00:10<00:04, 40.30it/s] 70%|██████▉   | 427/611 [00:10<00:04, 41.52it/s] 71%|███████   | 432/611 [00:11<00:04, 42.07it/s] 72%|███████▏  | 437/611 [00:11<00:04, 42.63it/s] 72%|███████▏  | 442/611 [00:11<00:03, 43.02it/s] 73%|███████▎  | 447/611 [00:11<00:03, 43.62it/s] 74%|███████▍  | 452/611 [00:11<00:03, 44.02it/s] 75%|███████▍  | 457/611 [00:11<00:03, 44.36it/s] 76%|███████▌  | 462/611 [00:11<00:03, 44.60it/s] 76%|███████▋  | 467/611 [00:11<00:03, 44.74it/s] 77%|███████▋  | 472/611 [00:11<00:03, 44.69it/s] 78%|███████▊  | 477/611 [00:12<00:03, 44.52it/s] 79%|███████▉  | 482/611 [00:12<00:02, 44.39it/s] 80%|███████▉  | 487/611 [00:12<00:02, 44.35it/s] 81%|████████  | 492/611 [00:12<00:02, 44.47it/s] 81%|████████▏ | 497/611 [00:12<00:02, 44.65it/s] 82%|████████▏ | 502/611 [00:12<00:02, 44.78it/s] 83%|████████▎ | 507/611 [00:13<00:05, 17.81it/s] 84%|████████▍ | 512/611 [00:13<00:04, 21.77it/s] 85%|████████▍ | 517/611 [00:13<00:03, 25.75it/s] 85%|████████▌ | 522/611 [00:13<00:03, 29.58it/s] 86%|████████▋ | 527/611 [00:13<00:02, 32.98it/s] 87%|████████▋ | 532/611 [00:13<00:02, 35.87it/s] 88%|████████▊ | 537/611 [00:13<00:01, 38.26it/s] 89%|████████▊ | 542/611 [00:14<00:01, 39.98it/s] 90%|████████▉ | 547/611 [00:14<00:01, 40.99it/s] 90%|█████████ | 552/611 [00:14<00:01, 41.77it/s] 91%|█████████ | 557/611 [00:14<00:01, 42.54it/s] 92%|█████████▏| 562/611 [00:14<00:01, 43.16it/s] 93%|█████████▎| 567/611 [00:14<00:01, 43.61it/s] 94%|█████████▎| 572/611 [00:14<00:00, 44.11it/s] 94%|█████████▍| 577/611 [00:14<00:00, 44.45it/s] 95%|█████████▌| 582/611 [00:14<00:00, 44.63it/s] 96%|█████████▌| 587/611 [00:15<00:00, 44.57it/s] 97%|█████████▋| 592/611 [00:15<00:00, 44.38it/s] 98%|█████████▊| 597/611 [00:15<00:00, 44.26it/s] 99%|█████████▊| 602/611 [00:15<00:00, 44.33it/s] 99%|█████████▉| 607/611 [00:15<00:00, 44.40it/s]100%|██████████| 611/611 [00:15<00:00, 39.17it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:43:16,267 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   eval_loss               =     0.9286
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   eval_runtime            = 0:00:15.63
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   eval_samples_per_second =    312.241
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   eval_steps_per_second   =     39.078
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:16,268 >>   perplexity              =     2.5311
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,807 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:43:46,730 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:43:46,732 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:47,616 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:48,838 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:48,921 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,579 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:43:52,786 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:43:53,001 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:53,604 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:53,901 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:53,901 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-25
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-100
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-50
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-75
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-125
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.75it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.72it/s]Extractor Predicting: 31it [00:18,  1.62it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.54it/s]Extractor Predicting: 37it [00:22,  1.61it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:24,  1.63it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:34,  1.57it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:35,  1.52it/s]Extractor Predicting: 59it [00:35,  1.57it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:37,  1.39it/s]Extractor Predicting: 62it [00:38,  1.42it/s]Extractor Predicting: 63it [00:38,  1.47it/s]Extractor Predicting: 64it [00:39,  1.51it/s]Extractor Predicting: 65it [00:40,  1.52it/s]Extractor Predicting: 66it [00:40,  1.40it/s]Extractor Predicting: 67it [00:41,  1.47it/s]Extractor Predicting: 68it [00:42,  1.53it/s]Extractor Predicting: 69it [00:42,  1.54it/s]Extractor Predicting: 70it [00:43,  1.52it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:44,  1.53it/s]Extractor Predicting: 73it [00:45,  1.56it/s]Extractor Predicting: 74it [00:46,  1.31it/s]Extractor Predicting: 75it [00:47,  1.38it/s]Extractor Predicting: 76it [00:47,  1.44it/s]Extractor Predicting: 77it [00:48,  1.32it/s]Extractor Predicting: 78it [00:49,  1.41it/s]Extractor Predicting: 79it [00:49,  1.48it/s]Extractor Predicting: 80it [00:50,  1.52it/s]Extractor Predicting: 81it [00:51,  1.48it/s]Extractor Predicting: 82it [00:51,  1.50it/s]Extractor Predicting: 83it [00:52,  1.53it/s]Extractor Predicting: 84it [00:52,  1.55it/s]Extractor Predicting: 85it [00:53,  1.55it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:54,  1.55it/s]Extractor Predicting: 88it [00:55,  1.52it/s]Extractor Predicting: 89it [00:56,  1.34it/s]Extractor Predicting: 90it [00:57,  1.39it/s]Extractor Predicting: 91it [00:57,  1.43it/s]Extractor Predicting: 92it [00:58,  1.39it/s]Extractor Predicting: 93it [00:59,  1.49it/s]Extractor Predicting: 94it [00:59,  1.53it/s]Extractor Predicting: 95it [01:00,  1.57it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:01,  1.44it/s]Extractor Predicting: 98it [01:02,  1.46it/s]Extractor Predicting: 99it [01:03,  1.45it/s]Extractor Predicting: 100it [01:03,  1.39it/s]Extractor Predicting: 101it [01:05,  1.22it/s]Extractor Predicting: 102it [01:05,  1.30it/s]Extractor Predicting: 103it [01:06,  1.37it/s]Extractor Predicting: 104it [01:06,  1.44it/s]Extractor Predicting: 105it [01:07,  1.49it/s]Extractor Predicting: 106it [01:08,  1.55it/s]Extractor Predicting: 107it [01:08,  1.58it/s]Extractor Predicting: 108it [01:09,  1.62it/s]Extractor Predicting: 109it [01:09,  1.61it/s]Extractor Predicting: 110it [01:10,  1.62it/s]Extractor Predicting: 111it [01:11,  1.63it/s]Extractor Predicting: 112it [01:11,  1.64it/s]Extractor Predicting: 113it [01:12,  1.60it/s]Extractor Predicting: 114it [01:13,  1.58it/s]Extractor Predicting: 115it [01:13,  1.60it/s]Extractor Predicting: 116it [01:14,  1.50it/s]Extractor Predicting: 117it [01:15,  1.51it/s]Extractor Predicting: 118it [01:15,  1.53it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:17,  1.50it/s]Extractor Predicting: 121it [01:17,  1.42it/s]Extractor Predicting: 122it [01:18,  1.45it/s]Extractor Predicting: 123it [01:19,  1.50it/s]Extractor Predicting: 124it [01:19,  1.53it/s]Extractor Predicting: 125it [01:20,  1.55it/s]Extractor Predicting: 126it [01:21,  1.33it/s]Extractor Predicting: 127it [01:22,  1.39it/s]Extractor Predicting: 128it [01:22,  1.45it/s]Extractor Predicting: 129it [01:23,  1.50it/s]Extractor Predicting: 130it [01:23,  1.49it/s]Extractor Predicting: 131it [01:25,  1.24it/s]Extractor Predicting: 132it [01:25,  1.34it/s]Extractor Predicting: 133it [01:26,  1.39it/s]Extractor Predicting: 134it [01:26,  1.44it/s]Extractor Predicting: 135it [01:27,  1.39it/s]Extractor Predicting: 136it [01:28,  1.45it/s]Extractor Predicting: 137it [01:29,  1.46it/s]Extractor Predicting: 138it [01:29,  1.50it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:31,  1.53it/s]Extractor Predicting: 142it [01:32,  1.50it/s]Extractor Predicting: 143it [01:32,  1.50it/s]Extractor Predicting: 144it [01:33,  1.55it/s]Extractor Predicting: 145it [01:34,  1.60it/s]Extractor Predicting: 146it [01:34,  1.58it/s]Extractor Predicting: 147it [01:35,  1.52it/s]Extractor Predicting: 148it [01:36,  1.54it/s]Extractor Predicting: 149it [01:36,  1.54it/s]Extractor Predicting: 150it [01:37,  1.53it/s]Extractor Predicting: 151it [01:38,  1.53it/s]Extractor Predicting: 152it [01:38,  1.48it/s]Extractor Predicting: 153it [01:39,  1.50it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:40,  1.53it/s]Extractor Predicting: 156it [01:41,  1.47it/s]Extractor Predicting: 157it [01:42,  1.34it/s]Extractor Predicting: 158it [01:43,  1.34it/s]Extractor Predicting: 159it [01:43,  1.40it/s]Extractor Predicting: 160it [01:44,  1.45it/s]Extractor Predicting: 161it [01:45,  1.48it/s]Extractor Predicting: 162it [01:45,  1.48it/s]Extractor Predicting: 163it [01:46,  1.49it/s]Extractor Predicting: 164it [01:47,  1.54it/s]Extractor Predicting: 165it [01:47,  1.54it/s]Extractor Predicting: 166it [01:48,  1.57it/s]Extractor Predicting: 167it [01:49,  1.43it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:50,  1.51it/s]Extractor Predicting: 170it [01:51,  1.52it/s]Extractor Predicting: 171it [01:51,  1.56it/s]Extractor Predicting: 172it [01:52,  1.54it/s]Extractor Predicting: 173it [01:52,  1.52it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:54,  1.53it/s]Extractor Predicting: 176it [01:55,  1.41it/s]Extractor Predicting: 177it [01:55,  1.42it/s]Extractor Predicting: 178it [01:56,  1.45it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:57,  1.50it/s]Extractor Predicting: 181it [01:58,  1.52it/s]Extractor Predicting: 182it [01:59,  1.32it/s]Extractor Predicting: 183it [02:00,  1.29it/s]Extractor Predicting: 184it [02:00,  1.36it/s]Extractor Predicting: 185it [02:01,  1.51it/s]Extractor Predicting: 185it [02:01,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:28,239 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:28,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:28,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:28,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:28,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:46:29,746 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:46:29,747 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:30,406 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:31,471 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:31,683 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:35,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:36,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:36,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:36,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:36,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:46:36,695 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:46:36,696 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:37,381 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:37,554 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:37,555 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3884393063583815,
  "recall": 0.13764850471118395,
  "score": 0.20326678765880216,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.58it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.32it/s]Extractor Predicting: 12it [00:07,  1.41it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.38it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.43it/s]Extractor Predicting: 26it [00:17,  1.40it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.65it/s]Extractor Predicting: 34it [00:22,  1.67it/s]Extractor Predicting: 35it [00:22,  1.69it/s]Extractor Predicting: 36it [00:23,  1.66it/s]Extractor Predicting: 37it [00:23,  1.69it/s]Extractor Predicting: 38it [00:24,  1.69it/s]Extractor Predicting: 39it [00:25,  1.64it/s]Extractor Predicting: 40it [00:25,  1.66it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.65it/s]Extractor Predicting: 44it [00:28,  1.69it/s]Extractor Predicting: 45it [00:28,  1.71it/s]Extractor Predicting: 46it [00:29,  1.75it/s]Extractor Predicting: 47it [00:29,  1.74it/s]Extractor Predicting: 48it [00:30,  1.74it/s]Extractor Predicting: 49it [00:30,  1.73it/s]Extractor Predicting: 50it [00:31,  1.71it/s]Extractor Predicting: 51it [00:32,  1.73it/s]Extractor Predicting: 52it [00:32,  1.73it/s]Extractor Predicting: 53it [00:33,  1.68it/s]Extractor Predicting: 54it [00:33,  1.66it/s]Extractor Predicting: 55it [00:34,  1.65it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:35,  1.69it/s]Extractor Predicting: 58it [00:36,  1.67it/s]Extractor Predicting: 59it [00:36,  1.66it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.46it/s]Extractor Predicting: 64it [00:40,  1.47it/s]Extractor Predicting: 65it [00:40,  1.48it/s]Extractor Predicting: 66it [00:41,  1.48it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:43,  1.48it/s]Extractor Predicting: 70it [00:44,  1.51it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:45,  1.57it/s]Extractor Predicting: 73it [00:46,  1.40it/s]Extractor Predicting: 74it [00:47,  1.46it/s]Extractor Predicting: 75it [00:47,  1.51it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.51it/s]Extractor Predicting: 79it [00:50,  1.60it/s]Extractor Predicting: 80it [00:50,  1.67it/s]Extractor Predicting: 81it [00:51,  1.62it/s]Extractor Predicting: 82it [00:51,  1.66it/s]Extractor Predicting: 83it [00:52,  1.62it/s]Extractor Predicting: 84it [00:53,  1.62it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:54,  1.43it/s]Extractor Predicting: 87it [00:55,  1.47it/s]Extractor Predicting: 88it [00:55,  1.53it/s]Extractor Predicting: 89it [00:56,  1.54it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:57,  1.49it/s]Extractor Predicting: 92it [00:58,  1.51it/s]Extractor Predicting: 93it [00:59,  1.56it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.58it/s]Extractor Predicting: 96it [01:01,  1.36it/s]Extractor Predicting: 97it [01:01,  1.44it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:03,  1.52it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.50it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.58it/s]Extractor Predicting: 105it [01:07,  1.60it/s]Extractor Predicting: 106it [01:07,  1.56it/s]Extractor Predicting: 107it [01:08,  1.56it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:09,  1.60it/s]Extractor Predicting: 110it [01:10,  1.63it/s]Extractor Predicting: 111it [01:10,  1.57it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:12,  1.57it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:14,  1.54it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:18,  1.56it/s]Extractor Predicting: 124it [01:19,  1.55it/s]Extractor Predicting: 125it [01:19,  1.58it/s]Extractor Predicting: 126it [01:20,  1.43it/s]Extractor Predicting: 127it [01:21,  1.51it/s]Extractor Predicting: 128it [01:22,  1.25it/s]Extractor Predicting: 129it [01:22,  1.35it/s]Extractor Predicting: 130it [01:23,  1.44it/s]Extractor Predicting: 131it [01:24,  1.47it/s]Extractor Predicting: 132it [01:24,  1.46it/s]Extractor Predicting: 133it [01:25,  1.50it/s]Extractor Predicting: 134it [01:26,  1.52it/s]Extractor Predicting: 135it [01:26,  1.56it/s]Extractor Predicting: 136it [01:27,  1.61it/s]Extractor Predicting: 137it [01:28,  1.52it/s]Extractor Predicting: 138it [01:28,  1.56it/s]Extractor Predicting: 139it [01:29,  1.59it/s]Extractor Predicting: 140it [01:29,  1.61it/s]Extractor Predicting: 141it [01:30,  1.43it/s]Extractor Predicting: 142it [01:31,  1.33it/s]Extractor Predicting: 143it [01:32,  1.34it/s]Extractor Predicting: 144it [01:32,  1.45it/s]Extractor Predicting: 145it [01:33,  1.47it/s]Extractor Predicting: 146it [01:34,  1.53it/s]Extractor Predicting: 147it [01:34,  1.57it/s]Extractor Predicting: 148it [01:35,  1.58it/s]Extractor Predicting: 149it [01:35,  1.63it/s]Extractor Predicting: 150it [01:36,  1.66it/s]Extractor Predicting: 151it [01:37,  1.69it/s]Extractor Predicting: 152it [01:37,  1.65it/s]Extractor Predicting: 153it [01:38,  1.41it/s]Extractor Predicting: 154it [01:39,  1.45it/s]Extractor Predicting: 155it [01:39,  1.50it/s]Extractor Predicting: 156it [01:40,  1.58it/s]Extractor Predicting: 157it [01:41,  1.57it/s]Extractor Predicting: 158it [01:41,  1.49it/s]Extractor Predicting: 159it [01:42,  1.55it/s]Extractor Predicting: 160it [01:43,  1.57it/s]Extractor Predicting: 161it [01:43,  1.62it/s]Extractor Predicting: 162it [01:44,  1.61it/s]Extractor Predicting: 163it [01:45,  1.36it/s]Extractor Predicting: 164it [01:45,  1.42it/s]Extractor Predicting: 165it [01:46,  1.43it/s]Extractor Predicting: 166it [01:47,  1.45it/s]Extractor Predicting: 167it [01:47,  1.48it/s]Extractor Predicting: 168it [01:48,  1.40it/s]Extractor Predicting: 169it [01:49,  1.48it/s]Extractor Predicting: 170it [01:49,  1.53it/s]Extractor Predicting: 171it [01:50,  1.56it/s]Extractor Predicting: 172it [01:51,  1.62it/s]Extractor Predicting: 173it [01:51,  1.55it/s]Extractor Predicting: 174it [01:52,  1.57it/s]Extractor Predicting: 175it [01:53,  1.53it/s]Extractor Predicting: 176it [01:53,  1.57it/s]Extractor Predicting: 177it [01:54,  1.57it/s]Extractor Predicting: 178it [01:54,  1.58it/s]Extractor Predicting: 179it [01:55,  1.56it/s]Extractor Predicting: 180it [01:56,  1.58it/s]Extractor Predicting: 181it [01:56,  1.59it/s]Extractor Predicting: 182it [01:57,  1.62it/s]Extractor Predicting: 183it [01:57,  1.66it/s]Extractor Predicting: 184it [01:58,  1.65it/s]Extractor Predicting: 185it [01:59,  1.50it/s]Extractor Predicting: 186it [02:00,  1.50it/s]Extractor Predicting: 187it [02:00,  1.55it/s]Extractor Predicting: 188it [02:01,  1.57it/s]Extractor Predicting: 189it [02:01,  1.58it/s]Extractor Predicting: 190it [02:02,  1.50it/s]Extractor Predicting: 191it [02:03,  1.54it/s]Extractor Predicting: 192it [02:03,  1.61it/s]Extractor Predicting: 193it [02:04,  1.62it/s]Extractor Predicting: 194it [02:05,  1.63it/s]Extractor Predicting: 195it [02:05,  1.41it/s]Extractor Predicting: 196it [02:06,  1.47it/s]Extractor Predicting: 197it [02:07,  1.52it/s]Extractor Predicting: 198it [02:07,  1.53it/s]Extractor Predicting: 199it [02:08,  1.55it/s]Extractor Predicting: 200it [02:09,  1.56it/s]Extractor Predicting: 201it [02:09,  1.60it/s]Extractor Predicting: 202it [02:10,  1.61it/s]Extractor Predicting: 203it [02:10,  1.64it/s]Extractor Predicting: 204it [02:11,  1.63it/s]Extractor Predicting: 205it [02:12,  1.61it/s]Extractor Predicting: 206it [02:12,  1.61it/s]Extractor Predicting: 207it [02:13,  1.64it/s]Extractor Predicting: 208it [02:13,  1.62it/s]Extractor Predicting: 209it [02:14,  1.60it/s]Extractor Predicting: 210it [02:15,  1.63it/s]Extractor Predicting: 211it [02:15,  1.63it/s]Extractor Predicting: 212it [02:16,  1.65it/s]Extractor Predicting: 213it [02:16,  1.65it/s]Extractor Predicting: 214it [02:17,  1.68it/s]Extractor Predicting: 215it [02:18,  1.66it/s]Extractor Predicting: 216it [02:18,  1.57it/s]Extractor Predicting: 217it [02:19,  1.60it/s]Extractor Predicting: 218it [02:20,  1.60it/s]Extractor Predicting: 219it [02:20,  1.60it/s]Extractor Predicting: 220it [02:21,  1.62it/s]Extractor Predicting: 221it [02:22,  1.54it/s]Extractor Predicting: 222it [02:22,  1.53it/s]Extractor Predicting: 223it [02:23,  1.49it/s]Extractor Predicting: 224it [02:24,  1.53it/s]Extractor Predicting: 225it [02:24,  1.57it/s]Extractor Predicting: 226it [02:25,  1.61it/s]Extractor Predicting: 227it [02:25,  1.62it/s]Extractor Predicting: 228it [02:26,  1.57it/s]Extractor Predicting: 229it [02:27,  1.62it/s]Extractor Predicting: 230it [02:27,  1.65it/s]Extractor Predicting: 231it [02:28,  1.67it/s]Extractor Predicting: 232it [02:28,  1.66it/s]Extractor Predicting: 233it [02:29,  1.60it/s]Extractor Predicting: 234it [02:30,  1.62it/s]Extractor Predicting: 235it [02:30,  1.61it/s]Extractor Predicting: 236it [02:31,  1.62it/s]Extractor Predicting: 237it [02:31,  1.64it/s]Extractor Predicting: 238it [02:32,  1.46it/s]Extractor Predicting: 239it [02:33,  1.53it/s]Extractor Predicting: 240it [02:34,  1.55it/s]Extractor Predicting: 241it [02:34,  1.59it/s]Extractor Predicting: 242it [02:35,  1.58it/s]Extractor Predicting: 243it [02:36,  1.32it/s]Extractor Predicting: 244it [02:36,  1.40it/s]Extractor Predicting: 245it [02:37,  1.48it/s]Extractor Predicting: 246it [02:38,  1.52it/s]Extractor Predicting: 247it [02:38,  1.57it/s]Extractor Predicting: 248it [02:39,  1.48it/s]Extractor Predicting: 249it [02:40,  1.50it/s]Extractor Predicting: 250it [02:40,  1.55it/s]Extractor Predicting: 251it [02:41,  1.56it/s]Extractor Predicting: 252it [02:42,  1.56it/s]Extractor Predicting: 253it [02:42,  1.49it/s]Extractor Predicting: 254it [02:43,  1.49it/s]Extractor Predicting: 255it [02:44,  1.53it/s]Extractor Predicting: 256it [02:44,  1.55it/s]Extractor Predicting: 257it [02:45,  1.40it/s]Extractor Predicting: 258it [02:46,  1.23it/s]Extractor Predicting: 259it [02:47,  1.32it/s]Extractor Predicting: 260it [02:47,  1.37it/s]Extractor Predicting: 261it [02:48,  1.44it/s]Extractor Predicting: 262it [02:49,  1.47it/s]Extractor Predicting: 263it [02:50,  1.35it/s]Extractor Predicting: 264it [02:50,  1.41it/s]Extractor Predicting: 265it [02:51,  1.48it/s]Extractor Predicting: 266it [02:51,  1.46it/s]Extractor Predicting: 267it [02:52,  1.49it/s]Extractor Predicting: 268it [02:53,  1.42it/s]Extractor Predicting: 269it [02:53,  1.47it/s]Extractor Predicting: 270it [02:54,  1.42it/s]Extractor Predicting: 271it [02:55,  1.46it/s]Extractor Predicting: 272it [02:56,  1.48it/s]Extractor Predicting: 273it [02:56,  1.51it/s]Extractor Predicting: 274it [02:57,  1.43it/s]Extractor Predicting: 275it [02:58,  1.47it/s]Extractor Predicting: 276it [02:58,  1.50it/s]Extractor Predicting: 277it [02:59,  1.52it/s]Extractor Predicting: 278it [02:59,  1.56it/s]Extractor Predicting: 279it [03:00,  1.51it/s]Extractor Predicting: 280it [03:01,  1.54it/s]Extractor Predicting: 281it [03:02,  1.50it/s]Extractor Predicting: 282it [03:02,  1.51it/s]Extractor Predicting: 283it [03:03,  1.53it/s]Extractor Predicting: 284it [03:04,  1.33it/s]Extractor Predicting: 285it [03:04,  1.35it/s]Extractor Predicting: 286it [03:05,  1.43it/s]Extractor Predicting: 287it [03:06,  1.43it/s]Extractor Predicting: 288it [03:06,  1.49it/s]Extractor Predicting: 289it [03:07,  1.53it/s]Extractor Predicting: 290it [03:08,  1.24it/s]Extractor Predicting: 291it [03:09,  1.29it/s]Extractor Predicting: 292it [03:09,  1.38it/s]Extractor Predicting: 293it [03:10,  1.43it/s]Extractor Predicting: 294it [03:11,  1.45it/s]Extractor Predicting: 295it [03:11,  1.48it/s]Extractor Predicting: 296it [03:12,  1.53it/s]Extractor Predicting: 297it [03:13,  1.46it/s]Extractor Predicting: 298it [03:13,  1.48it/s]Extractor Predicting: 299it [03:14,  1.51it/s]Extractor Predicting: 300it [03:15,  1.26it/s]Extractor Predicting: 301it [03:16,  1.35it/s]Extractor Predicting: 302it [03:16,  1.38it/s]Extractor Predicting: 303it [03:17,  1.46it/s]Extractor Predicting: 304it [03:18,  1.41it/s]Extractor Predicting: 305it [03:18,  1.49it/s]Extractor Predicting: 306it [03:19,  1.55it/s]Extractor Predicting: 307it [03:20,  1.55it/s]Extractor Predicting: 308it [03:20,  1.58it/s]Extractor Predicting: 309it [03:21,  1.56it/s]Extractor Predicting: 310it [03:22,  1.57it/s]Extractor Predicting: 311it [03:22,  1.54it/s]Extractor Predicting: 312it [03:23,  1.56it/s]Extractor Predicting: 313it [03:24,  1.44it/s]Extractor Predicting: 314it [03:24,  1.46it/s]Extractor Predicting: 315it [03:25,  1.51it/s]Extractor Predicting: 316it [03:26,  1.56it/s]Extractor Predicting: 317it [03:26,  1.58it/s]Extractor Predicting: 318it [03:27,  1.61it/s]Extractor Predicting: 319it [03:27,  1.58it/s]Extractor Predicting: 320it [03:28,  1.55it/s]Extractor Predicting: 321it [03:29,  1.41it/s]Extractor Predicting: 322it [03:30,  1.47it/s]Extractor Predicting: 323it [03:30,  1.46it/s]Extractor Predicting: 324it [03:31,  1.50it/s]Extractor Predicting: 325it [03:31,  1.54it/s]Extractor Predicting: 326it [03:32,  1.55it/s]Extractor Predicting: 327it [03:33,  1.57it/s]Extractor Predicting: 328it [03:33,  1.54it/s]Extractor Predicting: 329it [03:34,  1.51it/s]Extractor Predicting: 330it [03:35,  1.51it/s]Extractor Predicting: 331it [03:35,  1.54it/s]Extractor Predicting: 332it [03:36,  1.53it/s]Extractor Predicting: 333it [03:37,  1.53it/s]Extractor Predicting: 334it [03:38,  1.40it/s]Extractor Predicting: 335it [03:38,  1.43it/s]Extractor Predicting: 336it [03:39,  1.40it/s]Extractor Predicting: 337it [03:40,  1.43it/s]Extractor Predicting: 338it [03:40,  1.45it/s]Extractor Predicting: 339it [03:41,  1.27it/s]Extractor Predicting: 340it [03:42,  1.34it/s]Extractor Predicting: 341it [03:43,  1.38it/s]Extractor Predicting: 342it [03:43,  1.42it/s]Extractor Predicting: 343it [03:44,  1.48it/s]Extractor Predicting: 344it [03:45,  1.41it/s]Extractor Predicting: 345it [03:45,  1.41it/s]Extractor Predicting: 346it [03:46,  1.44it/s]Extractor Predicting: 347it [03:47,  1.48it/s]Extractor Predicting: 348it [03:47,  1.51it/s]Extractor Predicting: 349it [03:48,  1.27it/s]Extractor Predicting: 350it [03:49,  1.34it/s]Extractor Predicting: 351it [03:50,  1.42it/s]Extractor Predicting: 352it [03:51,  1.28it/s]Extractor Predicting: 353it [03:51,  1.24it/s]Extractor Predicting: 354it [03:52,  1.33it/s]Extractor Predicting: 355it [03:53,  1.41it/s]Extractor Predicting: 356it [03:53,  1.45it/s]Extractor Predicting: 357it [03:54,  1.49it/s]Extractor Predicting: 358it [03:55,  1.44it/s]Extractor Predicting: 359it [03:55,  1.47it/s]Extractor Predicting: 360it [03:56,  1.47it/s]Extractor Predicting: 361it [03:57,  1.48it/s]Extractor Predicting: 362it [03:57,  1.52it/s]Extractor Predicting: 363it [03:58,  1.57it/s]Extractor Predicting: 364it [03:59,  1.58it/s]Extractor Predicting: 365it [03:59,  1.56it/s]Extractor Predicting: 366it [04:00,  1.25it/s]Extractor Predicting: 367it [04:01,  1.31it/s]Extractor Predicting: 368it [04:02,  1.30it/s]Extractor Predicting: 369it [04:03,  1.32it/s]Extractor Predicting: 370it [04:03,  1.37it/s]Extractor Predicting: 371it [04:04,  1.42it/s]Extractor Predicting: 372it [04:04,  1.66it/s]Extractor Predicting: 372it [04:04,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:14,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:14,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:14,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:14,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:14,277 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:51:14,856 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:51:14,857 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:51:15,233 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:51:16,309 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:51:16,309 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:19,839 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:19,945 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:19,945 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:19,945 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:51:19,945 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:51:20,364 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:51:20,365 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:51:21,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:51:21,457 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:51:21,457 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2947183098591549,
  "recall": 0.1878366247755835,
  "score": 0.22944078947368424,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.39it/s]Extractor Predicting: 27it [00:17,  1.43it/s]Extractor Predicting: 28it [00:17,  1.45it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:19,  1.50it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:25,  1.44it/s]Extractor Predicting: 40it [00:25,  1.44it/s]Extractor Predicting: 41it [00:26,  1.45it/s]Extractor Predicting: 42it [00:27,  1.47it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.38it/s]Extractor Predicting: 46it [00:30,  1.37it/s]Extractor Predicting: 47it [00:30,  1.39it/s]Extractor Predicting: 48it [00:31,  1.42it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.44it/s]Extractor Predicting: 52it [00:34,  1.44it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.41it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.29it/s]Extractor Predicting: 58it [00:38,  1.33it/s]Extractor Predicting: 59it [00:39,  1.37it/s]Extractor Predicting: 60it [00:40,  1.40it/s]Extractor Predicting: 61it [00:41,  1.28it/s]Extractor Predicting: 62it [00:41,  1.32it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 63it [00:42,  1.49it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:52:13,317 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:13,424 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:52:13,513 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:13,514 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:52:13,569 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:53:05,699 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:53:05,771 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:53:07,233 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:53:07,234 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:53:08,249 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,499 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,500 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,500 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,500 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,500 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:08,500 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4626865671641791,
  "recall": 0.11147737488762362,
  "score": 0.17966674716252112,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:53:10,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:10,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:11,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:11,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:12,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:13,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:16,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:16,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:23,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:23,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:18, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:24,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:25,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:25,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:26,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:26,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:27,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:28,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:28,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:30,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:47, 12.90s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:36,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:36,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:38,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:39,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:39,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:41,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:41,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:36<02:21, 11.80s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:46,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:49,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:51,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:52,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:54,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:00,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:20, 12.74s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:01,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:03,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:07,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:07,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:09,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:11,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:13,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:15,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:16, 13.63s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:16,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:18,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:21,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:21,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:22,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:22,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:20<02:04, 13.86s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:30,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:31,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:31,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:35,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:39,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:42,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:33<01:47, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:43,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:44,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:49,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:50,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:50,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:51,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:52,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:46<01:34, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:56,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:57,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:00,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:07,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:08,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:08,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:02<01:24, 14.16s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:12,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:14,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:16,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:16,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:24,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:08, 13.67s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:25,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:30,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:30,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:35,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:25<00:50, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:35,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:42,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:43,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:43,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:50,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:40<00:40, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:50,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:51,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:53,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:56,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:59,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:00,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:02,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:55<00:28, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:06,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:07,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:10,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:10,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:07<00:13, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:17,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:18,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:20,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:23,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:25,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:28,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:31,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:21<00:00, 13.69s/it]Generating: 100%|██████████| 15/15 [03:21<00:00, 13.46s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:44,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:44,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:44,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:44,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:44,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:56:45,078 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:56:45,079 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:56:45,819 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:56:46,935 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:56:46,936 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:50,304 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:51,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:51,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:51,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:51,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:56:53,671 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:56:53,672 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:56:54,376 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:56:55,464 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:56:55,464 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8247282608695652, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.7994791666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7981770833333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8138020833333334, 'errors': {'', "('James C. F.', 'occupation', '', 'Joseph F. Doss and his students , including Robert G. Mays ( 1822 &ndash; 1899 ) , James C. F. ( 1831 &ndash; 1890 ) , John A. Hernández ( 1844 &ndash; 1893 ) , and George H. Biddle ( 1857 &ndash; 1866 ) , used a similar technique to F. Doss .')", 'not enough values to unpack (expected 2, got 1)', "('Joseph Anthony Girolamo', 'occupation', '', 'Joseph Anthony Girolamo was born on December 19 , 1852 in Cirencester , Gloucestershire , the son of John Girolamo and his wife , Lady Maria Girolamo .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 9911
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10011, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.54it/s]Extractor Estimating: 3it [00:01,  1.62it/s]Extractor Estimating: 4it [00:02,  1.34it/s]Extractor Estimating: 5it [00:03,  1.42it/s]Extractor Estimating: 6it [00:04,  1.49it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.62it/s]Extractor Estimating: 9it [00:06,  1.43it/s]Extractor Estimating: 10it [00:06,  1.47it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.67it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:09,  1.72it/s]Extractor Estimating: 16it [00:10,  1.29it/s]Extractor Estimating: 17it [00:11,  1.38it/s]Extractor Estimating: 18it [00:12,  1.12it/s]Extractor Estimating: 19it [00:13,  1.25it/s]Extractor Estimating: 20it [00:13,  1.37it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.37it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:16,  1.55it/s]Extractor Estimating: 25it [00:16,  1.59it/s]Extractor Estimating: 26it [00:17,  1.60it/s]Extractor Estimating: 27it [00:18,  1.63it/s]Extractor Estimating: 28it [00:18,  1.69it/s]Extractor Estimating: 29it [00:19,  1.70it/s]Extractor Estimating: 30it [00:19,  1.72it/s]Extractor Estimating: 31it [00:20,  1.76it/s]Extractor Estimating: 32it [00:21,  1.65it/s]Extractor Estimating: 33it [00:21,  1.72it/s]Extractor Estimating: 34it [00:22,  1.76it/s]Extractor Estimating: 35it [00:22,  1.75it/s]Extractor Estimating: 36it [00:23,  1.37it/s]Extractor Estimating: 37it [00:24,  1.51it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:25,  1.61it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:26,  1.60it/s]Extractor Estimating: 42it [00:27,  1.64it/s]Extractor Estimating: 43it [00:27,  1.64it/s]Extractor Estimating: 44it [00:28,  1.66it/s]Extractor Estimating: 45it [00:29,  1.69it/s]Extractor Estimating: 46it [00:29,  1.69it/s]Extractor Estimating: 47it [00:30,  1.69it/s]Extractor Estimating: 48it [00:30,  1.69it/s]Extractor Estimating: 49it [00:31,  1.59it/s]Extractor Estimating: 50it [00:32,  1.60it/s]Extractor Estimating: 51it [00:32,  1.72it/s]Extractor Estimating: 52it [00:33,  1.84it/s]Extractor Estimating: 53it [00:33,  1.88it/s]Extractor Estimating: 54it [00:34,  1.94it/s]Extractor Estimating: 55it [00:34,  2.03it/s]Extractor Estimating: 56it [00:34,  2.07it/s]Extractor Estimating: 57it [00:35,  2.14it/s]Extractor Estimating: 58it [00:35,  2.17it/s]Extractor Estimating: 59it [00:36,  2.15it/s]Extractor Estimating: 60it [00:36,  2.06it/s]Extractor Estimating: 61it [00:37,  2.04it/s]Extractor Estimating: 62it [00:37,  2.04it/s]Extractor Estimating: 63it [00:38,  1.93it/s]Extractor Estimating: 64it [00:38,  2.01it/s]Extractor Estimating: 65it [00:39,  2.13it/s]Extractor Estimating: 66it [00:39,  2.15it/s]Extractor Estimating: 67it [00:40,  2.16it/s]Extractor Estimating: 68it [00:40,  2.17it/s]Extractor Estimating: 69it [00:41,  2.18it/s]Extractor Estimating: 70it [00:41,  2.21it/s]Extractor Estimating: 71it [00:42,  1.92it/s]Extractor Estimating: 72it [00:42,  1.99it/s]Extractor Estimating: 73it [00:43,  1.97it/s]Extractor Estimating: 74it [00:43,  2.06it/s]Extractor Estimating: 75it [00:44,  1.89it/s]Extractor Estimating: 76it [00:44,  1.79it/s]Extractor Estimating: 77it [00:45,  1.69it/s]Extractor Estimating: 78it [00:46,  1.68it/s]Extractor Estimating: 79it [00:46,  1.75it/s]Extractor Estimating: 80it [00:47,  1.69it/s]Extractor Estimating: 81it [00:47,  1.70it/s]Extractor Estimating: 82it [00:48,  1.72it/s]Extractor Estimating: 83it [00:49,  1.77it/s]Extractor Estimating: 84it [00:50,  1.12it/s]Extractor Estimating: 85it [00:51,  1.23it/s]Extractor Estimating: 86it [00:51,  1.37it/s]Extractor Estimating: 87it [00:52,  1.39it/s]Extractor Estimating: 88it [00:53,  1.46it/s]Extractor Estimating: 89it [00:53,  1.49it/s]Extractor Estimating: 90it [00:54,  1.52it/s]Extractor Estimating: 91it [00:54,  1.57it/s]Extractor Estimating: 92it [00:56,  1.26it/s]Extractor Estimating: 93it [00:56,  1.41it/s]Extractor Estimating: 94it [00:57,  1.50it/s]Extractor Estimating: 95it [00:57,  1.55it/s]Extractor Estimating: 96it [00:58,  1.63it/s]Extractor Estimating: 97it [00:59,  1.46it/s]Extractor Estimating: 98it [00:59,  1.53it/s]Extractor Estimating: 99it [01:00,  1.56it/s]Extractor Estimating: 100it [01:01,  1.37it/s]Extractor Estimating: 101it [01:01,  1.50it/s]Extractor Estimating: 102it [01:02,  1.55it/s]Extractor Estimating: 103it [01:03,  1.60it/s]Extractor Estimating: 104it [01:03,  1.59it/s]Extractor Estimating: 105it [01:04,  1.31it/s]Extractor Estimating: 106it [01:05,  1.42it/s]Extractor Estimating: 107it [01:05,  1.48it/s]Extractor Estimating: 108it [01:06,  1.35it/s]Extractor Estimating: 109it [01:07,  1.46it/s]Extractor Estimating: 110it [01:07,  1.52it/s]Extractor Estimating: 111it [01:08,  1.56it/s]Extractor Estimating: 112it [01:09,  1.60it/s]Extractor Estimating: 113it [01:09,  1.44it/s]Extractor Estimating: 114it [01:10,  1.50it/s]Extractor Estimating: 115it [01:11,  1.52it/s]Extractor Estimating: 116it [01:12,  1.27it/s]Extractor Estimating: 117it [01:12,  1.37it/s]Extractor Estimating: 118it [01:13,  1.43it/s]Extractor Estimating: 119it [01:14,  1.48it/s]Extractor Estimating: 120it [01:15,  1.31it/s]Extractor Estimating: 121it [01:15,  1.41it/s]Extractor Estimating: 122it [01:16,  1.46it/s]Extractor Estimating: 123it [01:16,  1.51it/s]Extractor Estimating: 124it [01:17,  1.57it/s]Extractor Estimating: 125it [01:18,  1.61it/s]Extractor Estimating: 126it [01:18,  1.64it/s]Extractor Estimating: 127it [01:19,  1.66it/s]Extractor Estimating: 128it [01:19,  1.65it/s]Extractor Estimating: 129it [01:20,  1.54it/s]Extractor Estimating: 130it [01:21,  1.59it/s]Extractor Estimating: 131it [01:21,  1.64it/s]Extractor Estimating: 132it [01:22,  1.59it/s]Extractor Estimating: 133it [01:23,  1.63it/s]Extractor Estimating: 134it [01:23,  1.65it/s]Extractor Estimating: 135it [01:24,  1.66it/s]Extractor Estimating: 136it [01:24,  1.63it/s]Extractor Estimating: 137it [01:25,  1.56it/s]Extractor Estimating: 138it [01:26,  1.58it/s]Extractor Estimating: 139it [01:26,  1.64it/s]Extractor Estimating: 140it [01:27,  1.69it/s]Extractor Estimating: 141it [01:27,  1.67it/s]Extractor Estimating: 142it [01:28,  1.66it/s]Extractor Estimating: 143it [01:29,  1.66it/s]Extractor Estimating: 144it [01:29,  1.66it/s]Extractor Estimating: 145it [01:30,  1.65it/s]Extractor Estimating: 146it [01:31,  1.41it/s]Extractor Estimating: 147it [01:31,  1.51it/s]Extractor Estimating: 148it [01:32,  1.54it/s]Extractor Estimating: 149it [01:33,  1.62it/s]Extractor Estimating: 150it [01:33,  1.61it/s]Extractor Estimating: 151it [01:34,  1.65it/s]Extractor Estimating: 152it [01:34,  1.66it/s]Extractor Estimating: 153it [01:35,  1.64it/s]Extractor Estimating: 154it [01:36,  1.55it/s]Extractor Estimating: 155it [01:36,  1.59it/s]Extractor Estimating: 156it [01:37,  1.58it/s]Extractor Estimating: 157it [01:37,  1.64it/s]Extractor Estimating: 158it [01:38,  1.70it/s]Extractor Estimating: 159it [01:39,  1.72it/s]Extractor Estimating: 160it [01:39,  1.73it/s]Extractor Estimating: 161it [01:40,  1.70it/s]Extractor Estimating: 162it [01:40,  1.74it/s]Extractor Estimating: 163it [01:41,  1.70it/s]Extractor Estimating: 164it [01:42,  1.68it/s]Extractor Estimating: 165it [01:42,  1.68it/s]Extractor Estimating: 166it [01:43,  1.65it/s]Extractor Estimating: 167it [01:43,  1.69it/s]Extractor Estimating: 168it [01:44,  1.70it/s]Extractor Estimating: 169it [01:44,  1.75it/s]Extractor Estimating: 170it [01:45,  1.76it/s]Extractor Estimating: 171it [01:45,  1.80it/s]Extractor Estimating: 172it [01:46,  1.61it/s]Extractor Estimating: 173it [01:47,  1.65it/s]Extractor Estimating: 174it [01:47,  1.63it/s]Extractor Estimating: 175it [01:48,  1.70it/s]Extractor Estimating: 176it [01:49,  1.69it/s]Extractor Estimating: 177it [01:49,  1.68it/s]Extractor Estimating: 178it [01:50,  1.70it/s]Extractor Estimating: 179it [01:51,  1.54it/s]Extractor Estimating: 180it [01:52,  1.32it/s]Extractor Estimating: 181it [01:52,  1.42it/s]Extractor Estimating: 182it [01:53,  1.49it/s]Extractor Estimating: 183it [01:53,  1.44it/s]Extractor Estimating: 184it [01:54,  1.52it/s]Extractor Estimating: 185it [01:55,  1.55it/s]Extractor Estimating: 186it [01:55,  1.59it/s]Extractor Estimating: 187it [01:56,  1.60it/s]Extractor Estimating: 188it [01:57,  1.45it/s]Extractor Estimating: 189it [01:57,  1.50it/s]Extractor Estimating: 190it [01:58,  1.58it/s]Extractor Estimating: 191it [01:59,  1.60it/s]Extractor Estimating: 192it [01:59,  1.60it/s]Extractor Estimating: 193it [02:00,  1.66it/s]Extractor Estimating: 194it [02:00,  1.65it/s]Extractor Estimating: 195it [02:01,  1.65it/s]Extractor Estimating: 196it [02:02,  1.60it/s]Extractor Estimating: 197it [02:02,  1.63it/s]Extractor Estimating: 198it [02:03,  1.65it/s]Extractor Estimating: 199it [02:03,  1.67it/s]Extractor Estimating: 200it [02:04,  1.63it/s]Extractor Estimating: 201it [02:05,  1.66it/s]Extractor Estimating: 202it [02:05,  1.69it/s]Extractor Estimating: 203it [02:06,  1.70it/s]Extractor Estimating: 204it [02:06,  1.77it/s]Extractor Estimating: 205it [02:07,  1.39it/s]Extractor Estimating: 206it [02:08,  1.51it/s]Extractor Estimating: 207it [02:08,  1.53it/s]Extractor Estimating: 208it [02:10,  1.21it/s]Extractor Estimating: 209it [02:10,  1.32it/s]Extractor Estimating: 210it [02:11,  1.44it/s]Extractor Estimating: 211it [02:11,  1.49it/s]Extractor Estimating: 212it [02:13,  1.23it/s]Extractor Estimating: 213it [02:13,  1.35it/s]Extractor Estimating: 214it [02:14,  1.47it/s]Extractor Estimating: 215it [02:14,  1.53it/s]Extractor Estimating: 216it [02:15,  1.26it/s]Extractor Estimating: 217it [02:16,  1.39it/s]Extractor Estimating: 218it [02:19,  1.31s/it]Extractor Estimating: 219it [02:20,  1.26s/it]Extractor Estimating: 220it [02:21,  1.14s/it]Extractor Estimating: 221it [02:21,  1.04it/s]Extractor Estimating: 222it [02:22,  1.18it/s]Extractor Estimating: 223it [02:22,  1.31it/s]Extractor Estimating: 224it [02:23,  1.38it/s]Extractor Estimating: 225it [02:24,  1.23it/s]Extractor Estimating: 226it [02:25,  1.30it/s]Extractor Estimating: 227it [02:25,  1.39it/s]Extractor Estimating: 228it [02:26,  1.19it/s]Extractor Estimating: 229it [02:27,  1.30it/s]Extractor Estimating: 230it [02:28,  1.38it/s]Extractor Estimating: 231it [02:28,  1.48it/s]Extractor Estimating: 232it [02:29,  1.39it/s]Extractor Estimating: 233it [02:30,  1.45it/s]Extractor Estimating: 234it [02:30,  1.55it/s]Extractor Estimating: 235it [02:31,  1.59it/s]Extractor Estimating: 236it [02:31,  1.58it/s]Extractor Estimating: 237it [02:32,  1.51it/s]Extractor Estimating: 238it [02:33,  1.58it/s]Extractor Estimating: 239it [02:33,  1.57it/s]Extractor Estimating: 240it [02:34,  1.58it/s]Extractor Estimating: 241it [02:35,  1.62it/s]Extractor Estimating: 242it [02:35,  1.47it/s]Extractor Estimating: 243it [02:36,  1.51it/s]Extractor Estimating: 244it [02:37,  1.42it/s]Extractor Estimating: 245it [02:37,  1.48it/s]Extractor Estimating: 246it [02:38,  1.56it/s]Extractor Estimating: 247it [02:39,  1.55it/s]Extractor Estimating: 248it [02:39,  1.61it/s]Extractor Estimating: 249it [02:40,  1.60it/s]Extractor Estimating: 250it [02:40,  1.63it/s]Extractor Estimating: 251it [02:41,  1.62it/s]Extractor Estimating: 252it [02:42,  1.62it/s]Extractor Estimating: 253it [02:42,  1.61it/s]Extractor Estimating: 254it [02:43,  1.57it/s]Extractor Estimating: 255it [02:44,  1.58it/s]Extractor Estimating: 256it [02:44,  1.61it/s]Extractor Estimating: 257it [02:45,  1.60it/s]Extractor Estimating: 258it [02:45,  1.61it/s]Extractor Estimating: 259it [02:46,  1.33it/s]Extractor Estimating: 260it [02:47,  1.39it/s]Extractor Estimating: 261it [02:48,  1.46it/s]Extractor Estimating: 262it [02:48,  1.50it/s]Extractor Estimating: 263it [02:49,  1.56it/s]Extractor Estimating: 264it [02:50,  1.38it/s]Extractor Estimating: 265it [02:50,  1.45it/s]Extractor Estimating: 266it [02:51,  1.49it/s]Extractor Estimating: 267it [02:52,  1.53it/s]Extractor Estimating: 268it [02:52,  1.57it/s]Extractor Estimating: 269it [02:53,  1.55it/s]Extractor Estimating: 270it [02:54,  1.57it/s]Extractor Estimating: 271it [02:54,  1.58it/s]Extractor Estimating: 272it [02:55,  1.61it/s]Extractor Estimating: 273it [02:55,  1.62it/s]Extractor Estimating: 274it [02:56,  1.40it/s]Extractor Estimating: 275it [02:57,  1.23it/s]Extractor Estimating: 276it [02:58,  1.36it/s]Extractor Estimating: 277it [02:59,  1.44it/s]Extractor Estimating: 278it [02:59,  1.50it/s]Extractor Estimating: 279it [03:00,  1.57it/s]Extractor Estimating: 280it [03:01,  1.42it/s]Extractor Estimating: 281it [03:01,  1.55it/s]Extractor Estimating: 282it [03:02,  1.62it/s]Extractor Estimating: 283it [03:02,  1.67it/s]Extractor Estimating: 284it [03:03,  1.66it/s]Extractor Estimating: 285it [03:03,  1.65it/s]Extractor Estimating: 286it [03:04,  1.67it/s]Extractor Estimating: 287it [03:05,  1.66it/s]Extractor Estimating: 288it [03:05,  1.64it/s]Extractor Estimating: 289it [03:06,  1.57it/s]Extractor Estimating: 290it [03:06,  1.61it/s]Extractor Estimating: 291it [03:07,  1.63it/s]Extractor Estimating: 292it [03:08,  1.24it/s]Extractor Estimating: 293it [03:09,  1.34it/s]Extractor Estimating: 294it [03:10,  1.41it/s]Extractor Estimating: 295it [03:10,  1.49it/s]Extractor Estimating: 296it [03:11,  1.42it/s]Extractor Estimating: 297it [03:12,  1.48it/s]Extractor Estimating: 298it [03:12,  1.55it/s]Extractor Estimating: 299it [03:13,  1.58it/s]Extractor Estimating: 300it [03:14,  1.25it/s]Extractor Estimating: 301it [03:15,  1.34it/s]Extractor Estimating: 302it [03:15,  1.42it/s]Extractor Estimating: 303it [03:16,  1.51it/s]Extractor Estimating: 304it [03:17,  1.38it/s]Extractor Estimating: 305it [03:17,  1.49it/s]Extractor Estimating: 306it [03:18,  1.53it/s]Extractor Estimating: 307it [03:18,  1.61it/s]Extractor Estimating: 308it [03:19,  1.65it/s]Extractor Estimating: 309it [03:20,  1.54it/s]Extractor Estimating: 310it [03:20,  1.61it/s]Extractor Estimating: 311it [03:21,  1.67it/s]Extractor Estimating: 312it [03:21,  1.66it/s]Extractor Estimating: 313it [03:23,  1.21it/s]Extractor Estimating: 314it [03:23,  1.33it/s]Extractor Estimating: 315it [03:24,  1.46it/s]Extractor Estimating: 316it [03:24,  1.46it/s]Extractor Estimating: 317it [03:25,  1.54it/s]Extractor Estimating: 318it [03:26,  1.57it/s]Extractor Estimating: 319it [03:26,  1.58it/s]Extractor Estimating: 320it [03:27,  1.59it/s]Extractor Estimating: 321it [03:28,  1.47it/s]Extractor Estimating: 322it [03:28,  1.55it/s]Extractor Estimating: 323it [03:29,  1.59it/s]Extractor Estimating: 324it [03:29,  1.64it/s]Extractor Estimating: 325it [03:30,  1.58it/s]Extractor Estimating: 326it [03:31,  1.58it/s]Extractor Estimating: 327it [03:31,  1.55it/s]Extractor Estimating: 328it [03:32,  1.55it/s]Extractor Estimating: 329it [03:33,  1.56it/s]Extractor Estimating: 330it [03:33,  1.42it/s]Extractor Estimating: 331it [03:34,  1.49it/s]Extractor Estimating: 332it [03:35,  1.54it/s]Extractor Estimating: 333it [03:35,  1.47it/s]Extractor Estimating: 334it [03:36,  1.49it/s]Extractor Estimating: 335it [03:37,  1.51it/s]Extractor Estimating: 336it [03:38,  1.42it/s]Extractor Estimating: 337it [03:38,  1.40it/s]Extractor Estimating: 338it [03:39,  1.44it/s]Extractor Estimating: 339it [03:40,  1.48it/s]Extractor Estimating: 340it [03:40,  1.50it/s]Extractor Estimating: 341it [03:41,  1.40it/s]Extractor Estimating: 342it [03:42,  1.45it/s]Extractor Estimating: 343it [03:42,  1.50it/s]Extractor Estimating: 344it [03:43,  1.53it/s]Extractor Estimating: 345it [03:44,  1.27it/s]Extractor Estimating: 346it [03:45,  1.33it/s]Extractor Estimating: 347it [03:45,  1.41it/s]Extractor Estimating: 348it [03:46,  1.43it/s]Extractor Estimating: 349it [03:47,  1.48it/s]Extractor Estimating: 350it [03:47,  1.53it/s]Extractor Estimating: 351it [03:48,  1.56it/s]Extractor Estimating: 352it [03:48,  1.65it/s]Extractor Estimating: 353it [03:49,  1.55it/s]Extractor Estimating: 354it [03:50,  1.61it/s]Extractor Estimating: 355it [03:50,  1.66it/s]Extractor Estimating: 356it [03:51,  1.77it/s]Extractor Estimating: 357it [03:51,  1.64it/s]Extractor Estimating: 358it [03:52,  1.70it/s]Extractor Estimating: 359it [03:53,  1.69it/s]Extractor Estimating: 360it [03:53,  1.71it/s]Extractor Estimating: 361it [03:54,  1.76it/s]Extractor Estimating: 362it [03:55,  1.31it/s]Extractor Estimating: 363it [03:55,  1.44it/s]Extractor Estimating: 364it [03:56,  1.55it/s]Extractor Estimating: 365it [03:56,  1.67it/s]Extractor Estimating: 366it [03:58,  1.29it/s]Extractor Estimating: 367it [03:58,  1.44it/s]Extractor Estimating: 368it [03:59,  1.53it/s]Extractor Estimating: 369it [03:59,  1.62it/s]Extractor Estimating: 370it [04:01,  1.19it/s]Extractor Estimating: 371it [04:01,  1.33it/s]Extractor Estimating: 372it [04:02,  1.45it/s]Extractor Estimating: 373it [04:02,  1.60it/s]Extractor Estimating: 374it [04:03,  1.63it/s]Extractor Estimating: 375it [04:03,  1.90it/s]Extractor Estimating: 375it [04:03,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:44,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:44,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:44,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:44,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:44,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:01:45,097 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:01:45,098 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:45,791 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:46,905 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:46,905 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:50,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:50,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:50,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:50,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:50,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:01:51,696 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:01:51,697 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:52,390 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:52,570 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:52,570 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:55:35,961 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:55:37,028 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3026 mean pseudo reward: 0.9853347853153003
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 15430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15530, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.945, loss:214.1813
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 73, avg_time 0.955, loss:179.7259
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 46, avg_time 0.945, loss:168.7778
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 19, avg_time 0.940, loss:168.7255
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 119, avg_time 0.954, loss:156.6510
>> valid entity prec:0.5764, rec:0.5198, f1:0.5467
>> valid relation prec:0.2393, rec:0.1109, f1:0.1515
>> valid relation with NER prec:0.2393, rec:0.1109, f1:0.1515
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 92, avg_time 2.552, loss:148.6124
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 65, avg_time 0.953, loss:145.3128
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 38, avg_time 0.941, loss:143.4621
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 11, avg_time 0.957, loss:146.9425
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 111, avg_time 0.953, loss:150.3151
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5753, rec:0.5244, f1:0.5487
>> valid relation prec:0.2280, rec:0.1148, f1:0.1527
>> valid relation with NER prec:0.2280, rec:0.1148, f1:0.1527
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 84, avg_time 2.540, loss:135.6299
g_step 1200, step 57, avg_time 0.948, loss:137.8268
g_step 1300, step 30, avg_time 0.948, loss:144.7255
g_step 1400, step 3, avg_time 0.944, loss:140.3648
g_step 1500, step 103, avg_time 0.953, loss:125.8726
>> valid entity prec:0.5630, rec:0.4775, f1:0.5167
>> valid relation prec:0.2187, rec:0.0937, f1:0.1312
>> valid relation with NER prec:0.2187, rec:0.0937, f1:0.1312
g_step 1600, step 76, avg_time 2.522, loss:123.1341
g_step 1700, step 49, avg_time 0.950, loss:118.7047
g_step 1800, step 22, avg_time 0.952, loss:112.5024
g_step 1900, step 122, avg_time 0.948, loss:108.4867
g_step 2000, step 95, avg_time 0.948, loss:104.9085
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5387, rec:0.5657, f1:0.5519
>> valid relation prec:0.1963, rec:0.1353, f1:0.1602
>> valid relation with NER prec:0.1963, rec:0.1353, f1:0.1602
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 68, avg_time 2.567, loss:98.5968
g_step 2200, step 41, avg_time 0.942, loss:101.8004
g_step 2300, step 14, avg_time 0.951, loss:99.8744
g_step 2400, step 114, avg_time 0.948, loss:99.0345
g_step 2500, step 87, avg_time 0.959, loss:96.6848
>> valid entity prec:0.5623, rec:0.4459, f1:0.4974
>> valid relation prec:0.1876, rec:0.0951, f1:0.1262
>> valid relation with NER prec:0.1876, rec:0.0951, f1:0.1262
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:55:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:55:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-55-35_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:55:38 - WARNING - datasets.builder -   Using custom data configuration default-5e7156931e73e6d1
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5e7156931e73e6d1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:55:44,776 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:55:44,777 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:55:44,778 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:55:44,779 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:55:44,991 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:45,332 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:55:46,595 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:55:49,736 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:55:49,783 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5e7156931e73e6d1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.86ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.59ba/s]100%|██████████| 4/4 [00:01<00:00,  2.78ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.89ba/s] 40%|████      | 2/5 [00:00<00:00,  3.73ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.07ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.28ba/s]100%|██████████| 5/5 [00:01<00:00,  4.58ba/s]100%|██████████| 5/5 [00:01<00:00,  4.22ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.93ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.97ba/s]100%|██████████| 4/4 [00:00<00:00,  9.87ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.67ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.85ba/s]100%|██████████| 5/5 [00:00<00:00,  9.11ba/s]100%|██████████| 5/5 [00:00<00:00,  8.41ba/s]
[INFO|trainer.py:414] 2023-08-28 20:55:56,580 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:55:56,941 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:55:56,941 >>   Num examples = 3035
[INFO|trainer.py:1149] 2023-08-28 20:55:56,941 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:55:56,941 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:55:56,942 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:55:56,942 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:55:56,942 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:10,  3.33it/s]  1%|          | 2/235 [00:00<01:08,  3.41it/s]  1%|▏         | 3/235 [00:00<01:07,  3.43it/s]  2%|▏         | 4/235 [00:01<01:07,  3.44it/s]  2%|▏         | 5/235 [00:01<01:06,  3.45it/s]  3%|▎         | 6/235 [00:01<01:06,  3.44it/s]  3%|▎         | 7/235 [00:02<01:06,  3.43it/s]  3%|▎         | 8/235 [00:02<01:07,  3.34it/s]  4%|▍         | 9/235 [00:02<01:07,  3.36it/s]  4%|▍         | 10/235 [00:02<01:06,  3.38it/s]  5%|▍         | 11/235 [00:03<01:06,  3.39it/s]  5%|▌         | 12/235 [00:03<01:05,  3.40it/s]  6%|▌         | 13/235 [00:03<01:05,  3.40it/s]  6%|▌         | 14/235 [00:04<01:04,  3.40it/s]  6%|▋         | 15/235 [00:04<01:04,  3.40it/s]  7%|▋         | 16/235 [00:04<01:12,  3.02it/s]  7%|▋         | 17/235 [00:05<01:09,  3.13it/s]  8%|▊         | 18/235 [00:05<01:07,  3.21it/s]  8%|▊         | 19/235 [00:05<01:06,  3.27it/s]  9%|▊         | 20/235 [00:06<01:05,  3.31it/s]  9%|▉         | 21/235 [00:06<01:04,  3.34it/s]  9%|▉         | 22/235 [00:06<01:03,  3.36it/s] 10%|▉         | 23/235 [00:06<01:02,  3.37it/s] 10%|█         | 24/235 [00:07<01:02,  3.39it/s] 11%|█         | 25/235 [00:07<01:01,  3.40it/s] 11%|█         | 26/235 [00:08<01:29,  2.34it/s] 11%|█▏        | 27/235 [00:08<01:20,  2.58it/s] 12%|█▏        | 28/235 [00:08<01:14,  2.79it/s] 12%|█▏        | 29/235 [00:09<01:09,  2.95it/s] 13%|█▎        | 30/235 [00:09<01:06,  3.07it/s] 13%|█▎        | 31/235 [00:09<01:04,  3.17it/s] 14%|█▎        | 32/235 [00:10<01:29,  2.28it/s] 14%|█▍        | 33/235 [00:10<01:19,  2.53it/s] 14%|█▍        | 34/235 [00:10<01:13,  2.74it/s] 15%|█▍        | 35/235 [00:11<01:08,  2.91it/s] 15%|█▌        | 36/235 [00:11<01:05,  3.04it/s] 16%|█▌        | 37/235 [00:11<01:02,  3.15it/s] 16%|█▌        | 38/235 [00:12<01:01,  3.22it/s] 17%|█▋        | 39/235 [00:12<00:59,  3.27it/s] 17%|█▋        | 40/235 [00:12<00:58,  3.31it/s] 17%|█▋        | 41/235 [00:13<01:10,  2.76it/s] 18%|█▊        | 42/235 [00:13<01:05,  2.93it/s] 18%|█▊        | 43/235 [00:13<01:02,  3.06it/s] 19%|█▊        | 44/235 [00:14<01:00,  3.16it/s] 19%|█▉        | 45/235 [00:14<00:58,  3.23it/s] 20%|█▉        | 46/235 [00:14<00:57,  3.28it/s] 20%|██        | 47/235 [00:15<00:56,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 20:56:12,071 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:56:12,071 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 20:56:12,071 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.52it/s][A
  2%|▏         | 12/611 [00:00<00:40, 14.76it/s][A
  3%|▎         | 17/611 [00:00<00:29, 20.40it/s][A
  4%|▎         | 22/611 [00:00<00:23, 25.56it/s][A
  4%|▍         | 27/611 [00:01<00:19, 30.04it/s][A
  5%|▌         | 32/611 [00:01<00:17, 33.77it/s][A
  6%|▌         | 37/611 [00:01<00:15, 36.74it/s][A
  7%|▋         | 42/611 [00:01<00:14, 39.03it/s][A
  8%|▊         | 47/611 [00:01<00:13, 40.50it/s][A
  9%|▊         | 52/611 [00:01<00:13, 41.28it/s][A
  9%|▉         | 57/611 [00:01<00:13, 41.93it/s][A
 10%|█         | 62/611 [00:01<00:12, 42.61it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.13it/s][A
 12%|█▏        | 72/611 [00:02<00:12, 43.71it/s][A
 13%|█▎        | 77/611 [00:02<00:12, 44.08it/s][A
 13%|█▎        | 82/611 [00:02<00:11, 44.41it/s][A
 14%|█▍        | 87/611 [00:02<00:11, 44.58it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.33it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.05it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 43.89it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.01it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.24it/s][A
 19%|█▉        | 117/611 [00:03<00:11, 44.49it/s][A
 20%|█▉        | 122/611 [00:03<00:13, 35.18it/s][A
 21%|██        | 127/611 [00:03<00:12, 37.58it/s][A
 22%|██▏       | 132/611 [00:03<00:12, 39.58it/s][A
 22%|██▏       | 137/611 [00:03<00:11, 40.99it/s][A
 23%|██▎       | 142/611 [00:03<00:11, 42.19it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 42.92it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.56it/s][A
 26%|██▌       | 157/611 [00:04<00:10, 43.78it/s][A
 27%|██▋       | 162/611 [00:04<00:10, 43.60it/s][A
 27%|██▋       | 167/611 [00:04<00:10, 43.52it/s][A
 28%|██▊       | 172/611 [00:04<00:10, 43.72it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 43.94it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.28it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.45it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.57it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.73it/s][A
 33%|███▎      | 202/611 [00:05<00:09, 44.48it/s][A
 34%|███▍      | 207/611 [00:05<00:09, 44.16it/s][A
 35%|███▍      | 212/611 [00:05<00:21, 18.82it/s][A
 36%|███▌      | 217/611 [00:05<00:17, 22.81it/s][A
 36%|███▋      | 222/611 [00:06<00:14, 26.76it/s][A
 37%|███▋      | 227/611 [00:06<00:12, 30.43it/s][A
 38%|███▊      | 232/611 [00:06<00:11, 33.69it/s][A
 39%|███▉      | 237/611 [00:06<00:10, 36.49it/s][A
 40%|███▉      | 242/611 [00:06<00:09, 38.72it/s][A
 40%|████      | 247/611 [00:06<00:09, 40.32it/s][A
 41%|████      | 252/611 [00:06<00:08, 41.10it/s][A
 42%|████▏     | 257/611 [00:06<00:08, 41.72it/s][A
 43%|████▎     | 262/611 [00:06<00:08, 42.48it/s][A
 44%|████▎     | 267/611 [00:07<00:07, 43.10it/s][A
 45%|████▍     | 272/611 [00:07<00:07, 43.58it/s][A
 45%|████▌     | 277/611 [00:07<00:07, 43.90it/s][A
 46%|████▌     | 282/611 [00:07<00:07, 44.24it/s][A
 47%|████▋     | 287/611 [00:07<00:07, 44.45it/s][A
 48%|████▊     | 292/611 [00:07<00:07, 44.46it/s][A
 49%|████▊     | 297/611 [00:07<00:07, 44.23it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 43.98it/s][A
 50%|█████     | 307/611 [00:07<00:06, 44.04it/s][A
 51%|█████     | 312/611 [00:08<00:06, 44.20it/s][A
 52%|█████▏    | 317/611 [00:08<00:06, 44.34it/s][A
 53%|█████▎    | 322/611 [00:08<00:06, 44.45it/s][A
 54%|█████▎    | 327/611 [00:08<00:08, 33.29it/s][A
 54%|█████▍    | 332/611 [00:08<00:07, 36.10it/s][A
 55%|█████▌    | 337/611 [00:08<00:07, 38.38it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 40.09it/s][A
 57%|█████▋    | 347/611 [00:08<00:06, 41.49it/s][A
 58%|█████▊    | 352/611 [00:09<00:06, 42.52it/s][A
 58%|█████▊    | 357/611 [00:09<00:05, 43.27it/s][A
 59%|█████▉    | 362/611 [00:09<00:05, 43.59it/s][A
 60%|██████    | 367/611 [00:09<00:05, 43.48it/s][A
 61%|██████    | 372/611 [00:09<00:05, 43.40it/s][A
 62%|██████▏   | 377/611 [00:09<00:05, 43.58it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 43.92it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 44.15it/s][A
 64%|██████▍   | 392/611 [00:09<00:04, 44.44it/s][A
 65%|██████▍   | 397/611 [00:10<00:04, 44.68it/s][A
 66%|██████▌   | 402/611 [00:10<00:04, 44.68it/s][A
 67%|██████▋   | 407/611 [00:10<00:04, 44.55it/s][A
 67%|██████▋   | 412/611 [00:10<00:04, 44.17it/s][A
 68%|██████▊   | 417/611 [00:10<00:04, 44.05it/s][A
 69%|██████▉   | 422/611 [00:11<00:04, 43.98it/s][A
 70%|██████▉   | 427/611 [00:11<00:08, 21.17it/s][A
 71%|███████   | 432/611 [00:11<00:07, 25.13it/s][A
 72%|███████▏  | 437/611 [00:11<00:06, 28.95it/s][A
 72%|███████▏  | 442/611 [00:11<00:05, 32.43it/s][A
 73%|███████▎  | 447/611 [00:11<00:04, 35.39it/s][A
 74%|███████▍  | 452/611 [00:11<00:04, 37.83it/s][A
 75%|███████▍  | 457/611 [00:11<00:03, 39.73it/s][A
 76%|███████▌  | 462/611 [00:11<00:03, 41.04it/s][A
 76%|███████▋  | 467/611 [00:12<00:03, 41.64it/s][A
 77%|███████▋  | 472/611 [00:12<00:03, 42.13it/s][A
 78%|███████▊  | 477/611 [00:12<00:03, 42.51it/s][A
 79%|███████▉  | 482/611 [00:12<00:02, 43.19it/s][A
 80%|███████▉  | 487/611 [00:12<00:02, 43.49it/s][A
 81%|████████  | 492/611 [00:12<00:02, 44.02it/s][A
 81%|████████▏ | 497/611 [00:12<00:02, 44.28it/s][A
 82%|████████▏ | 502/611 [00:12<00:02, 44.46it/s][A
 83%|████████▎ | 507/611 [00:12<00:02, 44.47it/s][A
 84%|████████▍ | 512/611 [00:13<00:02, 44.22it/s][A
 85%|████████▍ | 517/611 [00:13<00:02, 43.99it/s][A
 85%|████████▌ | 522/611 [00:13<00:02, 43.91it/s][A
 86%|████████▋ | 527/611 [00:13<00:01, 44.11it/s][A
 87%|████████▋ | 532/611 [00:13<00:01, 44.32it/s][A
 88%|████████▊ | 537/611 [00:14<00:01, 44.44it/s][A
 89%|████████▊ | 542/611 [00:14<00:02, 23.46it/s][A
 90%|████████▉ | 547/611 [00:14<00:02, 27.40it/s][A
 90%|█████████ | 552/611 [00:14<00:01, 30.99it/s][A
 91%|█████████ | 557/611 [00:14<00:01, 34.10it/s][A
 92%|█████████▏| 562/611 [00:14<00:01, 36.75it/s][A
 93%|█████████▎| 567/611 [00:14<00:01, 38.89it/s][A
 94%|█████████▎| 572/611 [00:14<00:00, 40.49it/s][A
 94%|█████████▍| 577/611 [00:14<00:00, 41.63it/s][A
 95%|█████████▌| 582/611 [00:15<00:00, 42.13it/s][A
 96%|█████████▌| 587/611 [00:15<00:00, 42.52it/s][A
 97%|█████████▋| 592/611 [00:15<00:00, 42.94it/s][A
 98%|█████████▊| 597/611 [00:15<00:00, 43.36it/s][A
 99%|█████████▊| 602/611 [00:15<00:00, 43.87it/s][A
 99%|█████████▉| 607/611 [00:15<00:00, 44.13it/s][A                                                
                                                 [A 20%|██        | 47/235 [00:30<00:56,  3.32it/s]
100%|██████████| 611/611 [00:15<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:56:28,863 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 20:56:29,168 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:56:40,943 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:56:42,803 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:56:43,260 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [01:19<1:01:07, 19.61s/it] 21%|██        | 49/235 [01:20<43:27, 14.02s/it]   21%|██▏       | 50/235 [01:20<30:31,  9.90s/it] 22%|██▏       | 51/235 [01:21<21:31,  7.02s/it] 22%|██▏       | 52/235 [01:21<15:15,  5.00s/it] 23%|██▎       | 53/235 [01:21<10:53,  3.59s/it] 23%|██▎       | 54/235 [01:22<07:50,  2.60s/it] 23%|██▎       | 55/235 [01:22<05:43,  1.91s/it] 24%|██▍       | 56/235 [01:22<04:14,  1.42s/it] 24%|██▍       | 57/235 [01:23<03:28,  1.17s/it] 25%|██▍       | 58/235 [01:23<02:40,  1.10it/s] 25%|██▌       | 59/235 [01:23<02:07,  1.38it/s] 26%|██▌       | 60/235 [01:24<01:44,  1.68it/s] 26%|██▌       | 61/235 [01:24<01:27,  1.98it/s] 26%|██▋       | 62/235 [01:24<01:16,  2.27it/s] 27%|██▋       | 63/235 [01:25<01:08,  2.52it/s] 27%|██▋       | 64/235 [01:25<01:02,  2.74it/s] 28%|██▊       | 65/235 [01:25<00:58,  2.91it/s] 28%|██▊       | 66/235 [01:25<00:55,  3.04it/s] 29%|██▊       | 67/235 [01:26<01:04,  2.61it/s] 29%|██▉       | 68/235 [01:26<00:59,  2.81it/s] 29%|██▉       | 69/235 [01:27<00:55,  2.97it/s] 30%|██▉       | 70/235 [01:27<00:53,  3.08it/s] 30%|███       | 71/235 [01:27<00:51,  3.18it/s] 31%|███       | 72/235 [01:27<00:50,  3.24it/s] 31%|███       | 73/235 [01:28<00:49,  3.29it/s] 31%|███▏      | 74/235 [01:28<00:48,  3.32it/s] 32%|███▏      | 75/235 [01:28<00:47,  3.35it/s] 32%|███▏      | 76/235 [01:29<00:47,  3.36it/s] 33%|███▎      | 77/235 [01:29<00:54,  2.91it/s] 33%|███▎      | 78/235 [01:29<00:51,  3.04it/s] 34%|███▎      | 79/235 [01:30<00:49,  3.14it/s] 34%|███▍      | 80/235 [01:30<00:48,  3.22it/s] 34%|███▍      | 81/235 [01:30<00:47,  3.27it/s] 35%|███▍      | 82/235 [01:30<00:46,  3.31it/s] 35%|███▌      | 83/235 [01:31<00:45,  3.34it/s] 36%|███▌      | 84/235 [01:31<00:44,  3.36it/s] 36%|███▌      | 85/235 [01:31<00:44,  3.37it/s] 37%|███▋      | 86/235 [01:32<00:44,  3.39it/s] 37%|███▋      | 87/235 [01:32<00:45,  3.27it/s] 37%|███▋      | 88/235 [01:32<00:44,  3.31it/s] 38%|███▊      | 89/235 [01:33<00:43,  3.34it/s] 38%|███▊      | 90/235 [01:33<00:43,  3.35it/s] 39%|███▊      | 91/235 [01:33<00:42,  3.37it/s] 39%|███▉      | 92/235 [01:33<00:42,  3.38it/s] 40%|███▉      | 93/235 [01:34<00:41,  3.39it/s] 40%|████      | 94/235 [01:34<00:41,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 20:57:31,616 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:57:31,616 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 20:57:31,616 >>   Batch size = 8
{'eval_loss': 0.968847930431366, 'eval_runtime': 15.6863, 'eval_samples_per_second': 311.226, 'eval_steps_per_second': 38.951, 'epoch': 0.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.19it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.66it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.01it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.98it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.23it/s][A
  5%|▌         | 32/611 [00:00<00:19, 29.59it/s][A
  6%|▌         | 37/611 [00:00<00:17, 33.25it/s][A
  7%|▋         | 42/611 [00:01<00:15, 36.26it/s][A
  8%|▊         | 47/611 [00:01<00:14, 38.61it/s][A
  9%|▊         | 52/611 [00:01<00:13, 40.40it/s][A
  9%|▉         | 57/611 [00:01<00:13, 41.69it/s][A
 10%|█         | 62/611 [00:01<00:12, 42.72it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.11it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.04it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.03it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.37it/s][A
 14%|█▍        | 87/611 [00:02<00:11, 43.70it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.12it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.34it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.46it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.66it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.48it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.13it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.98it/s][A
 21%|██        | 127/611 [00:03<00:10, 44.11it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.27it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.39it/s][A
 23%|██▎       | 142/611 [00:03<00:22, 21.12it/s][A
 24%|██▍       | 147/611 [00:03<00:18, 25.09it/s][A
 25%|██▍       | 152/611 [00:03<00:15, 28.92it/s][A
 26%|██▌       | 157/611 [00:04<00:14, 32.38it/s][A
 27%|██▋       | 162/611 [00:04<00:12, 35.32it/s][A
 27%|██▋       | 167/611 [00:04<00:11, 37.71it/s][A
 28%|██▊       | 172/611 [00:04<00:11, 39.63it/s][A
 29%|██▉       | 177/611 [00:04<00:10, 40.95it/s][A
 30%|██▉       | 182/611 [00:04<00:10, 41.59it/s][A
 31%|███       | 187/611 [00:04<00:10, 42.13it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 42.73it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.25it/s][A
 33%|███▎      | 202/611 [00:05<00:09, 43.64it/s][A
 34%|███▍      | 207/611 [00:05<00:09, 43.94it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 44.24it/s][A
 36%|███▌      | 217/611 [00:05<00:08, 44.44it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.31it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.04it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.02it/s][A
 40%|███▉      | 242/611 [00:06<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:06<00:08, 44.30it/s][A
 41%|████      | 252/611 [00:06<00:08, 44.55it/s][A
 42%|████▏     | 257/611 [00:06<00:08, 39.39it/s][A
 43%|████▎     | 262/611 [00:06<00:08, 40.94it/s][A
 44%|████▎     | 267/611 [00:06<00:08, 41.96it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 42.78it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.30it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 43.67it/s][A
 47%|████▋     | 287/611 [00:07<00:07, 43.92it/s][A
 48%|████▊     | 292/611 [00:07<00:07, 44.03it/s][A
 49%|████▊     | 297/611 [00:07<00:07, 43.70it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 43.75it/s][A
 50%|█████     | 307/611 [00:07<00:06, 43.84it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.04it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.23it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.41it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.49it/s][A
 54%|█████▍    | 332/611 [00:08<00:06, 44.55it/s][A
 55%|█████▌    | 337/611 [00:08<00:10, 26.66it/s][A
 56%|█████▌    | 343/611 [00:08<00:08, 31.78it/s][A
 57%|█████▋    | 348/611 [00:08<00:07, 34.66it/s][A
 58%|█████▊    | 353/611 [00:08<00:06, 37.15it/s][A
 59%|█████▊    | 358/611 [00:08<00:06, 39.19it/s][A
 59%|█████▉    | 363/611 [00:09<00:06, 40.75it/s][A
 60%|██████    | 368/611 [00:09<00:05, 41.84it/s][A
 61%|██████    | 373/611 [00:09<00:05, 42.71it/s][A
 62%|██████▏   | 378/611 [00:09<00:05, 43.02it/s][A
 63%|██████▎   | 383/611 [00:09<00:05, 39.30it/s][A
 64%|██████▎   | 388/611 [00:09<00:05, 40.79it/s][A
 64%|██████▍   | 393/611 [00:09<00:05, 41.82it/s][A
 65%|██████▌   | 398/611 [00:09<00:04, 42.73it/s][A
 66%|██████▌   | 403/611 [00:09<00:04, 43.35it/s][A
 67%|██████▋   | 408/611 [00:10<00:04, 43.82it/s][A
 68%|██████▊   | 413/611 [00:10<00:04, 44.12it/s][A
 68%|██████▊   | 418/611 [00:10<00:04, 44.17it/s][A
 69%|██████▉   | 423/611 [00:10<00:04, 43.86it/s][A
 70%|███████   | 428/611 [00:10<00:04, 43.62it/s][A
 71%|███████   | 433/611 [00:10<00:04, 43.61it/s][A
 72%|███████▏  | 438/611 [00:10<00:03, 43.93it/s][A
 73%|███████▎  | 443/611 [00:10<00:03, 44.20it/s][A
 73%|███████▎  | 448/611 [00:10<00:03, 44.44it/s][A
 74%|███████▍  | 453/611 [00:11<00:03, 44.54it/s][A
 75%|███████▍  | 458/611 [00:11<00:03, 44.60it/s][A
 76%|███████▌  | 463/611 [00:11<00:03, 44.41it/s][A
 77%|███████▋  | 468/611 [00:11<00:03, 44.06it/s][A
 77%|███████▋  | 473/611 [00:11<00:03, 43.93it/s][A
 78%|███████▊  | 478/611 [00:11<00:03, 43.87it/s][A
 79%|███████▉  | 483/611 [00:11<00:02, 44.02it/s][A
 80%|███████▉  | 488/611 [00:11<00:02, 44.31it/s][A
 81%|████████  | 493/611 [00:11<00:02, 44.45it/s][A
 82%|████████▏ | 498/611 [00:12<00:02, 44.42it/s][A
 82%|████████▏ | 503/611 [00:12<00:02, 44.60it/s][A
 83%|████████▎ | 508/611 [00:12<00:02, 44.34it/s][A
 84%|████████▍ | 513/611 [00:12<00:02, 44.25it/s][A
 85%|████████▍ | 518/611 [00:12<00:02, 39.35it/s][A
 86%|████████▌ | 523/611 [00:12<00:02, 40.91it/s][A
 86%|████████▋ | 528/611 [00:12<00:01, 42.05it/s][A
 87%|████████▋ | 533/611 [00:12<00:01, 42.81it/s][A
 88%|████████▊ | 538/611 [00:13<00:01, 43.43it/s][A
 89%|████████▉ | 543/611 [00:13<00:01, 43.85it/s][A
 90%|████████▉ | 548/611 [00:13<00:01, 44.03it/s][A
 91%|█████████ | 553/611 [00:13<00:01, 43.97it/s][A
 91%|█████████▏| 558/611 [00:14<00:03, 14.16it/s][A
 92%|█████████▏| 563/611 [00:14<00:02, 17.83it/s][A
 93%|█████████▎| 568/611 [00:14<00:01, 21.77it/s][A
 94%|█████████▍| 573/611 [00:14<00:01, 25.72it/s][A
 95%|█████████▍| 578/611 [00:14<00:01, 29.50it/s][A
 95%|█████████▌| 583/611 [00:14<00:00, 32.86it/s][A
 96%|█████████▌| 588/611 [00:14<00:00, 35.73it/s][A
 97%|█████████▋| 593/611 [00:15<00:00, 38.00it/s][A
 98%|█████████▊| 598/611 [00:15<00:00, 39.35it/s][A
 99%|█████████▊| 603/611 [00:15<00:00, 40.35it/s][A
100%|█████████▉| 608/611 [00:15<00:00, 41.43it/s][A                                                
                                                 [A 40%|████      | 94/235 [01:50<00:41,  3.39it/s]
100%|██████████| 611/611 [00:15<00:00, 41.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:57:47,330 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 20:57:48,174 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:58:00,928 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:58:01,077 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:58:01,146 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [02:41<47:09, 20.21s/it] 41%|████      | 96/235 [02:41<33:07, 14.30s/it] 41%|████▏     | 97/235 [02:42<23:13, 10.10s/it] 42%|████▏     | 98/235 [02:42<16:20,  7.16s/it] 42%|████▏     | 99/235 [02:42<11:33,  5.10s/it] 43%|████▎     | 100/235 [02:43<08:21,  3.72s/it] 43%|████▎     | 101/235 [02:43<06:00,  2.69s/it] 43%|████▎     | 102/235 [02:43<04:22,  1.97s/it] 44%|████▍     | 103/235 [02:44<03:13,  1.47s/it] 44%|████▍     | 104/235 [02:44<02:26,  1.12s/it] 45%|████▍     | 105/235 [02:44<01:57,  1.11it/s] 45%|████▌     | 106/235 [02:44<01:32,  1.39it/s] 46%|████▌     | 107/235 [02:45<01:15,  1.69it/s] 46%|████▌     | 108/235 [02:45<01:03,  1.99it/s] 46%|████▋     | 109/235 [02:45<00:55,  2.28it/s] 47%|████▋     | 110/235 [02:46<00:49,  2.53it/s] 47%|████▋     | 111/235 [02:46<00:45,  2.74it/s] 48%|████▊     | 112/235 [02:46<00:42,  2.91it/s] 48%|████▊     | 113/235 [02:47<00:40,  3.04it/s] 49%|████▊     | 114/235 [02:47<00:38,  3.15it/s] 49%|████▉     | 115/235 [02:47<00:38,  3.10it/s] 49%|████▉     | 116/235 [02:47<00:37,  3.19it/s] 50%|████▉     | 117/235 [02:48<00:37,  3.13it/s] 50%|█████     | 118/235 [02:49<01:20,  1.46it/s] 51%|█████     | 119/235 [02:50<01:05,  1.76it/s] 51%|█████     | 120/235 [02:50<00:55,  2.06it/s] 51%|█████▏    | 121/235 [02:50<00:53,  2.11it/s] 52%|█████▏    | 122/235 [02:51<00:47,  2.38it/s] 52%|█████▏    | 123/235 [02:51<00:42,  2.62it/s] 53%|█████▎    | 124/235 [02:51<00:39,  2.82it/s] 53%|█████▎    | 125/235 [02:52<00:37,  2.97it/s] 54%|█████▎    | 126/235 [02:52<00:35,  3.09it/s] 54%|█████▍    | 127/235 [02:52<00:33,  3.18it/s] 54%|█████▍    | 128/235 [02:52<00:32,  3.25it/s] 55%|█████▍    | 129/235 [02:53<00:32,  3.30it/s] 55%|█████▌    | 130/235 [02:53<00:31,  3.33it/s] 56%|█████▌    | 131/235 [02:53<00:32,  3.15it/s] 56%|█████▌    | 132/235 [02:54<00:31,  3.23it/s] 57%|█████▋    | 133/235 [02:54<00:31,  3.28it/s] 57%|█████▋    | 134/235 [02:54<00:30,  3.32it/s] 57%|█████▋    | 135/235 [02:55<00:29,  3.34it/s] 58%|█████▊    | 136/235 [02:55<00:29,  3.36it/s] 58%|█████▊    | 137/235 [02:55<00:29,  3.38it/s] 59%|█████▊    | 138/235 [02:55<00:28,  3.39it/s] 59%|█████▉    | 139/235 [02:56<00:28,  3.39it/s] 60%|█████▉    | 140/235 [02:56<00:27,  3.39it/s] 60%|██████    | 141/235 [02:56<00:28,  3.24it/s][INFO|trainer.py:2140] 2023-08-28 20:58:53,912 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:58:53,912 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 20:58:53,912 >>   Batch size = 8
{'eval_loss': 0.9756728410720825, 'eval_runtime': 15.4932, 'eval_samples_per_second': 315.106, 'eval_steps_per_second': 39.437, 'epoch': 1.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.80it/s][A
  2%|▏         | 12/611 [00:00<00:22, 26.81it/s][A
  3%|▎         | 17/611 [00:00<00:18, 32.32it/s][A
  4%|▎         | 22/611 [00:00<00:16, 36.14it/s][A
  4%|▍         | 27/611 [00:00<00:15, 38.78it/s][A
  5%|▌         | 32/611 [00:00<00:14, 40.53it/s][A
  6%|▌         | 37/611 [00:00<00:13, 41.89it/s][A
  7%|▋         | 42/611 [00:01<00:13, 42.71it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.42it/s][A
  9%|▊         | 52/611 [00:01<00:12, 43.57it/s][A
  9%|▉         | 57/611 [00:01<00:12, 43.51it/s][A
 10%|█         | 62/611 [00:01<00:12, 43.72it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.90it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.17it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.32it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.51it/s][A
 14%|█▍        | 87/611 [00:02<00:11, 44.64it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.67it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.40it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.25it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.15it/s][A
 18%|█▊        | 112/611 [00:02<00:14, 34.80it/s][A
 19%|█▉        | 117/611 [00:02<00:13, 37.38it/s][A
 20%|█▉        | 122/611 [00:02<00:12, 39.33it/s][A
 21%|██        | 127/611 [00:03<00:11, 40.79it/s][A
 22%|██▏       | 132/611 [00:03<00:11, 41.99it/s][A
 22%|██▏       | 137/611 [00:03<00:11, 42.90it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.59it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 43.84it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.54it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 43.47it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 43.69it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.96it/s][A
 28%|██▊       | 172/611 [00:04<00:09, 44.30it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.53it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.66it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.68it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.35it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.08it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.76it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.86it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 44.02it/s][A
 36%|███▌      | 217/611 [00:05<00:08, 44.33it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.59it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.74it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.72it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.57it/s][A
 40%|███▉      | 242/611 [00:05<00:12, 29.29it/s][A
 40%|████      | 247/611 [00:05<00:11, 32.68it/s][A
 41%|████      | 252/611 [00:06<00:10, 35.52it/s][A
 42%|████▏     | 257/611 [00:06<00:09, 37.81it/s][A
 43%|████▎     | 262/611 [00:06<00:08, 39.72it/s][A
 44%|████▎     | 267/611 [00:06<00:08, 41.11it/s][A
 45%|████▍     | 272/611 [00:06<00:08, 42.25it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 42.90it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 42.93it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.05it/s][A
 48%|████▊     | 292/611 [00:07<00:07, 43.51it/s][A
 49%|████▊     | 297/611 [00:07<00:07, 43.87it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:07<00:06, 44.25it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.44it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.54it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.44it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.20it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.04it/s][A
 55%|█████▌    | 337/611 [00:08<00:06, 44.10it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 44.28it/s][A
 57%|█████▋    | 347/611 [00:08<00:05, 44.42it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.41it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.58it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.55it/s][A
 60%|██████    | 367/611 [00:08<00:07, 32.76it/s][A
 61%|██████    | 372/611 [00:08<00:06, 35.68it/s][A
 62%|██████▏   | 377/611 [00:09<00:06, 38.09it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 39.89it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 41.29it/s][A
 64%|██████▍   | 392/611 [00:09<00:05, 42.19it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 42.88it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.25it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.03it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.26it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.53it/s][A
 69%|██████▉   | 422/611 [00:10<00:04, 44.07it/s][A
 70%|██████▉   | 427/611 [00:10<00:04, 44.29it/s][A
 71%|███████   | 432/611 [00:10<00:04, 44.47it/s][A
 72%|███████▏  | 437/611 [00:10<00:03, 44.45it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.45it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 43.84it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 43.86it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.98it/s][A
 76%|███████▋  | 467/611 [00:11<00:03, 44.20it/s][A
 77%|███████▋  | 472/611 [00:11<00:03, 44.37it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 44.56it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.71it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.49it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.24it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 41.51it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 42.42it/s][A
 83%|████████▎ | 507/611 [00:12<00:02, 43.01it/s][A
 84%|████████▍ | 512/611 [00:12<00:02, 43.50it/s][A
 85%|████████▍ | 517/611 [00:12<00:02, 43.92it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.20it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.39it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.27it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.75it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.73it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.98it/s][A
 90%|█████████ | 552/611 [00:13<00:01, 44.13it/s][A
 91%|█████████ | 557/611 [00:13<00:01, 44.28it/s][A
 92%|█████████▏| 562/611 [00:13<00:01, 44.44it/s][A
 93%|█████████▎| 567/611 [00:13<00:00, 44.55it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.62it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.38it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.04it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.07it/s][A
 98%|█████████▊| 597/611 [00:14<00:00, 44.25it/s][A
 99%|█████████▊| 602/611 [00:14<00:00, 44.45it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 44.36it/s][A                                                 
                                                 [A 60%|██████    | 141/235 [03:11<00:28,  3.24it/s]
100%|██████████| 611/611 [00:14<00:00, 44.36it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:59:08,726 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 20:59:10,536 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:59:28,410 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:59:29,438 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:59:29,803 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [04:15<37:04, 23.92s/it] 61%|██████    | 143/235 [04:16<26:01, 16.98s/it] 61%|██████▏   | 144/235 [04:17<18:17, 12.06s/it] 62%|██████▏   | 145/235 [04:17<12:47,  8.53s/it] 62%|██████▏   | 146/235 [04:17<08:59,  6.06s/it] 63%|██████▎   | 147/235 [04:18<06:20,  4.33s/it] 63%|██████▎   | 148/235 [04:18<04:31,  3.12s/it] 63%|██████▎   | 149/235 [04:18<03:15,  2.27s/it] 64%|██████▍   | 150/235 [04:18<02:22,  1.68s/it] 64%|██████▍   | 151/235 [04:19<01:46,  1.26s/it] 65%|██████▍   | 152/235 [04:19<01:20,  1.03it/s] 65%|██████▌   | 153/235 [04:19<01:02,  1.30it/s] 66%|██████▌   | 154/235 [04:20<01:07,  1.20it/s] 66%|██████▌   | 155/235 [04:21<00:53,  1.49it/s] 66%|██████▋   | 156/235 [04:21<00:44,  1.79it/s] 67%|██████▋   | 157/235 [04:21<00:37,  2.09it/s] 67%|██████▋   | 158/235 [04:22<00:32,  2.37it/s] 68%|██████▊   | 159/235 [04:22<00:29,  2.61it/s] 68%|██████▊   | 160/235 [04:22<00:26,  2.80it/s] 69%|██████▊   | 161/235 [04:22<00:24,  2.96it/s] 69%|██████▉   | 162/235 [04:23<00:37,  1.96it/s] 69%|██████▉   | 163/235 [04:24<00:31,  2.25it/s] 70%|██████▉   | 164/235 [04:24<00:28,  2.51it/s] 70%|███████   | 165/235 [04:24<00:25,  2.72it/s] 71%|███████   | 166/235 [04:24<00:23,  2.90it/s] 71%|███████   | 167/235 [04:25<00:22,  3.04it/s] 71%|███████▏  | 168/235 [04:25<00:21,  3.14it/s] 72%|███████▏  | 169/235 [04:25<00:20,  3.22it/s] 72%|███████▏  | 170/235 [04:26<00:19,  3.28it/s] 73%|███████▎  | 171/235 [04:27<00:34,  1.87it/s] 73%|███████▎  | 172/235 [04:27<00:29,  2.16it/s] 74%|███████▎  | 173/235 [04:27<00:25,  2.43it/s] 74%|███████▍  | 174/235 [04:28<00:22,  2.66it/s] 74%|███████▍  | 175/235 [04:28<00:21,  2.85it/s] 75%|███████▍  | 176/235 [04:28<00:19,  3.00it/s] 75%|███████▌  | 177/235 [04:28<00:18,  3.11it/s] 76%|███████▌  | 178/235 [04:29<00:17,  3.20it/s] 76%|███████▌  | 179/235 [04:29<00:19,  2.85it/s] 77%|███████▋  | 180/235 [04:29<00:18,  3.00it/s] 77%|███████▋  | 181/235 [04:30<00:17,  3.11it/s] 77%|███████▋  | 182/235 [04:30<00:16,  3.20it/s] 78%|███████▊  | 183/235 [04:30<00:15,  3.26it/s] 78%|███████▊  | 184/235 [04:31<00:15,  3.31it/s] 79%|███████▊  | 185/235 [04:31<00:14,  3.34it/s] 79%|███████▉  | 186/235 [04:31<00:14,  3.36it/s] 80%|███████▉  | 187/235 [04:32<00:14,  3.38it/s] 80%|████████  | 188/235 [04:32<00:13,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 21:00:29,446 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:00:29,446 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 21:00:29,446 >>   Batch size = 8
{'eval_loss': 0.9837108850479126, 'eval_runtime': 14.3623, 'eval_samples_per_second': 339.918, 'eval_steps_per_second': 42.542, 'epoch': 2.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.14it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.86it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.25it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.49it/s][A
  4%|▍         | 27/611 [00:00<00:12, 46.05it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.73it/s][A
  6%|▌         | 37/611 [00:00<00:12, 45.02it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.44it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.37it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.53it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.55it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.69it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.84it/s][A
 12%|█▏        | 72/611 [00:01<00:11, 45.00it/s][A
 13%|█▎        | 77/611 [00:01<00:11, 44.87it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.45it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.26it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.25it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.42it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.44it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.60it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.80it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.90it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.69it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.43it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.32it/s][A
 22%|██▏       | 137/611 [00:03<00:16, 28.71it/s][A
 23%|██▎       | 142/611 [00:03<00:14, 32.26it/s][A
 24%|██▍       | 147/611 [00:03<00:13, 35.28it/s][A
 25%|██▍       | 152/611 [00:03<00:12, 37.77it/s][A
 26%|██▌       | 157/611 [00:03<00:11, 39.65it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 41.20it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 42.32it/s][A
 28%|██▊       | 172/611 [00:04<00:10, 43.01it/s][A
 29%|██▉       | 177/611 [00:04<00:10, 43.09it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.11it/s][A
 31%|███       | 187/611 [00:04<00:09, 43.40it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.80it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.05it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.27it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.51it/s][A
 35%|███▍      | 212/611 [00:04<00:08, 44.73it/s][A
 36%|███▌      | 217/611 [00:05<00:08, 44.79it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.37it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.20it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.18it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.24it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.47it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.48it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.63it/s][A
 42%|████▏     | 257/611 [00:06<00:07, 44.83it/s][A
 43%|████▎     | 262/611 [00:06<00:11, 30.11it/s][A
 44%|████▎     | 267/611 [00:06<00:10, 33.42it/s][A
 45%|████▍     | 272/611 [00:06<00:09, 36.23it/s][A
 45%|████▌     | 277/611 [00:06<00:08, 38.50it/s][A
 46%|████▌     | 282/611 [00:06<00:08, 40.24it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 41.61it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 42.63it/s][A
 49%|████▊     | 297/611 [00:07<00:07, 43.19it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 43.16it/s][A
 50%|█████     | 307/611 [00:07<00:07, 43.20it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.43it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.76it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.08it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.31it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.57it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.78it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 44.67it/s][A
 57%|█████▋    | 347/611 [00:08<00:05, 44.42it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.18it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.05it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.22it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.32it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.55it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.72it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.74it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 44.64it/s][A
 64%|██████▍   | 392/611 [00:09<00:05, 43.32it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 43.51it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.68it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.92it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.17it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.25it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.53it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.45it/s][A
 71%|███████   | 432/611 [00:10<00:04, 44.35it/s][A
 72%|███████▏  | 437/611 [00:10<00:03, 44.37it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.21it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.16it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.24it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.47it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.63it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.69it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.68it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 44.54it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.34it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.28it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.21it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.28it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.46it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.59it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.73it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.64it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.43it/s][A
 86%|████████▋ | 527/611 [00:12<00:02, 35.38it/s][A
 87%|████████▋ | 532/611 [00:12<00:02, 37.81it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 39.65it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 41.11it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 42.19it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.06it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.61it/s][A
 92%|█████████▏| 562/611 [00:13<00:01, 43.81it/s][A
 93%|█████████▎| 567/611 [00:13<00:01, 43.59it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 43.45it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.61it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.76it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.07it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.29it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.58it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.61it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 44.40it/s][A                                                 
                                                 [A 80%|████████  | 188/235 [04:46<00:13,  3.39it/s]
100%|██████████| 611/611 [00:14<00:00, 44.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:00:43,767 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 21:00:45,323 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:01:00,790 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:01:01,151 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:01:01,259 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [05:36<14:57, 19.52s/it] 81%|████████  | 190/235 [05:37<10:20, 13.78s/it] 81%|████████▏ | 191/235 [05:37<07:08,  9.73s/it] 82%|████████▏ | 192/235 [05:37<04:56,  6.90s/it] 82%|████████▏ | 193/235 [05:37<03:26,  4.92s/it] 83%|████████▎ | 194/235 [05:38<02:24,  3.53s/it] 83%|████████▎ | 195/235 [05:38<01:42,  2.56s/it] 83%|████████▎ | 196/235 [05:38<01:13,  1.88s/it] 84%|████████▍ | 197/235 [05:39<00:53,  1.40s/it] 84%|████████▍ | 198/235 [05:39<00:39,  1.07s/it] 85%|████████▍ | 199/235 [05:39<00:30,  1.19it/s] 85%|████████▌ | 200/235 [05:40<00:27,  1.26it/s] 86%|████████▌ | 201/235 [05:40<00:21,  1.55it/s] 86%|████████▌ | 202/235 [05:41<00:17,  1.85it/s] 86%|████████▋ | 203/235 [05:41<00:14,  2.15it/s] 87%|████████▋ | 204/235 [05:41<00:12,  2.42it/s] 87%|████████▋ | 205/235 [05:41<00:11,  2.65it/s] 88%|████████▊ | 206/235 [05:42<00:10,  2.84it/s] 88%|████████▊ | 207/235 [05:42<00:09,  2.99it/s] 89%|████████▊ | 208/235 [05:42<00:08,  3.12it/s] 89%|████████▉ | 209/235 [05:43<00:08,  3.09it/s] 89%|████████▉ | 210/235 [05:43<00:07,  3.19it/s] 90%|████████▉ | 211/235 [05:43<00:07,  3.27it/s] 90%|█████████ | 212/235 [05:43<00:06,  3.33it/s] 91%|█████████ | 213/235 [05:44<00:06,  3.36it/s] 91%|█████████ | 214/235 [05:44<00:06,  3.39it/s] 91%|█████████▏| 215/235 [05:44<00:05,  3.41it/s] 92%|█████████▏| 216/235 [05:45<00:05,  3.43it/s] 92%|█████████▏| 217/235 [05:45<00:05,  3.44it/s] 93%|█████████▎| 218/235 [05:45<00:04,  3.44it/s] 93%|█████████▎| 219/235 [05:45<00:04,  3.45it/s] 94%|█████████▎| 220/235 [05:46<00:05,  2.99it/s] 94%|█████████▍| 221/235 [05:46<00:04,  3.11it/s] 94%|█████████▍| 222/235 [05:47<00:04,  3.21it/s] 95%|█████████▍| 223/235 [05:47<00:03,  3.28it/s] 95%|█████████▌| 224/235 [05:47<00:03,  3.33it/s] 96%|█████████▌| 225/235 [05:47<00:02,  3.37it/s] 96%|█████████▌| 226/235 [05:48<00:02,  3.40it/s] 97%|█████████▋| 227/235 [05:48<00:02,  3.42it/s] 97%|█████████▋| 228/235 [05:48<00:02,  3.43it/s] 97%|█████████▋| 229/235 [05:49<00:01,  3.43it/s] 98%|█████████▊| 230/235 [05:49<00:01,  2.71it/s] 98%|█████████▊| 231/235 [05:49<00:01,  2.89it/s] 99%|█████████▊| 232/235 [05:50<00:00,  3.04it/s] 99%|█████████▉| 233/235 [05:50<00:00,  3.16it/s]100%|█████████▉| 234/235 [05:50<00:00,  3.24it/s]100%|██████████| 235/235 [05:51<00:00,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 21:01:47,987 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:01:47,987 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 21:01:47,987 >>   Batch size = 8
{'eval_loss': 0.9900437593460083, 'eval_runtime': 14.2005, 'eval_samples_per_second': 343.791, 'eval_steps_per_second': 43.027, 'epoch': 3.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.09it/s][A
  2%|▏         | 12/611 [00:00<00:12, 49.00it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.96it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.07it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.44it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.76it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.55it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.46it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.57it/s][A
  9%|▊         | 52/611 [00:01<00:12, 43.27it/s][A
  9%|▉         | 57/611 [00:01<00:12, 43.82it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.15it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.27it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.14it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.10it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.10it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.24it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.24it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.64it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.62it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.44it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.29it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.27it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.35it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.32it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.36it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.57it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.71it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.61it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.46it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.32it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.27it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.20it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.24it/s][A
 31%|███       | 187/611 [00:04<00:10, 40.69it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 41.92it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 42.87it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.37it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.78it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.03it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.16it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.09it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.71it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.93it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.24it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.43it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.53it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.58it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.60it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.53it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.15it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.94it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.95it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.13it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.42it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.53it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.57it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.71it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.42it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.26it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.06it/s][A
 53%|█████▎    | 322/611 [00:07<00:17, 16.50it/s][A
 54%|█████▎    | 327/611 [00:08<00:13, 20.39it/s][A
 54%|█████▍    | 332/611 [00:08<00:11, 24.38it/s][A
 55%|█████▌    | 337/611 [00:08<00:09, 28.24it/s][A
 56%|█████▌    | 342/611 [00:08<00:08, 31.79it/s][A
 57%|█████▋    | 347/611 [00:08<00:07, 34.83it/s][A
 58%|█████▊    | 352/611 [00:08<00:06, 37.34it/s][A
 58%|█████▊    | 357/611 [00:08<00:06, 39.24it/s][A
 59%|█████▉    | 362/611 [00:08<00:06, 40.34it/s][A
 60%|██████    | 367/611 [00:08<00:05, 41.12it/s][A
 61%|██████    | 372/611 [00:09<00:05, 42.01it/s][A
 62%|██████▏   | 377/611 [00:09<00:05, 42.91it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 43.40it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 43.80it/s][A
 64%|██████▍   | 392/611 [00:09<00:04, 44.15it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.33it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.27it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.88it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.84it/s][A
 68%|██████▊   | 417/611 [00:10<00:04, 44.00it/s][A
 69%|██████▉   | 422/611 [00:10<00:04, 44.15it/s][A
 70%|██████▉   | 427/611 [00:10<00:04, 44.37it/s][A
 71%|███████   | 432/611 [00:10<00:04, 42.54it/s][A
 72%|███████▏  | 437/611 [00:10<00:04, 43.27it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.71it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.80it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 43.77it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 43.78it/s][A
 76%|███████▌  | 462/611 [00:11<00:03, 43.92it/s][A
 76%|███████▋  | 467/611 [00:11<00:03, 44.11it/s][A
 77%|███████▋  | 472/611 [00:11<00:03, 44.14it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 44.39it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.55it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.57it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.30it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.22it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.16it/s][A
 83%|████████▎ | 507/611 [00:12<00:02, 44.26it/s][A
 84%|████████▍ | 512/611 [00:12<00:02, 44.15it/s][A
 85%|████████▍ | 517/611 [00:12<00:02, 44.34it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.42it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.58it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.49it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.31it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.06it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.04it/s][A
 90%|█████████ | 552/611 [00:13<00:01, 44.05it/s][A
 91%|█████████ | 557/611 [00:13<00:03, 15.36it/s][A
 92%|█████████▏| 562/611 [00:14<00:02, 19.12it/s][A
 93%|█████████▎| 567/611 [00:14<00:01, 23.10it/s][A
 94%|█████████▎| 572/611 [00:14<00:01, 27.02it/s][A
 94%|█████████▍| 577/611 [00:14<00:01, 30.70it/s][A
 95%|█████████▌| 582/611 [00:14<00:00, 33.93it/s][A
 96%|█████████▌| 587/611 [00:14<00:00, 36.60it/s][A
 97%|█████████▋| 592/611 [00:14<00:00, 38.63it/s][A
 98%|█████████▊| 597/611 [00:14<00:00, 39.90it/s][A
 99%|█████████▊| 602/611 [00:14<00:00, 40.82it/s][A
 99%|█████████▉| 607/611 [00:15<00:00, 41.84it/s][A                                                 
                                                 [A100%|██████████| 235/235 [06:06<00:00,  3.30it/s]
100%|██████████| 611/611 [00:15<00:00, 41.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:02:03,286 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 21:02:04,266 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:02:18,614 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:02:19,470 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:02:19,683 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:03:00,902 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:03:00,959 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47 (score: 0.968847930431366).
                                                 100%|██████████| 235/235 [07:37<00:00,  3.30it/s]100%|██████████| 235/235 [07:37<00:00,  1.95s/it]
[INFO|trainer.py:1894] 2023-08-28 21:03:35,132 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 21:03:35,618 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:03:48,323 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:03:48,741 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:03:49,594 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:03:50,876 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   train_loss               =     0.3681
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   train_runtime            = 0:07:37.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   train_samples            =       3035
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   train_samples_per_second =     33.147
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:50,876 >>   train_steps_per_second   =      0.513
{'eval_loss': 0.9966084361076355, 'eval_runtime': 15.1509, 'eval_samples_per_second': 322.225, 'eval_steps_per_second': 40.328, 'epoch': 4.99}
{'train_runtime': 457.8073, 'train_samples_per_second': 33.147, 'train_steps_per_second': 0.513, 'train_loss': 0.3681323112325465, 'epoch': 4.99}
08/28/2023 21:03:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:03:52,313 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:03:52,313 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 21:03:52,313 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.36it/s]  2%|▏         | 12/611 [00:00<00:12, 49.01it/s]  3%|▎         | 17/611 [00:00<00:12, 47.17it/s]  4%|▎         | 22/611 [00:00<00:12, 46.45it/s]  4%|▍         | 27/611 [00:00<00:12, 45.95it/s]  5%|▌         | 32/611 [00:00<00:12, 45.69it/s]  6%|▌         | 37/611 [00:00<00:12, 45.34it/s]  7%|▋         | 42/611 [00:00<00:12, 45.05it/s]  8%|▊         | 47/611 [00:01<00:12, 44.48it/s]  9%|▊         | 52/611 [00:01<00:12, 44.37it/s]  9%|▉         | 57/611 [00:01<00:12, 44.40it/s] 10%|█         | 62/611 [00:01<00:12, 44.45it/s] 11%|█         | 67/611 [00:01<00:12, 44.62it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.82it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.85it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.92it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.59it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.38it/s] 16%|█▌        | 97/611 [00:02<00:11, 44.33it/s] 17%|█▋        | 102/611 [00:02<00:11, 44.42it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.27it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.47it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.69it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.85it/s] 21%|██        | 127/611 [00:02<00:10, 44.84it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.59it/s] 22%|██▏       | 137/611 [00:03<00:18, 25.51it/s] 23%|██▎       | 141/611 [00:03<00:20, 22.95it/s] 24%|██▍       | 146/611 [00:03<00:17, 27.23it/s] 25%|██▍       | 151/611 [00:03<00:14, 31.03it/s] 26%|██▌       | 156/611 [00:03<00:13, 34.29it/s] 26%|██▋       | 161/611 [00:03<00:12, 36.94it/s] 27%|██▋       | 166/611 [00:04<00:11, 39.16it/s] 28%|██▊       | 171/611 [00:04<00:10, 40.82it/s] 29%|██▉       | 176/611 [00:04<00:10, 42.08it/s] 30%|██▉       | 181/611 [00:04<00:10, 42.72it/s] 30%|███       | 186/611 [00:04<00:09, 43.01it/s] 31%|███▏      | 191/611 [00:04<00:09, 43.29it/s] 32%|███▏      | 196/611 [00:04<00:09, 43.62it/s] 33%|███▎      | 201/611 [00:04<00:09, 44.11it/s] 34%|███▎      | 206/611 [00:04<00:09, 44.45it/s] 35%|███▍      | 211/611 [00:05<00:08, 44.77it/s] 35%|███▌      | 216/611 [00:05<00:08, 44.82it/s] 36%|███▌      | 221/611 [00:05<00:08, 45.01it/s] 37%|███▋      | 226/611 [00:05<00:09, 41.46it/s] 38%|███▊      | 231/611 [00:05<00:08, 42.47it/s] 39%|███▊      | 236/611 [00:05<00:08, 42.98it/s] 39%|███▉      | 241/611 [00:05<00:08, 43.52it/s] 40%|████      | 246/611 [00:05<00:08, 43.96it/s] 41%|████      | 251/611 [00:06<00:08, 44.34it/s] 42%|████▏     | 256/611 [00:06<00:08, 40.48it/s] 43%|████▎     | 261/611 [00:06<00:08, 41.82it/s] 44%|████▎     | 266/611 [00:06<00:08, 42.71it/s] 44%|████▍     | 271/611 [00:06<00:07, 43.46it/s] 45%|████▌     | 276/611 [00:06<00:07, 43.83it/s] 46%|████▌     | 281/611 [00:06<00:07, 44.10it/s] 47%|████▋     | 286/611 [00:06<00:07, 44.30it/s] 48%|████▊     | 291/611 [00:08<00:29, 10.75it/s] 48%|████▊     | 296/611 [00:08<00:22, 13.97it/s] 49%|████▉     | 301/611 [00:08<00:17, 17.63it/s] 50%|█████     | 306/611 [00:08<00:14, 21.57it/s] 51%|█████     | 311/611 [00:08<00:11, 25.59it/s] 52%|█████▏    | 316/611 [00:08<00:10, 29.43it/s] 53%|█████▎    | 321/611 [00:08<00:08, 32.86it/s] 53%|█████▎    | 326/611 [00:08<00:07, 35.87it/s] 54%|█████▍    | 331/611 [00:09<00:07, 38.00it/s] 55%|█████▍    | 336/611 [00:09<00:06, 39.49it/s] 56%|█████▌    | 341/611 [00:09<00:15, 17.08it/s] 57%|█████▋    | 346/611 [00:09<00:12, 21.02it/s] 57%|█████▋    | 351/611 [00:10<00:10, 25.04it/s] 58%|█████▊    | 356/611 [00:10<00:08, 28.92it/s] 59%|█████▉    | 361/611 [00:10<00:07, 32.44it/s] 60%|█████▉    | 366/611 [00:10<00:06, 35.45it/s] 61%|██████    | 371/611 [00:10<00:06, 37.89it/s] 62%|██████▏   | 376/611 [00:10<00:05, 39.70it/s] 62%|██████▏   | 381/611 [00:10<00:05, 40.74it/s] 63%|██████▎   | 386/611 [00:10<00:05, 41.55it/s] 64%|██████▍   | 391/611 [00:10<00:05, 42.36it/s] 65%|██████▍   | 396/611 [00:11<00:04, 43.19it/s] 66%|██████▌   | 401/611 [00:11<00:04, 43.85it/s] 66%|██████▋   | 406/611 [00:11<00:04, 44.24it/s] 67%|██████▋   | 411/611 [00:11<00:04, 44.52it/s] 68%|██████▊   | 416/611 [00:11<00:04, 44.62it/s] 69%|██████▉   | 421/611 [00:11<00:04, 44.52it/s] 70%|██████▉   | 426/611 [00:11<00:04, 44.28it/s] 71%|███████   | 431/611 [00:11<00:04, 44.14it/s] 71%|███████▏  | 436/611 [00:11<00:03, 44.21it/s] 72%|███████▏  | 441/611 [00:12<00:03, 44.51it/s] 73%|███████▎  | 446/611 [00:12<00:03, 44.72it/s] 74%|███████▍  | 451/611 [00:12<00:06, 23.79it/s] 75%|███████▍  | 456/611 [00:12<00:05, 27.72it/s] 75%|███████▌  | 461/611 [00:12<00:04, 31.34it/s] 76%|███████▋  | 466/611 [00:12<00:04, 34.45it/s] 77%|███████▋  | 471/611 [00:13<00:03, 37.09it/s] 78%|███████▊  | 476/611 [00:13<00:03, 39.21it/s] 79%|███████▊  | 481/611 [00:13<00:03, 40.79it/s] 80%|███████▉  | 486/611 [00:13<00:02, 41.93it/s] 80%|████████  | 491/611 [00:13<00:02, 42.41it/s] 81%|████████  | 496/611 [00:13<00:02, 42.81it/s] 82%|████████▏ | 501/611 [00:13<00:02, 43.38it/s] 83%|████████▎ | 506/611 [00:13<00:02, 43.83it/s] 84%|████████▎ | 511/611 [00:13<00:02, 44.08it/s] 84%|████████▍ | 516/611 [00:14<00:02, 44.43it/s] 85%|████████▌ | 521/611 [00:14<00:02, 44.61it/s] 86%|████████▌ | 526/611 [00:14<00:01, 44.68it/s] 87%|████████▋ | 531/611 [00:14<00:01, 44.60it/s] 88%|████████▊ | 536/611 [00:14<00:01, 44.30it/s] 89%|████████▊ | 541/611 [00:14<00:01, 44.20it/s] 89%|████████▉ | 546/611 [00:14<00:01, 44.43it/s] 90%|█████████ | 551/611 [00:14<00:01, 44.52it/s] 91%|█████████ | 556/611 [00:14<00:01, 44.71it/s] 92%|█████████▏| 561/611 [00:15<00:01, 44.81it/s] 93%|█████████▎| 566/611 [00:15<00:01, 44.87it/s] 93%|█████████▎| 571/611 [00:15<00:00, 42.45it/s] 94%|█████████▍| 576/611 [00:15<00:00, 42.99it/s] 95%|█████████▌| 581/611 [00:15<00:00, 43.39it/s] 96%|█████████▌| 586/611 [00:15<00:00, 43.57it/s] 97%|█████████▋| 591/611 [00:15<00:00, 43.80it/s] 98%|█████████▊| 596/611 [00:15<00:00, 44.16it/s] 98%|█████████▊| 601/611 [00:15<00:00, 44.48it/s] 99%|█████████▉| 606/611 [00:16<00:00, 44.66it/s]100%|██████████| 611/611 [00:16<00:00, 44.40it/s]100%|██████████| 611/611 [00:16<00:00, 37.73it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:04:08,529 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   eval_loss               =     0.9688
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   eval_runtime            = 0:00:16.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   eval_samples_per_second =    301.066
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   eval_steps_per_second   =      37.68
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:08,529 >>   perplexity              =     2.6349
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:33,320 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:33,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:33,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:33,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:33,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:04:34,201 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:04:34,202 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:04:34,795 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:04:35,938 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:04:35,938 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:04:40,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:04:40,770 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:04:41,359 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:04:41,642 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:04:41,642 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.70it/s]Extractor Predicting: 21it [00:13,  1.71it/s]Extractor Predicting: 22it [00:13,  1.71it/s]Extractor Predicting: 23it [00:14,  1.71it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:15,  1.71it/s]Extractor Predicting: 26it [00:16,  1.68it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.70it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:23,  1.69it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.50it/s]Extractor Predicting: 46it [00:28,  1.55it/s]Extractor Predicting: 47it [00:28,  1.58it/s]Extractor Predicting: 48it [00:29,  1.61it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:31,  1.39it/s]Extractor Predicting: 51it [00:31,  1.46it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.47it/s]Extractor Predicting: 56it [00:34,  1.48it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:36,  1.58it/s]Extractor Predicting: 60it [00:37,  1.36it/s]Extractor Predicting: 61it [00:38,  1.41it/s]Extractor Predicting: 62it [00:39,  1.44it/s]Extractor Predicting: 63it [00:39,  1.49it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:41,  1.49it/s]Extractor Predicting: 67it [00:42,  1.52it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:46,  1.56it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:48,  1.50it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.58it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:51,  1.60it/s]Extractor Predicting: 82it [00:52,  1.37it/s]Extractor Predicting: 83it [00:52,  1.44it/s]Extractor Predicting: 84it [00:53,  1.48it/s]Extractor Predicting: 85it [00:54,  1.50it/s]Extractor Predicting: 86it [00:54,  1.51it/s]Extractor Predicting: 87it [00:55,  1.49it/s]Extractor Predicting: 88it [00:56,  1.48it/s]Extractor Predicting: 89it [00:56,  1.50it/s]Extractor Predicting: 90it [00:57,  1.50it/s]Extractor Predicting: 91it [00:58,  1.51it/s]Extractor Predicting: 92it [00:58,  1.49it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:01,  1.61it/s]Extractor Predicting: 97it [01:01,  1.55it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.51it/s]Extractor Predicting: 100it [01:03,  1.55it/s]Extractor Predicting: 101it [01:04,  1.49it/s]Extractor Predicting: 102it [01:05,  1.40it/s]Extractor Predicting: 103it [01:06,  1.45it/s]Extractor Predicting: 104it [01:06,  1.49it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.57it/s]Extractor Predicting: 108it [01:09,  1.61it/s]Extractor Predicting: 109it [01:09,  1.61it/s]Extractor Predicting: 110it [01:10,  1.62it/s]Extractor Predicting: 111it [01:10,  1.65it/s]Extractor Predicting: 112it [01:11,  1.59it/s]Extractor Predicting: 113it [01:12,  1.56it/s]Extractor Predicting: 114it [01:12,  1.56it/s]Extractor Predicting: 115it [01:13,  1.59it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:14,  1.49it/s]Extractor Predicting: 118it [01:15,  1.51it/s]Extractor Predicting: 119it [01:16,  1.51it/s]Extractor Predicting: 120it [01:16,  1.49it/s]Extractor Predicting: 121it [01:17,  1.51it/s]Extractor Predicting: 122it [01:18,  1.39it/s]Extractor Predicting: 123it [01:18,  1.45it/s]Extractor Predicting: 124it [01:19,  1.47it/s]Extractor Predicting: 125it [01:20,  1.51it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.53it/s]Extractor Predicting: 128it [01:22,  1.55it/s]Extractor Predicting: 129it [01:22,  1.53it/s]Extractor Predicting: 130it [01:23,  1.50it/s]Extractor Predicting: 131it [01:24,  1.52it/s]Extractor Predicting: 132it [01:24,  1.57it/s]Extractor Predicting: 133it [01:25,  1.55it/s]Extractor Predicting: 134it [01:26,  1.45it/s]Extractor Predicting: 135it [01:26,  1.47it/s]Extractor Predicting: 136it [01:27,  1.50it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:28,  1.52it/s]Extractor Predicting: 139it [01:29,  1.29it/s]Extractor Predicting: 140it [01:30,  1.35it/s]Extractor Predicting: 141it [01:31,  1.41it/s]Extractor Predicting: 142it [01:31,  1.46it/s]Extractor Predicting: 143it [01:32,  1.47it/s]Extractor Predicting: 144it [01:33,  1.42it/s]Extractor Predicting: 145it [01:33,  1.50it/s]Extractor Predicting: 146it [01:34,  1.51it/s]Extractor Predicting: 147it [01:35,  1.50it/s]Extractor Predicting: 148it [01:35,  1.53it/s]Extractor Predicting: 149it [01:36,  1.25it/s]Extractor Predicting: 150it [01:37,  1.31it/s]Extractor Predicting: 151it [01:38,  1.37it/s]Extractor Predicting: 152it [01:38,  1.41it/s]Extractor Predicting: 153it [01:39,  1.41it/s]Extractor Predicting: 154it [01:40,  1.45it/s]Extractor Predicting: 155it [01:40,  1.48it/s]Extractor Predicting: 156it [01:41,  1.45it/s]Extractor Predicting: 157it [01:42,  1.42it/s]Extractor Predicting: 158it [01:43,  1.16it/s]Extractor Predicting: 159it [01:44,  1.25it/s]Extractor Predicting: 160it [01:44,  1.33it/s]Extractor Predicting: 161it [01:45,  1.39it/s]Extractor Predicting: 162it [01:46,  1.34it/s]Extractor Predicting: 163it [01:47,  1.34it/s]Extractor Predicting: 164it [01:47,  1.42it/s]Extractor Predicting: 165it [01:48,  1.45it/s]Extractor Predicting: 166it [01:48,  1.50it/s]Extractor Predicting: 167it [01:49,  1.51it/s]Extractor Predicting: 168it [01:50,  1.50it/s]Extractor Predicting: 169it [01:50,  1.52it/s]Extractor Predicting: 170it [01:51,  1.53it/s]Extractor Predicting: 171it [01:52,  1.56it/s]Extractor Predicting: 172it [01:52,  1.59it/s]Extractor Predicting: 173it [01:53,  1.46it/s]Extractor Predicting: 174it [01:54,  1.50it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.53it/s]Extractor Predicting: 177it [01:56,  1.51it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.50it/s]Extractor Predicting: 180it [01:58,  1.52it/s]Extractor Predicting: 181it [01:58,  1.53it/s]Extractor Predicting: 182it [01:59,  1.53it/s]Extractor Predicting: 183it [02:00,  1.47it/s]Extractor Predicting: 184it [02:00,  1.50it/s]Extractor Predicting: 185it [02:01,  1.61it/s]Extractor Predicting: 185it [02:01,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:16,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:16,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:16,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:16,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:16,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:07:17,404 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:07:17,734 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:18,600 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:19,657 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:19,657 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:22,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:23,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:23,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:23,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:23,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:07:23,940 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:07:23,941 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:24,586 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:24,762 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:24,762 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3157894736842105,
  "recall": 0.16837361736993037,
  "score": 0.2196392785571142,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:05,  1.48it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:15,  1.43it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.58it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:20,  1.68it/s]Extractor Predicting: 32it [00:20,  1.70it/s]Extractor Predicting: 33it [00:21,  1.70it/s]Extractor Predicting: 34it [00:21,  1.71it/s]Extractor Predicting: 35it [00:22,  1.68it/s]Extractor Predicting: 36it [00:23,  1.67it/s]Extractor Predicting: 37it [00:23,  1.70it/s]Extractor Predicting: 38it [00:24,  1.70it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.65it/s]Extractor Predicting: 44it [00:27,  1.68it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.67it/s]Extractor Predicting: 47it [00:29,  1.70it/s]Extractor Predicting: 48it [00:30,  1.71it/s]Extractor Predicting: 49it [00:30,  1.71it/s]Extractor Predicting: 50it [00:31,  1.70it/s]Extractor Predicting: 51it [00:32,  1.45it/s]Extractor Predicting: 52it [00:32,  1.52it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:36,  1.71it/s]Extractor Predicting: 59it [00:37,  1.68it/s]Extractor Predicting: 60it [00:37,  1.67it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:40,  1.51it/s]Extractor Predicting: 65it [00:41,  1.51it/s]Extractor Predicting: 66it [00:41,  1.50it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:44,  1.21it/s]Extractor Predicting: 70it [00:44,  1.30it/s]Extractor Predicting: 71it [00:45,  1.38it/s]Extractor Predicting: 72it [00:46,  1.44it/s]Extractor Predicting: 73it [00:46,  1.50it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:49,  1.63it/s]Extractor Predicting: 78it [00:50,  1.47it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.64it/s]Extractor Predicting: 81it [00:51,  1.62it/s]Extractor Predicting: 82it [00:52,  1.66it/s]Extractor Predicting: 83it [00:53,  1.42it/s]Extractor Predicting: 84it [00:53,  1.47it/s]Extractor Predicting: 85it [00:54,  1.48it/s]Extractor Predicting: 86it [00:55,  1.50it/s]Extractor Predicting: 87it [00:55,  1.52it/s]Extractor Predicting: 88it [00:56,  1.49it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:57,  1.54it/s]Extractor Predicting: 91it [00:58,  1.55it/s]Extractor Predicting: 92it [00:59,  1.56it/s]Extractor Predicting: 93it [01:00,  1.29it/s]Extractor Predicting: 94it [01:00,  1.38it/s]Extractor Predicting: 95it [01:01,  1.43it/s]Extractor Predicting: 96it [01:02,  1.47it/s]Extractor Predicting: 97it [01:02,  1.52it/s]Extractor Predicting: 98it [01:03,  1.51it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:04,  1.54it/s]Extractor Predicting: 101it [01:05,  1.57it/s]Extractor Predicting: 102it [01:05,  1.59it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:07,  1.59it/s]Extractor Predicting: 105it [01:07,  1.61it/s]Extractor Predicting: 106it [01:08,  1.64it/s]Extractor Predicting: 107it [01:08,  1.62it/s]Extractor Predicting: 108it [01:09,  1.65it/s]Extractor Predicting: 109it [01:10,  1.63it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.56it/s]Extractor Predicting: 112it [01:12,  1.56it/s]Extractor Predicting: 113it [01:12,  1.57it/s]Extractor Predicting: 114it [01:13,  1.57it/s]Extractor Predicting: 115it [01:14,  1.41it/s]Extractor Predicting: 116it [01:14,  1.47it/s]Extractor Predicting: 117it [01:15,  1.54it/s]Extractor Predicting: 118it [01:16,  1.54it/s]Extractor Predicting: 119it [01:16,  1.55it/s]Extractor Predicting: 120it [01:17,  1.53it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:18,  1.60it/s]Extractor Predicting: 123it [01:19,  1.59it/s]Extractor Predicting: 124it [01:20,  1.40it/s]Extractor Predicting: 125it [01:21,  1.24it/s]Extractor Predicting: 126it [01:21,  1.33it/s]Extractor Predicting: 127it [01:22,  1.42it/s]Extractor Predicting: 128it [01:23,  1.43it/s]Extractor Predicting: 129it [01:23,  1.49it/s]Extractor Predicting: 130it [01:24,  1.46it/s]Extractor Predicting: 131it [01:25,  1.48it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.53it/s]Extractor Predicting: 134it [01:26,  1.54it/s]Extractor Predicting: 135it [01:27,  1.50it/s]Extractor Predicting: 136it [01:28,  1.56it/s]Extractor Predicting: 137it [01:28,  1.53it/s]Extractor Predicting: 138it [01:29,  1.55it/s]Extractor Predicting: 139it [01:30,  1.59it/s]Extractor Predicting: 140it [01:31,  1.33it/s]Extractor Predicting: 141it [01:31,  1.39it/s]Extractor Predicting: 142it [01:32,  1.47it/s]Extractor Predicting: 143it [01:33,  1.43it/s]Extractor Predicting: 144it [01:33,  1.49it/s]Extractor Predicting: 145it [01:34,  1.50it/s]Extractor Predicting: 146it [01:35,  1.55it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.60it/s]Extractor Predicting: 149it [01:36,  1.59it/s]Extractor Predicting: 150it [01:37,  1.63it/s]Extractor Predicting: 151it [01:38,  1.65it/s]Extractor Predicting: 152it [01:38,  1.62it/s]Extractor Predicting: 153it [01:39,  1.64it/s]Extractor Predicting: 154it [01:39,  1.60it/s]Extractor Predicting: 155it [01:40,  1.60it/s]Extractor Predicting: 156it [01:41,  1.65it/s]Extractor Predicting: 157it [01:41,  1.58it/s]Extractor Predicting: 158it [01:42,  1.58it/s]Extractor Predicting: 159it [01:43,  1.60it/s]Extractor Predicting: 160it [01:43,  1.61it/s]Extractor Predicting: 161it [01:44,  1.65it/s]Extractor Predicting: 162it [01:45,  1.45it/s]Extractor Predicting: 163it [01:45,  1.50it/s]Extractor Predicting: 164it [01:46,  1.52it/s]Extractor Predicting: 165it [01:47,  1.50it/s]Extractor Predicting: 166it [01:47,  1.50it/s]Extractor Predicting: 167it [01:48,  1.29it/s]Extractor Predicting: 168it [01:49,  1.38it/s]Extractor Predicting: 169it [01:49,  1.46it/s]Extractor Predicting: 170it [01:50,  1.51it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:51,  1.51it/s]Extractor Predicting: 173it [01:52,  1.54it/s]Extractor Predicting: 174it [01:53,  1.56it/s]Extractor Predicting: 175it [01:53,  1.58it/s]Extractor Predicting: 176it [01:54,  1.60it/s]Extractor Predicting: 177it [01:55,  1.44it/s]Extractor Predicting: 178it [01:55,  1.49it/s]Extractor Predicting: 179it [01:56,  1.49it/s]Extractor Predicting: 180it [01:57,  1.55it/s]Extractor Predicting: 181it [01:57,  1.57it/s]Extractor Predicting: 182it [01:58,  1.50it/s]Extractor Predicting: 183it [01:58,  1.56it/s]Extractor Predicting: 184it [01:59,  1.58it/s]Extractor Predicting: 185it [02:00,  1.60it/s]Extractor Predicting: 186it [02:00,  1.57it/s]Extractor Predicting: 187it [02:02,  1.20it/s]Extractor Predicting: 188it [02:02,  1.30it/s]Extractor Predicting: 189it [02:03,  1.39it/s]Extractor Predicting: 190it [02:03,  1.47it/s]Extractor Predicting: 191it [02:04,  1.40it/s]Extractor Predicting: 192it [02:05,  1.50it/s]Extractor Predicting: 193it [02:05,  1.53it/s]Extractor Predicting: 194it [02:06,  1.57it/s]Extractor Predicting: 195it [02:07,  1.58it/s]Extractor Predicting: 196it [02:08,  1.30it/s]Extractor Predicting: 197it [02:08,  1.38it/s]Extractor Predicting: 198it [02:09,  1.43it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:10,  1.52it/s]Extractor Predicting: 201it [02:11,  1.56it/s]Extractor Predicting: 202it [02:12,  1.45it/s]Extractor Predicting: 203it [02:12,  1.52it/s]Extractor Predicting: 204it [02:13,  1.55it/s]Extractor Predicting: 205it [02:13,  1.59it/s]Extractor Predicting: 206it [02:14,  1.59it/s]Extractor Predicting: 207it [02:15,  1.60it/s]Extractor Predicting: 208it [02:15,  1.59it/s]Extractor Predicting: 209it [02:16,  1.58it/s]Extractor Predicting: 210it [02:16,  1.65it/s]Extractor Predicting: 211it [02:17,  1.64it/s]Extractor Predicting: 212it [02:18,  1.56it/s]Extractor Predicting: 213it [02:18,  1.59it/s]Extractor Predicting: 214it [02:19,  1.64it/s]Extractor Predicting: 215it [02:20,  1.64it/s]Extractor Predicting: 216it [02:20,  1.61it/s]Extractor Predicting: 217it [02:21,  1.57it/s]Extractor Predicting: 218it [02:22,  1.57it/s]Extractor Predicting: 219it [02:22,  1.59it/s]Extractor Predicting: 220it [02:23,  1.61it/s]Extractor Predicting: 221it [02:23,  1.63it/s]Extractor Predicting: 222it [02:24,  1.54it/s]Extractor Predicting: 223it [02:25,  1.51it/s]Extractor Predicting: 224it [02:25,  1.54it/s]Extractor Predicting: 225it [02:26,  1.57it/s]Extractor Predicting: 226it [02:27,  1.62it/s]Extractor Predicting: 227it [02:27,  1.60it/s]Extractor Predicting: 228it [02:28,  1.64it/s]Extractor Predicting: 229it [02:28,  1.66it/s]Extractor Predicting: 230it [02:29,  1.68it/s]Extractor Predicting: 231it [02:30,  1.68it/s]Extractor Predicting: 232it [02:30,  1.67it/s]Extractor Predicting: 233it [02:31,  1.57it/s]Extractor Predicting: 234it [02:32,  1.60it/s]Extractor Predicting: 235it [02:32,  1.59it/s]Extractor Predicting: 236it [02:33,  1.61it/s]Extractor Predicting: 237it [02:33,  1.63it/s]Extractor Predicting: 238it [02:34,  1.41it/s]Extractor Predicting: 239it [02:35,  1.48it/s]Extractor Predicting: 240it [02:35,  1.52it/s]Extractor Predicting: 241it [02:36,  1.56it/s]Extractor Predicting: 242it [02:37,  1.56it/s]Extractor Predicting: 243it [02:37,  1.50it/s]Extractor Predicting: 244it [02:38,  1.53it/s]Extractor Predicting: 245it [02:39,  1.58it/s]Extractor Predicting: 246it [02:39,  1.58it/s]Extractor Predicting: 247it [02:40,  1.62it/s]Extractor Predicting: 248it [02:41,  1.57it/s]Extractor Predicting: 249it [02:41,  1.57it/s]Extractor Predicting: 250it [02:42,  1.59it/s]Extractor Predicting: 251it [02:42,  1.58it/s]Extractor Predicting: 252it [02:43,  1.58it/s]Extractor Predicting: 253it [02:44,  1.55it/s]Extractor Predicting: 254it [02:44,  1.52it/s]Extractor Predicting: 255it [02:45,  1.55it/s]Extractor Predicting: 256it [02:46,  1.46it/s]Extractor Predicting: 257it [02:46,  1.51it/s]Extractor Predicting: 258it [02:47,  1.53it/s]Extractor Predicting: 259it [02:48,  1.55it/s]Extractor Predicting: 260it [02:48,  1.53it/s]Extractor Predicting: 261it [02:49,  1.35it/s]Extractor Predicting: 262it [02:50,  1.41it/s]Extractor Predicting: 263it [02:51,  1.45it/s]Extractor Predicting: 264it [02:51,  1.49it/s]Extractor Predicting: 265it [02:52,  1.54it/s]Extractor Predicting: 266it [02:53,  1.44it/s]Extractor Predicting: 267it [02:53,  1.48it/s]Extractor Predicting: 268it [02:54,  1.49it/s]Extractor Predicting: 269it [02:55,  1.52it/s]Extractor Predicting: 270it [02:55,  1.52it/s]Extractor Predicting: 271it [02:56,  1.21it/s]Extractor Predicting: 272it [02:57,  1.30it/s]Extractor Predicting: 273it [02:58,  1.37it/s]Extractor Predicting: 274it [02:58,  1.39it/s]Extractor Predicting: 275it [02:59,  1.33it/s]Extractor Predicting: 276it [03:00,  1.39it/s]Extractor Predicting: 277it [03:00,  1.44it/s]Extractor Predicting: 278it [03:01,  1.49it/s]Extractor Predicting: 279it [03:02,  1.50it/s]Extractor Predicting: 280it [03:02,  1.46it/s]Extractor Predicting: 281it [03:03,  1.45it/s]Extractor Predicting: 282it [03:04,  1.47it/s]Extractor Predicting: 283it [03:04,  1.50it/s]Extractor Predicting: 284it [03:05,  1.54it/s]Extractor Predicting: 285it [03:06,  1.47it/s]Extractor Predicting: 286it [03:06,  1.52it/s]Extractor Predicting: 287it [03:07,  1.49it/s]Extractor Predicting: 288it [03:08,  1.53it/s]Extractor Predicting: 289it [03:08,  1.56it/s]Extractor Predicting: 290it [03:09,  1.46it/s]Extractor Predicting: 291it [03:10,  1.46it/s]Extractor Predicting: 292it [03:10,  1.51it/s]Extractor Predicting: 293it [03:11,  1.52it/s]Extractor Predicting: 294it [03:12,  1.51it/s]Extractor Predicting: 295it [03:12,  1.52it/s]Extractor Predicting: 296it [03:13,  1.46it/s]Extractor Predicting: 297it [03:14,  1.49it/s]Extractor Predicting: 298it [03:14,  1.50it/s]Extractor Predicting: 299it [03:15,  1.52it/s]Extractor Predicting: 300it [03:16,  1.51it/s]Extractor Predicting: 301it [03:17,  1.45it/s]Extractor Predicting: 302it [03:17,  1.45it/s]Extractor Predicting: 303it [03:18,  1.51it/s]Extractor Predicting: 304it [03:18,  1.54it/s]Extractor Predicting: 305it [03:19,  1.58it/s]Extractor Predicting: 306it [03:20,  1.53it/s]Extractor Predicting: 307it [03:20,  1.54it/s]Extractor Predicting: 308it [03:21,  1.57it/s]Extractor Predicting: 309it [03:22,  1.56it/s]Extractor Predicting: 310it [03:22,  1.57it/s]Extractor Predicting: 311it [03:23,  1.37it/s]Extractor Predicting: 312it [03:24,  1.44it/s]Extractor Predicting: 313it [03:25,  1.42it/s]Extractor Predicting: 314it [03:25,  1.45it/s]Extractor Predicting: 315it [03:26,  1.50it/s]Extractor Predicting: 316it [03:26,  1.50it/s]Extractor Predicting: 317it [03:27,  1.53it/s]Extractor Predicting: 318it [03:28,  1.58it/s]Extractor Predicting: 319it [03:28,  1.56it/s]Extractor Predicting: 320it [03:29,  1.53it/s]Extractor Predicting: 321it [03:30,  1.50it/s]Extractor Predicting: 322it [03:30,  1.53it/s]Extractor Predicting: 323it [03:31,  1.50it/s]Extractor Predicting: 324it [03:32,  1.52it/s]Extractor Predicting: 325it [03:32,  1.56it/s]Extractor Predicting: 326it [03:33,  1.55it/s]Extractor Predicting: 327it [03:34,  1.56it/s]Extractor Predicting: 328it [03:34,  1.54it/s]Extractor Predicting: 329it [03:35,  1.56it/s]Extractor Predicting: 330it [03:36,  1.54it/s]Extractor Predicting: 331it [03:36,  1.52it/s]Extractor Predicting: 332it [03:37,  1.36it/s]Extractor Predicting: 333it [03:38,  1.40it/s]Extractor Predicting: 334it [03:38,  1.46it/s]Extractor Predicting: 335it [03:39,  1.40it/s]Extractor Predicting: 336it [03:40,  1.37it/s]Extractor Predicting: 337it [03:41,  1.41it/s]Extractor Predicting: 338it [03:41,  1.44it/s]Extractor Predicting: 339it [03:42,  1.47it/s]Extractor Predicting: 340it [03:43,  1.39it/s]Extractor Predicting: 341it [03:43,  1.42it/s]Extractor Predicting: 342it [03:44,  1.44it/s]Extractor Predicting: 343it [03:45,  1.49it/s]Extractor Predicting: 344it [03:45,  1.51it/s]Extractor Predicting: 345it [03:46,  1.26it/s]Extractor Predicting: 346it [03:47,  1.34it/s]Extractor Predicting: 347it [03:48,  1.40it/s]Extractor Predicting: 348it [03:48,  1.45it/s]Extractor Predicting: 349it [03:49,  1.45it/s]Extractor Predicting: 350it [03:50,  1.47it/s]Extractor Predicting: 351it [03:50,  1.52it/s]Extractor Predicting: 352it [03:51,  1.49it/s]Extractor Predicting: 353it [03:52,  1.48it/s]Extractor Predicting: 354it [03:52,  1.48it/s]Extractor Predicting: 355it [03:53,  1.52it/s]Extractor Predicting: 356it [03:54,  1.53it/s]Extractor Predicting: 357it [03:54,  1.54it/s]Extractor Predicting: 358it [03:55,  1.56it/s]Extractor Predicting: 359it [03:56,  1.31it/s]Extractor Predicting: 360it [03:57,  1.35it/s]Extractor Predicting: 361it [03:57,  1.40it/s]Extractor Predicting: 362it [03:58,  1.45it/s]Extractor Predicting: 363it [03:59,  1.40it/s]Extractor Predicting: 364it [03:59,  1.46it/s]Extractor Predicting: 365it [04:00,  1.47it/s]Extractor Predicting: 366it [04:01,  1.45it/s]Extractor Predicting: 367it [04:01,  1.47it/s]Extractor Predicting: 368it [04:03,  1.14it/s]Extractor Predicting: 369it [04:03,  1.20it/s]Extractor Predicting: 370it [04:04,  1.27it/s]Extractor Predicting: 371it [04:05,  1.34it/s]Extractor Predicting: 372it [04:05,  1.56it/s]Extractor Predicting: 372it [04:05,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:58,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:58,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:58,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:58,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:58,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:11:58,891 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:11:58,892 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:59,189 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:12:00,275 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:12:00,275 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:02,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:02,483 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:02,483 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:02,483 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:02,483 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:12:02,958 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:12:02,959 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:12:03,368 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:12:03,534 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:12:03,535 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2765802505097582,
  "recall": 0.2130834829443447,
  "score": 0.24071491950817595,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:08,  1.66it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.58it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:25,  1.46it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:26,  1.45it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:29,  1.18it/s]Extractor Predicting: 46it [00:30,  1.28it/s]Extractor Predicting: 47it [00:31,  1.33it/s]Extractor Predicting: 48it [00:31,  1.37it/s]Extractor Predicting: 49it [00:32,  1.27it/s]Extractor Predicting: 50it [00:33,  1.32it/s]Extractor Predicting: 51it [00:34,  1.35it/s]Extractor Predicting: 52it [00:34,  1.38it/s]Extractor Predicting: 53it [00:35,  1.35it/s]Extractor Predicting: 54it [00:36,  1.38it/s]Extractor Predicting: 55it [00:37,  1.44it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:38,  1.47it/s]Extractor Predicting: 58it [00:39,  1.40it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:40,  1.44it/s]Extractor Predicting: 61it [00:41,  1.41it/s]Extractor Predicting: 62it [00:41,  1.42it/s]Extractor Predicting: 63it [00:42,  1.56it/s]Extractor Predicting: 63it [00:42,  1.48it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:12:53,166 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:53,200 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:12:53,255 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:53,256 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:12:53,289 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:13:23,726 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:13:23,875 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:13:24,247 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:13:24,248 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:13:24,406 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,681 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,682 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,682 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,682 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,682 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:24,682 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45859305431878894,
  "recall": 0.15433023673958646,
  "score": 0.2309417040358744,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:13:25,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:26,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:27,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:27,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:28,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:29,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:29,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:30,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:31,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:31,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:32,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:32,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:33,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:34,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:35,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:35,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:37,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:38,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:39,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:39,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:20, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:40,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:42,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:42,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:43,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:44,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:44,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:45,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:45,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:46,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:46,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:47,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:47,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:48,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:49,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:49,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:50,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:51,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:51,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:52,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:52,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:53,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<03:01, 13.93s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:53,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:55,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:55,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:58,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:58,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:59,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:59,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:00,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:00,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:01,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:01,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:02,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:02,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:03,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:38<02:26, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:03,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:04,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:05,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:05,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:06,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:08,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:08,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:09,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:10,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:10,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:11,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:13,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:13,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:14,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:15,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:15,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:15, 12.31s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:16,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:17,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:17,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:19,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:19,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:20,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:22,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:22,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:23,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:25,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:26,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:26,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:27,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:29,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:29,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:30,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:31,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:05<02:13, 13.39s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:31,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:32,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:32,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:33,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:34,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:34,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:35,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:35,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:36,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:37,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:37,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:38,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:39,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:39,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:40,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:43,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:44,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:18<01:59, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:44,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:45,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:46,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:47,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:51,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:51,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:52,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:52,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:55,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:56,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:56,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:57,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:31<01:45, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:57,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:58,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:59,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:59,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:00,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:00,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:01,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:03,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:03,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:04,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:04,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:05,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:06,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:06,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:08,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:09,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:10,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:45<01:32, 13.17s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:11,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:11,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:13,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:14,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:14,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:15,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:15,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:16,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:16,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:17,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:19,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:19,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:20,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:22,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:22,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:58<01:19, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:24,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:25,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:25,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:27,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:28,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:29,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:30,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:30,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:32,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:34,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:34,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:35,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:36,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:36,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:37,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:11<01:06, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:37,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:38,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:38,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:40,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:40,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:41,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:42,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:42,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:42,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:43,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:43,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:44,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:45,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:45,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:47,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:22<00:49, 12.40s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:48,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:48,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:50,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:51,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:51,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:52,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:53,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:53,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:55,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:55,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:56,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:57,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:57,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:59,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:35<00:37, 12.61s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:01,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:01,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:03,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:06,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:07,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:08,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:08,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:09,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:09,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:10,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:11,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:11,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:12,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:48<00:25, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:14,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:14,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:15,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:15,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:16,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:19,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:19,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:20,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:20,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:22,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:22,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:23,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:23,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:58<00:11, 11.94s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:24,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:25,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:25,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:27,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:28,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:28,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:29,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:30,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:30,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:31,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:33,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:34,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:35,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:35,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:36,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:37,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:38,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:13<00:00, 12.73s/it]Generating: 100%|██████████| 15/15 [03:13<00:00, 12.87s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:51,268 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:51,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:51,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:51,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:51,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:16:52,248 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:16:52,249 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:16:53,123 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:16:54,235 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:16:54,235 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:59,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:00,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:00,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:00,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:00,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:17:00,967 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:17:00,968 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:17:01,543 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:17:01,977 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:17:01,978 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 502, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.978125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 606, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7575, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 631, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8963068181818182, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8220108695652174, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.978125, 'errors': {''}}
['Relation : occupation . Context : John C. Scott ( born March 19 , 1975 ) is an American former professional wrestler who competed in the World Wrestling Federation ( WWF ) . Head Entity : John C. Scott , Tail Entity : American wrestler .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 539, 'raw': 544}
{'target': 600, 'success': 571, 'raw': 576}
{'target': 600, 'success': 603, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9917763157894737, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 8415
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8515, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.72it/s]Extractor Estimating: 5it [00:02,  1.73it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:04,  1.62it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:09,  1.55it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:11,  1.68it/s]Extractor Estimating: 19it [00:11,  1.69it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.58it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:14,  1.55it/s]Extractor Estimating: 25it [00:15,  1.59it/s]Extractor Estimating: 26it [00:16,  1.59it/s]Extractor Estimating: 27it [00:16,  1.69it/s]Extractor Estimating: 28it [00:17,  1.67it/s]Extractor Estimating: 29it [00:17,  1.55it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.61it/s]Extractor Estimating: 32it [00:19,  1.65it/s]Extractor Estimating: 33it [00:20,  1.70it/s]Extractor Estimating: 34it [00:20,  1.63it/s]Extractor Estimating: 35it [00:21,  1.71it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:22,  1.71it/s]Extractor Estimating: 38it [00:23,  1.68it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:24,  1.68it/s]Extractor Estimating: 41it [00:24,  1.76it/s]Extractor Estimating: 42it [00:25,  1.81it/s]Extractor Estimating: 43it [00:26,  1.78it/s]Extractor Estimating: 44it [00:26,  1.81it/s]Extractor Estimating: 45it [00:27,  1.78it/s]Extractor Estimating: 46it [00:27,  1.76it/s]Extractor Estimating: 47it [00:28,  1.77it/s]Extractor Estimating: 48it [00:28,  1.76it/s]Extractor Estimating: 49it [00:29,  1.76it/s]Extractor Estimating: 50it [00:30,  1.76it/s]Extractor Estimating: 51it [00:30,  1.85it/s]Extractor Estimating: 52it [00:31,  1.55it/s]Extractor Estimating: 53it [00:31,  1.69it/s]Extractor Estimating: 54it [00:32,  1.82it/s]Extractor Estimating: 55it [00:32,  1.93it/s]Extractor Estimating: 56it [00:33,  1.95it/s]Extractor Estimating: 57it [00:33,  2.07it/s]Extractor Estimating: 58it [00:34,  1.98it/s]Extractor Estimating: 59it [00:34,  2.04it/s]Extractor Estimating: 60it [00:35,  2.02it/s]Extractor Estimating: 61it [00:35,  2.10it/s]Extractor Estimating: 62it [00:36,  2.14it/s]Extractor Estimating: 63it [00:36,  2.06it/s]Extractor Estimating: 64it [00:37,  2.08it/s]Extractor Estimating: 65it [00:37,  1.74it/s]Extractor Estimating: 66it [00:38,  1.89it/s]Extractor Estimating: 67it [00:38,  1.95it/s]Extractor Estimating: 68it [00:39,  2.01it/s]Extractor Estimating: 69it [00:39,  2.02it/s]Extractor Estimating: 70it [00:40,  2.05it/s]Extractor Estimating: 71it [00:40,  1.79it/s]Extractor Estimating: 72it [00:41,  1.87it/s]Extractor Estimating: 73it [00:41,  1.94it/s]Extractor Estimating: 74it [00:42,  1.98it/s]Extractor Estimating: 75it [00:42,  2.03it/s]Extractor Estimating: 76it [00:43,  1.98it/s]Extractor Estimating: 77it [00:44,  1.80it/s]Extractor Estimating: 78it [00:44,  1.79it/s]Extractor Estimating: 79it [00:45,  1.46it/s]Extractor Estimating: 80it [00:46,  1.48it/s]Extractor Estimating: 81it [00:46,  1.59it/s]Extractor Estimating: 82it [00:47,  1.64it/s]Extractor Estimating: 83it [00:47,  1.70it/s]Extractor Estimating: 84it [00:48,  1.49it/s]Extractor Estimating: 85it [00:49,  1.58it/s]Extractor Estimating: 86it [00:49,  1.61it/s]Extractor Estimating: 87it [00:50,  1.69it/s]Extractor Estimating: 88it [00:51,  1.63it/s]Extractor Estimating: 89it [00:51,  1.45it/s]Extractor Estimating: 90it [00:52,  1.49it/s]Extractor Estimating: 91it [00:53,  1.55it/s]Extractor Estimating: 92it [00:53,  1.65it/s]Extractor Estimating: 93it [00:54,  1.71it/s]Extractor Estimating: 94it [00:54,  1.61it/s]Extractor Estimating: 95it [00:55,  1.62it/s]Extractor Estimating: 96it [00:56,  1.62it/s]Extractor Estimating: 97it [00:56,  1.67it/s]Extractor Estimating: 98it [00:57,  1.74it/s]Extractor Estimating: 99it [00:58,  1.52it/s]Extractor Estimating: 100it [00:58,  1.57it/s]Extractor Estimating: 101it [00:59,  1.61it/s]Extractor Estimating: 102it [00:59,  1.63it/s]Extractor Estimating: 103it [01:00,  1.61it/s]Extractor Estimating: 104it [01:01,  1.32it/s]Extractor Estimating: 105it [01:02,  1.43it/s]Extractor Estimating: 106it [01:02,  1.49it/s]Extractor Estimating: 107it [01:03,  1.56it/s]Extractor Estimating: 108it [01:03,  1.61it/s]Extractor Estimating: 109it [01:04,  1.62it/s]Extractor Estimating: 110it [01:05,  1.61it/s]Extractor Estimating: 111it [01:05,  1.68it/s]Extractor Estimating: 112it [01:06,  1.66it/s]Extractor Estimating: 113it [01:06,  1.67it/s]Extractor Estimating: 114it [01:07,  1.77it/s]Extractor Estimating: 115it [01:08,  1.35it/s]Extractor Estimating: 116it [01:09,  1.43it/s]Extractor Estimating: 117it [01:09,  1.50it/s]Extractor Estimating: 118it [01:10,  1.45it/s]Extractor Estimating: 119it [01:11,  1.37it/s]Extractor Estimating: 120it [01:11,  1.48it/s]Extractor Estimating: 121it [01:12,  1.53it/s]Extractor Estimating: 122it [01:12,  1.58it/s]Extractor Estimating: 123it [01:13,  1.60it/s]Extractor Estimating: 124it [01:14,  1.36it/s]Extractor Estimating: 125it [01:15,  1.45it/s]Extractor Estimating: 126it [01:15,  1.51it/s]Extractor Estimating: 127it [01:16,  1.54it/s]Extractor Estimating: 128it [01:16,  1.55it/s]Extractor Estimating: 129it [01:17,  1.61it/s]Extractor Estimating: 130it [01:18,  1.60it/s]Extractor Estimating: 131it [01:18,  1.61it/s]Extractor Estimating: 132it [01:19,  1.57it/s]Extractor Estimating: 133it [01:20,  1.60it/s]Extractor Estimating: 134it [01:20,  1.64it/s]Extractor Estimating: 135it [01:21,  1.65it/s]Extractor Estimating: 136it [01:21,  1.63it/s]Extractor Estimating: 137it [01:22,  1.56it/s]Extractor Estimating: 138it [01:23,  1.58it/s]Extractor Estimating: 139it [01:23,  1.59it/s]Extractor Estimating: 140it [01:24,  1.61it/s]Extractor Estimating: 141it [01:24,  1.63it/s]Extractor Estimating: 142it [01:25,  1.51it/s]Extractor Estimating: 143it [01:26,  1.55it/s]Extractor Estimating: 144it [01:26,  1.59it/s]Extractor Estimating: 145it [01:27,  1.63it/s]Extractor Estimating: 146it [01:28,  1.65it/s]Extractor Estimating: 147it [01:29,  1.37it/s]Extractor Estimating: 148it [01:29,  1.45it/s]Extractor Estimating: 149it [01:30,  1.53it/s]Extractor Estimating: 150it [01:30,  1.57it/s]Extractor Estimating: 151it [01:31,  1.65it/s]Extractor Estimating: 152it [01:32,  1.46it/s]Extractor Estimating: 153it [01:32,  1.52it/s]Extractor Estimating: 154it [01:33,  1.57it/s]Extractor Estimating: 155it [01:34,  1.63it/s]Extractor Estimating: 156it [01:34,  1.64it/s]Extractor Estimating: 157it [01:35,  1.55it/s]Extractor Estimating: 158it [01:35,  1.59it/s]Extractor Estimating: 159it [01:36,  1.63it/s]Extractor Estimating: 160it [01:37,  1.65it/s]Extractor Estimating: 161it [01:37,  1.65it/s]Extractor Estimating: 162it [01:38,  1.50it/s]Extractor Estimating: 163it [01:39,  1.55it/s]Extractor Estimating: 164it [01:39,  1.63it/s]Extractor Estimating: 165it [01:40,  1.69it/s]Extractor Estimating: 166it [01:40,  1.66it/s]Extractor Estimating: 167it [01:41,  1.57it/s]Extractor Estimating: 168it [01:42,  1.66it/s]Extractor Estimating: 169it [01:42,  1.63it/s]Extractor Estimating: 170it [01:43,  1.66it/s]Extractor Estimating: 171it [01:43,  1.66it/s]Extractor Estimating: 172it [01:44,  1.48it/s]Extractor Estimating: 173it [01:45,  1.54it/s]Extractor Estimating: 174it [01:45,  1.59it/s]Extractor Estimating: 175it [01:46,  1.62it/s]Extractor Estimating: 176it [01:47,  1.64it/s]Extractor Estimating: 177it [01:47,  1.61it/s]Extractor Estimating: 178it [01:48,  1.60it/s]Extractor Estimating: 179it [01:48,  1.64it/s]Extractor Estimating: 180it [01:49,  1.61it/s]Extractor Estimating: 181it [01:50,  1.60it/s]Extractor Estimating: 182it [01:50,  1.61it/s]Extractor Estimating: 183it [01:51,  1.66it/s]Extractor Estimating: 184it [01:51,  1.71it/s]Extractor Estimating: 185it [01:52,  1.72it/s]Extractor Estimating: 186it [01:53,  1.75it/s]Extractor Estimating: 187it [01:53,  1.70it/s]Extractor Estimating: 188it [01:54,  1.70it/s]Extractor Estimating: 189it [01:54,  1.68it/s]Extractor Estimating: 190it [01:55,  1.68it/s]Extractor Estimating: 191it [01:56,  1.68it/s]Extractor Estimating: 192it [01:56,  1.52it/s]Extractor Estimating: 193it [01:57,  1.59it/s]Extractor Estimating: 194it [01:58,  1.59it/s]Extractor Estimating: 195it [01:58,  1.56it/s]Extractor Estimating: 196it [01:59,  1.62it/s]Extractor Estimating: 197it [02:00,  1.53it/s]Extractor Estimating: 198it [02:00,  1.59it/s]Extractor Estimating: 199it [02:01,  1.58it/s]Extractor Estimating: 200it [02:02,  1.50it/s]Extractor Estimating: 201it [02:02,  1.52it/s]Extractor Estimating: 202it [02:03,  1.51it/s]Extractor Estimating: 203it [02:03,  1.56it/s]Extractor Estimating: 204it [02:04,  1.56it/s]Extractor Estimating: 205it [02:05,  1.60it/s]Extractor Estimating: 206it [02:05,  1.62it/s]Extractor Estimating: 207it [02:06,  1.40it/s]Extractor Estimating: 208it [02:07,  1.49it/s]Extractor Estimating: 209it [02:07,  1.53it/s]Extractor Estimating: 210it [02:08,  1.58it/s]Extractor Estimating: 211it [02:08,  1.65it/s]Extractor Estimating: 212it [02:09,  1.55it/s]Extractor Estimating: 213it [02:10,  1.61it/s]Extractor Estimating: 214it [02:10,  1.64it/s]Extractor Estimating: 215it [02:11,  1.64it/s]Extractor Estimating: 216it [02:12,  1.67it/s]Extractor Estimating: 217it [02:12,  1.59it/s]Extractor Estimating: 218it [02:13,  1.57it/s]Extractor Estimating: 219it [02:14,  1.58it/s]Extractor Estimating: 220it [02:14,  1.64it/s]Extractor Estimating: 221it [02:15,  1.67it/s]Extractor Estimating: 222it [02:16,  1.45it/s]Extractor Estimating: 223it [02:16,  1.52it/s]Extractor Estimating: 224it [02:17,  1.53it/s]Extractor Estimating: 225it [02:17,  1.59it/s]Extractor Estimating: 226it [02:18,  1.60it/s]Extractor Estimating: 227it [02:19,  1.60it/s]Extractor Estimating: 228it [02:19,  1.62it/s]Extractor Estimating: 229it [02:20,  1.48it/s]Extractor Estimating: 230it [02:21,  1.52it/s]Extractor Estimating: 231it [02:21,  1.54it/s]Extractor Estimating: 232it [02:22,  1.56it/s]Extractor Estimating: 233it [02:22,  1.60it/s]Extractor Estimating: 234it [02:23,  1.38it/s]Extractor Estimating: 235it [02:24,  1.42it/s]Extractor Estimating: 236it [02:25,  1.47it/s]Extractor Estimating: 237it [02:25,  1.52it/s]Extractor Estimating: 238it [02:26,  1.50it/s]Extractor Estimating: 239it [02:27,  1.42it/s]Extractor Estimating: 240it [02:27,  1.48it/s]Extractor Estimating: 241it [02:28,  1.50it/s]Extractor Estimating: 242it [02:29,  1.52it/s]Extractor Estimating: 243it [02:29,  1.54it/s]Extractor Estimating: 244it [02:30,  1.26it/s]Extractor Estimating: 245it [02:31,  1.37it/s]Extractor Estimating: 246it [02:32,  1.45it/s]Extractor Estimating: 247it [02:32,  1.49it/s]Extractor Estimating: 248it [02:33,  1.51it/s]Extractor Estimating: 249it [02:34,  1.44it/s]Extractor Estimating: 250it [02:34,  1.49it/s]Extractor Estimating: 251it [02:35,  1.52it/s]Extractor Estimating: 252it [02:36,  1.54it/s]Extractor Estimating: 253it [02:36,  1.56it/s]Extractor Estimating: 254it [02:37,  1.29it/s]Extractor Estimating: 255it [02:38,  1.37it/s]Extractor Estimating: 256it [02:38,  1.46it/s]Extractor Estimating: 257it [02:39,  1.49it/s]Extractor Estimating: 258it [02:40,  1.53it/s]Extractor Estimating: 259it [02:40,  1.52it/s]Extractor Estimating: 260it [02:41,  1.54it/s]Extractor Estimating: 261it [02:42,  1.54it/s]Extractor Estimating: 262it [02:42,  1.55it/s]Extractor Estimating: 263it [02:43,  1.58it/s]Extractor Estimating: 264it [02:44,  1.18it/s]Extractor Estimating: 265it [02:45,  1.29it/s]Extractor Estimating: 266it [02:45,  1.39it/s]Extractor Estimating: 267it [02:46,  1.45it/s]Extractor Estimating: 268it [02:47,  1.33it/s]Extractor Estimating: 269it [02:48,  1.36it/s]Extractor Estimating: 270it [02:48,  1.43it/s]Extractor Estimating: 271it [02:49,  1.47it/s]Extractor Estimating: 272it [02:50,  1.51it/s]Extractor Estimating: 273it [02:50,  1.54it/s]Extractor Estimating: 274it [02:51,  1.37it/s]Extractor Estimating: 275it [02:52,  1.45it/s]Extractor Estimating: 276it [02:52,  1.45it/s]Extractor Estimating: 277it [02:53,  1.49it/s]Extractor Estimating: 278it [02:54,  1.54it/s]Extractor Estimating: 279it [02:55,  1.24it/s]Extractor Estimating: 280it [02:55,  1.33it/s]Extractor Estimating: 281it [02:56,  1.44it/s]Extractor Estimating: 282it [02:57,  1.51it/s]Extractor Estimating: 283it [02:57,  1.51it/s]Extractor Estimating: 284it [02:59,  1.13it/s]Extractor Estimating: 285it [02:59,  1.26it/s]Extractor Estimating: 286it [03:00,  1.34it/s]Extractor Estimating: 287it [03:00,  1.46it/s]Extractor Estimating: 288it [03:01,  1.47it/s]Extractor Estimating: 289it [03:02,  1.50it/s]Extractor Estimating: 290it [03:02,  1.53it/s]Extractor Estimating: 291it [03:03,  1.57it/s]Extractor Estimating: 292it [03:03,  1.58it/s]Extractor Estimating: 293it [03:04,  1.48it/s]Extractor Estimating: 294it [03:05,  1.55it/s]Extractor Estimating: 295it [03:05,  1.62it/s]Extractor Estimating: 296it [03:06,  1.65it/s]Extractor Estimating: 297it [03:07,  1.64it/s]Extractor Estimating: 298it [03:07,  1.57it/s]Extractor Estimating: 299it [03:08,  1.62it/s]Extractor Estimating: 300it [03:08,  1.65it/s]Extractor Estimating: 301it [03:09,  1.66it/s]Extractor Estimating: 302it [03:10,  1.74it/s]Extractor Estimating: 303it [03:10,  1.78it/s]Extractor Estimating: 304it [03:11,  1.63it/s]Extractor Estimating: 305it [03:11,  1.70it/s]Extractor Estimating: 306it [03:12,  1.70it/s]Extractor Estimating: 307it [03:12,  1.71it/s]Extractor Estimating: 308it [03:13,  1.74it/s]Extractor Estimating: 309it [03:14,  1.70it/s]Extractor Estimating: 310it [03:14,  1.62it/s]Extractor Estimating: 311it [03:15,  1.64it/s]Extractor Estimating: 312it [03:16,  1.62it/s]Extractor Estimating: 313it [03:16,  1.68it/s]Extractor Estimating: 314it [03:17,  1.71it/s]Extractor Estimating: 315it [03:17,  1.65it/s]Extractor Estimating: 316it [03:18,  1.70it/s]Extractor Estimating: 317it [03:19,  1.34it/s]Extractor Estimating: 318it [03:20,  1.41it/s]Extractor Estimating: 319it [03:20,  1.47it/s]Extractor Estimating: 320it [03:21,  1.58it/s]Extractor Estimating: 321it [03:22,  1.47it/s]Extractor Estimating: 322it [03:22,  1.58it/s]Extractor Estimating: 323it [03:23,  1.64it/s]Extractor Estimating: 324it [03:23,  1.68it/s]Extractor Estimating: 325it [03:24,  1.69it/s]Extractor Estimating: 326it [03:24,  1.59it/s]Extractor Estimating: 327it [03:25,  1.57it/s]Extractor Estimating: 328it [03:26,  1.58it/s]Extractor Estimating: 329it [03:26,  1.58it/s]Extractor Estimating: 330it [03:27,  1.57it/s]Extractor Estimating: 331it [03:28,  1.50it/s]Extractor Estimating: 332it [03:28,  1.52it/s]Extractor Estimating: 333it [03:29,  1.54it/s]Extractor Estimating: 334it [03:30,  1.49it/s]Extractor Estimating: 335it [03:30,  1.51it/s]Extractor Estimating: 336it [03:31,  1.50it/s]Extractor Estimating: 337it [03:32,  1.52it/s]Extractor Estimating: 338it [03:32,  1.54it/s]Extractor Estimating: 339it [03:33,  1.57it/s]Extractor Estimating: 340it [03:34,  1.57it/s]Extractor Estimating: 341it [03:34,  1.51it/s]Extractor Estimating: 342it [03:35,  1.53it/s]Extractor Estimating: 343it [03:36,  1.55it/s]Extractor Estimating: 344it [03:36,  1.56it/s]Extractor Estimating: 345it [03:37,  1.58it/s]Extractor Estimating: 346it [03:38,  1.38it/s]Extractor Estimating: 347it [03:38,  1.42it/s]Extractor Estimating: 348it [03:39,  1.47it/s]Extractor Estimating: 349it [03:40,  1.49it/s]Extractor Estimating: 350it [03:40,  1.62it/s]Extractor Estimating: 351it [03:41,  1.62it/s]Extractor Estimating: 352it [03:41,  1.72it/s]Extractor Estimating: 353it [03:42,  1.81it/s]Extractor Estimating: 354it [03:42,  1.85it/s]Extractor Estimating: 355it [03:43,  1.83it/s]Extractor Estimating: 356it [03:43,  1.85it/s]Extractor Estimating: 357it [03:44,  1.86it/s]Extractor Estimating: 358it [03:44,  1.89it/s]Extractor Estimating: 359it [03:45,  1.86it/s]Extractor Estimating: 360it [03:46,  1.87it/s]Extractor Estimating: 361it [03:46,  1.81it/s]Extractor Estimating: 362it [03:47,  1.85it/s]Extractor Estimating: 363it [03:47,  1.83it/s]Extractor Estimating: 364it [03:48,  1.91it/s]Extractor Estimating: 365it [03:48,  1.86it/s]Extractor Estimating: 366it [03:49,  1.81it/s]Extractor Estimating: 367it [03:49,  1.85it/s]Extractor Estimating: 368it [03:50,  1.87it/s]Extractor Estimating: 369it [03:50,  1.85it/s]Extractor Estimating: 370it [03:51,  1.85it/s]Extractor Estimating: 371it [03:51,  1.86it/s]Extractor Estimating: 372it [03:52,  1.73it/s]Extractor Estimating: 373it [03:53,  1.73it/s]Extractor Estimating: 374it [03:53,  1.76it/s]Extractor Estimating: 374it [03:53,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:31,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:31,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:31,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:31,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:31,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:21:32,397 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:21:32,398 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:32,975 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:34,287 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:34,387 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:37,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:37,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:37,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:37,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:37,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:21:38,896 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:21:38,897 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:39,323 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:40,195 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:40,195 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:40:33,482 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:40:33,691 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4488 mean pseudo reward: 0.9904740073559498
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 15728
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15828, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15828, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.959, loss:194.7848
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 13, avg_time 0.951, loss:163.1307
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 113, avg_time 0.962, loss:146.9892
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 26, avg_time 0.950, loss:143.6983
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 126, avg_time 0.963, loss:139.7879
>> valid entity prec:0.5608, rec:0.5213, f1:0.5404
>> valid relation prec:0.2141, rec:0.1258, f1:0.1585
>> valid relation with NER prec:0.2141, rec:0.1258, f1:0.1585
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 39, avg_time 2.567, loss:130.2908
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 139, avg_time 0.960, loss:130.8967
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 52, avg_time 0.963, loss:134.9743
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 152, avg_time 0.965, loss:133.5319
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 65, avg_time 0.977, loss:135.8044
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5236, rec:0.5893, f1:0.5545
>> valid relation prec:0.2033, rec:0.1345, f1:0.1619
>> valid relation with NER prec:0.2033, rec:0.1345, f1:0.1619
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 165, avg_time 2.571, loss:134.6974
g_step 1200, step 78, avg_time 0.954, loss:128.1638
g_step 1300, step 178, avg_time 0.970, loss:127.7909
g_step 1400, step 91, avg_time 0.968, loss:124.0322
g_step 1500, step 4, avg_time 0.969, loss:128.6253
>> valid entity prec:0.5326, rec:0.5347, f1:0.5337
>> valid relation prec:0.1716, rec:0.1150, f1:0.1377
>> valid relation with NER prec:0.1716, rec:0.1150, f1:0.1377
g_step 1600, step 104, avg_time 2.553, loss:107.7765
g_step 1700, step 17, avg_time 0.953, loss:121.5482
g_step 1800, step 117, avg_time 0.968, loss:106.1145
g_step 1900, step 30, avg_time 0.982, loss:121.8092
g_step 2000, step 130, avg_time 0.963, loss:112.2050
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5156, rec:0.5077, f1:0.5116
>> valid relation prec:0.1675, rec:0.1070, f1:0.1306
>> valid relation with NER prec:0.1675, rec:0.1070, f1:0.1306
g_step 2100, step 43, avg_time 2.536, loss:113.4412
g_step 2200, step 143, avg_time 0.968, loss:105.5299
g_step 2300, step 56, avg_time 0.974, loss:99.9293
g_step 2400, step 156, avg_time 0.967, loss:108.7119
g_step 2500, step 69, avg_time 0.959, loss:94.9884
>> valid entity prec:0.5534, rec:0.4586, f1:0.5016
>> valid relation prec:0.2112, rec:0.1160, f1:0.1498
>> valid relation with NER prec:0.2112, rec:0.1160, f1:0.1498
g_step 2600, step 169, avg_time 2.550, loss:97.5431
g_step 2700, step 82, avg_time 0.964, loss:93.9798
g_step 2800, step 182, avg_time 0.975, loss:93.1736
g_step 2900, step 95, avg_time 0.967, loss:86.6773
g_step 3000, step 8, avg_time 0.970, loss:95.2771
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5530, rec:0.4086, f1:0.4699
>> valid relation prec:0.1740, rec:0.0869, f1:0.1159
>> valid relation with NER prec:0.1740, rec:0.0869, f1:0.1159
g_step 3100, step 108, avg_time 2.555, loss:88.9814
g_step 3200, step 21, avg_time 0.961, loss:91.8025
g_step 3300, step 121, avg_time 0.964, loss:82.6672
g_step 3400, step 34, avg_time 0.985, loss:88.0975
g_step 3500, step 134, avg_time 0.960, loss:74.2039
>> valid entity prec:0.4886, rec:0.4410, f1:0.4636
>> valid relation prec:0.1484, rec:0.0957, f1:0.1164
>> valid relation with NER prec:0.1484, rec:0.0957, f1:0.1164
g_step 3600, step 47, avg_time 2.547, loss:82.0513
g_step 3700, step 147, avg_time 0.968, loss:85.8535
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:40:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:40:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-40-33_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:40:36 - WARNING - datasets.builder -   Using custom data configuration default-977ff7b1799ee9eb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-977ff7b1799ee9eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:40:45,264 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:40:45,319 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:40:45,320 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:40:45,321 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:40:45,688 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,972 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,972 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,973 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:40:45,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:40:46,986 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:40:50,363 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:40:50,431 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-977ff7b1799ee9eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.71ba/s] 40%|████      | 2/5 [00:00<00:00,  3.70ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  5.52ba/s]100%|██████████| 5/5 [00:01<00:00,  4.64ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.15ba/s] 40%|████      | 2/5 [00:00<00:00,  3.16ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.71ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.06ba/s]100%|██████████| 5/5 [00:01<00:00,  4.44ba/s]100%|██████████| 5/5 [00:01<00:00,  3.88ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.82ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.10ba/s]100%|██████████| 5/5 [00:00<00:00,  8.52ba/s]100%|██████████| 5/5 [00:00<00:00,  7.18ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.09ba/s] 60%|██████    | 3/5 [00:00<00:00,  5.09ba/s]100%|██████████| 5/5 [00:00<00:00,  7.03ba/s]100%|██████████| 5/5 [00:00<00:00,  5.84ba/s]
[INFO|trainer.py:414] 2023-08-28 22:40:56,976 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:40:57,141 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:40:57,141 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 22:40:57,141 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:40:57,141 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:40:57,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:40:57,141 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:40:57,141 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:45,  3.32it/s]  1%|          | 2/350 [00:00<01:42,  3.40it/s]  1%|          | 3/350 [00:00<01:41,  3.43it/s]  1%|          | 4/350 [00:01<01:40,  3.45it/s]  1%|▏         | 5/350 [00:01<01:40,  3.43it/s]  2%|▏         | 6/350 [00:01<01:40,  3.43it/s]  2%|▏         | 7/350 [00:02<01:40,  3.42it/s]  2%|▏         | 8/350 [00:02<01:40,  3.41it/s]  3%|▎         | 9/350 [00:02<01:39,  3.41it/s]  3%|▎         | 10/350 [00:02<01:39,  3.41it/s]  3%|▎         | 11/350 [00:03<01:42,  3.29it/s]  3%|▎         | 12/350 [00:03<01:41,  3.33it/s]  4%|▎         | 13/350 [00:03<01:40,  3.35it/s]  4%|▍         | 14/350 [00:04<01:39,  3.36it/s]  4%|▍         | 15/350 [00:04<01:39,  3.38it/s]  5%|▍         | 16/350 [00:04<01:38,  3.39it/s]  5%|▍         | 17/350 [00:05<01:38,  3.39it/s]  5%|▌         | 18/350 [00:05<01:37,  3.40it/s]  5%|▌         | 19/350 [00:05<01:37,  3.40it/s]  6%|▌         | 20/350 [00:05<01:37,  3.40it/s]  6%|▌         | 21/350 [00:06<01:36,  3.40it/s]  6%|▋         | 22/350 [00:06<01:40,  3.25it/s]  7%|▋         | 23/350 [00:06<01:39,  3.30it/s]  7%|▋         | 24/350 [00:07<01:37,  3.33it/s]  7%|▋         | 25/350 [00:07<01:36,  3.35it/s]  7%|▋         | 26/350 [00:07<01:36,  3.37it/s]  8%|▊         | 27/350 [00:07<01:35,  3.38it/s]  8%|▊         | 28/350 [00:08<01:35,  3.39it/s]  8%|▊         | 29/350 [00:08<01:34,  3.39it/s]  9%|▊         | 30/350 [00:08<01:34,  3.39it/s]  9%|▉         | 31/350 [00:09<01:33,  3.40it/s]  9%|▉         | 32/350 [00:09<01:33,  3.40it/s]  9%|▉         | 33/350 [00:09<01:33,  3.40it/s] 10%|▉         | 34/350 [00:10<01:32,  3.40it/s] 10%|█         | 35/350 [00:10<01:38,  3.18it/s] 10%|█         | 36/350 [00:10<01:36,  3.25it/s] 11%|█         | 37/350 [00:11<01:34,  3.30it/s] 11%|█         | 38/350 [00:11<01:33,  3.33it/s] 11%|█         | 39/350 [00:11<01:32,  3.35it/s] 11%|█▏        | 40/350 [00:11<01:32,  3.37it/s] 12%|█▏        | 41/350 [00:12<01:31,  3.38it/s] 12%|█▏        | 42/350 [00:12<01:31,  3.38it/s] 12%|█▏        | 43/350 [00:12<01:30,  3.39it/s] 13%|█▎        | 44/350 [00:13<01:30,  3.39it/s] 13%|█▎        | 45/350 [00:13<01:35,  3.19it/s] 13%|█▎        | 46/350 [00:13<01:33,  3.25it/s] 13%|█▎        | 47/350 [00:14<01:31,  3.30it/s] 14%|█▎        | 48/350 [00:14<01:30,  3.32it/s] 14%|█▍        | 49/350 [00:14<01:30,  3.34it/s] 14%|█▍        | 50/350 [00:14<01:29,  3.36it/s] 15%|█▍        | 51/350 [00:15<01:45,  2.82it/s] 15%|█▍        | 52/350 [00:15<01:40,  2.98it/s] 15%|█▌        | 53/350 [00:15<01:36,  3.09it/s] 15%|█▌        | 54/350 [00:16<01:33,  3.18it/s] 16%|█▌        | 55/350 [00:16<01:34,  3.12it/s] 16%|█▌        | 56/350 [00:16<01:31,  3.20it/s] 16%|█▋        | 57/350 [00:17<01:29,  3.26it/s] 17%|█▋        | 58/350 [00:17<01:28,  3.30it/s] 17%|█▋        | 59/350 [00:17<01:27,  3.33it/s] 17%|█▋        | 60/350 [00:18<01:26,  3.35it/s] 17%|█▋        | 61/350 [00:18<01:25,  3.36it/s] 18%|█▊        | 62/350 [00:18<01:25,  3.37it/s] 18%|█▊        | 63/350 [00:18<01:24,  3.38it/s] 18%|█▊        | 64/350 [00:19<01:24,  3.39it/s] 19%|█▊        | 65/350 [00:19<01:24,  3.39it/s] 19%|█▉        | 66/350 [00:19<01:29,  3.16it/s] 19%|█▉        | 67/350 [00:20<01:27,  3.22it/s] 19%|█▉        | 68/350 [00:20<01:26,  3.27it/s] 20%|█▉        | 69/350 [00:20<01:24,  3.31it/s] 20%|██        | 70/350 [00:21<01:23,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 22:41:18,263 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:41:18,264 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:41:18,264 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.36it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.26it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.71it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.82it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.08it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.62it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.45it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.18it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.32it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.37it/s][A
  9%|▉         | 57/611 [00:06<00:12, 44.48it/s][A
 10%|█         | 62/611 [00:07<03:21,  2.72it/s][A
 11%|█         | 67/611 [00:07<02:23,  3.80it/s][A
 12%|█▏        | 72/611 [00:07<01:42,  5.25it/s][A
 13%|█▎        | 77/611 [00:07<01:14,  7.15it/s][A
 13%|█▎        | 82/611 [00:07<00:55,  9.57it/s][A
 14%|█▍        | 87/611 [00:07<00:41, 12.53it/s][A
 15%|█▌        | 92/611 [00:07<00:32, 16.00it/s][A
 16%|█▌        | 97/611 [00:07<00:25, 19.84it/s][A
 17%|█▋        | 102/611 [00:07<00:21, 23.69it/s][A
 18%|█▊        | 107/611 [00:08<00:18, 27.40it/s][A
 18%|█▊        | 112/611 [00:08<00:16, 30.84it/s][A
 19%|█▉        | 117/611 [00:08<00:14, 33.95it/s][A
 20%|█▉        | 122/611 [00:08<00:13, 36.67it/s][A
 21%|██        | 127/611 [00:08<00:12, 38.73it/s][A
 22%|██▏       | 132/611 [00:08<00:11, 40.34it/s][A
 22%|██▏       | 137/611 [00:08<00:11, 41.57it/s][A
 23%|██▎       | 142/611 [00:08<00:11, 42.45it/s][A
 24%|██▍       | 147/611 [00:08<00:10, 42.68it/s][A
 25%|██▍       | 152/611 [00:09<00:10, 43.02it/s][A
 26%|██▌       | 157/611 [00:09<00:10, 43.15it/s][A
 27%|██▋       | 162/611 [00:09<00:10, 43.51it/s][A
 27%|██▋       | 167/611 [00:09<00:10, 41.01it/s][A
 28%|██▊       | 172/611 [00:09<00:10, 42.08it/s][A
 29%|██▉       | 177/611 [00:09<00:10, 42.96it/s][A
 30%|██▉       | 182/611 [00:09<00:09, 43.48it/s][A
 31%|███       | 187/611 [00:09<00:09, 43.72it/s][A
 31%|███▏      | 192/611 [00:10<00:09, 43.82it/s][A
 32%|███▏      | 197/611 [00:10<00:09, 43.70it/s][A
 33%|███▎      | 202/611 [00:10<00:09, 43.67it/s][A
 34%|███▍      | 207/611 [00:10<00:09, 43.55it/s][A
 35%|███▍      | 212/611 [00:10<00:09, 43.66it/s][A
 36%|███▌      | 217/611 [00:10<00:08, 44.08it/s][A
 36%|███▋      | 222/611 [00:10<00:08, 44.35it/s][A
 37%|███▋      | 227/611 [00:10<00:08, 44.54it/s][A
 38%|███▊      | 232/611 [00:10<00:08, 44.51it/s][A
 39%|███▉      | 237/611 [00:11<00:08, 44.41it/s][A
 40%|███▉      | 242/611 [00:11<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:11<00:08, 44.06it/s][A
 41%|████      | 252/611 [00:11<00:08, 43.99it/s][A
 42%|████▏     | 257/611 [00:11<00:08, 44.06it/s][A
 43%|████▎     | 262/611 [00:11<00:07, 44.12it/s][A
 44%|████▎     | 267/611 [00:11<00:07, 44.38it/s][A
 45%|████▍     | 272/611 [00:11<00:07, 44.53it/s][A
 45%|████▌     | 277/611 [00:11<00:07, 44.57it/s][A
 46%|████▌     | 282/611 [00:12<00:07, 44.44it/s][A
 47%|████▋     | 287/611 [00:12<00:07, 44.23it/s][A
 48%|████▊     | 292/611 [00:12<00:07, 44.08it/s][A
 49%|████▊     | 297/611 [00:12<00:07, 44.09it/s][A
 49%|████▉     | 302/611 [00:12<00:11, 26.12it/s][A
 50%|█████     | 307/611 [00:12<00:10, 29.90it/s][A
 51%|█████     | 312/611 [00:12<00:09, 33.21it/s][A
 52%|█████▏    | 317/611 [00:13<00:08, 35.96it/s][A
 53%|█████▎    | 322/611 [00:13<00:07, 38.29it/s][A
 54%|█████▎    | 327/611 [00:13<00:07, 40.04it/s][A
 54%|█████▍    | 332/611 [00:13<00:06, 41.39it/s][A
 55%|█████▌    | 337/611 [00:13<00:06, 42.20it/s][A
 56%|█████▌    | 342/611 [00:13<00:06, 42.41it/s][A
 57%|█████▋    | 347/611 [00:13<00:06, 42.61it/s][A
 58%|█████▊    | 352/611 [00:13<00:06, 43.07it/s][A
 58%|█████▊    | 357/611 [00:14<00:05, 43.38it/s][A
 59%|█████▉    | 362/611 [00:14<00:05, 43.88it/s][A
 60%|██████    | 367/611 [00:14<00:05, 44.17it/s][A
 61%|██████    | 372/611 [00:14<00:05, 44.42it/s][A
 62%|██████▏   | 377/611 [00:14<00:05, 44.45it/s][A
 63%|██████▎   | 382/611 [00:14<00:05, 44.34it/s][A
 63%|██████▎   | 387/611 [00:14<00:05, 43.97it/s][A
 64%|██████▍   | 392/611 [00:14<00:05, 43.76it/s][A
 65%|██████▍   | 397/611 [00:14<00:04, 43.86it/s][A
 66%|██████▌   | 402/611 [00:15<00:04, 44.01it/s][A
 67%|██████▋   | 407/611 [00:15<00:04, 44.24it/s][A
 67%|██████▋   | 412/611 [00:15<00:04, 44.44it/s][A
 68%|██████▊   | 417/611 [00:15<00:04, 44.55it/s][A
 69%|██████▉   | 422/611 [00:15<00:04, 44.62it/s][A
 70%|██████▉   | 427/611 [00:15<00:05, 36.07it/s][A
 71%|███████   | 432/611 [00:15<00:04, 38.38it/s][A
 72%|███████▏  | 437/611 [00:15<00:04, 40.03it/s][A
 72%|███████▏  | 442/611 [00:16<00:04, 41.39it/s][A
 73%|███████▎  | 447/611 [00:16<00:03, 42.32it/s][A
 74%|███████▍  | 452/611 [00:16<00:03, 43.08it/s][A
 75%|███████▍  | 457/611 [00:16<00:03, 43.62it/s][A
 76%|███████▌  | 462/611 [00:16<00:03, 43.74it/s][A
 76%|███████▋  | 467/611 [00:16<00:03, 43.55it/s][A
 77%|███████▋  | 472/611 [00:16<00:03, 43.30it/s][A
 78%|███████▊  | 477/611 [00:16<00:03, 43.51it/s][A
 79%|███████▉  | 482/611 [00:16<00:02, 43.74it/s][A
 80%|███████▉  | 487/611 [00:17<00:02, 44.08it/s][A
 81%|████████  | 492/611 [00:17<00:02, 44.27it/s][A
 81%|████████▏ | 497/611 [00:17<00:02, 44.45it/s][A
 82%|████████▏ | 502/611 [00:17<00:02, 44.52it/s][A
 83%|████████▎ | 507/611 [00:17<00:02, 44.33it/s][A
 84%|████████▍ | 512/611 [00:17<00:02, 43.96it/s][A
 85%|████████▍ | 517/611 [00:17<00:02, 43.79it/s][A
 85%|████████▌ | 522/611 [00:17<00:02, 43.82it/s][A
 86%|████████▋ | 527/611 [00:17<00:01, 44.05it/s][A
 87%|████████▋ | 532/611 [00:18<00:01, 44.10it/s][A
 88%|████████▊ | 537/611 [00:18<00:01, 44.36it/s][A
 89%|████████▊ | 542/611 [00:18<00:01, 44.54it/s][A
 90%|████████▉ | 547/611 [00:18<00:01, 44.53it/s][A
 90%|█████████ | 552/611 [00:18<00:01, 44.35it/s][A
 91%|█████████ | 557/611 [00:18<00:01, 30.87it/s][A
 92%|█████████▏| 562/611 [00:18<00:01, 34.02it/s][A
 93%|█████████▎| 567/611 [00:18<00:01, 36.63it/s][A
 94%|█████████▎| 572/611 [00:19<00:01, 38.79it/s][A
 94%|█████████▍| 577/611 [00:19<00:00, 40.45it/s][A
 95%|█████████▌| 582/611 [00:19<00:00, 41.70it/s][A
 96%|█████████▌| 587/611 [00:19<00:00, 42.63it/s][A
 97%|█████████▋| 592/611 [00:19<00:00, 42.96it/s][A
 98%|█████████▊| 597/611 [00:19<00:00, 42.98it/s][A
 99%|█████████▊| 602/611 [00:19<00:00, 43.02it/s][A
 99%|█████████▉| 607/611 [00:19<00:00, 43.22it/s][A
                                                 [A                                                
100%|██████████| 611/611 [00:19<00:00, 43.22it/s][A 20%|██        | 70/350 [00:41<01:23,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:41:39,233 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 22:41:40,011 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:41:48,957 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:41:49,688 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:41:50,325 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [01:07<1:05:50, 14.16s/it] 21%|██        | 72/350 [01:08<46:41, 10.08s/it]   21%|██        | 73/350 [01:08<32:58,  7.14s/it] 21%|██        | 74/350 [01:08<23:24,  5.09s/it] 21%|██▏       | 75/350 [01:09<16:43,  3.65s/it] 22%|██▏       | 76/350 [01:09<12:04,  2.64s/it] 22%|██▏       | 77/350 [01:09<08:49,  1.94s/it] 22%|██▏       | 78/350 [01:09<06:32,  1.44s/it] 23%|██▎       | 79/350 [01:10<04:57,  1.10s/it] 23%|██▎       | 80/350 [01:10<03:51,  1.17it/s] 23%|██▎       | 81/350 [01:10<03:05,  1.45it/s] 23%|██▎       | 82/350 [01:11<02:35,  1.73it/s] 24%|██▎       | 83/350 [01:11<02:11,  2.03it/s] 24%|██▍       | 84/350 [01:11<01:55,  2.30it/s] 24%|██▍       | 85/350 [01:11<01:43,  2.55it/s] 25%|██▍       | 86/350 [01:12<01:35,  2.76it/s] 25%|██▍       | 87/350 [01:12<01:29,  2.92it/s] 25%|██▌       | 88/350 [01:12<01:25,  3.05it/s] 25%|██▌       | 89/350 [01:13<01:22,  3.15it/s] 26%|██▌       | 90/350 [01:13<01:20,  3.22it/s] 26%|██▌       | 91/350 [01:13<01:19,  3.28it/s] 26%|██▋       | 92/350 [01:14<01:17,  3.31it/s] 27%|██▋       | 93/350 [01:14<01:20,  3.18it/s] 27%|██▋       | 94/350 [01:14<01:19,  3.24it/s] 27%|██▋       | 95/350 [01:14<01:17,  3.29it/s] 27%|██▋       | 96/350 [01:15<01:30,  2.79it/s] 28%|██▊       | 97/350 [01:15<01:25,  2.95it/s] 28%|██▊       | 98/350 [01:16<01:21,  3.07it/s] 28%|██▊       | 99/350 [01:16<01:19,  3.16it/s] 29%|██▊       | 100/350 [01:16<01:17,  3.23it/s] 29%|██▉       | 101/350 [01:16<01:15,  3.28it/s] 29%|██▉       | 102/350 [01:17<01:14,  3.31it/s] 29%|██▉       | 103/350 [01:17<01:17,  3.19it/s] 30%|██▉       | 104/350 [01:17<01:15,  3.25it/s] 30%|███       | 105/350 [01:18<01:14,  3.30it/s] 30%|███       | 106/350 [01:18<01:13,  3.33it/s] 31%|███       | 107/350 [01:18<01:12,  3.34it/s] 31%|███       | 108/350 [01:19<01:12,  3.36it/s] 31%|███       | 109/350 [01:19<01:11,  3.38it/s] 31%|███▏      | 110/350 [01:19<01:10,  3.38it/s] 32%|███▏      | 111/350 [01:19<01:10,  3.38it/s] 32%|███▏      | 112/350 [01:20<01:10,  3.39it/s] 32%|███▏      | 113/350 [01:20<01:09,  3.39it/s] 33%|███▎      | 114/350 [01:20<01:12,  3.24it/s] 33%|███▎      | 115/350 [01:21<01:11,  3.28it/s] 33%|███▎      | 116/350 [01:21<01:10,  3.32it/s] 33%|███▎      | 117/350 [01:21<01:09,  3.34it/s] 34%|███▎      | 118/350 [01:22<01:09,  3.36it/s] 34%|███▍      | 119/350 [01:22<01:08,  3.37it/s] 34%|███▍      | 120/350 [01:22<01:08,  3.38it/s] 35%|███▍      | 121/350 [01:22<01:07,  3.39it/s] 35%|███▍      | 122/350 [01:23<01:07,  3.39it/s] 35%|███▌      | 123/350 [01:23<01:06,  3.39it/s] 35%|███▌      | 124/350 [01:23<01:06,  3.39it/s] 36%|███▌      | 125/350 [01:24<01:17,  2.89it/s] 36%|███▌      | 126/350 [01:24<01:14,  3.02it/s] 36%|███▋      | 127/350 [01:24<01:11,  3.13it/s] 37%|███▋      | 128/350 [01:25<01:09,  3.20it/s] 37%|███▋      | 129/350 [01:25<01:07,  3.26it/s] 37%|███▋      | 130/350 [01:25<01:06,  3.30it/s] 37%|███▋      | 131/350 [01:26<01:05,  3.33it/s] 38%|███▊      | 132/350 [01:26<01:05,  3.35it/s] 38%|███▊      | 133/350 [01:26<01:04,  3.36it/s] 38%|███▊      | 134/350 [01:26<01:03,  3.38it/s] 39%|███▊      | 135/350 [01:27<01:07,  3.21it/s] 39%|███▉      | 136/350 [01:27<01:09,  3.08it/s] 39%|███▉      | 137/350 [01:28<01:40,  2.12it/s] 39%|███▉      | 138/350 [01:28<01:28,  2.39it/s] 40%|███▉      | 139/350 [01:29<01:20,  2.62it/s] 40%|████      | 140/350 [01:29<01:14,  2.82it/s][INFO|trainer.py:2140] 2023-08-28 22:42:26,488 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:42:26,488 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:42:26,488 >>   Batch size = 8
{'eval_loss': 0.9869425296783447, 'eval_runtime': 20.0483, 'eval_samples_per_second': 243.511, 'eval_steps_per_second': 30.476, 'epoch': 0.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.30it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.26it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.62it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.66it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.96it/s][A
  5%|▌         | 32/611 [00:00<00:18, 31.65it/s][A
  6%|▌         | 37/611 [00:00<00:16, 35.00it/s][A
  7%|▋         | 42/611 [00:01<00:15, 37.51it/s][A
  8%|▊         | 47/611 [00:01<00:14, 39.54it/s][A
  9%|▊         | 52/611 [00:01<00:13, 41.05it/s][A
  9%|▉         | 57/611 [00:01<00:13, 42.19it/s][A
 10%|█         | 62/611 [00:01<00:12, 42.95it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.27it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.16it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.14it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.36it/s][A
 14%|█▍        | 87/611 [00:02<00:12, 43.64it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.01it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.19it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.47it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.55it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.97it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.90it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.86it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.14it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.25it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.47it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.59it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.57it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.24it/s][A
 27%|██▋       | 162/611 [00:03<00:13, 32.60it/s][A
 27%|██▋       | 167/611 [00:04<00:12, 35.51it/s][A
 28%|██▊       | 172/611 [00:04<00:11, 37.79it/s][A
 29%|██▉       | 177/611 [00:04<00:10, 39.69it/s][A
 30%|██▉       | 182/611 [00:04<00:10, 41.14it/s][A
 31%|███       | 187/611 [00:04<00:10, 42.23it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 42.89it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.24it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.17it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.10it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 43.35it/s][A
 36%|███▌      | 217/611 [00:05<00:09, 43.72it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.05it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.35it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.42it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.48it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.33it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.96it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.72it/s][A
 42%|████▏     | 257/611 [00:06<00:08, 43.89it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 44.08it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.36it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.44it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.60it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.42it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.25it/s][A
 48%|████▊     | 292/611 [00:06<00:09, 32.41it/s][A
 49%|████▊     | 297/611 [00:07<00:08, 35.40it/s][A
 49%|████▉     | 302/611 [00:07<00:08, 37.76it/s][A
 50%|█████     | 307/611 [00:07<00:07, 39.62it/s][A
 51%|█████     | 312/611 [00:07<00:07, 41.02it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 42.17it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 42.99it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 43.21it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.16it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.15it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 43.32it/s][A
 57%|█████▋    | 347/611 [00:08<00:06, 43.56it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 43.85it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.12it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.32it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.49it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.50it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.23it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 43.87it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 43.87it/s][A
 64%|██████▍   | 392/611 [00:09<00:04, 43.95it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.10it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.24it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.44it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.59it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.45it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 38.05it/s][A
 70%|██████▉   | 427/611 [00:10<00:04, 39.83it/s][A
 71%|███████   | 432/611 [00:10<00:04, 41.27it/s][A
 72%|███████▏  | 437/611 [00:10<00:04, 42.27it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 42.99it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.58it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.09it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.16it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.84it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.65it/s][A
 77%|███████▋  | 472/611 [00:11<00:03, 43.79it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 44.13it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.37it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.44it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.59it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.74it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.55it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.17it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.88it/s][A
 85%|████████▍ | 517/611 [00:12<00:02, 44.06it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.27it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.35it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.62it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.75it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.61it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.50it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.14it/s][A
 91%|█████████ | 557/611 [00:13<00:01, 37.78it/s][A
 92%|█████████▏| 562/611 [00:13<00:01, 39.68it/s][A
 93%|█████████▎| 567/611 [00:13<00:01, 41.09it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 42.19it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.00it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.60it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.01it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.08it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.78it/s][A
 99%|█████████▊| 602/611 [00:14<00:00, 43.59it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 43.73it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 43.73it/s][A 40%|████      | 140/350 [01:43<01:14,  2.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:42:41,146 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 22:42:41,926 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:42:49,018 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:42:49,228 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:42:49,317 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [02:08<42:10, 12.11s/it] 41%|████      | 142/350 [02:09<29:42,  8.57s/it] 41%|████      | 143/350 [02:09<21:00,  6.09s/it] 41%|████      | 144/350 [02:09<14:56,  4.35s/it] 41%|████▏     | 145/350 [02:10<10:42,  3.13s/it] 42%|████▏     | 146/350 [02:10<07:45,  2.28s/it] 42%|████▏     | 147/350 [02:10<05:42,  1.69s/it] 42%|████▏     | 148/350 [02:10<04:16,  1.27s/it] 43%|████▎     | 149/350 [02:11<03:16,  1.03it/s] 43%|████▎     | 150/350 [02:11<02:34,  1.30it/s] 43%|████▎     | 151/350 [02:11<02:04,  1.59it/s] 43%|████▎     | 152/350 [02:12<01:44,  1.90it/s] 44%|████▎     | 153/350 [02:12<01:32,  2.13it/s] 44%|████▍     | 154/350 [02:12<01:21,  2.40it/s] 44%|████▍     | 155/350 [02:13<01:13,  2.64it/s] 45%|████▍     | 156/350 [02:13<01:08,  2.83it/s] 45%|████▍     | 157/350 [02:13<01:04,  2.98it/s] 45%|████▌     | 158/350 [02:13<01:02,  3.09it/s] 45%|████▌     | 159/350 [02:14<01:00,  3.18it/s] 46%|████▌     | 160/350 [02:14<00:58,  3.24it/s] 46%|████▌     | 161/350 [02:14<00:57,  3.29it/s] 46%|████▋     | 162/350 [02:15<00:56,  3.32it/s] 47%|████▋     | 163/350 [02:15<01:00,  3.08it/s] 47%|████▋     | 164/350 [02:15<01:01,  3.04it/s] 47%|████▋     | 165/350 [02:16<00:58,  3.14it/s] 47%|████▋     | 166/350 [02:16<00:57,  3.21it/s] 48%|████▊     | 167/350 [02:16<00:55,  3.27it/s] 48%|████▊     | 168/350 [02:16<00:55,  3.31it/s] 48%|████▊     | 169/350 [02:17<00:54,  3.33it/s] 49%|████▊     | 170/350 [02:17<00:53,  3.36it/s] 49%|████▉     | 171/350 [02:17<00:53,  3.37it/s] 49%|████▉     | 172/350 [02:18<00:52,  3.38it/s] 49%|████▉     | 173/350 [02:18<00:52,  3.38it/s] 50%|████▉     | 174/350 [02:18<00:51,  3.39it/s] 50%|█████     | 175/350 [02:19<00:58,  2.97it/s] 50%|█████     | 176/350 [02:19<00:56,  3.08it/s] 51%|█████     | 177/350 [02:19<00:54,  3.17it/s] 51%|█████     | 178/350 [02:20<00:53,  3.24it/s] 51%|█████     | 179/350 [02:20<00:52,  3.28it/s] 51%|█████▏    | 180/350 [02:20<00:51,  3.32it/s] 52%|█████▏    | 181/350 [02:20<00:50,  3.34it/s] 52%|█████▏    | 182/350 [02:21<00:50,  3.36it/s] 52%|█████▏    | 183/350 [02:21<00:49,  3.37it/s] 53%|█████▎    | 184/350 [02:21<00:49,  3.38it/s] 53%|█████▎    | 185/350 [02:22<00:51,  3.18it/s] 53%|█████▎    | 186/350 [02:22<00:50,  3.25it/s] 53%|█████▎    | 187/350 [02:22<00:49,  3.31it/s] 54%|█████▎    | 188/350 [02:23<00:48,  3.35it/s] 54%|█████▍    | 189/350 [02:23<00:47,  3.38it/s] 54%|█████▍    | 190/350 [02:23<00:47,  3.40it/s] 55%|█████▍    | 191/350 [02:23<00:46,  3.41it/s] 55%|█████▍    | 192/350 [02:24<00:46,  3.42it/s] 55%|█████▌    | 193/350 [02:24<00:45,  3.43it/s] 55%|█████▌    | 194/350 [02:24<00:45,  3.43it/s] 56%|█████▌    | 195/350 [02:25<00:45,  3.44it/s] 56%|█████▌    | 196/350 [02:25<00:47,  3.22it/s] 56%|█████▋    | 197/350 [02:25<00:46,  3.28it/s] 57%|█████▋    | 198/350 [02:26<00:45,  3.33it/s] 57%|█████▋    | 199/350 [02:26<00:44,  3.36it/s] 57%|█████▋    | 200/350 [02:26<00:44,  3.39it/s] 57%|█████▋    | 201/350 [02:26<00:43,  3.40it/s] 58%|█████▊    | 202/350 [02:27<00:43,  3.42it/s] 58%|█████▊    | 203/350 [02:27<00:42,  3.43it/s] 58%|█████▊    | 204/350 [02:28<01:37,  1.50it/s] 59%|█████▊    | 205/350 [02:29<01:23,  1.74it/s] 59%|█████▉    | 206/350 [02:29<01:10,  2.04it/s] 59%|█████▉    | 207/350 [02:29<01:01,  2.32it/s] 59%|█████▉    | 208/350 [02:30<00:55,  2.56it/s] 60%|█████▉    | 209/350 [02:30<00:50,  2.77it/s] 60%|██████    | 210/350 [02:30<00:47,  2.93it/s][INFO|trainer.py:2140] 2023-08-28 22:43:28,016 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:43:28,017 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:43:28,017 >>   Batch size = 8
{'eval_loss': 0.9999995827674866, 'eval_runtime': 14.3483, 'eval_samples_per_second': 340.25, 'eval_steps_per_second': 42.583, 'epoch': 1.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.48it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.35it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.60it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.78it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.01it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.66it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.35it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.21it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.36it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.33it/s][A
  9%|▉         | 57/611 [00:01<00:14, 39.12it/s][A
 10%|█         | 62/611 [00:01<00:13, 40.85it/s][A
 11%|█         | 67/611 [00:01<00:12, 41.96it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 42.94it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.40it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.60it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.75it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.70it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 43.54it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 43.52it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 43.91it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.12it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.38it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.55it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.48it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 44.38it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.14it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.85it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 43.78it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.95it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.18it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.39it/s][A
 27%|██▋       | 167/611 [00:03<00:09, 44.51it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.62it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.46it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.19it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.07it/s][A
 31%|███▏      | 192/611 [00:04<00:13, 31.59it/s][A
 32%|███▏      | 197/611 [00:04<00:11, 34.68it/s][A
 33%|███▎      | 202/611 [00:04<00:11, 37.17it/s][A
 34%|███▍      | 207/611 [00:04<00:10, 39.22it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 40.74it/s][A
 36%|███▌      | 217/611 [00:05<00:09, 41.92it/s][A
 36%|███▋      | 222/611 [00:05<00:09, 42.73it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.08it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.11it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.11it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 43.42it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.67it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.06it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.31it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 44.49it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.53it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.29it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.94it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 43.88it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.84it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.01it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.18it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.39it/s][A
 50%|█████     | 307/611 [00:07<00:06, 44.58it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.56it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.43it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 41.78it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 42.45it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 42.89it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.34it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.75it/s][A
 57%|█████▋    | 347/611 [00:08<00:05, 44.04it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.28it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.26it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.94it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.85it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.93it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.03it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.15it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.34it/s][A
 64%|██████▍   | 392/611 [00:09<00:04, 44.52it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.58it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.03it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.96it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.98it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.05it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.26it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.42it/s][A
 72%|███████▏  | 437/611 [00:10<00:03, 44.54it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.49it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.09it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 41.88it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 42.60it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.04it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.47it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.94it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 43.92it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.14it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.03it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.62it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.94it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.03it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.05it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.22it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.44it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.53it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.42it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.30it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.21it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.21it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.15it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.23it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.33it/s][A
 93%|█████████▎| 567/611 [00:13<00:00, 44.53it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.55it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.54it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.44it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.33it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 35.46it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 37.91it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 39.84it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 41.25it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 41.25it/s][A 60%|██████    | 210/350 [02:45<00:47,  2.93it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:43:42,935 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 22:43:43,442 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:43:52,834 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:43:53,242 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:43:53,589 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [03:08<26:32, 11.46s/it] 61%|██████    | 212/350 [03:08<18:41,  8.13s/it] 61%|██████    | 213/350 [03:08<13:11,  5.78s/it] 61%|██████    | 214/350 [03:09<09:21,  4.13s/it] 61%|██████▏   | 215/350 [03:09<06:42,  2.98s/it] 62%|██████▏   | 216/350 [03:09<04:51,  2.17s/it] 62%|██████▏   | 217/350 [03:10<03:34,  1.61s/it] 62%|██████▏   | 218/350 [03:10<02:40,  1.21s/it] 63%|██████▎   | 219/350 [03:10<02:02,  1.07it/s] 63%|██████▎   | 220/350 [03:10<01:36,  1.34it/s] 63%|██████▎   | 221/350 [03:11<01:18,  1.64it/s] 63%|██████▎   | 222/350 [03:11<01:05,  1.94it/s] 64%|██████▎   | 223/350 [03:11<01:04,  1.98it/s] 64%|██████▍   | 224/350 [03:12<01:02,  2.00it/s] 64%|██████▍   | 225/350 [03:12<00:54,  2.29it/s] 65%|██████▍   | 226/350 [03:13<00:48,  2.54it/s] 65%|██████▍   | 227/350 [03:13<00:44,  2.74it/s] 65%|██████▌   | 228/350 [03:13<00:41,  2.91it/s] 65%|██████▌   | 229/350 [03:13<00:39,  3.04it/s] 66%|██████▌   | 230/350 [03:14<00:38,  3.14it/s] 66%|██████▌   | 231/350 [03:14<00:37,  3.21it/s] 66%|██████▋   | 232/350 [03:14<00:37,  3.16it/s] 67%|██████▋   | 233/350 [03:15<00:36,  3.23it/s] 67%|██████▋   | 234/350 [03:15<00:39,  2.91it/s] 67%|██████▋   | 235/350 [03:15<00:37,  3.04it/s] 67%|██████▋   | 236/350 [03:16<00:36,  3.14it/s] 68%|██████▊   | 237/350 [03:16<00:35,  3.22it/s] 68%|██████▊   | 238/350 [03:16<00:34,  3.27it/s] 68%|██████▊   | 239/350 [03:17<00:33,  3.31it/s] 69%|██████▊   | 240/350 [03:17<00:33,  3.33it/s] 69%|██████▉   | 241/350 [03:17<00:32,  3.35it/s] 69%|██████▉   | 242/350 [03:18<00:35,  3.06it/s] 69%|██████▉   | 243/350 [03:18<00:33,  3.16it/s] 70%|██████▉   | 244/350 [03:18<00:32,  3.24it/s] 70%|███████   | 245/350 [03:18<00:31,  3.30it/s] 70%|███████   | 246/350 [03:19<00:31,  3.34it/s] 71%|███████   | 247/350 [03:19<00:30,  3.38it/s] 71%|███████   | 248/350 [03:19<00:30,  3.39it/s] 71%|███████   | 249/350 [03:20<00:29,  3.41it/s] 71%|███████▏  | 250/350 [03:20<00:29,  3.42it/s] 72%|███████▏  | 251/350 [03:20<00:28,  3.43it/s] 72%|███████▏  | 252/350 [03:21<00:36,  2.67it/s] 72%|███████▏  | 253/350 [03:21<00:33,  2.86it/s] 73%|███████▎  | 254/350 [03:21<00:31,  3.02it/s] 73%|███████▎  | 255/350 [03:22<00:30,  3.14it/s] 73%|███████▎  | 256/350 [03:22<00:29,  3.22it/s] 73%|███████▎  | 257/350 [03:22<00:28,  3.28it/s] 74%|███████▎  | 258/350 [03:22<00:27,  3.33it/s] 74%|███████▍  | 259/350 [03:23<00:27,  3.36it/s] 74%|███████▍  | 260/350 [03:23<00:26,  3.39it/s] 75%|███████▍  | 261/350 [03:23<00:26,  3.41it/s] 75%|███████▍  | 262/350 [03:24<00:29,  2.97it/s] 75%|███████▌  | 263/350 [03:24<00:28,  3.10it/s] 75%|███████▌  | 264/350 [03:24<00:26,  3.20it/s] 76%|███████▌  | 265/350 [03:25<00:26,  3.27it/s] 76%|███████▌  | 266/350 [03:25<00:25,  3.32it/s] 76%|███████▋  | 267/350 [03:25<00:24,  3.36it/s] 77%|███████▋  | 268/350 [03:26<00:24,  3.39it/s] 77%|███████▋  | 269/350 [03:26<00:23,  3.40it/s] 77%|███████▋  | 270/350 [03:26<00:23,  3.42it/s] 77%|███████▋  | 271/350 [03:26<00:23,  3.42it/s] 78%|███████▊  | 272/350 [03:27<00:24,  3.23it/s] 78%|███████▊  | 273/350 [03:27<00:23,  3.29it/s] 78%|███████▊  | 274/350 [03:28<00:29,  2.61it/s] 79%|███████▊  | 275/350 [03:28<00:26,  2.81it/s] 79%|███████▉  | 276/350 [03:29<00:51,  1.43it/s] 79%|███████▉  | 277/350 [03:30<00:43,  1.66it/s] 79%|███████▉  | 278/350 [03:30<00:36,  1.97it/s] 80%|███████▉  | 279/350 [03:30<00:31,  2.26it/s] 80%|████████  | 280/350 [03:31<00:27,  2.52it/s][INFO|trainer.py:2140] 2023-08-28 22:44:28,304 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:44:28,305 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:44:28,305 >>   Batch size = 8
{'eval_loss': 1.0194402933120728, 'eval_runtime': 14.1578, 'eval_samples_per_second': 344.827, 'eval_steps_per_second': 43.156, 'epoch': 2.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.21it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.04it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.25it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.39it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.86it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.68it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.49it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.37it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.50it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.56it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.48it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.39it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.27it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.20it/s][A
 13%|█▎        | 82/611 [00:01<00:13, 37.88it/s][A
 14%|█▍        | 87/611 [00:02<00:13, 39.80it/s][A
 15%|█▌        | 92/611 [00:02<00:12, 41.16it/s][A
 16%|█▌        | 97/611 [00:02<00:12, 42.15it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 42.96it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 43.52it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.90it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.94it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.61it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.38it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 43.69it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.80it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.12it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.31it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.46it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.57it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.43it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.05it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.85it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 43.87it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.06it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.20it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.38it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.53it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.58it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.46it/s][A
 35%|███▍      | 212/611 [00:05<00:09, 44.08it/s][A
 36%|███▌      | 217/611 [00:05<00:15, 25.96it/s][A
 36%|███▋      | 222/611 [00:05<00:13, 29.70it/s][A
 37%|███▋      | 227/611 [00:05<00:11, 32.99it/s][A
 38%|███▊      | 232/611 [00:05<00:10, 35.89it/s][A
 39%|███▉      | 237/611 [00:05<00:09, 38.15it/s][A
 40%|███▉      | 242/611 [00:05<00:09, 39.95it/s][A
 40%|████      | 247/611 [00:05<00:08, 41.36it/s][A
 41%|████      | 252/611 [00:05<00:08, 42.17it/s][A
 42%|████▏     | 257/611 [00:06<00:08, 42.37it/s][A
 43%|████▎     | 262/611 [00:06<00:08, 42.53it/s][A
 44%|████▎     | 267/611 [00:06<00:08, 42.93it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.38it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.70it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 43.94it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.25it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.43it/s][A
 49%|████▊     | 297/611 [00:07<00:07, 44.38it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:07<00:06, 43.82it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.85it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.88it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.21it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.36it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.45it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.55it/s][A
 56%|█████▌    | 342/611 [00:08<00:07, 38.03it/s][A
 57%|█████▋    | 347/611 [00:08<00:06, 39.82it/s][A
 58%|█████▊    | 352/611 [00:08<00:06, 41.21it/s][A
 58%|█████▊    | 357/611 [00:08<00:06, 42.22it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.02it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.53it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.93it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.01it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.64it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 43.35it/s][A
 64%|██████▍   | 392/611 [00:09<00:05, 43.61it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 43.88it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.14it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.34it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.55it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.57it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.30it/s][A
 70%|██████▉   | 427/611 [00:10<00:04, 43.94it/s][A
 71%|███████   | 432/611 [00:10<00:04, 43.72it/s][A
 72%|███████▏  | 437/611 [00:10<00:03, 43.79it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.99it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.24it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.44it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.59it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.68it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.36it/s][A
 77%|███████▋  | 472/611 [00:11<00:03, 44.10it/s][A
 78%|███████▊  | 477/611 [00:11<00:04, 31.48it/s][A
 79%|███████▉  | 482/611 [00:11<00:03, 34.55it/s][A
 80%|███████▉  | 487/611 [00:11<00:03, 37.11it/s][A
 81%|████████  | 492/611 [00:11<00:03, 39.21it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 40.82it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 41.89it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 42.88it/s][A
 84%|████████▍ | 512/611 [00:12<00:02, 43.30it/s][A
 85%|████████▍ | 517/611 [00:12<00:02, 43.20it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 43.15it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 43.57it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.68it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.09it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.34it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.46it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.67it/s][A
 91%|█████████ | 557/611 [00:13<00:01, 44.67it/s][A
 92%|█████████▏| 562/611 [00:13<00:01, 44.25it/s][A
 93%|█████████▎| 567/611 [00:13<00:00, 44.05it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 43.99it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.11it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.37it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.54it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.69it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.78it/s][A
 99%|█████████▊| 602/611 [00:14<00:00, 44.66it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 40.74it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 40.74it/s][A 80%|████████  | 280/350 [03:45<00:27,  2.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:44:43,709 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 22:44:44,867 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:44:57,367 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:44:58,396 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:44:59,300 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [04:25<19:09, 16.66s/it] 81%|████████  | 282/350 [04:26<13:21, 11.79s/it] 81%|████████  | 283/350 [04:26<09:18,  8.34s/it] 81%|████████  | 284/350 [04:26<06:31,  5.93s/it] 81%|████████▏ | 285/350 [04:27<04:35,  4.24s/it] 82%|████████▏ | 286/350 [04:27<03:15,  3.05s/it] 82%|████████▏ | 287/350 [04:27<02:20,  2.23s/it] 82%|████████▏ | 288/350 [04:28<01:52,  1.81s/it] 83%|████████▎ | 289/350 [04:28<01:23,  1.37s/it] 83%|████████▎ | 290/350 [04:29<01:17,  1.28s/it] 83%|████████▎ | 291/350 [04:30<00:58,  1.01it/s] 83%|████████▎ | 292/350 [04:30<00:45,  1.28it/s] 84%|████████▎ | 293/350 [04:30<00:36,  1.58it/s] 84%|████████▍ | 294/350 [04:31<00:29,  1.88it/s] 84%|████████▍ | 295/350 [04:31<00:25,  2.17it/s] 85%|████████▍ | 296/350 [04:31<00:24,  2.19it/s] 85%|████████▍ | 297/350 [04:32<00:21,  2.45it/s] 85%|████████▌ | 298/350 [04:32<00:19,  2.67it/s] 85%|████████▌ | 299/350 [04:32<00:17,  2.86it/s] 86%|████████▌ | 300/350 [04:32<00:16,  3.00it/s] 86%|████████▌ | 301/350 [04:33<00:15,  3.11it/s] 86%|████████▋ | 302/350 [04:33<00:15,  3.19it/s] 87%|████████▋ | 303/350 [04:33<00:14,  3.25it/s] 87%|████████▋ | 304/350 [04:34<00:13,  3.29it/s] 87%|████████▋ | 305/350 [04:34<00:13,  3.32it/s] 87%|████████▋ | 306/350 [04:34<00:14,  3.10it/s] 88%|████████▊ | 307/350 [04:35<00:13,  3.18it/s] 88%|████████▊ | 308/350 [04:35<00:12,  3.24it/s] 88%|████████▊ | 309/350 [04:35<00:12,  3.29it/s] 89%|████████▊ | 310/350 [04:36<00:12,  3.32it/s] 89%|████████▉ | 311/350 [04:36<00:11,  3.34it/s] 89%|████████▉ | 312/350 [04:36<00:11,  3.36it/s] 89%|████████▉ | 313/350 [04:36<00:10,  3.37it/s] 90%|████████▉ | 314/350 [04:37<00:10,  3.38it/s] 90%|█████████ | 315/350 [04:37<00:10,  3.38it/s] 90%|█████████ | 316/350 [04:37<00:10,  3.18it/s] 91%|█████████ | 317/350 [04:38<00:10,  3.24it/s] 91%|█████████ | 318/350 [04:38<00:09,  3.29it/s] 91%|█████████ | 319/350 [04:38<00:09,  3.32it/s] 91%|█████████▏| 320/350 [04:39<00:08,  3.34it/s] 92%|█████████▏| 321/350 [04:39<00:08,  3.36it/s] 92%|█████████▏| 322/350 [04:39<00:08,  3.37it/s] 92%|█████████▏| 323/350 [04:39<00:07,  3.38it/s] 93%|█████████▎| 324/350 [04:40<00:07,  3.38it/s] 93%|█████████▎| 325/350 [04:40<00:07,  3.39it/s] 93%|█████████▎| 326/350 [04:40<00:07,  3.21it/s] 93%|█████████▎| 327/350 [04:41<00:07,  3.27it/s] 94%|█████████▎| 328/350 [04:41<00:06,  3.31it/s] 94%|█████████▍| 329/350 [04:41<00:06,  3.33it/s] 94%|█████████▍| 330/350 [04:42<00:05,  3.35it/s] 95%|█████████▍| 331/350 [04:42<00:05,  3.36it/s] 95%|█████████▍| 332/350 [04:42<00:05,  3.38it/s] 95%|█████████▌| 333/350 [04:42<00:05,  3.38it/s] 95%|█████████▌| 334/350 [04:43<00:04,  3.39it/s] 96%|█████████▌| 335/350 [04:43<00:04,  3.39it/s] 96%|█████████▌| 336/350 [04:43<00:04,  3.29it/s] 96%|█████████▋| 337/350 [04:44<00:03,  3.33it/s] 97%|█████████▋| 338/350 [04:44<00:03,  3.36it/s] 97%|█████████▋| 339/350 [04:44<00:03,  3.39it/s] 97%|█████████▋| 340/350 [04:44<00:02,  3.41it/s] 97%|█████████▋| 341/350 [04:45<00:02,  3.42it/s] 98%|█████████▊| 342/350 [04:45<00:02,  3.43it/s] 98%|█████████▊| 343/350 [04:45<00:02,  3.43it/s] 98%|█████████▊| 344/350 [04:46<00:01,  3.43it/s] 99%|█████████▊| 345/350 [04:46<00:01,  3.44it/s] 99%|█████████▉| 346/350 [04:46<00:01,  3.44it/s] 99%|█████████▉| 347/350 [04:47<00:00,  3.04it/s] 99%|█████████▉| 348/350 [04:47<00:00,  3.15it/s]100%|█████████▉| 349/350 [04:47<00:00,  3.23it/s]100%|██████████| 350/350 [04:47<00:00,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 22:45:45,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:45:45,140 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:45:45,140 >>   Batch size = 8
{'eval_loss': 1.0343108177185059, 'eval_runtime': 14.3935, 'eval_samples_per_second': 339.181, 'eval_steps_per_second': 42.45, 'epoch': 3.99}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.88it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.77it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.83it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.02it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.32it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.72it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.48it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.30it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.43it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.62it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.70it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.64it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.64it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.45it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.19it/s][A
 13%|█▎        | 82/611 [00:01<00:14, 35.70it/s][A
 14%|█▍        | 87/611 [00:02<00:13, 38.13it/s][A
 15%|█▌        | 92/611 [00:02<00:12, 39.96it/s][A
 16%|█▌        | 97/611 [00:02<00:12, 41.32it/s][A
 17%|█▋        | 102/611 [00:02<00:12, 42.25it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 42.76it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.53it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.76it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.54it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.44it/s][A
 22%|██▏       | 132/611 [00:03<00:10, 43.67it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.93it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.26it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.30it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.44it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.59it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.52it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.26it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.08it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.01it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.17it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.25it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.43it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.54it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.59it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.48it/s][A
 35%|███▍      | 212/611 [00:04<00:10, 38.74it/s][A
 36%|███▌      | 217/611 [00:05<00:09, 40.43it/s][A
 36%|███▋      | 222/611 [00:05<00:09, 41.68it/s][A
 37%|███▋      | 227/611 [00:05<00:09, 42.53it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.30it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.68it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.00it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.12it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.74it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.57it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 43.78it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.95it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.37it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.41it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.61it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.47it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.18it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.03it/s][A
 50%|█████     | 307/611 [00:07<00:06, 44.03it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.16it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.34it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.38it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.53it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.54it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.42it/s][A
 56%|█████▌    | 342/611 [00:08<00:06, 44.21it/s][A
 57%|█████▋    | 347/611 [00:08<00:11, 23.26it/s][A
 58%|█████▊    | 352/611 [00:08<00:09, 27.17it/s][A
 58%|█████▊    | 357/611 [00:08<00:08, 30.85it/s][A
 59%|█████▉    | 362/611 [00:08<00:07, 34.11it/s][A
 60%|██████    | 367/611 [00:08<00:06, 36.81it/s][A
 61%|██████    | 372/611 [00:08<00:06, 38.96it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 40.64it/s][A
 63%|██████▎   | 382/611 [00:09<00:05, 41.56it/s][A
 63%|██████▎   | 387/611 [00:09<00:05, 42.07it/s][A
 64%|██████▍   | 392/611 [00:09<00:05, 42.29it/s][A
 65%|██████▍   | 397/611 [00:09<00:05, 42.54it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.11it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.56it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.99it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.26it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.43it/s][A
 70%|██████▉   | 427/611 [00:10<00:04, 44.52it/s][A
 71%|███████   | 432/611 [00:10<00:04, 44.14it/s][A
 72%|███████▏  | 437/611 [00:10<00:03, 43.83it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.77it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.93it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.19it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.44it/s][A
 76%|███████▌  | 462/611 [00:11<00:03, 44.58it/s][A
 76%|███████▋  | 467/611 [00:11<00:04, 33.15it/s][A
 77%|███████▋  | 472/611 [00:11<00:03, 35.89it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 38.15it/s][A
 79%|███████▉  | 482/611 [00:11<00:03, 39.94it/s][A
 80%|███████▉  | 487/611 [00:11<00:03, 41.32it/s][A
 81%|████████  | 492/611 [00:11<00:02, 42.31it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.10it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.39it/s][A
 83%|████████▎ | 507/611 [00:12<00:02, 43.30it/s][A
 84%|████████▍ | 512/611 [00:12<00:02, 43.20it/s][A
 85%|████████▍ | 517/611 [00:12<00:02, 43.41it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 43.71it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.05it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.43it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.55it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.41it/s][A
 90%|█████████ | 552/611 [00:13<00:01, 44.20it/s][A
 91%|█████████ | 557/611 [00:13<00:01, 43.91it/s][A
 92%|█████████▏| 562/611 [00:13<00:01, 43.80it/s][A
 93%|█████████▎| 567/611 [00:13<00:01, 43.88it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.21it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.32it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.55it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.57it/s][A
 97%|█████████▋| 592/611 [00:14<00:00, 44.32it/s][A
 98%|█████████▊| 597/611 [00:14<00:00, 32.09it/s][A
 99%|█████████▊| 602/611 [00:14<00:00, 35.03it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 37.44it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 37.44it/s][A100%|██████████| 350/350 [05:02<00:00,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:45:59,886 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 22:46:00,227 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:46:07,552 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:46:07,890 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:46:07,977 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:46:24,475 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:46:24,549 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70 (score: 0.9869425296783447).
                                                 100%|██████████| 350/350 [05:45<00:00,  3.30it/s]100%|██████████| 350/350 [05:45<00:00,  1.01it/s]
[INFO|trainer.py:1894] 2023-08-28 22:46:43,010 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 22:46:43,458 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:46:51,636 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:46:52,737 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:46:53,327 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:46:55,148 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,209 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,210 >>   train_loss               =     0.3464
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,210 >>   train_runtime            = 0:05:45.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,210 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,210 >>   train_samples_per_second =      65.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:46:55,210 >>   train_steps_per_second   =      1.013
{'eval_loss': 1.0389224290847778, 'eval_runtime': 14.5108, 'eval_samples_per_second': 336.44, 'eval_steps_per_second': 42.107, 'epoch': 4.99}
{'train_runtime': 345.6767, 'train_samples_per_second': 65.09, 'train_steps_per_second': 1.013, 'train_loss': 0.3464281572614397, 'epoch': 4.99}
08/28/2023 22:46:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:46:56,008 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:46:56,008 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:46:56,008 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.11it/s]  2%|▏         | 12/611 [00:00<00:12, 49.06it/s]  3%|▎         | 17/611 [00:00<00:12, 47.65it/s]  4%|▎         | 22/611 [00:00<00:12, 46.68it/s]  4%|▍         | 27/611 [00:00<00:12, 46.21it/s]  5%|▌         | 32/611 [00:00<00:12, 45.90it/s]  6%|▌         | 37/611 [00:00<00:12, 45.73it/s]  7%|▋         | 42/611 [00:00<00:12, 45.35it/s]  8%|▊         | 47/611 [00:01<00:12, 44.70it/s]  9%|▊         | 52/611 [00:01<00:12, 44.43it/s]  9%|▉         | 57/611 [00:01<00:12, 44.59it/s] 10%|█         | 62/611 [00:01<00:12, 44.78it/s] 11%|█         | 67/611 [00:01<00:12, 44.92it/s] 12%|█▏        | 72/611 [00:01<00:15, 34.06it/s] 13%|█▎        | 77/611 [00:01<00:14, 36.87it/s] 13%|█▎        | 82/611 [00:01<00:13, 38.98it/s] 14%|█▍        | 87/611 [00:02<00:12, 40.69it/s] 15%|█▌        | 92/611 [00:02<00:12, 41.89it/s] 16%|█▌        | 97/611 [00:02<00:11, 42.88it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.58it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.05it/s] 18%|█▊        | 112/611 [00:02<00:11, 43.76it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.00it/s] 20%|█▉        | 122/611 [00:02<00:11, 44.24it/s] 21%|██        | 127/611 [00:02<00:10, 44.56it/s] 22%|██▏       | 132/611 [00:03<00:10, 44.77it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.73it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.94it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.98it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.71it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.47it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.48it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.60it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.79it/s] 29%|██▉       | 177/611 [00:04<00:09, 44.88it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.96it/s] 31%|███       | 187/611 [00:04<00:09, 44.99it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.91it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.76it/s] 33%|███▎      | 202/611 [00:04<00:11, 36.10it/s] 34%|███▍      | 207/611 [00:04<00:10, 38.41it/s] 35%|███▍      | 212/611 [00:04<00:09, 40.25it/s] 36%|███▌      | 217/611 [00:05<00:09, 41.61it/s] 36%|███▋      | 222/611 [00:05<00:09, 42.60it/s] 37%|███▋      | 227/611 [00:05<00:08, 43.44it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.03it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.12it/s] 40%|███▉      | 242/611 [00:05<00:08, 43.93it/s] 40%|████      | 247/611 [00:05<00:08, 43.80it/s] 41%|████      | 252/611 [00:05<00:08, 44.02it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.26it/s] 43%|████▎     | 262/611 [00:06<00:07, 44.52it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.85it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.94it/s] 45%|████▌     | 277/611 [00:06<00:07, 45.07it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.92it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.54it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.31it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.30it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.47it/s] 50%|█████     | 307/611 [00:07<00:06, 44.70it/s] 51%|█████     | 312/611 [00:07<00:06, 44.73it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.83it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.86it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.69it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.39it/s] 55%|█████▌    | 337/611 [00:07<00:06, 41.60it/s] 56%|█████▌    | 342/611 [00:07<00:06, 42.64it/s] 57%|█████▋    | 347/611 [00:07<00:06, 43.46it/s] 58%|█████▊    | 352/611 [00:08<00:05, 43.87it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.24it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.59it/s] 60%|██████    | 367/611 [00:08<00:05, 44.62it/s] 61%|██████    | 372/611 [00:08<00:05, 44.56it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.21it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.12it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.27it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.45it/s] 65%|██████▍   | 397/611 [00:09<00:04, 44.80it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.94it/s] 67%|██████▋   | 407/611 [00:09<00:04, 45.04it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.94it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.67it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.37it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.30it/s] 71%|███████   | 432/611 [00:09<00:04, 44.32it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.52it/s] 72%|███████▏  | 442/611 [00:10<00:03, 44.74it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.77it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.94it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.90it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.63it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.43it/s] 77%|███████▋  | 472/611 [00:10<00:03, 35.32it/s] 78%|███████▊  | 477/611 [00:10<00:03, 37.82it/s] 79%|███████▉  | 482/611 [00:11<00:03, 39.76it/s] 80%|███████▉  | 487/611 [00:11<00:03, 41.18it/s] 81%|████████  | 492/611 [00:11<00:02, 42.38it/s] 81%|████████▏ | 497/611 [00:11<00:02, 43.10it/s] 82%|████████▏ | 502/611 [00:11<00:02, 43.59it/s] 83%|████████▎ | 507/611 [00:11<00:02, 43.90it/s] 84%|████████▍ | 512/611 [00:11<00:02, 43.66it/s] 85%|████████▍ | 517/611 [00:11<00:02, 43.60it/s] 85%|████████▌ | 522/611 [00:11<00:02, 43.81it/s] 86%|████████▋ | 527/611 [00:12<00:01, 44.17it/s] 87%|████████▋ | 532/611 [00:12<00:01, 44.55it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.63it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.77it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.84it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.58it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.24it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.12it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.10it/s] 94%|█████████▎| 572/611 [00:13<00:00, 44.35it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.46it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.74it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.69it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.80it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.66it/s] 99%|█████████▊| 602/611 [00:14<00:00, 23.68it/s] 99%|█████████▉| 607/611 [00:14<00:00, 27.60it/s]100%|██████████| 611/611 [00:14<00:00, 42.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:47:10,314 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   eval_loss               =     0.9869
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   eval_runtime            = 0:00:14.30
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   eval_samples_per_second =    341.251
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   eval_steps_per_second   =     42.709
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:47:10,314 >>   perplexity              =      2.683
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:26,468 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:26,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:26,537 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:26,537 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:26,537 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:47:27,256 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:47:27,257 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:47:27,836 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:47:28,866 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:47:28,867 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:30,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:31,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:31,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:31,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:47:31,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:47:31,516 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:47:31,517 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:47:31,900 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:47:32,061 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:47:32,061 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:13,  1.43it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:16,  1.39it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.66it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:26,  1.64it/s]Extractor Predicting: 43it [00:27,  1.64it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:28,  1.69it/s]Extractor Predicting: 46it [00:28,  1.68it/s]Extractor Predicting: 47it [00:29,  1.68it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:30,  1.65it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:32,  1.67it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:33,  1.60it/s]Extractor Predicting: 55it [00:34,  1.56it/s]Extractor Predicting: 56it [00:35,  1.54it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:39,  1.54it/s]Extractor Predicting: 64it [00:40,  1.55it/s]Extractor Predicting: 65it [00:40,  1.55it/s]Extractor Predicting: 66it [00:41,  1.52it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:43,  1.55it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.52it/s]Extractor Predicting: 73it [00:46,  1.52it/s]Extractor Predicting: 74it [00:46,  1.53it/s]Extractor Predicting: 75it [00:47,  1.55it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.53it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:51,  1.54it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:55,  1.47it/s]Extractor Predicting: 89it [00:56,  1.49it/s]Extractor Predicting: 90it [00:57,  1.50it/s]Extractor Predicting: 91it [00:57,  1.51it/s]Extractor Predicting: 92it [00:58,  1.58it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [01:00,  1.61it/s]Extractor Predicting: 96it [01:00,  1.62it/s]Extractor Predicting: 97it [01:01,  1.65it/s]Extractor Predicting: 98it [01:02,  1.58it/s]Extractor Predicting: 99it [01:02,  1.53it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:04,  1.49it/s]Extractor Predicting: 102it [01:04,  1.49it/s]Extractor Predicting: 103it [01:05,  1.48it/s]Extractor Predicting: 104it [01:06,  1.52it/s]Extractor Predicting: 105it [01:06,  1.54it/s]Extractor Predicting: 106it [01:07,  1.60it/s]Extractor Predicting: 107it [01:07,  1.61it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:09,  1.61it/s]Extractor Predicting: 110it [01:09,  1.62it/s]Extractor Predicting: 111it [01:10,  1.65it/s]Extractor Predicting: 112it [01:11,  1.65it/s]Extractor Predicting: 113it [01:11,  1.50it/s]Extractor Predicting: 114it [01:12,  1.52it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:13,  1.53it/s]Extractor Predicting: 117it [01:14,  1.52it/s]Extractor Predicting: 118it [01:15,  1.43it/s]Extractor Predicting: 119it [01:15,  1.45it/s]Extractor Predicting: 120it [01:16,  1.45it/s]Extractor Predicting: 121it [01:17,  1.48it/s]Extractor Predicting: 122it [01:17,  1.50it/s]Extractor Predicting: 123it [01:18,  1.46it/s]Extractor Predicting: 124it [01:19,  1.49it/s]Extractor Predicting: 125it [01:19,  1.52it/s]Extractor Predicting: 126it [01:20,  1.54it/s]Extractor Predicting: 127it [01:21,  1.40it/s]Extractor Predicting: 128it [01:22,  1.38it/s]Extractor Predicting: 129it [01:22,  1.45it/s]Extractor Predicting: 130it [01:23,  1.43it/s]Extractor Predicting: 131it [01:24,  1.47it/s]Extractor Predicting: 132it [01:24,  1.52it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:26,  1.52it/s]Extractor Predicting: 135it [01:26,  1.49it/s]Extractor Predicting: 136it [01:27,  1.52it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:28,  1.53it/s]Extractor Predicting: 139it [01:29,  1.52it/s]Extractor Predicting: 140it [01:30,  1.47it/s]Extractor Predicting: 141it [01:30,  1.50it/s]Extractor Predicting: 142it [01:31,  1.53it/s]Extractor Predicting: 143it [01:31,  1.51it/s]Extractor Predicting: 144it [01:32,  1.55it/s]Extractor Predicting: 145it [01:33,  1.49it/s]Extractor Predicting: 146it [01:33,  1.50it/s]Extractor Predicting: 147it [01:34,  1.50it/s]Extractor Predicting: 148it [01:35,  1.53it/s]Extractor Predicting: 149it [01:35,  1.52it/s]Extractor Predicting: 150it [01:36,  1.50it/s]Extractor Predicting: 151it [01:37,  1.50it/s]Extractor Predicting: 152it [01:37,  1.51it/s]Extractor Predicting: 153it [01:38,  1.51it/s]Extractor Predicting: 154it [01:39,  1.53it/s]Extractor Predicting: 155it [01:39,  1.48it/s]Extractor Predicting: 156it [01:40,  1.44it/s]Extractor Predicting: 157it [01:41,  1.42it/s]Extractor Predicting: 158it [01:42,  1.40it/s]Extractor Predicting: 159it [01:42,  1.44it/s]Extractor Predicting: 160it [01:43,  1.45it/s]Extractor Predicting: 161it [01:44,  1.48it/s]Extractor Predicting: 162it [01:44,  1.49it/s]Extractor Predicting: 163it [01:45,  1.50it/s]Extractor Predicting: 164it [01:46,  1.54it/s]Extractor Predicting: 165it [01:46,  1.49it/s]Extractor Predicting: 166it [01:47,  1.54it/s]Extractor Predicting: 167it [01:48,  1.54it/s]Extractor Predicting: 168it [01:48,  1.54it/s]Extractor Predicting: 169it [01:49,  1.56it/s]Extractor Predicting: 170it [01:50,  1.51it/s]Extractor Predicting: 171it [01:50,  1.55it/s]Extractor Predicting: 172it [01:51,  1.58it/s]Extractor Predicting: 173it [01:51,  1.54it/s]Extractor Predicting: 174it [01:52,  1.56it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:53,  1.54it/s]Extractor Predicting: 177it [01:54,  1.52it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:55,  1.53it/s]Extractor Predicting: 180it [01:56,  1.54it/s]Extractor Predicting: 181it [01:57,  1.53it/s]Extractor Predicting: 182it [01:57,  1.52it/s]Extractor Predicting: 183it [01:58,  1.48it/s]Extractor Predicting: 184it [01:59,  1.50it/s]Extractor Predicting: 185it [01:59,  1.62it/s]Extractor Predicting: 185it [01:59,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:55,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:55,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:55,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:55,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:55,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:49:56,766 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:49:56,767 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:49:57,290 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:49:58,594 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:49:58,889 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:02,222 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:02,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:02,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:02,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:50:03,761 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:50:03,762 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:50:04,513 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:50:04,681 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:50:04,681 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.31598667776852624,
  "recall": 0.15546907005325686,
  "score": 0.20840197693574955,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.44it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:19,  1.68it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:21,  1.66it/s]Extractor Predicting: 34it [00:21,  1.68it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:26,  1.64it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:27,  1.65it/s]Extractor Predicting: 44it [00:27,  1.68it/s]Extractor Predicting: 45it [00:28,  1.70it/s]Extractor Predicting: 46it [00:29,  1.74it/s]Extractor Predicting: 47it [00:29,  1.74it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:31,  1.64it/s]Extractor Predicting: 51it [00:32,  1.67it/s]Extractor Predicting: 52it [00:32,  1.68it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:33,  1.60it/s]Extractor Predicting: 55it [00:34,  1.61it/s]Extractor Predicting: 56it [00:35,  1.62it/s]Extractor Predicting: 57it [00:35,  1.66it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:39,  1.49it/s]Extractor Predicting: 64it [00:40,  1.48it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:41,  1.49it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.37it/s]Extractor Predicting: 69it [00:43,  1.43it/s]Extractor Predicting: 70it [00:44,  1.47it/s]Extractor Predicting: 71it [00:45,  1.51it/s]Extractor Predicting: 72it [00:45,  1.54it/s]Extractor Predicting: 73it [00:46,  1.46it/s]Extractor Predicting: 74it [00:47,  1.50it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:49,  1.61it/s]Extractor Predicting: 79it [00:50,  1.68it/s]Extractor Predicting: 80it [00:50,  1.72it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:51,  1.64it/s]Extractor Predicting: 83it [00:52,  1.61it/s]Extractor Predicting: 84it [00:53,  1.61it/s]Extractor Predicting: 85it [00:53,  1.58it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.56it/s]Extractor Predicting: 90it [00:57,  1.59it/s]Extractor Predicting: 91it [00:57,  1.51it/s]Extractor Predicting: 92it [00:58,  1.52it/s]Extractor Predicting: 93it [00:59,  1.56it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.58it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:01,  1.55it/s]Extractor Predicting: 98it [01:02,  1.57it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:04,  1.52it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.58it/s]Extractor Predicting: 105it [01:06,  1.60it/s]Extractor Predicting: 106it [01:07,  1.56it/s]Extractor Predicting: 107it [01:08,  1.56it/s]Extractor Predicting: 108it [01:08,  1.60it/s]Extractor Predicting: 109it [01:09,  1.59it/s]Extractor Predicting: 110it [01:09,  1.62it/s]Extractor Predicting: 111it [01:10,  1.59it/s]Extractor Predicting: 112it [01:11,  1.58it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:13,  1.54it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:14,  1.58it/s]Extractor Predicting: 119it [01:15,  1.58it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:16,  1.55it/s]Extractor Predicting: 122it [01:17,  1.57it/s]Extractor Predicting: 123it [01:18,  1.57it/s]Extractor Predicting: 124it [01:18,  1.52it/s]Extractor Predicting: 125it [01:19,  1.55it/s]Extractor Predicting: 126it [01:20,  1.56it/s]Extractor Predicting: 127it [01:20,  1.61it/s]Extractor Predicting: 128it [01:21,  1.56it/s]Extractor Predicting: 129it [01:22,  1.56it/s]Extractor Predicting: 130it [01:22,  1.60it/s]Extractor Predicting: 131it [01:23,  1.58it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:24,  1.58it/s]Extractor Predicting: 134it [01:25,  1.55it/s]Extractor Predicting: 135it [01:25,  1.57it/s]Extractor Predicting: 136it [01:26,  1.44it/s]Extractor Predicting: 137it [01:27,  1.44it/s]Extractor Predicting: 138it [01:27,  1.49it/s]Extractor Predicting: 139it [01:28,  1.52it/s]Extractor Predicting: 140it [01:29,  1.54it/s]Extractor Predicting: 141it [01:29,  1.55it/s]Extractor Predicting: 142it [01:30,  1.58it/s]Extractor Predicting: 143it [01:31,  1.50it/s]Extractor Predicting: 144it [01:31,  1.49it/s]Extractor Predicting: 145it [01:32,  1.49it/s]Extractor Predicting: 146it [01:33,  1.54it/s]Extractor Predicting: 147it [01:33,  1.60it/s]Extractor Predicting: 148it [01:34,  1.59it/s]Extractor Predicting: 149it [01:35,  1.56it/s]Extractor Predicting: 150it [01:35,  1.59it/s]Extractor Predicting: 151it [01:36,  1.63it/s]Extractor Predicting: 152it [01:36,  1.60it/s]Extractor Predicting: 153it [01:37,  1.62it/s]Extractor Predicting: 154it [01:38,  1.53it/s]Extractor Predicting: 155it [01:38,  1.54it/s]Extractor Predicting: 156it [01:39,  1.61it/s]Extractor Predicting: 157it [01:40,  1.58it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:41,  1.51it/s]Extractor Predicting: 160it [01:42,  1.54it/s]Extractor Predicting: 161it [01:42,  1.59it/s]Extractor Predicting: 162it [01:43,  1.59it/s]Extractor Predicting: 163it [01:43,  1.60it/s]Extractor Predicting: 164it [01:44,  1.41it/s]Extractor Predicting: 165it [01:45,  1.42it/s]Extractor Predicting: 166it [01:46,  1.44it/s]Extractor Predicting: 167it [01:46,  1.47it/s]Extractor Predicting: 168it [01:47,  1.51it/s]Extractor Predicting: 169it [01:48,  1.50it/s]Extractor Predicting: 170it [01:48,  1.54it/s]Extractor Predicting: 171it [01:49,  1.57it/s]Extractor Predicting: 172it [01:49,  1.61it/s]Extractor Predicting: 173it [01:50,  1.62it/s]Extractor Predicting: 174it [01:51,  1.60it/s]Extractor Predicting: 175it [01:51,  1.60it/s]Extractor Predicting: 176it [01:52,  1.62it/s]Extractor Predicting: 177it [01:53,  1.57it/s]Extractor Predicting: 178it [01:53,  1.57it/s]Extractor Predicting: 179it [01:54,  1.55it/s]Extractor Predicting: 180it [01:54,  1.59it/s]Extractor Predicting: 181it [01:55,  1.60it/s]Extractor Predicting: 182it [01:56,  1.59it/s]Extractor Predicting: 183it [01:56,  1.63it/s]Extractor Predicting: 184it [01:57,  1.63it/s]Extractor Predicting: 185it [01:57,  1.64it/s]Extractor Predicting: 186it [01:58,  1.59it/s]Extractor Predicting: 187it [01:59,  1.57it/s]Extractor Predicting: 188it [01:59,  1.58it/s]Extractor Predicting: 189it [02:00,  1.59it/s]Extractor Predicting: 190it [02:01,  1.62it/s]Extractor Predicting: 191it [02:01,  1.63it/s]Extractor Predicting: 192it [02:02,  1.65it/s]Extractor Predicting: 193it [02:02,  1.64it/s]Extractor Predicting: 194it [02:03,  1.65it/s]Extractor Predicting: 195it [02:04,  1.63it/s]Extractor Predicting: 196it [02:04,  1.64it/s]Extractor Predicting: 197it [02:05,  1.55it/s]Extractor Predicting: 198it [02:06,  1.56it/s]Extractor Predicting: 199it [02:06,  1.56it/s]Extractor Predicting: 200it [02:07,  1.59it/s]Extractor Predicting: 201it [02:08,  1.61it/s]Extractor Predicting: 202it [02:08,  1.58it/s]Extractor Predicting: 203it [02:09,  1.61it/s]Extractor Predicting: 204it [02:09,  1.62it/s]Extractor Predicting: 205it [02:10,  1.63it/s]Extractor Predicting: 206it [02:11,  1.63it/s]Extractor Predicting: 207it [02:11,  1.45it/s]Extractor Predicting: 208it [02:12,  1.49it/s]Extractor Predicting: 209it [02:13,  1.50it/s]Extractor Predicting: 210it [02:13,  1.59it/s]Extractor Predicting: 211it [02:14,  1.59it/s]Extractor Predicting: 212it [02:15,  1.56it/s]Extractor Predicting: 213it [02:15,  1.59it/s]Extractor Predicting: 214it [02:16,  1.64it/s]Extractor Predicting: 215it [02:16,  1.63it/s]Extractor Predicting: 216it [02:17,  1.61it/s]Extractor Predicting: 217it [02:18,  1.56it/s]Extractor Predicting: 218it [02:18,  1.56it/s]Extractor Predicting: 219it [02:19,  1.58it/s]Extractor Predicting: 220it [02:20,  1.60it/s]Extractor Predicting: 221it [02:20,  1.62it/s]Extractor Predicting: 222it [02:21,  1.57it/s]Extractor Predicting: 223it [02:22,  1.52it/s]Extractor Predicting: 224it [02:22,  1.55it/s]Extractor Predicting: 225it [02:23,  1.51it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:24,  1.59it/s]Extractor Predicting: 228it [02:25,  1.63it/s]Extractor Predicting: 229it [02:25,  1.66it/s]Extractor Predicting: 230it [02:26,  1.63it/s]Extractor Predicting: 231it [02:26,  1.65it/s]Extractor Predicting: 232it [02:27,  1.65it/s]Extractor Predicting: 233it [02:28,  1.66it/s]Extractor Predicting: 234it [02:28,  1.66it/s]Extractor Predicting: 235it [02:29,  1.59it/s]Extractor Predicting: 236it [02:30,  1.61it/s]Extractor Predicting: 237it [02:30,  1.63it/s]Extractor Predicting: 238it [02:31,  1.60it/s]Extractor Predicting: 239it [02:31,  1.63it/s]Extractor Predicting: 240it [02:32,  1.56it/s]Extractor Predicting: 241it [02:33,  1.59it/s]Extractor Predicting: 242it [02:33,  1.58it/s]Extractor Predicting: 243it [02:34,  1.55it/s]Extractor Predicting: 244it [02:35,  1.57it/s]Extractor Predicting: 245it [02:35,  1.59it/s]Extractor Predicting: 246it [02:36,  1.60it/s]Extractor Predicting: 247it [02:36,  1.64it/s]Extractor Predicting: 248it [02:37,  1.58it/s]Extractor Predicting: 249it [02:38,  1.57it/s]Extractor Predicting: 250it [02:39,  1.35it/s]Extractor Predicting: 251it [02:39,  1.42it/s]Extractor Predicting: 252it [02:40,  1.46it/s]Extractor Predicting: 253it [02:41,  1.46it/s]Extractor Predicting: 254it [02:41,  1.46it/s]Extractor Predicting: 255it [02:42,  1.50it/s]Extractor Predicting: 256it [02:43,  1.53it/s]Extractor Predicting: 257it [02:43,  1.40it/s]Extractor Predicting: 258it [02:44,  1.44it/s]Extractor Predicting: 259it [02:45,  1.48it/s]Extractor Predicting: 260it [02:46,  1.39it/s]Extractor Predicting: 261it [02:46,  1.45it/s]Extractor Predicting: 262it [02:47,  1.48it/s]Extractor Predicting: 263it [02:47,  1.50it/s]Extractor Predicting: 264it [02:48,  1.52it/s]Extractor Predicting: 265it [02:49,  1.53it/s]Extractor Predicting: 266it [02:49,  1.50it/s]Extractor Predicting: 267it [02:50,  1.51it/s]Extractor Predicting: 268it [02:51,  1.45it/s]Extractor Predicting: 269it [02:51,  1.49it/s]Extractor Predicting: 270it [02:52,  1.49it/s]Extractor Predicting: 271it [02:53,  1.50it/s]Extractor Predicting: 272it [02:53,  1.51it/s]Extractor Predicting: 273it [02:54,  1.52it/s]Extractor Predicting: 274it [02:55,  1.50it/s]Extractor Predicting: 275it [02:55,  1.52it/s]Extractor Predicting: 276it [02:56,  1.53it/s]Extractor Predicting: 277it [02:57,  1.55it/s]Extractor Predicting: 278it [02:57,  1.57it/s]Extractor Predicting: 279it [02:58,  1.55it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [02:59,  1.48it/s]Extractor Predicting: 282it [03:00,  1.49it/s]Extractor Predicting: 283it [03:01,  1.52it/s]Extractor Predicting: 284it [03:01,  1.55it/s]Extractor Predicting: 285it [03:02,  1.55it/s]Extractor Predicting: 286it [03:03,  1.57it/s]Extractor Predicting: 287it [03:03,  1.53it/s]Extractor Predicting: 288it [03:04,  1.56it/s]Extractor Predicting: 289it [03:05,  1.51it/s]Extractor Predicting: 290it [03:05,  1.50it/s]Extractor Predicting: 291it [03:06,  1.48it/s]Extractor Predicting: 292it [03:07,  1.53it/s]Extractor Predicting: 293it [03:07,  1.53it/s]Extractor Predicting: 294it [03:08,  1.52it/s]Extractor Predicting: 295it [03:08,  1.52it/s]Extractor Predicting: 296it [03:09,  1.50it/s]Extractor Predicting: 297it [03:10,  1.52it/s]Extractor Predicting: 298it [03:10,  1.51it/s]Extractor Predicting: 299it [03:11,  1.53it/s]Extractor Predicting: 300it [03:12,  1.52it/s]Extractor Predicting: 301it [03:12,  1.55it/s]Extractor Predicting: 302it [03:13,  1.52it/s]Extractor Predicting: 303it [03:14,  1.56it/s]Extractor Predicting: 304it [03:14,  1.55it/s]Extractor Predicting: 305it [03:15,  1.59it/s]Extractor Predicting: 306it [03:16,  1.62it/s]Extractor Predicting: 307it [03:16,  1.60it/s]Extractor Predicting: 308it [03:17,  1.62it/s]Extractor Predicting: 309it [03:17,  1.58it/s]Extractor Predicting: 310it [03:18,  1.59it/s]Extractor Predicting: 311it [03:19,  1.55it/s]Extractor Predicting: 312it [03:19,  1.57it/s]Extractor Predicting: 313it [03:20,  1.45it/s]Extractor Predicting: 314it [03:21,  1.48it/s]Extractor Predicting: 315it [03:21,  1.52it/s]Extractor Predicting: 316it [03:22,  1.56it/s]Extractor Predicting: 317it [03:23,  1.58it/s]Extractor Predicting: 318it [03:23,  1.61it/s]Extractor Predicting: 319it [03:24,  1.58it/s]Extractor Predicting: 320it [03:25,  1.55it/s]Extractor Predicting: 321it [03:25,  1.54it/s]Extractor Predicting: 322it [03:26,  1.56it/s]Extractor Predicting: 323it [03:27,  1.50it/s]Extractor Predicting: 324it [03:27,  1.52it/s]Extractor Predicting: 325it [03:28,  1.56it/s]Extractor Predicting: 326it [03:28,  1.56it/s]Extractor Predicting: 327it [03:29,  1.58it/s]Extractor Predicting: 328it [03:30,  1.54it/s]Extractor Predicting: 329it [03:30,  1.52it/s]Extractor Predicting: 330it [03:31,  1.51it/s]Extractor Predicting: 331it [03:32,  1.54it/s]Extractor Predicting: 332it [03:32,  1.54it/s]Extractor Predicting: 333it [03:33,  1.52it/s]Extractor Predicting: 334it [03:34,  1.54it/s]Extractor Predicting: 335it [03:34,  1.53it/s]Extractor Predicting: 336it [03:35,  1.45it/s]Extractor Predicting: 337it [03:36,  1.47it/s]Extractor Predicting: 338it [03:36,  1.47it/s]Extractor Predicting: 339it [03:37,  1.44it/s]Extractor Predicting: 340it [03:38,  1.46it/s]Extractor Predicting: 341it [03:39,  1.46it/s]Extractor Predicting: 342it [03:39,  1.47it/s]Extractor Predicting: 343it [03:40,  1.50it/s]Extractor Predicting: 344it [03:41,  1.46it/s]Extractor Predicting: 345it [03:42,  1.28it/s]Extractor Predicting: 346it [03:42,  1.32it/s]Extractor Predicting: 347it [03:43,  1.38it/s]Extractor Predicting: 348it [03:44,  1.43it/s]Extractor Predicting: 349it [03:44,  1.46it/s]Extractor Predicting: 350it [03:45,  1.47it/s]Extractor Predicting: 351it [03:46,  1.48it/s]Extractor Predicting: 352it [03:46,  1.46it/s]Extractor Predicting: 353it [03:47,  1.45it/s]Extractor Predicting: 354it [03:48,  1.48it/s]Extractor Predicting: 355it [03:48,  1.52it/s]Extractor Predicting: 356it [03:49,  1.52it/s]Extractor Predicting: 357it [03:50,  1.53it/s]Extractor Predicting: 358it [03:50,  1.55it/s]Extractor Predicting: 359it [03:51,  1.49it/s]Extractor Predicting: 360it [03:52,  1.48it/s]Extractor Predicting: 361it [03:52,  1.49it/s]Extractor Predicting: 362it [03:53,  1.50it/s]Extractor Predicting: 363it [03:53,  1.55it/s]Extractor Predicting: 364it [03:54,  1.57it/s]Extractor Predicting: 365it [03:55,  1.55it/s]Extractor Predicting: 366it [03:55,  1.51it/s]Extractor Predicting: 367it [03:56,  1.44it/s]Extractor Predicting: 368it [03:57,  1.44it/s]Extractor Predicting: 369it [03:58,  1.41it/s]Extractor Predicting: 370it [03:58,  1.43it/s]Extractor Predicting: 371it [03:59,  1.47it/s]Extractor Predicting: 372it [03:59,  1.75it/s]Extractor Predicting: 372it [03:59,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:33,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:54:35,666 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:54:35,667 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:54:36,535 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:54:37,972 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:54:37,972 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:43,108 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:43,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:43,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:43,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:43,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:54:44,212 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:54:44,213 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:54:45,660 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:54:45,819 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:54:45,819 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27256994519757716,
  "recall": 0.21207360861759425,
  "score": 0.23854600530102232,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:08,  1.66it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:10,  1.49it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:14,  1.49it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.41it/s]Extractor Predicting: 39it [00:25,  1.40it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:26,  1.43it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.41it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:31,  1.46it/s]Extractor Predicting: 48it [00:31,  1.45it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:34,  1.44it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:35,  1.44it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.47it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.41it/s]Extractor Predicting: 62it [00:41,  1.42it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 63it [00:41,  1.50it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:55:33,322 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:55:33,452 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:55:33,622 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:55:33,623 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:55:33,688 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:55:58,353 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:55:58,594 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:55:59,121 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:55:59,122 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:55:59,405 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:55:59,651 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45652173913043476,
  "recall": 0.15732694036559786,
  "score": 0.234009360374415,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:56:00,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:00,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:01,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:02,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:02,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:03,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:03,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:04,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:04,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:05,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:06,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:06,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:07,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:08,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:08,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:09,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:10,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:10,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:11,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:11,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:12,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:13,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:07, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:13,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:14,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:15,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:15,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:16,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:16,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:17,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:17,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:18,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:18,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:19,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:19,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:20,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:21,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:21,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:22,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:22,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:23,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:23,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:24,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:24,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:24<02:39, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:25,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:25,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:26,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:26,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:27,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:27,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:28,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:28,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:29,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:29,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:30,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:30,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:31,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:31,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:32,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:32,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:33,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:33,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:34,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:34<02:11, 10.98s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:34,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:35,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:35,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:36,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:37,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:38,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:38,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:39,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:39,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:40,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:41,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:42,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:42,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:43,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:43,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:44,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:44,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:45,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:45,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:46<02:04, 11.31s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:46,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:47,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:47,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:48,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:48,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:49,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:49,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:50,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:51,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:51,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:52,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:52,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:53,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:54,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:54,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:55,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:55,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:56,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:57,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:57,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:58,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:58,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [00:59<01:58, 11.89s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:59,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:00,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:00,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:01,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:01,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:02,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:03,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:03,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:04,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:04,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:05,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:06,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:06,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:07,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:07,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:08,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:09,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:09,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:10,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:11,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:11,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:11<01:49, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:12,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:12,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:13,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:14,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:14,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:15,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:15,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:16,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:16,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:17,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:18,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:18,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:19,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:19,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:20,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:20,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:21,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:22,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:22,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:23<01:35, 11.89s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:23,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:24,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:24,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:25,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:25,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:27,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:27,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:28,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:28,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:29,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:29,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:30,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:30,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:31,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:32,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:32,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:33,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:33,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:34,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:34<01:22, 11.74s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:34,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:35,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:36,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:37,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:37,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:38,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:39,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:39,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:40,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:40,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:41,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:41,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:42,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:43,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:43,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:44,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:44,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:45,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:46,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:46,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:47,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:47<01:12, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:47,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:48,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:49,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:49,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:50,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:50,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:51,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:52,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:52,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:53,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:53,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:54,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:55,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:55,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:56,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:56,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:57,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:58,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:58,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:59,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [01:59<01:00, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:00,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:00,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:00,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:01,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:02,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:02,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:02,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:03,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:03,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:04,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:04,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:05,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:06,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:06,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:07,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:07,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:07,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:08,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:09,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:09<00:45, 11.37s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:10,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:10,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:11,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:12,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:12,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:13,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:13,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:14,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:15,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:15,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:16,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:16,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:17,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:17,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:18,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:18,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:19,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:20,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:20,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:21<00:34, 11.54s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:21,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:22,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:22,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:23,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:24,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:24,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:25,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:25,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:26,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:26,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:27,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:28,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:28,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:29,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:29,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:30,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:31,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:31,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:32,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:32,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:33,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:33<00:23, 11.84s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:34,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:34,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:35,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:35,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:36,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:37,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:37,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:38,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:38,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:39,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:39,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:40,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:40,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:41,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:41,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:42,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:42,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:43,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:43<00:11, 11.15s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:43,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:44,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:44,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:45,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:46,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:47,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:48,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:48,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:49,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:49,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:50,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:51,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:51,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:52,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:53,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:53,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:54,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:54,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:55,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:56,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [02:56<00:00, 11.81s/it]Generating: 100%|██████████| 15/15 [02:56<00:00, 11.77s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:07,080 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:07,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:07,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:07,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:07,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:59:08,203 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:59:08,204 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:59:08,909 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:59:10,020 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:59:10,020 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:12,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:12,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:12,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:12,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:59:12,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:59:12,686 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:59:12,687 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:59:13,057 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:59:13,237 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:59:13,237 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : conflict . Context : On 17 March 2014 , the United States and its allies launched an air strike on a convoy of armoured vehicle s in the eastern part of the country . Head Entity : United States , Tail Entity : Ukraine .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 382, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 539, 'raw': 544}
{'target': 600, 'success': 570, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 445, 'raw': 448}
{'target': 600, 'success': 476, 'raw': 480}
{'target': 600, 'success': 506, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : creator .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.95, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : occupation .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 7335
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7435, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.69it/s]Extractor Estimating: 4it [00:02,  1.64it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.74it/s]Extractor Estimating: 7it [00:04,  1.80it/s]Extractor Estimating: 8it [00:04,  1.79it/s]Extractor Estimating: 9it [00:05,  1.81it/s]Extractor Estimating: 10it [00:05,  1.81it/s]Extractor Estimating: 11it [00:06,  1.80it/s]Extractor Estimating: 12it [00:06,  1.76it/s]Extractor Estimating: 13it [00:07,  1.65it/s]Extractor Estimating: 14it [00:08,  1.68it/s]Extractor Estimating: 15it [00:08,  1.68it/s]Extractor Estimating: 16it [00:09,  1.73it/s]Extractor Estimating: 17it [00:09,  1.72it/s]Extractor Estimating: 18it [00:10,  1.70it/s]Extractor Estimating: 19it [00:10,  1.76it/s]Extractor Estimating: 20it [00:11,  1.78it/s]Extractor Estimating: 21it [00:12,  1.76it/s]Extractor Estimating: 22it [00:12,  1.68it/s]Extractor Estimating: 23it [00:13,  1.70it/s]Extractor Estimating: 24it [00:13,  1.66it/s]Extractor Estimating: 25it [00:14,  1.64it/s]Extractor Estimating: 26it [00:15,  1.53it/s]Extractor Estimating: 27it [00:15,  1.66it/s]Extractor Estimating: 28it [00:16,  1.68it/s]Extractor Estimating: 29it [00:17,  1.68it/s]Extractor Estimating: 30it [00:17,  1.64it/s]Extractor Estimating: 31it [00:18,  1.58it/s]Extractor Estimating: 32it [00:18,  1.63it/s]Extractor Estimating: 33it [00:19,  1.65it/s]Extractor Estimating: 34it [00:20,  1.68it/s]Extractor Estimating: 35it [00:20,  1.68it/s]Extractor Estimating: 36it [00:21,  1.70it/s]Extractor Estimating: 37it [00:21,  1.70it/s]Extractor Estimating: 38it [00:22,  1.73it/s]Extractor Estimating: 39it [00:22,  1.72it/s]Extractor Estimating: 40it [00:23,  1.71it/s]Extractor Estimating: 41it [00:24,  1.66it/s]Extractor Estimating: 42it [00:24,  1.66it/s]Extractor Estimating: 43it [00:25,  1.70it/s]Extractor Estimating: 44it [00:25,  1.69it/s]Extractor Estimating: 45it [00:26,  1.72it/s]Extractor Estimating: 46it [00:27,  1.76it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.67it/s]Extractor Estimating: 49it [00:28,  1.67it/s]Extractor Estimating: 50it [00:29,  1.69it/s]Extractor Estimating: 51it [00:30,  1.76it/s]Extractor Estimating: 52it [00:30,  1.88it/s]Extractor Estimating: 53it [00:30,  1.99it/s]Extractor Estimating: 54it [00:31,  2.11it/s]Extractor Estimating: 55it [00:31,  2.14it/s]Extractor Estimating: 56it [00:32,  2.13it/s]Extractor Estimating: 57it [00:32,  2.18it/s]Extractor Estimating: 58it [00:33,  2.14it/s]Extractor Estimating: 59it [00:33,  2.06it/s]Extractor Estimating: 60it [00:34,  2.10it/s]Extractor Estimating: 61it [00:34,  2.09it/s]Extractor Estimating: 62it [00:35,  1.91it/s]Extractor Estimating: 63it [00:35,  1.99it/s]Extractor Estimating: 64it [00:36,  2.01it/s]Extractor Estimating: 65it [00:36,  1.91it/s]Extractor Estimating: 66it [00:37,  1.97it/s]Extractor Estimating: 67it [00:37,  2.05it/s]Extractor Estimating: 68it [00:38,  1.97it/s]Extractor Estimating: 69it [00:38,  1.99it/s]Extractor Estimating: 70it [00:39,  2.11it/s]Extractor Estimating: 71it [00:39,  2.13it/s]Extractor Estimating: 72it [00:40,  2.17it/s]Extractor Estimating: 73it [00:40,  2.21it/s]Extractor Estimating: 74it [00:40,  2.21it/s]Extractor Estimating: 75it [00:41,  2.11it/s]Extractor Estimating: 76it [00:41,  2.01it/s]Extractor Estimating: 77it [00:42,  1.90it/s]Extractor Estimating: 78it [00:43,  1.87it/s]Extractor Estimating: 79it [00:43,  1.75it/s]Extractor Estimating: 80it [00:44,  1.72it/s]Extractor Estimating: 81it [00:44,  1.74it/s]Extractor Estimating: 82it [00:45,  1.72it/s]Extractor Estimating: 83it [00:46,  1.80it/s]Extractor Estimating: 84it [00:46,  1.86it/s]Extractor Estimating: 85it [00:47,  1.88it/s]Extractor Estimating: 86it [00:47,  1.88it/s]Extractor Estimating: 87it [00:48,  1.89it/s]Extractor Estimating: 88it [00:48,  1.74it/s]Extractor Estimating: 89it [00:49,  1.73it/s]Extractor Estimating: 90it [00:49,  1.79it/s]Extractor Estimating: 91it [00:50,  1.59it/s]Extractor Estimating: 92it [00:51,  1.64it/s]Extractor Estimating: 93it [00:51,  1.69it/s]Extractor Estimating: 94it [00:52,  1.76it/s]Extractor Estimating: 95it [00:52,  1.75it/s]Extractor Estimating: 96it [00:53,  1.64it/s]Extractor Estimating: 97it [00:54,  1.72it/s]Extractor Estimating: 98it [00:54,  1.80it/s]Extractor Estimating: 99it [00:55,  1.82it/s]Extractor Estimating: 100it [00:55,  1.78it/s]Extractor Estimating: 101it [00:56,  1.72it/s]Extractor Estimating: 102it [00:56,  1.72it/s]Extractor Estimating: 103it [00:57,  1.71it/s]Extractor Estimating: 104it [00:58,  1.67it/s]Extractor Estimating: 105it [00:58,  1.68it/s]Extractor Estimating: 106it [00:59,  1.66it/s]Extractor Estimating: 107it [00:59,  1.68it/s]Extractor Estimating: 108it [01:00,  1.63it/s]Extractor Estimating: 109it [01:01,  1.62it/s]Extractor Estimating: 110it [01:01,  1.68it/s]Extractor Estimating: 111it [01:02,  1.69it/s]Extractor Estimating: 112it [01:02,  1.67it/s]Extractor Estimating: 113it [01:03,  1.72it/s]Extractor Estimating: 114it [01:04,  1.65it/s]Extractor Estimating: 115it [01:04,  1.63it/s]Extractor Estimating: 116it [01:05,  1.64it/s]Extractor Estimating: 117it [01:06,  1.64it/s]Extractor Estimating: 118it [01:06,  1.63it/s]Extractor Estimating: 119it [01:07,  1.63it/s]Extractor Estimating: 120it [01:07,  1.63it/s]Extractor Estimating: 121it [01:08,  1.68it/s]Extractor Estimating: 122it [01:09,  1.70it/s]Extractor Estimating: 123it [01:09,  1.66it/s]Extractor Estimating: 124it [01:10,  1.70it/s]Extractor Estimating: 125it [01:10,  1.69it/s]Extractor Estimating: 126it [01:11,  1.68it/s]Extractor Estimating: 127it [01:12,  1.65it/s]Extractor Estimating: 128it [01:12,  1.66it/s]Extractor Estimating: 129it [01:13,  1.65it/s]Extractor Estimating: 130it [01:13,  1.66it/s]Extractor Estimating: 131it [01:14,  1.59it/s]Extractor Estimating: 132it [01:15,  1.60it/s]Extractor Estimating: 133it [01:15,  1.65it/s]Extractor Estimating: 134it [01:16,  1.70it/s]Extractor Estimating: 135it [01:16,  1.68it/s]Extractor Estimating: 136it [01:17,  1.64it/s]Extractor Estimating: 137it [01:18,  1.69it/s]Extractor Estimating: 138it [01:18,  1.67it/s]Extractor Estimating: 139it [01:19,  1.69it/s]Extractor Estimating: 140it [01:19,  1.61it/s]Extractor Estimating: 141it [01:20,  1.59it/s]Extractor Estimating: 142it [01:21,  1.60it/s]Extractor Estimating: 143it [01:21,  1.61it/s]Extractor Estimating: 144it [01:22,  1.62it/s]Extractor Estimating: 145it [01:22,  1.65it/s]Extractor Estimating: 146it [01:23,  1.70it/s]Extractor Estimating: 147it [01:24,  1.71it/s]Extractor Estimating: 148it [01:24,  1.52it/s]Extractor Estimating: 149it [01:25,  1.58it/s]Extractor Estimating: 150it [01:26,  1.58it/s]Extractor Estimating: 151it [01:26,  1.63it/s]Extractor Estimating: 152it [01:27,  1.68it/s]Extractor Estimating: 153it [01:27,  1.65it/s]Extractor Estimating: 154it [01:28,  1.67it/s]Extractor Estimating: 155it [01:29,  1.69it/s]Extractor Estimating: 156it [01:29,  1.68it/s]Extractor Estimating: 157it [01:30,  1.67it/s]Extractor Estimating: 158it [01:30,  1.68it/s]Extractor Estimating: 159it [01:31,  1.72it/s]Extractor Estimating: 160it [01:32,  1.65it/s]Extractor Estimating: 161it [01:32,  1.64it/s]Extractor Estimating: 162it [01:33,  1.67it/s]Extractor Estimating: 163it [01:33,  1.67it/s]Extractor Estimating: 164it [01:34,  1.68it/s]Extractor Estimating: 165it [01:35,  1.63it/s]Extractor Estimating: 166it [01:35,  1.65it/s]Extractor Estimating: 167it [01:36,  1.67it/s]Extractor Estimating: 168it [01:37,  1.53it/s]Extractor Estimating: 169it [01:37,  1.55it/s]Extractor Estimating: 170it [01:38,  1.61it/s]Extractor Estimating: 171it [01:38,  1.62it/s]Extractor Estimating: 172it [01:39,  1.62it/s]Extractor Estimating: 173it [01:40,  1.58it/s]Extractor Estimating: 174it [01:40,  1.61it/s]Extractor Estimating: 175it [01:41,  1.63it/s]Extractor Estimating: 176it [01:41,  1.58it/s]Extractor Estimating: 177it [01:42,  1.58it/s]Extractor Estimating: 178it [01:43,  1.62it/s]Extractor Estimating: 179it [01:43,  1.64it/s]Extractor Estimating: 180it [01:44,  1.67it/s]Extractor Estimating: 181it [01:44,  1.66it/s]Extractor Estimating: 182it [01:45,  1.58it/s]Extractor Estimating: 183it [01:46,  1.62it/s]Extractor Estimating: 184it [01:46,  1.65it/s]Extractor Estimating: 185it [01:47,  1.65it/s]Extractor Estimating: 186it [01:48,  1.68it/s]Extractor Estimating: 187it [01:48,  1.71it/s]Extractor Estimating: 188it [01:49,  1.72it/s]Extractor Estimating: 189it [01:49,  1.76it/s]Extractor Estimating: 190it [01:50,  1.75it/s]Extractor Estimating: 191it [01:51,  1.47it/s]Extractor Estimating: 192it [01:51,  1.56it/s]Extractor Estimating: 193it [01:52,  1.61it/s]Extractor Estimating: 194it [01:52,  1.66it/s]Extractor Estimating: 195it [01:53,  1.69it/s]Extractor Estimating: 196it [01:54,  1.67it/s]Extractor Estimating: 197it [01:54,  1.66it/s]Extractor Estimating: 198it [01:55,  1.63it/s]Extractor Estimating: 199it [01:55,  1.61it/s]Extractor Estimating: 200it [01:56,  1.59it/s]Extractor Estimating: 201it [01:57,  1.61it/s]Extractor Estimating: 202it [01:57,  1.60it/s]Extractor Estimating: 203it [01:58,  1.62it/s]Extractor Estimating: 204it [01:59,  1.67it/s]Extractor Estimating: 205it [01:59,  1.69it/s]Extractor Estimating: 206it [02:00,  1.68it/s]Extractor Estimating: 207it [02:00,  1.60it/s]Extractor Estimating: 208it [02:01,  1.56it/s]Extractor Estimating: 209it [02:02,  1.60it/s]Extractor Estimating: 210it [02:02,  1.63it/s]Extractor Estimating: 211it [02:03,  1.66it/s]Extractor Estimating: 212it [02:03,  1.68it/s]Extractor Estimating: 213it [02:04,  1.73it/s]Extractor Estimating: 214it [02:04,  1.74it/s]Extractor Estimating: 215it [02:05,  1.71it/s]Extractor Estimating: 216it [02:06,  1.71it/s]Extractor Estimating: 217it [02:06,  1.66it/s]Extractor Estimating: 218it [02:07,  1.64it/s]Extractor Estimating: 219it [02:08,  1.66it/s]Extractor Estimating: 220it [02:08,  1.65it/s]Extractor Estimating: 221it [02:09,  1.67it/s]Extractor Estimating: 222it [02:09,  1.64it/s]Extractor Estimating: 223it [02:10,  1.69it/s]Extractor Estimating: 224it [02:10,  1.72it/s]Extractor Estimating: 225it [02:11,  1.60it/s]Extractor Estimating: 226it [02:12,  1.58it/s]Extractor Estimating: 227it [02:12,  1.57it/s]Extractor Estimating: 228it [02:13,  1.64it/s]Extractor Estimating: 229it [02:14,  1.70it/s]Extractor Estimating: 230it [02:14,  1.67it/s]Extractor Estimating: 231it [02:15,  1.64it/s]Extractor Estimating: 232it [02:15,  1.63it/s]Extractor Estimating: 233it [02:16,  1.55it/s]Extractor Estimating: 234it [02:17,  1.58it/s]Extractor Estimating: 235it [02:17,  1.61it/s]Extractor Estimating: 236it [02:18,  1.64it/s]Extractor Estimating: 237it [02:19,  1.62it/s]Extractor Estimating: 238it [02:19,  1.60it/s]Extractor Estimating: 239it [02:20,  1.58it/s]Extractor Estimating: 240it [02:21,  1.56it/s]Extractor Estimating: 241it [02:21,  1.51it/s]Extractor Estimating: 242it [02:22,  1.56it/s]Extractor Estimating: 243it [02:23,  1.55it/s]Extractor Estimating: 244it [02:23,  1.60it/s]Extractor Estimating: 245it [02:24,  1.63it/s]Extractor Estimating: 246it [02:24,  1.61it/s]Extractor Estimating: 247it [02:25,  1.61it/s]Extractor Estimating: 248it [02:26,  1.60it/s]Extractor Estimating: 249it [02:26,  1.61it/s]Extractor Estimating: 250it [02:27,  1.54it/s]Extractor Estimating: 251it [02:28,  1.56it/s]Extractor Estimating: 252it [02:28,  1.56it/s]Extractor Estimating: 253it [02:29,  1.58it/s]Extractor Estimating: 254it [02:29,  1.59it/s]Extractor Estimating: 255it [02:30,  1.60it/s]Extractor Estimating: 256it [02:31,  1.61it/s]Extractor Estimating: 257it [02:31,  1.60it/s]Extractor Estimating: 258it [02:32,  1.53it/s]Extractor Estimating: 259it [02:33,  1.55it/s]Extractor Estimating: 260it [02:33,  1.57it/s]Extractor Estimating: 261it [02:34,  1.58it/s]Extractor Estimating: 262it [02:34,  1.58it/s]Extractor Estimating: 263it [02:35,  1.45it/s]Extractor Estimating: 264it [02:36,  1.49it/s]Extractor Estimating: 265it [02:37,  1.46it/s]Extractor Estimating: 266it [02:37,  1.49it/s]Extractor Estimating: 267it [02:38,  1.53it/s]Extractor Estimating: 268it [02:39,  1.55it/s]Extractor Estimating: 269it [02:39,  1.56it/s]Extractor Estimating: 270it [02:40,  1.56it/s]Extractor Estimating: 271it [02:40,  1.58it/s]Extractor Estimating: 272it [02:41,  1.59it/s]Extractor Estimating: 273it [02:42,  1.57it/s]Extractor Estimating: 274it [02:42,  1.58it/s]Extractor Estimating: 275it [02:43,  1.60it/s]Extractor Estimating: 276it [02:44,  1.63it/s]Extractor Estimating: 277it [02:44,  1.62it/s]Extractor Estimating: 278it [02:45,  1.60it/s]Extractor Estimating: 279it [02:45,  1.60it/s]Extractor Estimating: 280it [02:46,  1.64it/s]Extractor Estimating: 281it [02:47,  1.61it/s]Extractor Estimating: 282it [02:47,  1.64it/s]Extractor Estimating: 283it [02:48,  1.49it/s]Extractor Estimating: 284it [02:49,  1.56it/s]Extractor Estimating: 285it [02:49,  1.58it/s]Extractor Estimating: 286it [02:50,  1.62it/s]Extractor Estimating: 287it [02:50,  1.62it/s]Extractor Estimating: 288it [02:51,  1.63it/s]Extractor Estimating: 289it [02:52,  1.65it/s]Extractor Estimating: 290it [02:52,  1.49it/s]Extractor Estimating: 291it [02:53,  1.53it/s]Extractor Estimating: 292it [02:54,  1.59it/s]Extractor Estimating: 293it [02:54,  1.60it/s]Extractor Estimating: 294it [02:55,  1.63it/s]Extractor Estimating: 295it [02:56,  1.52it/s]Extractor Estimating: 296it [02:56,  1.54it/s]Extractor Estimating: 297it [02:57,  1.56it/s]Extractor Estimating: 298it [02:57,  1.59it/s]Extractor Estimating: 299it [02:58,  1.61it/s]Extractor Estimating: 300it [02:59,  1.61it/s]Extractor Estimating: 301it [02:59,  1.64it/s]Extractor Estimating: 302it [03:00,  1.73it/s]Extractor Estimating: 303it [03:00,  1.74it/s]Extractor Estimating: 304it [03:01,  1.76it/s]Extractor Estimating: 305it [03:01,  1.74it/s]Extractor Estimating: 306it [03:02,  1.74it/s]Extractor Estimating: 307it [03:03,  1.76it/s]Extractor Estimating: 308it [03:03,  1.81it/s]Extractor Estimating: 309it [03:04,  1.79it/s]Extractor Estimating: 310it [03:04,  1.78it/s]Extractor Estimating: 311it [03:05,  1.78it/s]Extractor Estimating: 312it [03:05,  1.71it/s]Extractor Estimating: 313it [03:06,  1.72it/s]Extractor Estimating: 314it [03:07,  1.76it/s]Extractor Estimating: 315it [03:07,  1.78it/s]Extractor Estimating: 316it [03:08,  1.80it/s]Extractor Estimating: 317it [03:08,  1.86it/s]Extractor Estimating: 318it [03:09,  1.74it/s]Extractor Estimating: 319it [03:09,  1.76it/s]Extractor Estimating: 320it [03:10,  1.81it/s]Extractor Estimating: 321it [03:10,  1.82it/s]Extractor Estimating: 322it [03:11,  1.84it/s]Extractor Estimating: 323it [03:12,  1.77it/s]Extractor Estimating: 324it [03:12,  1.72it/s]Extractor Estimating: 325it [03:13,  1.69it/s]Extractor Estimating: 326it [03:13,  1.65it/s]Extractor Estimating: 327it [03:14,  1.63it/s]Extractor Estimating: 328it [03:15,  1.56it/s]Extractor Estimating: 329it [03:15,  1.56it/s]Extractor Estimating: 330it [03:16,  1.56it/s]Extractor Estimating: 331it [03:17,  1.56it/s]Extractor Estimating: 332it [03:17,  1.57it/s]Extractor Estimating: 333it [03:18,  1.44it/s]Extractor Estimating: 334it [03:19,  1.48it/s]Extractor Estimating: 335it [03:19,  1.50it/s]Extractor Estimating: 336it [03:20,  1.53it/s]Extractor Estimating: 337it [03:21,  1.55it/s]Extractor Estimating: 338it [03:21,  1.55it/s]Extractor Estimating: 339it [03:22,  1.55it/s]Extractor Estimating: 340it [03:23,  1.52it/s]Extractor Estimating: 341it [03:23,  1.53it/s]Extractor Estimating: 342it [03:24,  1.52it/s]Extractor Estimating: 343it [03:25,  1.54it/s]Extractor Estimating: 344it [03:25,  1.55it/s]Extractor Estimating: 345it [03:26,  1.43it/s]Extractor Estimating: 346it [03:27,  1.47it/s]Extractor Estimating: 347it [03:27,  1.49it/s]Extractor Estimating: 348it [03:28,  1.45it/s]Extractor Estimating: 349it [03:29,  1.54it/s]Extractor Estimating: 350it [03:29,  1.64it/s]Extractor Estimating: 351it [03:30,  1.75it/s]Extractor Estimating: 352it [03:30,  1.78it/s]Extractor Estimating: 353it [03:31,  1.84it/s]Extractor Estimating: 354it [03:31,  1.91it/s]Extractor Estimating: 355it [03:32,  1.93it/s]Extractor Estimating: 356it [03:32,  1.96it/s]Extractor Estimating: 357it [03:33,  1.96it/s]Extractor Estimating: 358it [03:33,  1.95it/s]Extractor Estimating: 359it [03:34,  1.96it/s]Extractor Estimating: 360it [03:34,  1.92it/s]Extractor Estimating: 361it [03:35,  1.95it/s]Extractor Estimating: 362it [03:35,  1.98it/s]Extractor Estimating: 363it [03:36,  1.97it/s]Extractor Estimating: 364it [03:36,  2.01it/s]Extractor Estimating: 365it [03:37,  2.00it/s]Extractor Estimating: 366it [03:37,  1.98it/s]Extractor Estimating: 367it [03:38,  2.04it/s]Extractor Estimating: 368it [03:38,  1.96it/s]Extractor Estimating: 369it [03:39,  2.02it/s]Extractor Estimating: 370it [03:39,  1.97it/s]Extractor Estimating: 371it [03:40,  1.97it/s]Extractor Estimating: 372it [03:40,  1.96it/s]Extractor Estimating: 373it [03:41,  1.99it/s]Extractor Estimating: 374it [03:41,  2.33it/s]Extractor Estimating: 374it [03:41,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:25,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:25,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:25,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:25,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:25,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:03:25,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:03:25,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:03:26,379 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:03:27,484 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:03:27,484 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:33,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:33,514 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:33,515 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:33,515 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:03:33,515 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:03:34,595 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:03:34,596 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:03:35,632 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:03:35,807 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:03:35,807 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:48:20,583 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:48:20,613 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5974 mean pseudo reward: 0.9877407982974316
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 15859
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15959, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15959, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.982, loss:185.5914
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.968, loss:157.2059
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 51, avg_time 0.977, loss:160.9460
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 151, avg_time 0.969, loss:145.3476
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 2, avg_time 0.975, loss:146.5195
>> valid entity prec:0.5348, rec:0.5401, f1:0.5374
>> valid relation prec:0.1900, rec:0.1123, f1:0.1412
>> valid relation with NER prec:0.1900, rec:0.1123, f1:0.1412
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 102, avg_time 2.587, loss:135.8515
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 202, avg_time 0.981, loss:138.6195
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 53, avg_time 0.978, loss:135.9820
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 153, avg_time 0.987, loss:146.2964
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 4, avg_time 0.967, loss:151.7359
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5339, rec:0.6101, f1:0.5695
>> valid relation prec:0.2172, rec:0.1400, f1:0.1703
>> valid relation with NER prec:0.2172, rec:0.1400, f1:0.1703
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 104, avg_time 2.567, loss:138.9481
g_step 1200, step 204, avg_time 0.971, loss:143.0701
g_step 1300, step 55, avg_time 0.987, loss:152.3279
g_step 1400, step 155, avg_time 0.973, loss:134.9437
g_step 1500, step 6, avg_time 0.965, loss:138.9489
>> valid entity prec:0.5671, rec:0.5423, f1:0.5544
>> valid relation prec:0.2283, rec:0.1318, f1:0.1671
>> valid relation with NER prec:0.2283, rec:0.1318, f1:0.1671
g_step 1600, step 106, avg_time 2.552, loss:142.4966
g_step 1700, step 206, avg_time 0.979, loss:130.7049
g_step 1800, step 57, avg_time 0.967, loss:117.0583
g_step 1900, step 157, avg_time 0.974, loss:128.1885
g_step 2000, step 8, avg_time 0.970, loss:129.1009
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5747, rec:0.4987, f1:0.5340
>> valid relation prec:0.1950, rec:0.1027, f1:0.1345
>> valid relation with NER prec:0.1950, rec:0.1027, f1:0.1345
g_step 2100, step 108, avg_time 2.555, loss:118.3516
g_step 2200, step 208, avg_time 0.971, loss:121.5349
g_step 2300, step 59, avg_time 0.973, loss:116.1907
g_step 2400, step 159, avg_time 0.980, loss:115.6173
g_step 2500, step 10, avg_time 0.969, loss:120.5273
>> valid entity prec:0.5579, rec:0.5071, f1:0.5313
>> valid relation prec:0.2036, rec:0.0980, f1:0.1323
>> valid relation with NER prec:0.2036, rec:0.0980, f1:0.1323
g_step 2600, step 110, avg_time 2.558, loss:110.2598
g_step 2700, step 210, avg_time 0.971, loss:109.5885
g_step 2800, step 61, avg_time 0.980, loss:106.4903
g_step 2900, step 161, avg_time 0.972, loss:112.2782
g_step 3000, step 12, avg_time 0.981, loss:104.2533
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5424, rec:0.5610, f1:0.5516
>> valid relation prec:0.1715, rec:0.1256, f1:0.1450
>> valid relation with NER prec:0.1715, rec:0.1256, f1:0.1450
g_step 3100, step 112, avg_time 2.548, loss:98.7382
g_step 3200, step 212, avg_time 0.973, loss:97.7172
g_step 3300, step 63, avg_time 0.966, loss:96.9086
g_step 3400, step 163, avg_time 0.980, loss:96.8466
g_step 3500, step 14, avg_time 0.972, loss:103.1936
>> valid entity prec:0.5591, rec:0.5434, f1:0.5511
>> valid relation prec:0.1657, rec:0.1127, f1:0.1342
>> valid relation with NER prec:0.1657, rec:0.1127, f1:0.1342
g_step 3600, step 114, avg_time 2.557, loss:88.8663
g_step 3700, step 214, avg_time 0.972, loss:104.1025
g_step 3800, step 65, avg_time 0.977, loss:96.8837
g_step 3900, step 165, avg_time 0.982, loss:92.4063
g_step 4000, step 16, avg_time 0.963, loss:99.3387
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5304, rec:0.5291, f1:0.5297
>> valid relation prec:0.1811, rec:0.1070, f1:0.1345
>> valid relation with NER prec:0.1811, rec:0.1070, f1:0.1345
g_step 4100, step 116, avg_time 2.557, loss:89.8770
g_step 4200, step 216, avg_time 0.975, loss:100.3630
g_step 4300, step 67, avg_time 0.978, loss:92.4388
g_step 4400, step 167, avg_time 0.970, loss:86.4131
g_step 4500, step 18, avg_time 0.972, loss:95.6817
>> valid entity prec:0.5759, rec:0.5021, f1:0.5365
>> valid relation prec:0.1838, rec:0.1222, f1:0.1468
>> valid relation with NER prec:0.1838, rec:0.1222, f1:0.1468
g_step 4600, step 118, avg_time 2.552, loss:85.7985
g_step 4700, step 218, avg_time 0.977, loss:93.0181
g_step 4800, step 69, avg_time 0.973, loss:88.4705
g_step 4900, step 169, avg_time 0.981, loss:91.6569
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:48:20 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:48:20 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-48-20_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:48:21 - WARNING - datasets.builder -   Using custom data configuration default-79bdb3a7528239fc
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-79bdb3a7528239fc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:48:23,397 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:23,398 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:48:23,399 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:23,400 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:48:23,475 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:23,524 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:48:23,929 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:48:27,128 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:48:27,163 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-79bdb3a7528239fc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.59ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.62ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.15ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.46ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.65ba/s]100%|██████████| 6/6 [00:01<00:00,  4.76ba/s]100%|██████████| 6/6 [00:01<00:00,  4.34ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.87ba/s] 40%|████      | 2/5 [00:00<00:00,  3.70ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.09ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.32ba/s]100%|██████████| 5/5 [00:01<00:00,  4.63ba/s]100%|██████████| 5/5 [00:01<00:00,  4.24ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.78ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.64ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.46ba/s]100%|██████████| 6/6 [00:00<00:00,  9.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.71ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.89ba/s]100%|██████████| 5/5 [00:00<00:00,  9.23ba/s]100%|██████████| 5/5 [00:00<00:00,  8.50ba/s]
[INFO|trainer.py:414] 2023-08-29 00:48:32,161 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:48:32,168 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:48:32,169 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-29 00:48:32,169 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:48:32,169 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:48:32,169 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:48:32,169 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:48:32,169 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:18,  3.39it/s]  0%|          | 2/470 [00:00<02:16,  3.44it/s]  1%|          | 3/470 [00:00<02:15,  3.44it/s]  1%|          | 4/470 [00:01<02:15,  3.45it/s]  1%|          | 5/470 [00:01<02:18,  3.36it/s]  1%|▏         | 6/470 [00:01<02:17,  3.38it/s]  1%|▏         | 7/470 [00:02<02:16,  3.38it/s]  2%|▏         | 8/470 [00:02<02:16,  3.38it/s]  2%|▏         | 9/470 [00:02<02:16,  3.39it/s]  2%|▏         | 10/470 [00:02<02:15,  3.39it/s]  2%|▏         | 11/470 [00:03<02:15,  3.39it/s]  3%|▎         | 12/470 [00:03<02:14,  3.40it/s]  3%|▎         | 13/470 [00:03<02:14,  3.40it/s]  3%|▎         | 14/470 [00:04<02:14,  3.40it/s]  3%|▎         | 15/470 [00:04<02:13,  3.40it/s]  3%|▎         | 16/470 [00:04<02:20,  3.22it/s]  4%|▎         | 17/470 [00:05<02:18,  3.27it/s]  4%|▍         | 18/470 [00:05<02:16,  3.31it/s]  4%|▍         | 19/470 [00:05<02:15,  3.34it/s]  4%|▍         | 20/470 [00:05<02:14,  3.35it/s]  4%|▍         | 21/470 [00:06<02:13,  3.37it/s]  5%|▍         | 22/470 [00:06<02:12,  3.38it/s]  5%|▍         | 23/470 [00:06<02:11,  3.39it/s]  5%|▌         | 24/470 [00:07<02:11,  3.39it/s]  5%|▌         | 25/470 [00:07<02:11,  3.39it/s]  6%|▌         | 26/470 [00:07<02:10,  3.39it/s]  6%|▌         | 27/470 [00:08<02:18,  3.21it/s]  6%|▌         | 28/470 [00:08<02:15,  3.26it/s]  6%|▌         | 29/470 [00:08<02:13,  3.31it/s]  6%|▋         | 30/470 [00:08<02:12,  3.33it/s]  7%|▋         | 31/470 [00:09<02:10,  3.35it/s]  7%|▋         | 32/470 [00:09<02:09,  3.37it/s]  7%|▋         | 33/470 [00:09<02:09,  3.38it/s]  7%|▋         | 34/470 [00:10<02:08,  3.38it/s]  7%|▋         | 35/470 [00:10<02:08,  3.39it/s]  8%|▊         | 36/470 [00:10<02:07,  3.40it/s]  8%|▊         | 37/470 [00:10<02:07,  3.40it/s]  8%|▊         | 38/470 [00:11<02:14,  3.22it/s]  8%|▊         | 39/470 [00:11<02:11,  3.27it/s]  9%|▊         | 40/470 [00:11<02:09,  3.31it/s]  9%|▊         | 41/470 [00:12<02:08,  3.34it/s]  9%|▉         | 42/470 [00:12<02:07,  3.36it/s]  9%|▉         | 43/470 [00:12<02:06,  3.37it/s]  9%|▉         | 44/470 [00:13<02:06,  3.38it/s] 10%|▉         | 45/470 [00:13<02:05,  3.39it/s] 10%|▉         | 46/470 [00:13<02:05,  3.39it/s] 10%|█         | 47/470 [00:13<02:04,  3.39it/s] 10%|█         | 48/470 [00:14<02:04,  3.39it/s] 10%|█         | 49/470 [00:14<02:09,  3.25it/s] 11%|█         | 50/470 [00:14<02:07,  3.29it/s] 11%|█         | 51/470 [00:15<02:06,  3.32it/s] 11%|█         | 52/470 [00:15<02:05,  3.34it/s] 11%|█▏        | 53/470 [00:15<02:04,  3.36it/s] 11%|█▏        | 54/470 [00:16<02:03,  3.37it/s] 12%|█▏        | 55/470 [00:16<02:02,  3.38it/s] 12%|█▏        | 56/470 [00:16<02:02,  3.39it/s] 12%|█▏        | 57/470 [00:16<02:01,  3.39it/s] 12%|█▏        | 58/470 [00:17<02:01,  3.39it/s] 13%|█▎        | 59/470 [00:17<02:01,  3.39it/s] 13%|█▎        | 60/470 [00:17<02:07,  3.22it/s] 13%|█▎        | 61/470 [00:18<02:05,  3.27it/s] 13%|█▎        | 62/470 [00:18<02:03,  3.31it/s] 13%|█▎        | 63/470 [00:18<02:02,  3.34it/s] 14%|█▎        | 64/470 [00:19<02:01,  3.35it/s] 14%|█▍        | 65/470 [00:19<02:00,  3.36it/s] 14%|█▍        | 66/470 [00:19<01:59,  3.37it/s] 14%|█▍        | 67/470 [00:19<01:59,  3.38it/s] 14%|█▍        | 68/470 [00:20<01:58,  3.38it/s] 15%|█▍        | 69/470 [00:20<01:58,  3.39it/s] 15%|█▍        | 70/470 [00:20<01:57,  3.39it/s] 15%|█▌        | 71/470 [00:21<01:57,  3.39it/s] 15%|█▌        | 72/470 [00:21<01:57,  3.39it/s] 16%|█▌        | 73/470 [00:21<01:56,  3.40it/s] 16%|█▌        | 74/470 [00:22<01:56,  3.40it/s] 16%|█▌        | 75/470 [00:22<01:56,  3.39it/s] 16%|█▌        | 76/470 [00:22<01:56,  3.39it/s] 16%|█▋        | 77/470 [00:22<01:55,  3.40it/s] 17%|█▋        | 78/470 [00:23<01:55,  3.40it/s] 17%|█▋        | 79/470 [00:23<01:55,  3.39it/s] 17%|█▋        | 80/470 [00:23<01:58,  3.30it/s] 17%|█▋        | 81/470 [00:24<01:56,  3.33it/s] 17%|█▋        | 82/470 [00:24<01:55,  3.35it/s] 18%|█▊        | 83/470 [00:24<01:55,  3.36it/s] 18%|█▊        | 84/470 [00:25<01:54,  3.37it/s] 18%|█▊        | 85/470 [00:25<01:54,  3.37it/s] 18%|█▊        | 86/470 [00:25<01:53,  3.38it/s] 19%|█▊        | 87/470 [00:25<01:53,  3.39it/s] 19%|█▊        | 88/470 [00:26<01:52,  3.39it/s] 19%|█▉        | 89/470 [00:26<01:52,  3.39it/s] 19%|█▉        | 90/470 [00:26<01:52,  3.39it/s] 19%|█▉        | 91/470 [00:27<01:54,  3.32it/s] 20%|█▉        | 92/470 [00:27<01:53,  3.34it/s] 20%|█▉        | 93/470 [00:27<01:52,  3.35it/s] 20%|██        | 94/470 [00:27<01:44,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 00:49:00,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:49:00,091 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:49:00,091 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:11, 54.94it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.51it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.82it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.66it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.23it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.84it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.64it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.68it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.65it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.75it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.86it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.74it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.62it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.46it/s][A
 13%|█▎        | 77/611 [00:01<00:13, 38.92it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 40.78it/s][A
 14%|█▍        | 87/611 [00:02<00:14, 35.95it/s][A
 15%|█▌        | 92/611 [00:02<00:13, 38.33it/s][A
 16%|█▌        | 97/611 [00:02<00:12, 40.07it/s][A
 17%|█▋        | 102/611 [00:02<00:12, 41.58it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 42.59it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.33it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.60it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.79it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.49it/s][A
 22%|██▏       | 132/611 [00:03<00:11, 43.48it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.75it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.11it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.40it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.66it/s][A
 26%|██▌       | 157/611 [00:03<00:11, 39.48it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 41.28it/s][A
 27%|██▋       | 167/611 [00:03<00:11, 37.28it/s][A
 28%|██▊       | 171/611 [00:04<00:26, 16.77it/s][A
 29%|██▉       | 176/611 [00:04<00:20, 20.91it/s][A
 30%|██▉       | 181/611 [00:04<00:17, 25.05it/s][A
 30%|███       | 186/611 [00:04<00:14, 28.97it/s][A
 31%|███       | 190/611 [00:05<00:14, 29.93it/s][A
 32%|███▏      | 195/611 [00:05<00:12, 33.61it/s][A
 33%|███▎      | 200/611 [00:05<00:11, 36.51it/s][A
 34%|███▎      | 205/611 [00:05<00:10, 38.80it/s][A
 34%|███▍      | 210/611 [00:05<00:09, 40.27it/s][A
 35%|███▌      | 215/611 [00:05<00:09, 41.23it/s][A
 36%|███▌      | 220/611 [00:05<00:09, 42.16it/s][A
 37%|███▋      | 225/611 [00:05<00:08, 42.90it/s][A
 38%|███▊      | 230/611 [00:05<00:08, 43.16it/s][A
 38%|███▊      | 235/611 [00:06<00:08, 43.54it/s][A
 39%|███▉      | 240/611 [00:06<00:08, 43.77it/s][A
 40%|████      | 245/611 [00:06<00:08, 43.91it/s][A
 41%|████      | 250/611 [00:06<00:08, 44.25it/s][A
 42%|████▏     | 255/611 [00:06<00:08, 44.32it/s][A
 43%|████▎     | 260/611 [00:06<00:07, 44.40it/s][A
 43%|████▎     | 265/611 [00:06<00:07, 44.45it/s][A
 44%|████▍     | 270/611 [00:06<00:07, 44.26it/s][A
 45%|████▌     | 275/611 [00:06<00:07, 44.25it/s][A
 46%|████▌     | 280/611 [00:07<00:07, 44.29it/s][A
 47%|████▋     | 285/611 [00:07<00:07, 44.33it/s][A
 47%|████▋     | 290/611 [00:07<00:07, 44.48it/s][A
 48%|████▊     | 295/611 [00:07<00:07, 44.46it/s][A
 49%|████▉     | 300/611 [00:07<00:06, 44.63it/s][A
 50%|████▉     | 305/611 [00:07<00:06, 44.52it/s][A
 51%|█████     | 310/611 [00:07<00:06, 44.46it/s][A
 52%|█████▏    | 315/611 [00:07<00:06, 44.44it/s][A
 52%|█████▏    | 320/611 [00:07<00:06, 44.33it/s][A
 53%|█████▎    | 325/611 [00:08<00:06, 42.59it/s][A
 54%|█████▍    | 330/611 [00:08<00:06, 43.13it/s][A
 55%|█████▍    | 335/611 [00:08<00:06, 43.64it/s][A
 56%|█████▌    | 340/611 [00:08<00:06, 44.10it/s][A
 56%|█████▋    | 345/611 [00:08<00:06, 44.16it/s][A
 57%|█████▋    | 350/611 [00:08<00:05, 44.34it/s][A
 58%|█████▊    | 355/611 [00:08<00:05, 44.35it/s][A
 59%|█████▉    | 360/611 [00:08<00:05, 44.24it/s][A
 60%|█████▉    | 365/611 [00:08<00:05, 44.16it/s][A
 61%|██████    | 370/611 [00:09<00:05, 44.03it/s][A
 61%|██████▏   | 375/611 [00:09<00:05, 44.26it/s][A
 62%|██████▏   | 380/611 [00:09<00:05, 44.45it/s][A
 63%|██████▎   | 385/611 [00:09<00:05, 44.42it/s][A
 64%|██████▍   | 390/611 [00:09<00:04, 44.72it/s][A
 65%|██████▍   | 395/611 [00:09<00:04, 44.65it/s][A
 65%|██████▌   | 400/611 [00:09<00:04, 44.31it/s][A
 66%|██████▋   | 405/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 410/611 [00:09<00:04, 44.25it/s][A
 68%|██████▊   | 415/611 [00:10<00:04, 44.05it/s][A
 69%|██████▊   | 420/611 [00:10<00:04, 44.24it/s][A
 70%|██████▉   | 425/611 [00:10<00:04, 44.44it/s][A
 70%|███████   | 430/611 [00:10<00:04, 44.59it/s][A
 71%|███████   | 435/611 [00:10<00:03, 44.65it/s][A
 72%|███████▏  | 440/611 [00:10<00:03, 44.61it/s][A
 73%|███████▎  | 445/611 [00:10<00:03, 44.47it/s][A
 74%|███████▎  | 450/611 [00:10<00:03, 44.39it/s][A
 74%|███████▍  | 455/611 [00:11<00:03, 44.19it/s][A
 75%|███████▌  | 460/611 [00:11<00:03, 41.60it/s][A
 76%|███████▌  | 465/611 [00:11<00:03, 42.60it/s][A
 77%|███████▋  | 470/611 [00:11<00:03, 43.11it/s][A
 78%|███████▊  | 475/611 [00:11<00:03, 43.66it/s][A
 79%|███████▊  | 480/611 [00:11<00:02, 44.09it/s][A
 79%|███████▉  | 485/611 [00:11<00:02, 44.26it/s][A
 80%|████████  | 490/611 [00:11<00:02, 44.36it/s][A
 81%|████████  | 495/611 [00:11<00:02, 44.15it/s][A
 82%|████████▏ | 500/611 [00:12<00:02, 43.80it/s][A
 83%|████████▎ | 505/611 [00:12<00:02, 43.98it/s][A
 83%|████████▎ | 510/611 [00:12<00:02, 44.15it/s][A
 84%|████████▍ | 515/611 [00:12<00:02, 44.40it/s][A
 85%|████████▌ | 520/611 [00:12<00:02, 44.57it/s][A
 86%|████████▌ | 525/611 [00:12<00:01, 44.34it/s][A
 87%|████████▋ | 530/611 [00:12<00:01, 44.57it/s][A
 88%|████████▊ | 535/611 [00:12<00:01, 44.44it/s][A
 88%|████████▊ | 540/611 [00:12<00:01, 44.17it/s][A
 89%|████████▉ | 545/611 [00:13<00:01, 43.95it/s][A
 90%|█████████ | 550/611 [00:13<00:01, 44.16it/s][A
 91%|█████████ | 555/611 [00:13<00:01, 44.23it/s][A
 92%|█████████▏| 560/611 [00:13<00:01, 44.44it/s][A
 92%|█████████▏| 565/611 [00:13<00:01, 44.52it/s][A
 93%|█████████▎| 570/611 [00:13<00:00, 44.66it/s][A
 94%|█████████▍| 575/611 [00:13<00:00, 44.70it/s][A
 95%|█████████▍| 580/611 [00:13<00:00, 44.39it/s][A
 96%|█████████▌| 585/611 [00:13<00:00, 44.16it/s][A
 97%|█████████▋| 590/611 [00:14<00:00, 44.12it/s][A
 97%|█████████▋| 595/611 [00:14<00:00, 42.93it/s][A
 98%|█████████▊| 600/611 [00:14<00:00, 43.54it/s][A
 99%|█████████▉| 605/611 [00:14<00:00, 43.99it/s][A
100%|█████████▉| 610/611 [00:14<00:00, 44.17it/s][A
                                                 [A                                                
100%|██████████| 611/611 [00:14<00:00, 44.17it/s][A 20%|██        | 94/470 [00:42<01:44,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:49:14,739 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 00:49:14,870 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:49:18,044 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:49:18,234 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:49:18,328 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:53<48:16,  7.72s/it] 20%|██        | 96/470 [00:53<34:14,  5.49s/it] 21%|██        | 97/470 [00:53<24:27,  3.93s/it] 21%|██        | 98/470 [00:53<17:37,  2.84s/it] 21%|██        | 99/470 [00:54<12:50,  2.08s/it] 21%|██▏       | 100/470 [00:54<09:32,  1.55s/it] 21%|██▏       | 101/470 [00:54<07:12,  1.17s/it] 22%|██▏       | 102/470 [00:55<05:34,  1.10it/s] 22%|██▏       | 103/470 [00:55<04:25,  1.38it/s] 22%|██▏       | 104/470 [00:55<03:38,  1.68it/s] 22%|██▏       | 105/470 [00:55<03:04,  1.98it/s] 23%|██▎       | 106/470 [00:56<02:40,  2.26it/s] 23%|██▎       | 107/470 [00:56<02:24,  2.51it/s] 23%|██▎       | 108/470 [00:56<02:12,  2.73it/s] 23%|██▎       | 109/470 [00:57<02:04,  2.90it/s] 23%|██▎       | 110/470 [00:57<01:58,  3.03it/s] 24%|██▎       | 111/470 [00:57<01:56,  3.09it/s] 24%|██▍       | 112/470 [00:58<01:52,  3.18it/s] 24%|██▍       | 113/470 [00:58<01:50,  3.23it/s] 24%|██▍       | 114/470 [00:58<01:48,  3.28it/s] 24%|██▍       | 115/470 [00:58<01:47,  3.31it/s] 25%|██▍       | 116/470 [00:59<01:45,  3.34it/s] 25%|██▍       | 117/470 [00:59<01:45,  3.36it/s] 25%|██▌       | 118/470 [00:59<01:44,  3.37it/s] 25%|██▌       | 119/470 [01:00<01:43,  3.38it/s] 26%|██▌       | 120/470 [01:00<01:43,  3.38it/s] 26%|██▌       | 121/470 [01:00<01:43,  3.38it/s] 26%|██▌       | 122/470 [01:01<01:43,  3.35it/s] 26%|██▌       | 123/470 [01:01<01:43,  3.36it/s] 26%|██▋       | 124/470 [01:01<01:42,  3.37it/s] 27%|██▋       | 125/470 [01:01<01:42,  3.37it/s] 27%|██▋       | 126/470 [01:02<01:41,  3.38it/s] 27%|██▋       | 127/470 [01:02<01:41,  3.38it/s] 27%|██▋       | 128/470 [01:02<01:41,  3.38it/s] 27%|██▋       | 129/470 [01:03<01:40,  3.39it/s] 28%|██▊       | 130/470 [01:03<01:40,  3.39it/s] 28%|██▊       | 131/470 [01:03<01:40,  3.39it/s] 28%|██▊       | 132/470 [01:03<01:39,  3.39it/s] 28%|██▊       | 133/470 [01:04<01:44,  3.22it/s] 29%|██▊       | 134/470 [01:04<01:42,  3.26it/s] 29%|██▊       | 135/470 [01:04<01:41,  3.30it/s] 29%|██▉       | 136/470 [01:05<01:40,  3.33it/s] 29%|██▉       | 137/470 [01:05<01:39,  3.35it/s] 29%|██▉       | 138/470 [01:05<01:38,  3.36it/s] 30%|██▉       | 139/470 [01:06<01:38,  3.36it/s] 30%|██▉       | 140/470 [01:06<01:37,  3.37it/s] 30%|███       | 141/470 [01:06<01:37,  3.37it/s] 30%|███       | 142/470 [01:06<01:37,  3.38it/s] 30%|███       | 143/470 [01:07<01:41,  3.23it/s] 31%|███       | 144/470 [01:07<01:39,  3.27it/s] 31%|███       | 145/470 [01:07<01:38,  3.30it/s] 31%|███       | 146/470 [01:08<01:37,  3.32it/s] 31%|███▏      | 147/470 [01:08<01:36,  3.34it/s] 31%|███▏      | 148/470 [01:08<01:35,  3.36it/s] 32%|███▏      | 149/470 [01:09<01:35,  3.36it/s] 32%|███▏      | 150/470 [01:09<01:35,  3.36it/s] 32%|███▏      | 151/470 [01:09<01:34,  3.37it/s] 32%|███▏      | 152/470 [01:09<01:34,  3.37it/s] 33%|███▎      | 153/470 [01:10<01:38,  3.22it/s] 33%|███▎      | 154/470 [01:10<01:36,  3.27it/s] 33%|███▎      | 155/470 [01:10<01:35,  3.31it/s] 33%|███▎      | 156/470 [01:11<01:34,  3.33it/s] 33%|███▎      | 157/470 [01:11<01:33,  3.34it/s] 34%|███▎      | 158/470 [01:11<01:32,  3.36it/s] 34%|███▍      | 159/470 [01:12<01:32,  3.37it/s] 34%|███▍      | 160/470 [01:12<01:31,  3.37it/s] 34%|███▍      | 161/470 [01:12<01:31,  3.37it/s] 34%|███▍      | 162/470 [01:12<01:31,  3.37it/s] 35%|███▍      | 163/470 [01:13<01:36,  3.20it/s] 35%|███▍      | 164/470 [01:13<01:34,  3.25it/s] 35%|███▌      | 165/470 [01:13<01:32,  3.28it/s] 35%|███▌      | 166/470 [01:14<01:31,  3.31it/s] 36%|███▌      | 167/470 [01:14<01:30,  3.33it/s] 36%|███▌      | 168/470 [01:14<01:30,  3.35it/s] 36%|███▌      | 169/470 [01:15<01:29,  3.36it/s] 36%|███▌      | 170/470 [01:15<01:29,  3.36it/s] 36%|███▋      | 171/470 [01:15<01:28,  3.37it/s] 37%|███▋      | 172/470 [01:15<01:28,  3.38it/s] 37%|███▋      | 173/470 [01:16<01:29,  3.32it/s] 37%|███▋      | 174/470 [01:16<01:28,  3.34it/s] 37%|███▋      | 175/470 [01:16<01:28,  3.35it/s] 37%|███▋      | 176/470 [01:17<01:27,  3.36it/s] 38%|███▊      | 177/470 [01:17<01:27,  3.37it/s] 38%|███▊      | 178/470 [01:17<01:26,  3.37it/s] 38%|███▊      | 179/470 [01:18<01:26,  3.37it/s] 38%|███▊      | 180/470 [01:18<01:26,  3.37it/s] 39%|███▊      | 181/470 [01:18<01:25,  3.37it/s] 39%|███▊      | 182/470 [01:18<01:25,  3.38it/s] 39%|███▉      | 183/470 [01:19<01:24,  3.38it/s] 39%|███▉      | 184/470 [01:19<01:29,  3.19it/s] 39%|███▉      | 185/470 [01:19<01:27,  3.25it/s] 40%|███▉      | 186/470 [01:20<01:26,  3.29it/s] 40%|███▉      | 187/470 [01:20<01:25,  3.32it/s] 40%|████      | 188/470 [01:20<01:19,  3.56it/s][INFO|trainer.py:2140] 2023-08-29 00:49:52,906 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:49:52,906 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:49:52,906 >>   Batch size = 8
{'eval_loss': 1.0186160802841187, 'eval_runtime': 14.5472, 'eval_samples_per_second': 335.597, 'eval_steps_per_second': 42.001, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.03it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.58it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.96it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.06it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.63it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.07it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.60it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.21it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.19it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.42it/s][A
  9%|▉         | 57/611 [00:01<00:13, 42.57it/s][A
 10%|█         | 62/611 [00:01<00:12, 43.27it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.85it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.90it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.04it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.17it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.34it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.40it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.59it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.44it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.38it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.11it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.07it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.17it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.37it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.45it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.56it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.54it/s][A
 27%|██▋       | 167/611 [00:03<00:09, 44.58it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.49it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.26it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.13it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.20it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 42.65it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.41it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.82it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.16it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.09it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.09it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.17it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.14it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.99it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.11it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.27it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.61it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.66it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.46it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.45it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.31it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.19it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.15it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.28it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.43it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.55it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.59it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.53it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.45it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.27it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.21it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.17it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 42.26it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.11it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.55it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.02it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.01it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.08it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.96it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.15it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.34it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.56it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.59it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 42.60it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.26it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.25it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.21it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.12it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.31it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.36it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.54it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.56it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.38it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.36it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.33it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.18it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.18it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 42.22it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.09it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.66it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 39.61it/s][A
 79%|███████▉  | 482/611 [00:10<00:03, 41.62it/s][A
 80%|███████▉  | 487/611 [00:11<00:03, 40.39it/s][A
 81%|████████  | 492/611 [00:11<00:02, 42.73it/s][A
 81%|████████▏ | 497/611 [00:11<00:03, 31.55it/s][A
 82%|████████▏ | 502/611 [00:11<00:03, 34.20it/s][A
 83%|████████▎ | 506/611 [00:11<00:03, 33.56it/s][A
 84%|████████▎ | 511/611 [00:11<00:02, 36.53it/s][A
 84%|████████▍ | 516/611 [00:11<00:02, 38.79it/s][A
 85%|████████▌ | 521/611 [00:12<00:02, 40.41it/s][A
 86%|████████▌ | 526/611 [00:12<00:02, 41.68it/s][A
 87%|████████▋ | 531/611 [00:12<00:01, 42.65it/s][A
 88%|████████▊ | 536/611 [00:12<00:01, 43.03it/s][A
 89%|████████▊ | 541/611 [00:12<00:01, 43.17it/s][A
 89%|████████▉ | 546/611 [00:12<00:01, 43.24it/s][A
 90%|█████████ | 551/611 [00:12<00:01, 43.48it/s][A
 91%|█████████ | 556/611 [00:12<00:01, 43.79it/s][A
 92%|█████████▏| 561/611 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 566/611 [00:13<00:01, 44.37it/s][A
 93%|█████████▎| 571/611 [00:13<00:00, 44.42it/s][A
 94%|█████████▍| 576/611 [00:13<00:00, 44.52it/s][A
 95%|█████████▌| 581/611 [00:13<00:00, 44.43it/s][A
 96%|█████████▌| 586/611 [00:13<00:00, 43.33it/s][A
 97%|█████████▋| 591/611 [00:13<00:00, 43.50it/s][A
 98%|█████████▊| 596/611 [00:13<00:00, 43.70it/s][A
 98%|█████████▊| 601/611 [00:13<00:00, 43.90it/s][A
 99%|█████████▉| 606/611 [00:13<00:00, 44.12it/s][A
100%|██████████| 611/611 [00:14<00:00, 44.38it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 44.38it/s][A 40%|████      | 188/470 [01:34<01:19,  3.56it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:50:07,079 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 00:50:07,269 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:50:10,443 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:50:10,574 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:50:10,638 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:45<35:01,  7.48s/it] 40%|████      | 190/470 [01:45<24:52,  5.33s/it] 41%|████      | 191/470 [01:45<17:45,  3.82s/it] 41%|████      | 192/470 [01:45<12:47,  2.76s/it] 41%|████      | 193/470 [01:46<09:20,  2.02s/it] 41%|████▏     | 194/470 [01:46<06:55,  1.50s/it] 41%|████▏     | 195/470 [01:46<05:13,  1.14s/it] 42%|████▏     | 196/470 [01:47<04:03,  1.13it/s] 42%|████▏     | 197/470 [01:47<03:13,  1.41it/s] 42%|████▏     | 198/470 [01:47<02:39,  1.71it/s] 42%|████▏     | 199/470 [01:47<02:15,  2.01it/s] 43%|████▎     | 200/470 [01:48<01:57,  2.29it/s] 43%|████▎     | 201/470 [01:48<01:49,  2.45it/s] 43%|████▎     | 202/470 [01:48<01:40,  2.67it/s] 43%|████▎     | 203/470 [01:49<01:33,  2.85it/s] 43%|████▎     | 204/470 [01:49<01:28,  2.99it/s] 44%|████▎     | 205/470 [01:49<01:25,  3.10it/s] 44%|████▍     | 206/470 [01:50<01:22,  3.19it/s] 44%|████▍     | 207/470 [01:50<01:21,  3.24it/s] 44%|████▍     | 208/470 [01:50<01:19,  3.29it/s] 44%|████▍     | 209/470 [01:50<01:18,  3.32it/s] 45%|████▍     | 210/470 [01:51<01:17,  3.34it/s] 45%|████▍     | 211/470 [01:51<01:17,  3.35it/s] 45%|████▌     | 212/470 [01:51<01:18,  3.30it/s] 45%|████▌     | 213/470 [01:52<01:17,  3.32it/s] 46%|████▌     | 214/470 [01:52<01:16,  3.34it/s] 46%|████▌     | 215/470 [01:52<01:16,  3.35it/s] 46%|████▌     | 216/470 [01:53<01:15,  3.36it/s] 46%|████▌     | 217/470 [01:53<01:15,  3.37it/s] 46%|████▋     | 218/470 [01:53<01:14,  3.38it/s] 47%|████▋     | 219/470 [01:53<01:14,  3.38it/s] 47%|████▋     | 220/470 [01:54<01:13,  3.38it/s] 47%|████▋     | 221/470 [01:54<01:13,  3.39it/s] 47%|████▋     | 222/470 [01:54<01:13,  3.39it/s] 47%|████▋     | 223/470 [01:55<01:12,  3.39it/s] 48%|████▊     | 224/470 [01:55<01:12,  3.41it/s] 48%|████▊     | 225/470 [01:55<01:11,  3.42it/s] 48%|████▊     | 226/470 [01:56<01:12,  3.35it/s] 48%|████▊     | 227/470 [01:56<01:11,  3.38it/s] 49%|████▊     | 228/470 [01:56<01:11,  3.39it/s] 49%|████▊     | 229/470 [01:56<01:10,  3.41it/s] 49%|████▉     | 230/470 [01:57<01:10,  3.41it/s] 49%|████▉     | 231/470 [01:57<01:09,  3.42it/s] 49%|████▉     | 232/470 [01:57<01:09,  3.42it/s] 50%|████▉     | 233/470 [01:58<01:09,  3.43it/s] 50%|████▉     | 234/470 [01:58<01:08,  3.43it/s] 50%|█████     | 235/470 [01:58<01:08,  3.43it/s] 50%|█████     | 236/470 [01:58<01:08,  3.44it/s] 50%|█████     | 237/470 [01:59<01:10,  3.30it/s] 51%|█████     | 238/470 [01:59<01:09,  3.34it/s] 51%|█████     | 239/470 [01:59<01:08,  3.36it/s] 51%|█████     | 240/470 [02:00<01:08,  3.38it/s] 51%|█████▏    | 241/470 [02:00<01:07,  3.40it/s] 51%|█████▏    | 242/470 [02:00<01:06,  3.41it/s] 52%|█████▏    | 243/470 [02:01<01:06,  3.42it/s] 52%|█████▏    | 244/470 [02:01<01:06,  3.42it/s] 52%|█████▏    | 245/470 [02:01<01:05,  3.42it/s] 52%|█████▏    | 246/470 [02:01<01:05,  3.42it/s] 53%|█████▎    | 247/470 [02:02<01:05,  3.42it/s] 53%|█████▎    | 248/470 [02:02<01:07,  3.30it/s] 53%|█████▎    | 249/470 [02:02<01:06,  3.33it/s] 53%|█████▎    | 250/470 [02:03<01:05,  3.36it/s] 53%|█████▎    | 251/470 [02:03<01:04,  3.38it/s] 54%|█████▎    | 252/470 [02:03<01:04,  3.40it/s] 54%|█████▍    | 253/470 [02:03<01:03,  3.41it/s] 54%|█████▍    | 254/470 [02:04<01:03,  3.41it/s] 54%|█████▍    | 255/470 [02:04<01:02,  3.42it/s] 54%|█████▍    | 256/470 [02:04<01:02,  3.42it/s] 55%|█████▍    | 257/470 [02:05<01:02,  3.42it/s] 55%|█████▍    | 258/470 [02:05<01:02,  3.42it/s] 55%|█████▌    | 259/470 [02:05<01:03,  3.32it/s] 55%|█████▌    | 260/470 [02:06<01:02,  3.35it/s] 56%|█████▌    | 261/470 [02:06<01:01,  3.38it/s] 56%|█████▌    | 262/470 [02:06<01:01,  3.39it/s] 56%|█████▌    | 263/470 [02:06<01:00,  3.40it/s] 56%|█████▌    | 264/470 [02:07<01:00,  3.41it/s] 56%|█████▋    | 265/470 [02:07<01:00,  3.41it/s] 57%|█████▋    | 266/470 [02:07<00:59,  3.42it/s] 57%|█████▋    | 267/470 [02:08<00:59,  3.42it/s] 57%|█████▋    | 268/470 [02:08<00:59,  3.42it/s] 57%|█████▋    | 269/470 [02:08<00:58,  3.42it/s] 57%|█████▋    | 270/470 [02:09<01:02,  3.20it/s] 58%|█████▊    | 271/470 [02:09<01:00,  3.27it/s] 58%|█████▊    | 272/470 [02:09<00:59,  3.32it/s] 58%|█████▊    | 273/470 [02:09<00:58,  3.35it/s] 58%|█████▊    | 274/470 [02:10<00:58,  3.37it/s] 59%|█████▊    | 275/470 [02:10<00:57,  3.39it/s] 59%|█████▊    | 276/470 [02:10<00:56,  3.40it/s] 59%|█████▉    | 277/470 [02:11<00:56,  3.41it/s] 59%|█████▉    | 278/470 [02:11<00:56,  3.42it/s] 59%|█████▉    | 279/470 [02:11<00:55,  3.42it/s] 60%|█████▉    | 280/470 [02:11<00:55,  3.42it/s] 60%|█████▉    | 281/470 [02:12<00:58,  3.25it/s] 60%|██████    | 282/470 [02:12<00:53,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 00:50:44,682 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:50:44,683 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:50:44,683 >>   Batch size = 8
{'eval_loss': 1.0398932695388794, 'eval_runtime': 14.0534, 'eval_samples_per_second': 347.389, 'eval_steps_per_second': 43.477, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.85it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.44it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.01it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.17it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.50it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.98it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.47it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.25it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.28it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.41it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.59it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.64it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.63it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.74it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.44it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.21it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.19it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.34it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.50it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.61it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.76it/s][A
 19%|█▉        | 117/611 [00:02<00:12, 41.11it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 42.15it/s][A
 21%|██        | 127/611 [00:02<00:11, 42.72it/s][A
 22%|██▏       | 132/611 [00:02<00:11, 43.26it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.62it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.92it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.16it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.32it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.16it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.18it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.18it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.27it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.33it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.38it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.50it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.53it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.44it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.34it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.27it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.30it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.46it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.37it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.46it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.52it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.59it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.47it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.34it/s][A
 41%|████      | 252/611 [00:05<00:08, 40.44it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 41.72it/s][A
 43%|████▎     | 262/611 [00:05<00:08, 42.67it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.29it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.81it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.10it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.06it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.74it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.66it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.82it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.00it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.18it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.47it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.65it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.56it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.36it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.12it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.04it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.36it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.47it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.61it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.68it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.41it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.12it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 40.24it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 41.58it/s][A
 65%|██████▍   | 397/611 [00:09<00:05, 42.59it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.23it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.76it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.09it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.11it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.02it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.78it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.68it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.01it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.22it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.44it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.56it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.71it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.62it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.33it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.96it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.84it/s][A
 79%|███████▉  | 482/611 [00:10<00:03, 42.37it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.00it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.51it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.88it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.16it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.15it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.97it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.80it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.73it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.98it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.38it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.58it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.69it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.62it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.45it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.22it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.06it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.19it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.21it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.46it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.64it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.70it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.58it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.14it/s][A 60%|██████    | 282/470 [02:26<00:53,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:50:58,810 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 00:50:59,096 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:51:03,603 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:51:03,934 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:51:04,073 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:42<29:03,  9.32s/it] 60%|██████    | 284/470 [02:43<20:31,  6.62s/it] 61%|██████    | 285/470 [02:43<14:33,  4.72s/it] 61%|██████    | 286/470 [02:43<10:24,  3.40s/it] 61%|██████    | 287/470 [02:44<07:31,  2.46s/it] 61%|██████▏   | 288/470 [02:44<05:30,  1.81s/it] 61%|██████▏   | 289/470 [02:44<04:05,  1.36s/it] 62%|██████▏   | 290/470 [02:45<03:07,  1.04s/it] 62%|██████▏   | 291/470 [02:45<02:26,  1.23it/s] 62%|██████▏   | 292/470 [02:45<01:57,  1.52it/s] 62%|██████▏   | 293/470 [02:45<01:37,  1.82it/s] 63%|██████▎   | 294/470 [02:46<01:23,  2.11it/s] 63%|██████▎   | 295/470 [02:46<01:15,  2.30it/s] 63%|██████▎   | 296/470 [02:46<01:08,  2.55it/s] 63%|██████▎   | 297/470 [02:47<01:02,  2.76it/s] 63%|██████▎   | 298/470 [02:47<00:58,  2.92it/s] 64%|██████▎   | 299/470 [02:47<00:56,  3.05it/s] 64%|██████▍   | 300/470 [02:48<00:54,  3.15it/s] 64%|██████▍   | 301/470 [02:48<00:52,  3.22it/s] 64%|██████▍   | 302/470 [02:48<00:51,  3.27it/s] 64%|██████▍   | 303/470 [02:48<00:50,  3.31it/s] 65%|██████▍   | 304/470 [02:49<00:49,  3.33it/s] 65%|██████▍   | 305/470 [02:49<00:49,  3.35it/s] 65%|██████▌   | 306/470 [02:49<00:49,  3.30it/s] 65%|██████▌   | 307/470 [02:50<00:49,  3.33it/s] 66%|██████▌   | 308/470 [02:50<00:48,  3.35it/s] 66%|██████▌   | 309/470 [02:50<00:47,  3.36it/s] 66%|██████▌   | 310/470 [02:50<00:47,  3.37it/s] 66%|██████▌   | 311/470 [02:51<00:47,  3.38it/s] 66%|██████▋   | 312/470 [02:51<00:46,  3.38it/s] 67%|██████▋   | 313/470 [02:51<00:46,  3.39it/s] 67%|██████▋   | 314/470 [02:52<00:45,  3.39it/s] 67%|██████▋   | 315/470 [02:52<00:45,  3.41it/s] 67%|██████▋   | 316/470 [02:52<00:45,  3.42it/s] 67%|██████▋   | 317/470 [02:53<00:45,  3.39it/s] 68%|██████▊   | 318/470 [02:53<00:44,  3.41it/s] 68%|██████▊   | 319/470 [02:53<00:44,  3.42it/s] 68%|██████▊   | 320/470 [02:53<00:43,  3.43it/s] 68%|██████▊   | 321/470 [02:54<00:43,  3.43it/s] 69%|██████▊   | 322/470 [02:54<00:43,  3.43it/s] 69%|██████▊   | 323/470 [02:54<00:42,  3.44it/s] 69%|██████▉   | 324/470 [02:55<00:42,  3.44it/s] 69%|██████▉   | 325/470 [02:55<00:42,  3.44it/s] 69%|██████▉   | 326/470 [02:55<00:41,  3.44it/s] 70%|██████▉   | 327/470 [02:55<00:41,  3.44it/s] 70%|██████▉   | 328/470 [02:56<00:41,  3.44it/s] 70%|███████   | 329/470 [02:56<00:40,  3.44it/s] 70%|███████   | 330/470 [02:56<00:40,  3.44it/s] 70%|███████   | 331/470 [02:57<00:40,  3.44it/s] 71%|███████   | 332/470 [02:57<00:40,  3.39it/s] 71%|███████   | 333/470 [02:57<00:40,  3.40it/s] 71%|███████   | 334/470 [02:57<00:39,  3.41it/s] 71%|███████▏  | 335/470 [02:58<00:39,  3.42it/s] 71%|███████▏  | 336/470 [02:58<00:39,  3.43it/s] 72%|███████▏  | 337/470 [02:58<00:38,  3.43it/s] 72%|███████▏  | 338/470 [02:59<00:38,  3.42it/s] 72%|███████▏  | 339/470 [02:59<00:38,  3.43it/s] 72%|███████▏  | 340/470 [02:59<00:37,  3.43it/s] 73%|███████▎  | 341/470 [03:00<00:37,  3.43it/s] 73%|███████▎  | 342/470 [03:00<00:37,  3.44it/s] 73%|███████▎  | 343/470 [03:00<00:37,  3.35it/s] 73%|███████▎  | 344/470 [03:00<00:37,  3.38it/s] 73%|███████▎  | 345/470 [03:01<00:36,  3.40it/s] 74%|███████▎  | 346/470 [03:01<00:36,  3.41it/s] 74%|███████▍  | 347/470 [03:01<00:35,  3.42it/s] 74%|███████▍  | 348/470 [03:02<00:35,  3.43it/s] 74%|███████▍  | 349/470 [03:02<00:35,  3.43it/s] 74%|███████▍  | 350/470 [03:02<00:34,  3.43it/s] 75%|███████▍  | 351/470 [03:02<00:34,  3.43it/s] 75%|███████▍  | 352/470 [03:03<00:34,  3.43it/s] 75%|███████▌  | 353/470 [03:03<00:34,  3.44it/s] 75%|███████▌  | 354/470 [03:03<00:34,  3.35it/s] 76%|███████▌  | 355/470 [03:04<00:34,  3.38it/s] 76%|███████▌  | 356/470 [03:04<00:33,  3.40it/s] 76%|███████▌  | 357/470 [03:04<00:33,  3.41it/s] 76%|███████▌  | 358/470 [03:05<00:32,  3.42it/s] 76%|███████▋  | 359/470 [03:05<00:32,  3.42it/s] 77%|███████▋  | 360/470 [03:05<00:32,  3.43it/s] 77%|███████▋  | 361/470 [03:05<00:31,  3.43it/s] 77%|███████▋  | 362/470 [03:06<00:31,  3.44it/s] 77%|███████▋  | 363/470 [03:06<00:31,  3.44it/s] 77%|███████▋  | 364/470 [03:06<00:30,  3.44it/s] 78%|███████▊  | 365/470 [03:07<00:31,  3.37it/s] 78%|███████▊  | 366/470 [03:07<00:30,  3.39it/s] 78%|███████▊  | 367/470 [03:07<00:30,  3.41it/s] 78%|███████▊  | 368/470 [03:07<00:29,  3.41it/s] 79%|███████▊  | 369/470 [03:08<00:29,  3.42it/s] 79%|███████▊  | 370/470 [03:08<00:29,  3.43it/s] 79%|███████▉  | 371/470 [03:08<00:28,  3.43it/s] 79%|███████▉  | 372/470 [03:09<00:28,  3.43it/s] 79%|███████▉  | 373/470 [03:09<00:28,  3.44it/s] 80%|███████▉  | 374/470 [03:09<00:27,  3.44it/s] 80%|███████▉  | 375/470 [03:09<00:27,  3.44it/s] 80%|████████  | 376/470 [03:10<00:26,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 00:51:42,412 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:51:42,412 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:51:42,412 >>   Batch size = 8
{'eval_loss': 1.0516771078109741, 'eval_runtime': 13.8716, 'eval_samples_per_second': 351.943, 'eval_steps_per_second': 44.047, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.00it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.59it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.10it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.34it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.75it/s][A
  5%|▌         | 32/611 [00:00<00:12, 45.09it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.44it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.15it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.27it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.37it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.42it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.59it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.72it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.76it/s][A
 13%|█▎        | 77/611 [00:01<00:11, 44.52it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.19it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.22it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.34it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.34it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.54it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.60it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.67it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.51it/s][A
 21%|██        | 127/611 [00:02<00:11, 41.12it/s][A
 22%|██▏       | 132/611 [00:02<00:11, 42.19it/s][A
 22%|██▏       | 137/611 [00:03<00:11, 42.95it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.33it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 43.77it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.07it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.23it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.21it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.98it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.24it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.36it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.35it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.50it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.54it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.49it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.36it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.10it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.24it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.41it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.43it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.50it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.50it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.64it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.50it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.38it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.18it/s][A
 43%|████▎     | 262/611 [00:05<00:08, 41.08it/s][A
 44%|████▎     | 267/611 [00:06<00:08, 42.23it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 42.88it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.60it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.03it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.27it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.27it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.12it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.84it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.91it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.91it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.22it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.37it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.54it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.67it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.57it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.38it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.19it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.38it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.50it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.52it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.66it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.52it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.36it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.20it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.32it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.54it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.65it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.59it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.52it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.31it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.21it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.20it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.20it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.30it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.55it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.56it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.65it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.50it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.33it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.31it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.18it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.16it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.40it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.52it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.64it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.57it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.34it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.16it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.29it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.29it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.52it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.92it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.20it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.23it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.33it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.25it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.25it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.19it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.36it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.47it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.51it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.57it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.48it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.43it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.43it/s][A 80%|████████  | 376/470 [03:24<00:26,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:51:56,338 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 00:51:56,472 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:51:59,107 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:51:59,241 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:51:59,327 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:39<13:50,  8.93s/it] 80%|████████  | 378/470 [03:39<09:44,  6.35s/it] 81%|████████  | 379/470 [03:39<06:52,  4.53s/it] 81%|████████  | 380/470 [03:40<04:53,  3.26s/it] 81%|████████  | 381/470 [03:40<03:31,  2.37s/it] 81%|████████▏ | 382/470 [03:40<02:33,  1.75s/it] 81%|████████▏ | 383/470 [03:41<01:54,  1.31s/it] 82%|████████▏ | 384/470 [03:41<01:26,  1.01s/it] 82%|████████▏ | 385/470 [03:41<01:07,  1.26it/s] 82%|████████▏ | 386/470 [03:42<00:54,  1.55it/s] 82%|████████▏ | 387/470 [03:42<00:44,  1.86it/s] 83%|████████▎ | 388/470 [03:42<00:38,  2.15it/s] 83%|████████▎ | 389/470 [03:42<00:34,  2.35it/s] 83%|████████▎ | 390/470 [03:43<00:30,  2.59it/s] 83%|████████▎ | 391/470 [03:43<00:28,  2.79it/s] 83%|████████▎ | 392/470 [03:43<00:26,  2.95it/s] 84%|████████▎ | 393/470 [03:44<00:25,  3.07it/s] 84%|████████▍ | 394/470 [03:44<00:24,  3.16it/s] 84%|████████▍ | 395/470 [03:44<00:23,  3.23it/s] 84%|████████▍ | 396/470 [03:45<00:22,  3.28it/s] 84%|████████▍ | 397/470 [03:45<00:22,  3.31it/s] 85%|████████▍ | 398/470 [03:45<00:21,  3.34it/s] 85%|████████▍ | 399/470 [03:45<00:21,  3.36it/s] 85%|████████▌ | 400/470 [03:46<00:21,  3.19it/s] 85%|████████▌ | 401/470 [03:46<00:21,  3.25it/s] 86%|████████▌ | 402/470 [03:46<00:20,  3.30it/s] 86%|████████▌ | 403/470 [03:47<00:20,  3.32it/s] 86%|████████▌ | 404/470 [03:47<00:19,  3.34it/s] 86%|████████▌ | 405/470 [03:47<00:19,  3.36it/s] 86%|████████▋ | 406/470 [03:48<00:18,  3.37it/s] 87%|████████▋ | 407/470 [03:48<00:18,  3.38it/s] 87%|████████▋ | 408/470 [03:48<00:18,  3.38it/s] 87%|████████▋ | 409/470 [03:48<00:18,  3.39it/s] 87%|████████▋ | 410/470 [03:49<00:18,  3.28it/s] 87%|████████▋ | 411/470 [03:49<00:17,  3.31it/s] 88%|████████▊ | 412/470 [03:49<00:17,  3.34it/s] 88%|████████▊ | 413/470 [03:50<00:16,  3.36it/s] 88%|████████▊ | 414/470 [03:50<00:16,  3.37it/s] 88%|████████▊ | 415/470 [03:50<00:16,  3.38it/s] 89%|████████▊ | 416/470 [03:50<00:15,  3.38it/s] 89%|████████▊ | 417/470 [03:51<00:15,  3.38it/s] 89%|████████▉ | 418/470 [03:51<00:15,  3.38it/s] 89%|████████▉ | 419/470 [03:51<00:15,  3.39it/s] 89%|████████▉ | 420/470 [03:52<00:14,  3.39it/s] 90%|████████▉ | 421/470 [03:52<00:14,  3.31it/s] 90%|████████▉ | 422/470 [03:52<00:14,  3.33it/s] 90%|█████████ | 423/470 [03:53<00:14,  3.35it/s] 90%|█████████ | 424/470 [03:53<00:13,  3.37it/s] 90%|█████████ | 425/470 [03:53<00:13,  3.37it/s] 91%|█████████ | 426/470 [03:53<00:13,  3.38it/s] 91%|█████████ | 427/470 [03:54<00:12,  3.38it/s] 91%|█████████ | 428/470 [03:54<00:12,  3.39it/s] 91%|█████████▏| 429/470 [03:54<00:12,  3.39it/s] 91%|█████████▏| 430/470 [03:55<00:11,  3.39it/s] 92%|█████████▏| 431/470 [03:55<00:11,  3.39it/s] 92%|█████████▏| 432/470 [03:55<00:11,  3.33it/s] 92%|█████████▏| 433/470 [03:56<00:11,  3.35it/s] 92%|█████████▏| 434/470 [03:56<00:10,  3.36it/s] 93%|█████████▎| 435/470 [03:56<00:10,  3.37it/s] 93%|█████████▎| 436/470 [03:56<00:10,  3.38it/s] 93%|█████████▎| 437/470 [03:57<00:09,  3.38it/s] 93%|█████████▎| 438/470 [03:57<00:09,  3.39it/s] 93%|█████████▎| 439/470 [03:57<00:09,  3.39it/s] 94%|█████████▎| 440/470 [03:58<00:08,  3.39it/s] 94%|█████████▍| 441/470 [03:58<00:08,  3.39it/s] 94%|█████████▍| 442/470 [03:58<00:08,  3.39it/s] 94%|█████████▍| 443/470 [03:59<00:08,  3.26it/s] 94%|█████████▍| 444/470 [03:59<00:07,  3.29it/s] 95%|█████████▍| 445/470 [03:59<00:07,  3.32it/s] 95%|█████████▍| 446/470 [03:59<00:07,  3.34it/s] 95%|█████████▌| 447/470 [04:00<00:06,  3.35it/s] 95%|█████████▌| 448/470 [04:00<00:06,  3.36it/s] 96%|█████████▌| 449/470 [04:00<00:06,  3.36it/s] 96%|█████████▌| 450/470 [04:01<00:05,  3.37it/s] 96%|█████████▌| 451/470 [04:01<00:05,  3.37it/s] 96%|█████████▌| 452/470 [04:01<00:05,  3.38it/s] 96%|█████████▋| 453/470 [04:01<00:05,  3.38it/s] 97%|█████████▋| 454/470 [04:02<00:04,  3.32it/s] 97%|█████████▋| 455/470 [04:02<00:04,  3.34it/s] 97%|█████████▋| 456/470 [04:02<00:04,  3.35it/s] 97%|█████████▋| 457/470 [04:03<00:03,  3.36it/s] 97%|█████████▋| 458/470 [04:03<00:03,  3.37it/s] 98%|█████████▊| 459/470 [04:03<00:03,  3.37it/s] 98%|█████████▊| 460/470 [04:04<00:02,  3.38it/s] 98%|█████████▊| 461/470 [04:04<00:02,  3.38it/s] 98%|█████████▊| 462/470 [04:04<00:02,  3.38it/s] 99%|█████████▊| 463/470 [04:04<00:02,  3.38it/s] 99%|█████████▊| 464/470 [04:05<00:01,  3.38it/s] 99%|█████████▉| 465/470 [04:05<00:01,  3.29it/s] 99%|█████████▉| 466/470 [04:05<00:01,  3.32it/s] 99%|█████████▉| 467/470 [04:06<00:00,  3.34it/s]100%|█████████▉| 468/470 [04:06<00:00,  3.35it/s]100%|█████████▉| 469/470 [04:06<00:00,  3.36it/s]100%|██████████| 470/470 [04:06<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 00:52:39,168 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:52:39,168 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:52:39,168 >>   Batch size = 8
{'eval_loss': 1.0599639415740967, 'eval_runtime': 13.8125, 'eval_samples_per_second': 353.449, 'eval_steps_per_second': 44.235, 'epoch': 4.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.76it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.40it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.13it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.21it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.64it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.97it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.60it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.23it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.39it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.44it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.56it/s][A
 10%|█         | 62/611 [00:01<00:12, 42.75it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.35it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.86it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.80it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.82it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.88it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.00it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.19it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.20it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.26it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.46it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.59it/s][A
 20%|█▉        | 122/611 [00:02<00:10, 44.51it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.30it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.20it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.12it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.21it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.29it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.36it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.56it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.58it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.39it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.23it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.29it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.34it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.24it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.31it/s][A
 32%|███▏      | 197/611 [00:04<00:10, 40.20it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 41.60it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 42.59it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.16it/s][A
 36%|███▌      | 217/611 [00:04<00:09, 43.54it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.84it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.93it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.02it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.76it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 43.83it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.20it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.45it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.42it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.40it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.37it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.22it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.07it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.03it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.32it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.46it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.64it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.54it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.48it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.34it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.14it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.05it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 41.13it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 42.19it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.00it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.52it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.93it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.17it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.25it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.12it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.76it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.83it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.01it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.29it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.54it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.63it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.72it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.54it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.18it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.91it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.01it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.15it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.43it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.45it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.57it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.69it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.47it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.24it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.14it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 37.38it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 39.35it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 40.96it/s][A
 79%|███████▉  | 482/611 [00:10<00:03, 42.10it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 42.91it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.41it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.80it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.98it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.73it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.57it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.77it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.02it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.18it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.32it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.49it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.56it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.46it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.07it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.93it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.06it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.20it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.34it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.52it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.57it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.58it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.37it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.07it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.77it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.01it/s][A100%|██████████| 470/470 [04:20<00:00,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:52:53,221 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 00:52:53,392 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:52:56,647 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:52:56,779 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:52:56,852 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:53:03,912 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:53:03,942 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94 (score: 1.0186160802841187).
                                                 100%|██████████| 470/470 [04:44<00:00,  3.60it/s]100%|██████████| 470/470 [04:44<00:00,  1.65it/s]
[INFO|trainer.py:1894] 2023-08-29 00:53:16,525 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 00:53:16,683 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:53:19,882 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:53:20,076 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:53:20,180 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:53:20,959 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   train_loss               =     0.3326
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   train_runtime            = 0:04:44.30
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   train_samples_per_second =    105.521
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:20,959 >>   train_steps_per_second   =      1.653
{'eval_loss': 1.070593237876892, 'eval_runtime': 13.9116, 'eval_samples_per_second': 350.93, 'eval_steps_per_second': 43.92, 'epoch': 5.0}
{'train_runtime': 284.3047, 'train_samples_per_second': 105.521, 'train_steps_per_second': 1.653, 'train_loss': 0.33262488182554856, 'epoch': 5.0}
08/29/2023 00:53:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:53:21,384 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:53:21,384 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:53:21,384 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:11, 54.95it/s]  2%|▏         | 12/611 [00:00<00:12, 48.67it/s]  3%|▎         | 17/611 [00:00<00:12, 47.02it/s]  4%|▎         | 22/611 [00:00<00:12, 46.31it/s]  4%|▍         | 27/611 [00:00<00:12, 45.96it/s]  5%|▌         | 32/611 [00:00<00:12, 45.63it/s]  6%|▌         | 37/611 [00:00<00:12, 45.52it/s]  7%|▋         | 42/611 [00:00<00:12, 45.19it/s]  8%|▊         | 47/611 [00:01<00:12, 44.61it/s]  9%|▊         | 52/611 [00:01<00:12, 44.17it/s]  9%|▉         | 57/611 [00:01<00:12, 44.03it/s] 10%|█         | 62/611 [00:01<00:12, 44.12it/s] 11%|█         | 67/611 [00:01<00:12, 44.33it/s] 12%|█▏        | 72/611 [00:01<00:13, 41.36it/s] 13%|█▎        | 77/611 [00:01<00:12, 42.46it/s] 13%|█▎        | 82/611 [00:01<00:12, 43.22it/s] 14%|█▍        | 87/611 [00:01<00:11, 43.71it/s] 15%|█▌        | 92/611 [00:02<00:11, 43.95it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.91it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.94it/s] 18%|█▊        | 107/611 [00:02<00:11, 43.90it/s] 18%|█▊        | 112/611 [00:02<00:11, 43.86it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.04it/s] 20%|█▉        | 122/611 [00:02<00:11, 44.28it/s] 21%|██        | 127/611 [00:02<00:10, 44.53it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.60it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.67it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.47it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.19it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.23it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.21it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.28it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.51it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.67it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.76it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.59it/s] 31%|███       | 187/611 [00:04<00:09, 44.43it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.42it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.34it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.19it/s] 34%|███▍      | 207/611 [00:04<00:09, 43.49it/s] 35%|███▍      | 212/611 [00:04<00:09, 43.96it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.31it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.54it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.40it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.17it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.20it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.25it/s] 40%|████      | 247/611 [00:05<00:08, 44.12it/s] 41%|████      | 252/611 [00:05<00:08, 44.19it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.40it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.46it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.64it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.50it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.48it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.40it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.26it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.16it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.26it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.47it/s] 50%|█████     | 307/611 [00:06<00:06, 44.57it/s] 51%|█████     | 312/611 [00:07<00:06, 44.58it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.50it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.44it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.27it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.19it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.12it/s] 56%|█████▌    | 342/611 [00:07<00:06, 42.91it/s] 57%|█████▋    | 347/611 [00:07<00:06, 41.45it/s] 58%|█████▊    | 352/611 [00:07<00:06, 42.80it/s] 58%|█████▊    | 357/611 [00:08<00:05, 43.37it/s] 59%|█████▉    | 362/611 [00:08<00:05, 43.83it/s] 60%|██████    | 367/611 [00:08<00:05, 44.03it/s] 61%|██████    | 372/611 [00:08<00:05, 43.97it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.00it/s] 63%|██████▎   | 382/611 [00:08<00:05, 43.91it/s] 63%|██████▎   | 387/611 [00:08<00:05, 43.89it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.11it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.39it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.44it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.58it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.54it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.44it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.14it/s] 71%|███████   | 432/611 [00:09<00:04, 44.09it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.20it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.34it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.47it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.61it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.56it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.35it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.23it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.17it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.10it/s] 79%|███████▉  | 482/611 [00:10<00:03, 41.57it/s] 80%|███████▉  | 487/611 [00:11<00:02, 42.46it/s] 81%|████████  | 492/611 [00:11<00:02, 43.26it/s] 81%|████████▏ | 497/611 [00:11<00:02, 43.76it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.06it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.12it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.07it/s] 85%|████████▍ | 517/611 [00:11<00:02, 43.98it/s] 85%|████████▌ | 522/611 [00:11<00:02, 43.83it/s] 86%|████████▋ | 527/611 [00:11<00:01, 43.99it/s] 87%|████████▋ | 532/611 [00:12<00:01, 44.26it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.47it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.61it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.59it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.38it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.33it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.08it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.11it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.16it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.31it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.48it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.65it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.52it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.40it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.26it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.10it/s]100%|██████████| 611/611 [00:13<00:00, 44.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:53:35,246 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   eval_loss               =     1.0186
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   eval_runtime            = 0:00:13.83
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   eval_samples_per_second =     352.78
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   eval_steps_per_second   =     44.152
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:53:35,247 >>   perplexity              =     2.7694
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:44,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:44,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:44,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:44,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:44,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:53:45,280 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:53:45,281 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:53:45,593 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:53:46,649 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:53:46,649 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:48,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:48,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:48,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:48,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:48,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:53:49,765 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:53:49,766 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:53:50,094 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:53:50,246 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:53:50,246 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.74it/s]Extractor Predicting: 23it [00:14,  1.74it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.70it/s]Extractor Predicting: 33it [00:20,  1.69it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:29,  1.65it/s]Extractor Predicting: 49it [00:29,  1.66it/s]Extractor Predicting: 50it [00:30,  1.61it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:32,  1.64it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:33,  1.54it/s]Extractor Predicting: 56it [00:34,  1.53it/s]Extractor Predicting: 57it [00:34,  1.56it/s]Extractor Predicting: 58it [00:35,  1.56it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:36,  1.59it/s]Extractor Predicting: 61it [00:37,  1.58it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:38,  1.56it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:40,  1.56it/s]Extractor Predicting: 66it [00:40,  1.53it/s]Extractor Predicting: 67it [00:41,  1.55it/s]Extractor Predicting: 68it [00:41,  1.58it/s]Extractor Predicting: 69it [00:42,  1.58it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:44,  1.53it/s]Extractor Predicting: 73it [00:45,  1.54it/s]Extractor Predicting: 74it [00:45,  1.55it/s]Extractor Predicting: 75it [00:46,  1.56it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:48,  1.61it/s]Extractor Predicting: 80it [00:49,  1.61it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:50,  1.58it/s]Extractor Predicting: 83it [00:51,  1.55it/s]Extractor Predicting: 84it [00:52,  1.56it/s]Extractor Predicting: 85it [00:52,  1.56it/s]Extractor Predicting: 86it [00:53,  1.55it/s]Extractor Predicting: 87it [00:54,  1.55it/s]Extractor Predicting: 88it [00:54,  1.48it/s]Extractor Predicting: 89it [00:55,  1.50it/s]Extractor Predicting: 90it [00:56,  1.51it/s]Extractor Predicting: 91it [00:56,  1.51it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:57,  1.61it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [00:59,  1.62it/s]Extractor Predicting: 97it [01:00,  1.65it/s]Extractor Predicting: 98it [01:01,  1.60it/s]Extractor Predicting: 99it [01:01,  1.54it/s]Extractor Predicting: 100it [01:02,  1.58it/s]Extractor Predicting: 101it [01:03,  1.50it/s]Extractor Predicting: 102it [01:03,  1.49it/s]Extractor Predicting: 103it [01:04,  1.51it/s]Extractor Predicting: 104it [01:05,  1.53it/s]Extractor Predicting: 105it [01:05,  1.55it/s]Extractor Predicting: 106it [01:06,  1.61it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:07,  1.64it/s]Extractor Predicting: 109it [01:08,  1.62it/s]Extractor Predicting: 110it [01:08,  1.63it/s]Extractor Predicting: 111it [01:09,  1.63it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:10,  1.59it/s]Extractor Predicting: 114it [01:11,  1.58it/s]Extractor Predicting: 115it [01:11,  1.59it/s]Extractor Predicting: 116it [01:12,  1.52it/s]Extractor Predicting: 117it [01:13,  1.51it/s]Extractor Predicting: 118it [01:13,  1.53it/s]Extractor Predicting: 119it [01:14,  1.52it/s]Extractor Predicting: 120it [01:15,  1.49it/s]Extractor Predicting: 121it [01:15,  1.46it/s]Extractor Predicting: 122it [01:16,  1.48it/s]Extractor Predicting: 123it [01:17,  1.52it/s]Extractor Predicting: 124it [01:17,  1.54it/s]Extractor Predicting: 125it [01:18,  1.56it/s]Extractor Predicting: 126it [01:19,  1.49it/s]Extractor Predicting: 127it [01:19,  1.50it/s]Extractor Predicting: 128it [01:20,  1.52it/s]Extractor Predicting: 129it [01:21,  1.56it/s]Extractor Predicting: 130it [01:21,  1.50it/s]Extractor Predicting: 131it [01:22,  1.52it/s]Extractor Predicting: 132it [01:23,  1.42it/s]Extractor Predicting: 133it [01:23,  1.44it/s]Extractor Predicting: 134it [01:24,  1.45it/s]Extractor Predicting: 135it [01:25,  1.46it/s]Extractor Predicting: 136it [01:25,  1.50it/s]Extractor Predicting: 137it [01:26,  1.48it/s]Extractor Predicting: 138it [01:27,  1.50it/s]Extractor Predicting: 139it [01:27,  1.49it/s]Extractor Predicting: 140it [01:28,  1.49it/s]Extractor Predicting: 141it [01:29,  1.51it/s]Extractor Predicting: 142it [01:29,  1.51it/s]Extractor Predicting: 143it [01:30,  1.50it/s]Extractor Predicting: 144it [01:31,  1.54it/s]Extractor Predicting: 145it [01:31,  1.58it/s]Extractor Predicting: 146it [01:32,  1.56it/s]Extractor Predicting: 147it [01:33,  1.53it/s]Extractor Predicting: 148it [01:33,  1.55it/s]Extractor Predicting: 149it [01:34,  1.54it/s]Extractor Predicting: 150it [01:35,  1.50it/s]Extractor Predicting: 151it [01:35,  1.50it/s]Extractor Predicting: 152it [01:36,  1.51it/s]Extractor Predicting: 153it [01:37,  1.51it/s]Extractor Predicting: 154it [01:37,  1.52it/s]Extractor Predicting: 155it [01:38,  1.53it/s]Extractor Predicting: 156it [01:39,  1.47it/s]Extractor Predicting: 157it [01:39,  1.42it/s]Extractor Predicting: 158it [01:40,  1.39it/s]Extractor Predicting: 159it [01:41,  1.43it/s]Extractor Predicting: 160it [01:41,  1.47it/s]Extractor Predicting: 161it [01:42,  1.49it/s]Extractor Predicting: 162it [01:43,  1.50it/s]Extractor Predicting: 163it [01:43,  1.50it/s]Extractor Predicting: 164it [01:44,  1.54it/s]Extractor Predicting: 165it [01:45,  1.52it/s]Extractor Predicting: 166it [01:45,  1.55it/s]Extractor Predicting: 167it [01:46,  1.54it/s]Extractor Predicting: 168it [01:47,  1.54it/s]Extractor Predicting: 169it [01:47,  1.56it/s]Extractor Predicting: 170it [01:48,  1.56it/s]Extractor Predicting: 171it [01:49,  1.58it/s]Extractor Predicting: 172it [01:49,  1.60it/s]Extractor Predicting: 173it [01:50,  1.54it/s]Extractor Predicting: 174it [01:50,  1.56it/s]Extractor Predicting: 175it [01:51,  1.54it/s]Extractor Predicting: 176it [01:52,  1.55it/s]Extractor Predicting: 177it [01:52,  1.53it/s]Extractor Predicting: 178it [01:53,  1.53it/s]Extractor Predicting: 179it [01:54,  1.53it/s]Extractor Predicting: 180it [01:54,  1.53it/s]Extractor Predicting: 181it [01:55,  1.50it/s]Extractor Predicting: 182it [01:56,  1.51it/s]Extractor Predicting: 183it [01:56,  1.47it/s]Extractor Predicting: 184it [01:57,  1.50it/s]Extractor Predicting: 185it [01:58,  1.60it/s]Extractor Predicting: 185it [01:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:04,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:04,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:04,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:04,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:04,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:56:04,954 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:56:04,955 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:56:05,557 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:56:06,613 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:56:06,613 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:09,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:09,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:09,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:09,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:09,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:56:10,384 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:56:10,385 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:56:10,978 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:56:11,131 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:56:11,131 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3164655507979759,
  "recall": 0.16653011061040557,
  "score": 0.21822574151120655,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.56it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:19,  1.73it/s]Extractor Predicting: 32it [00:19,  1.73it/s]Extractor Predicting: 33it [00:20,  1.70it/s]Extractor Predicting: 34it [00:20,  1.71it/s]Extractor Predicting: 35it [00:21,  1.71it/s]Extractor Predicting: 36it [00:22,  1.70it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:23,  1.70it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.55it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:29,  1.68it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.65it/s]Extractor Predicting: 51it [00:31,  1.68it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:32,  1.67it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:33,  1.64it/s]Extractor Predicting: 56it [00:34,  1.63it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.73it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:36,  1.65it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.55it/s]Extractor Predicting: 64it [00:39,  1.50it/s]Extractor Predicting: 65it [00:39,  1.50it/s]Extractor Predicting: 66it [00:40,  1.49it/s]Extractor Predicting: 67it [00:41,  1.49it/s]Extractor Predicting: 68it [00:41,  1.50it/s]Extractor Predicting: 69it [00:42,  1.50it/s]Extractor Predicting: 70it [00:43,  1.52it/s]Extractor Predicting: 71it [00:43,  1.55it/s]Extractor Predicting: 72it [00:44,  1.56it/s]Extractor Predicting: 73it [00:45,  1.61it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:47,  1.65it/s]Extractor Predicting: 78it [00:48,  1.64it/s]Extractor Predicting: 79it [00:48,  1.67it/s]Extractor Predicting: 80it [00:49,  1.72it/s]Extractor Predicting: 81it [00:49,  1.68it/s]Extractor Predicting: 82it [00:50,  1.70it/s]Extractor Predicting: 83it [00:51,  1.65it/s]Extractor Predicting: 84it [00:51,  1.63it/s]Extractor Predicting: 85it [00:52,  1.58it/s]Extractor Predicting: 86it [00:53,  1.55it/s]Extractor Predicting: 87it [00:53,  1.56it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.56it/s]Extractor Predicting: 92it [00:56,  1.56it/s]Extractor Predicting: 93it [00:57,  1.59it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:58,  1.60it/s]Extractor Predicting: 96it [00:59,  1.59it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [01:00,  1.60it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:01,  1.57it/s]Extractor Predicting: 101it [01:02,  1.59it/s]Extractor Predicting: 102it [01:03,  1.60it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:04,  1.61it/s]Extractor Predicting: 105it [01:04,  1.62it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:06,  1.62it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.62it/s]Extractor Predicting: 110it [01:08,  1.63it/s]Extractor Predicting: 111it [01:08,  1.63it/s]Extractor Predicting: 112it [01:09,  1.61it/s]Extractor Predicting: 113it [01:09,  1.60it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.59it/s]Extractor Predicting: 116it [01:11,  1.58it/s]Extractor Predicting: 117it [01:12,  1.62it/s]Extractor Predicting: 118it [01:13,  1.60it/s]Extractor Predicting: 119it [01:13,  1.60it/s]Extractor Predicting: 120it [01:14,  1.62it/s]Extractor Predicting: 121it [01:14,  1.66it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:16,  1.62it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:19,  1.59it/s]Extractor Predicting: 130it [01:20,  1.62it/s]Extractor Predicting: 131it [01:21,  1.59it/s]Extractor Predicting: 132it [01:21,  1.56it/s]Extractor Predicting: 133it [01:22,  1.57it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:23,  1.59it/s]Extractor Predicting: 136it [01:24,  1.63it/s]Extractor Predicting: 137it [01:24,  1.58it/s]Extractor Predicting: 138it [01:25,  1.60it/s]Extractor Predicting: 139it [01:26,  1.62it/s]Extractor Predicting: 140it [01:26,  1.62it/s]Extractor Predicting: 141it [01:27,  1.58it/s]Extractor Predicting: 142it [01:28,  1.61it/s]Extractor Predicting: 143it [01:28,  1.52it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:30,  1.39it/s]Extractor Predicting: 146it [01:30,  1.46it/s]Extractor Predicting: 147it [01:31,  1.54it/s]Extractor Predicting: 148it [01:32,  1.52it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:33,  1.64it/s]Extractor Predicting: 152it [01:34,  1.61it/s]Extractor Predicting: 153it [01:35,  1.58it/s]Extractor Predicting: 154it [01:35,  1.55it/s]Extractor Predicting: 155it [01:36,  1.57it/s]Extractor Predicting: 156it [01:37,  1.62it/s]Extractor Predicting: 157it [01:37,  1.58it/s]Extractor Predicting: 158it [01:38,  1.52it/s]Extractor Predicting: 159it [01:39,  1.55it/s]Extractor Predicting: 160it [01:39,  1.57it/s]Extractor Predicting: 161it [01:40,  1.61it/s]Extractor Predicting: 162it [01:40,  1.60it/s]Extractor Predicting: 163it [01:41,  1.54it/s]Extractor Predicting: 164it [01:42,  1.55it/s]Extractor Predicting: 165it [01:42,  1.52it/s]Extractor Predicting: 166it [01:43,  1.50it/s]Extractor Predicting: 167it [01:44,  1.51it/s]Extractor Predicting: 168it [01:44,  1.53it/s]Extractor Predicting: 169it [01:45,  1.57it/s]Extractor Predicting: 170it [01:46,  1.60it/s]Extractor Predicting: 171it [01:46,  1.61it/s]Extractor Predicting: 172it [01:47,  1.64it/s]Extractor Predicting: 173it [01:47,  1.61it/s]Extractor Predicting: 174it [01:48,  1.61it/s]Extractor Predicting: 175it [01:49,  1.61it/s]Extractor Predicting: 176it [01:49,  1.62it/s]Extractor Predicting: 177it [01:50,  1.60it/s]Extractor Predicting: 178it [01:51,  1.58it/s]Extractor Predicting: 179it [01:51,  1.56it/s]Extractor Predicting: 180it [01:52,  1.60it/s]Extractor Predicting: 181it [01:52,  1.60it/s]Extractor Predicting: 182it [01:53,  1.63it/s]Extractor Predicting: 183it [01:54,  1.64it/s]Extractor Predicting: 184it [01:54,  1.63it/s]Extractor Predicting: 185it [01:55,  1.64it/s]Extractor Predicting: 186it [01:56,  1.59it/s]Extractor Predicting: 187it [01:56,  1.62it/s]Extractor Predicting: 188it [01:57,  1.60it/s]Extractor Predicting: 189it [01:57,  1.61it/s]Extractor Predicting: 190it [01:58,  1.63it/s]Extractor Predicting: 191it [01:59,  1.63it/s]Extractor Predicting: 192it [01:59,  1.65it/s]Extractor Predicting: 193it [02:00,  1.64it/s]Extractor Predicting: 194it [02:00,  1.65it/s]Extractor Predicting: 195it [02:01,  1.63it/s]Extractor Predicting: 196it [02:02,  1.63it/s]Extractor Predicting: 197it [02:02,  1.62it/s]Extractor Predicting: 198it [02:03,  1.60it/s]Extractor Predicting: 199it [02:04,  1.60it/s]Extractor Predicting: 200it [02:04,  1.61it/s]Extractor Predicting: 201it [02:05,  1.62it/s]Extractor Predicting: 202it [02:05,  1.62it/s]Extractor Predicting: 203it [02:06,  1.64it/s]Extractor Predicting: 204it [02:07,  1.64it/s]Extractor Predicting: 205it [02:07,  1.64it/s]Extractor Predicting: 206it [02:08,  1.63it/s]Extractor Predicting: 207it [02:08,  1.64it/s]Extractor Predicting: 208it [02:09,  1.62it/s]Extractor Predicting: 209it [02:10,  1.60it/s]Extractor Predicting: 210it [02:10,  1.66it/s]Extractor Predicting: 211it [02:11,  1.64it/s]Extractor Predicting: 212it [02:11,  1.65it/s]Extractor Predicting: 213it [02:12,  1.65it/s]Extractor Predicting: 214it [02:13,  1.68it/s]Extractor Predicting: 215it [02:13,  1.66it/s]Extractor Predicting: 216it [02:14,  1.63it/s]Extractor Predicting: 217it [02:14,  1.64it/s]Extractor Predicting: 218it [02:15,  1.62it/s]Extractor Predicting: 219it [02:16,  1.62it/s]Extractor Predicting: 220it [02:16,  1.63it/s]Extractor Predicting: 221it [02:17,  1.64it/s]Extractor Predicting: 222it [02:18,  1.58it/s]Extractor Predicting: 223it [02:18,  1.53it/s]Extractor Predicting: 224it [02:19,  1.56it/s]Extractor Predicting: 225it [02:20,  1.58it/s]Extractor Predicting: 226it [02:20,  1.62it/s]Extractor Predicting: 227it [02:21,  1.63it/s]Extractor Predicting: 228it [02:21,  1.65it/s]Extractor Predicting: 229it [02:22,  1.67it/s]Extractor Predicting: 230it [02:22,  1.69it/s]Extractor Predicting: 231it [02:23,  1.69it/s]Extractor Predicting: 232it [02:24,  1.68it/s]Extractor Predicting: 233it [02:24,  1.66it/s]Extractor Predicting: 234it [02:25,  1.67it/s]Extractor Predicting: 235it [02:25,  1.64it/s]Extractor Predicting: 236it [02:26,  1.65it/s]Extractor Predicting: 237it [02:27,  1.65it/s]Extractor Predicting: 238it [02:27,  1.61it/s]Extractor Predicting: 239it [02:28,  1.64it/s]Extractor Predicting: 240it [02:29,  1.63it/s]Extractor Predicting: 241it [02:29,  1.64it/s]Extractor Predicting: 242it [02:30,  1.62it/s]Extractor Predicting: 243it [02:30,  1.57it/s]Extractor Predicting: 244it [02:31,  1.58it/s]Extractor Predicting: 245it [02:32,  1.62it/s]Extractor Predicting: 246it [02:32,  1.62it/s]Extractor Predicting: 247it [02:33,  1.65it/s]Extractor Predicting: 248it [02:34,  1.58it/s]Extractor Predicting: 249it [02:34,  1.58it/s]Extractor Predicting: 250it [02:35,  1.60it/s]Extractor Predicting: 251it [02:35,  1.60it/s]Extractor Predicting: 252it [02:36,  1.57it/s]Extractor Predicting: 253it [02:37,  1.53it/s]Extractor Predicting: 254it [02:37,  1.51it/s]Extractor Predicting: 255it [02:38,  1.54it/s]Extractor Predicting: 256it [02:39,  1.55it/s]Extractor Predicting: 257it [02:39,  1.55it/s]Extractor Predicting: 258it [02:40,  1.56it/s]Extractor Predicting: 259it [02:41,  1.57it/s]Extractor Predicting: 260it [02:41,  1.55it/s]Extractor Predicting: 261it [02:42,  1.57it/s]Extractor Predicting: 262it [02:43,  1.56it/s]Extractor Predicting: 263it [02:43,  1.57it/s]Extractor Predicting: 264it [02:44,  1.39it/s]Extractor Predicting: 265it [02:45,  1.46it/s]Extractor Predicting: 266it [02:45,  1.45it/s]Extractor Predicting: 267it [02:46,  1.47it/s]Extractor Predicting: 268it [02:47,  1.48it/s]Extractor Predicting: 269it [02:47,  1.51it/s]Extractor Predicting: 270it [02:48,  1.50it/s]Extractor Predicting: 271it [02:49,  1.50it/s]Extractor Predicting: 272it [02:49,  1.50it/s]Extractor Predicting: 273it [02:50,  1.52it/s]Extractor Predicting: 274it [02:51,  1.49it/s]Extractor Predicting: 275it [02:51,  1.51it/s]Extractor Predicting: 276it [02:52,  1.52it/s]Extractor Predicting: 277it [02:53,  1.53it/s]Extractor Predicting: 278it [02:53,  1.56it/s]Extractor Predicting: 279it [02:54,  1.52it/s]Extractor Predicting: 280it [02:55,  1.54it/s]Extractor Predicting: 281it [02:55,  1.49it/s]Extractor Predicting: 282it [02:56,  1.50it/s]Extractor Predicting: 283it [02:57,  1.52it/s]Extractor Predicting: 284it [02:57,  1.55it/s]Extractor Predicting: 285it [02:58,  1.54it/s]Extractor Predicting: 286it [02:59,  1.55it/s]Extractor Predicting: 287it [02:59,  1.50it/s]Extractor Predicting: 288it [03:00,  1.54it/s]Extractor Predicting: 289it [03:00,  1.56it/s]Extractor Predicting: 290it [03:01,  1.53it/s]Extractor Predicting: 291it [03:02,  1.50it/s]Extractor Predicting: 292it [03:02,  1.54it/s]Extractor Predicting: 293it [03:03,  1.54it/s]Extractor Predicting: 294it [03:04,  1.51it/s]Extractor Predicting: 295it [03:04,  1.52it/s]Extractor Predicting: 296it [03:05,  1.56it/s]Extractor Predicting: 297it [03:06,  1.56it/s]Extractor Predicting: 298it [03:06,  1.55it/s]Extractor Predicting: 299it [03:07,  1.52it/s]Extractor Predicting: 300it [03:08,  1.52it/s]Extractor Predicting: 301it [03:08,  1.55it/s]Extractor Predicting: 302it [03:09,  1.51it/s]Extractor Predicting: 303it [03:10,  1.56it/s]Extractor Predicting: 304it [03:10,  1.56it/s]Extractor Predicting: 305it [03:11,  1.60it/s]Extractor Predicting: 306it [03:11,  1.62it/s]Extractor Predicting: 307it [03:12,  1.59it/s]Extractor Predicting: 308it [03:13,  1.61it/s]Extractor Predicting: 309it [03:13,  1.56it/s]Extractor Predicting: 310it [03:14,  1.57it/s]Extractor Predicting: 311it [03:15,  1.54it/s]Extractor Predicting: 312it [03:15,  1.56it/s]Extractor Predicting: 313it [03:16,  1.51it/s]Extractor Predicting: 314it [03:17,  1.49it/s]Extractor Predicting: 315it [03:17,  1.53it/s]Extractor Predicting: 316it [03:18,  1.57it/s]Extractor Predicting: 317it [03:19,  1.59it/s]Extractor Predicting: 318it [03:19,  1.62it/s]Extractor Predicting: 319it [03:20,  1.57it/s]Extractor Predicting: 320it [03:20,  1.55it/s]Extractor Predicting: 321it [03:21,  1.55it/s]Extractor Predicting: 322it [03:22,  1.57it/s]Extractor Predicting: 323it [03:22,  1.51it/s]Extractor Predicting: 324it [03:23,  1.51it/s]Extractor Predicting: 325it [03:24,  1.55it/s]Extractor Predicting: 326it [03:24,  1.56it/s]Extractor Predicting: 327it [03:25,  1.57it/s]Extractor Predicting: 328it [03:26,  1.54it/s]Extractor Predicting: 329it [03:26,  1.54it/s]Extractor Predicting: 330it [03:27,  1.53it/s]Extractor Predicting: 331it [03:28,  1.55it/s]Extractor Predicting: 332it [03:28,  1.55it/s]Extractor Predicting: 333it [03:29,  1.53it/s]Extractor Predicting: 334it [03:30,  1.54it/s]Extractor Predicting: 335it [03:30,  1.53it/s]Extractor Predicting: 336it [03:31,  1.45it/s]Extractor Predicting: 337it [03:32,  1.46it/s]Extractor Predicting: 338it [03:32,  1.48it/s]Extractor Predicting: 339it [03:33,  1.50it/s]Extractor Predicting: 340it [03:34,  1.50it/s]Extractor Predicting: 341it [03:34,  1.49it/s]Extractor Predicting: 342it [03:35,  1.47it/s]Extractor Predicting: 343it [03:36,  1.51it/s]Extractor Predicting: 344it [03:36,  1.52it/s]Extractor Predicting: 345it [03:37,  1.48it/s]Extractor Predicting: 346it [03:38,  1.49it/s]Extractor Predicting: 347it [03:38,  1.51it/s]Extractor Predicting: 348it [03:39,  1.53it/s]Extractor Predicting: 349it [03:40,  1.51it/s]Extractor Predicting: 350it [03:40,  1.52it/s]Extractor Predicting: 351it [03:41,  1.55it/s]Extractor Predicting: 352it [03:42,  1.51it/s]Extractor Predicting: 353it [03:42,  1.49it/s]Extractor Predicting: 354it [03:43,  1.53it/s]Extractor Predicting: 355it [03:43,  1.56it/s]Extractor Predicting: 356it [03:44,  1.56it/s]Extractor Predicting: 357it [03:45,  1.54it/s]Extractor Predicting: 358it [03:45,  1.56it/s]Extractor Predicting: 359it [03:46,  1.55it/s]Extractor Predicting: 360it [03:47,  1.51it/s]Extractor Predicting: 361it [03:48,  1.34it/s]Extractor Predicting: 362it [03:48,  1.40it/s]Extractor Predicting: 363it [03:49,  1.47it/s]Extractor Predicting: 364it [03:50,  1.51it/s]Extractor Predicting: 365it [03:50,  1.49it/s]Extractor Predicting: 366it [03:51,  1.46it/s]Extractor Predicting: 367it [03:52,  1.46it/s]Extractor Predicting: 368it [03:52,  1.46it/s]Extractor Predicting: 369it [03:53,  1.42it/s]Extractor Predicting: 370it [03:54,  1.43it/s]Extractor Predicting: 371it [03:54,  1.47it/s]Extractor Predicting: 372it [03:55,  1.72it/s]Extractor Predicting: 372it [03:55,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:21,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:21,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:21,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:21,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:21,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:00:22,210 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:00:22,211 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:22,812 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:23,846 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:23,846 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:26,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:26,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:00:27,560 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:00:27,561 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:27,863 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:28,022 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:28,022 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.26597131681877445,
  "recall": 0.18312387791741472,
  "score": 0.2169059011164274,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:25,  1.42it/s]Extractor Predicting: 40it [00:25,  1.43it/s]Extractor Predicting: 41it [00:26,  1.44it/s]Extractor Predicting: 42it [00:27,  1.47it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.45it/s]Extractor Predicting: 45it [00:29,  1.44it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.47it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:33,  1.42it/s]Extractor Predicting: 53it [00:34,  1.44it/s]Extractor Predicting: 54it [00:35,  1.44it/s]Extractor Predicting: 55it [00:35,  1.48it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.35it/s]Extractor Predicting: 59it [00:38,  1.38it/s]Extractor Predicting: 60it [00:39,  1.39it/s]Extractor Predicting: 61it [00:40,  1.37it/s]Extractor Predicting: 62it [00:41,  1.38it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 63it [00:41,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:01:12,109 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:01:12,110 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:01:12,214 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:01:12,215 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:01:12,251 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:01:25,902 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:01:25,930 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:01:26,103 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:01:26,103 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:01:26,201 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:01:26,271 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4657113613101331,
  "recall": 0.13635001498351812,
  "score": 0.21094112192860454,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:01:26,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:27,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:27,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:28,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:28,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:29,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:29,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:30,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:31,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:31,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:32,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:32,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:33,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:33,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:34,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:35,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:35,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:36,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:36,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:37,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:38,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:11<02:47, 11.98s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:38,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:39,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:39,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:40,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:40,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:41,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:42,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:42,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:43,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:43,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:44,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:44,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:45,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:45,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:46,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:46,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:47,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:47,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:48,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:48,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:49,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:23<02:30, 11.59s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:49,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:50,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:50,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:51,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:51,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:52,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:52,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:53,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:53,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:54,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:54,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:55,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:55,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:56,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:56,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:57,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:57,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:58,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:58,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:32<02:06, 10.51s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:59,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:59,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:00,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:00,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:01,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:02,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:02,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:03,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:03,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:04,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:05,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:06,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:06,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:07,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:07,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:10,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:44<02:00, 10.99s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:10,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:12,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:12,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:13,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:13,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:14,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:14,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:15,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:15,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:16,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:18,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:18,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:20,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:21,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:21,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [00:55<01:51, 11.15s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:22,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:22,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:23,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:24,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:24,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:26,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:27,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:27,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:29,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:30,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:30,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:32,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:33,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:34,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:08<01:44, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:34,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:35,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:36,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:36,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:37,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:37,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:38,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:39,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:39,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:40,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:40,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:41,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:42,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:42,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:43,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:43,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:44,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:44,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:45,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:19<01:31, 11.47s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:46,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:46,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:48,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:50,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:50,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:51,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:52,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:52,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:53,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:53,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:55,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:56,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:56,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:57,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:31<01:20, 11.54s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:57,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:58,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:58,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:59,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:01,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:01,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:03,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:04,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:04,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:05,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:05,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:06,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:06,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:07,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:08,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:08,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:42<01:08, 11.48s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:09,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:09,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:10,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:10,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:11,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:12,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:12,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:13,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:13,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:15,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:16,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:16,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:17,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:18,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:19,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:19,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:20,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [01:54<00:58, 11.60s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:20,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:21,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:21,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:22,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:22,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:23,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:23,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:24,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:24,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:25,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:25,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:26,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:26,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:27,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:27,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:28,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:28,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:29,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:29,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:03<00:43, 10.88s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:30,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:30,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:31,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:31,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:32,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:33,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:33,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:34,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:34,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:35,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:36,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:36,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:37,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:37,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:38,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:38,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:39,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:40,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:40,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:41,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:15<00:33, 11.10s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:42,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:43,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:43,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:44,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:44,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:45,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:46,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:47,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:47,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:48,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:48,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:49,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:50,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:51,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:52,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:52,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:53,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:27<00:22, 11.33s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:53,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:54,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:54,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:55,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:55,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:56,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:56,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:57,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:57,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:58,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:58,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:59,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:59,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:00,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:00,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:01,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:01,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:02,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:02,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:36<00:10, 10.84s/it][WARNING|generation_utils.py:914] 2023-08-29 01:04:03,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:03,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:04,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:05,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:05,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:06,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:07,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:07,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:08,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:08,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:09,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:10,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:10,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:11,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:12,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:12,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:13,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:14,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:14,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:15,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [02:49<00:00, 11.48s/it]Generating: 100%|██████████| 15/15 [02:49<00:00, 11.31s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:24,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:24,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:24,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:24,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:24,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:04:25,153 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:04:25,155 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:04:25,784 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:04:26,893 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:04:26,893 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:30,005 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:30,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:30,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:30,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:30,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:04:30,674 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:04:30,675 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:04:31,277 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:04:31,448 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:04:31,449 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : conflict . Context : On 17 March 2014 , the United States and its allies launched an air strike on a convoy of armoured vehicle s in the eastern part of the Kolkata district of West Bengal , killing 17 people . Head Entity : West Bengal , Tail Entity : India .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 538, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 445, 'raw': 448}
{'target': 600, 'success': 477, 'raw': 480}
{'target': 600, 'success': 509, 'raw': 512}
{'target': 600, 'success': 541, 'raw': 544}
{'target': 600, 'success': 573, 'raw': 576}
{'target': 600, 'success': 605, 'raw': 608}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9950657894736842, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : creator .', 'success_rate': 0.959375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 502, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 384, 'raw': 384}
{'target': 600, 'success': 416, 'raw': 416}
{'target': 600, 'success': 448, 'raw': 448}
{'target': 600, 'success': 480, 'raw': 480}
{'target': 600, 'success': 512, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : residence .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.978125, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 6634
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6734, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.63it/s]Extractor Estimating: 3it [00:01,  1.66it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:02,  1.76it/s]Extractor Estimating: 6it [00:03,  1.77it/s]Extractor Estimating: 7it [00:03,  1.81it/s]Extractor Estimating: 8it [00:04,  1.81it/s]Extractor Estimating: 9it [00:05,  1.76it/s]Extractor Estimating: 10it [00:05,  1.76it/s]Extractor Estimating: 11it [00:06,  1.74it/s]Extractor Estimating: 12it [00:06,  1.79it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:08,  1.77it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:09,  1.72it/s]Extractor Estimating: 18it [00:10,  1.72it/s]Extractor Estimating: 19it [00:10,  1.74it/s]Extractor Estimating: 20it [00:11,  1.76it/s]Extractor Estimating: 21it [00:12,  1.75it/s]Extractor Estimating: 22it [00:12,  1.79it/s]Extractor Estimating: 23it [00:13,  1.82it/s]Extractor Estimating: 24it [00:13,  1.81it/s]Extractor Estimating: 25it [00:14,  1.82it/s]Extractor Estimating: 26it [00:14,  1.79it/s]Extractor Estimating: 27it [00:15,  1.78it/s]Extractor Estimating: 28it [00:15,  1.76it/s]Extractor Estimating: 29it [00:16,  1.75it/s]Extractor Estimating: 30it [00:17,  1.77it/s]Extractor Estimating: 31it [00:17,  1.71it/s]Extractor Estimating: 32it [00:18,  1.73it/s]Extractor Estimating: 33it [00:18,  1.76it/s]Extractor Estimating: 34it [00:19,  1.81it/s]Extractor Estimating: 35it [00:19,  1.72it/s]Extractor Estimating: 36it [00:20,  1.69it/s]Extractor Estimating: 37it [00:21,  1.68it/s]Extractor Estimating: 38it [00:21,  1.70it/s]Extractor Estimating: 39it [00:22,  1.72it/s]Extractor Estimating: 40it [00:22,  1.75it/s]Extractor Estimating: 41it [00:23,  1.76it/s]Extractor Estimating: 42it [00:24,  1.76it/s]Extractor Estimating: 43it [00:24,  1.82it/s]Extractor Estimating: 44it [00:25,  1.84it/s]Extractor Estimating: 45it [00:25,  1.83it/s]Extractor Estimating: 46it [00:26,  1.84it/s]Extractor Estimating: 47it [00:26,  1.80it/s]Extractor Estimating: 48it [00:27,  1.76it/s]Extractor Estimating: 49it [00:27,  1.74it/s]Extractor Estimating: 50it [00:28,  1.79it/s]Extractor Estimating: 51it [00:28,  1.89it/s]Extractor Estimating: 52it [00:29,  1.97it/s]Extractor Estimating: 53it [00:29,  2.03it/s]Extractor Estimating: 54it [00:30,  2.00it/s]Extractor Estimating: 55it [00:30,  2.07it/s]Extractor Estimating: 56it [00:31,  2.15it/s]Extractor Estimating: 57it [00:31,  2.16it/s]Extractor Estimating: 58it [00:32,  2.23it/s]Extractor Estimating: 59it [00:32,  2.18it/s]Extractor Estimating: 60it [00:33,  2.18it/s]Extractor Estimating: 61it [00:33,  2.23it/s]Extractor Estimating: 62it [00:33,  2.21it/s]Extractor Estimating: 63it [00:34,  2.27it/s]Extractor Estimating: 64it [00:34,  2.25it/s]Extractor Estimating: 65it [00:35,  2.19it/s]Extractor Estimating: 66it [00:35,  2.18it/s]Extractor Estimating: 67it [00:36,  2.21it/s]Extractor Estimating: 68it [00:36,  2.14it/s]Extractor Estimating: 69it [00:37,  2.17it/s]Extractor Estimating: 70it [00:37,  2.02it/s]Extractor Estimating: 71it [00:38,  2.07it/s]Extractor Estimating: 72it [00:38,  2.09it/s]Extractor Estimating: 73it [00:39,  2.15it/s]Extractor Estimating: 74it [00:39,  2.17it/s]Extractor Estimating: 75it [00:39,  2.13it/s]Extractor Estimating: 76it [00:40,  2.11it/s]Extractor Estimating: 77it [00:41,  2.01it/s]Extractor Estimating: 78it [00:41,  2.00it/s]Extractor Estimating: 79it [00:42,  1.92it/s]Extractor Estimating: 80it [00:42,  1.94it/s]Extractor Estimating: 81it [00:43,  1.91it/s]Extractor Estimating: 82it [00:43,  1.85it/s]Extractor Estimating: 83it [00:44,  1.88it/s]Extractor Estimating: 84it [00:44,  1.88it/s]Extractor Estimating: 85it [00:45,  1.90it/s]Extractor Estimating: 86it [00:45,  1.84it/s]Extractor Estimating: 87it [00:46,  1.81it/s]Extractor Estimating: 88it [00:46,  1.87it/s]Extractor Estimating: 89it [00:47,  1.88it/s]Extractor Estimating: 90it [00:47,  1.94it/s]Extractor Estimating: 91it [00:48,  1.93it/s]Extractor Estimating: 92it [00:48,  1.95it/s]Extractor Estimating: 93it [00:49,  1.96it/s]Extractor Estimating: 94it [00:49,  1.96it/s]Extractor Estimating: 95it [00:50,  1.89it/s]Extractor Estimating: 96it [00:51,  1.83it/s]Extractor Estimating: 97it [00:51,  1.87it/s]Extractor Estimating: 98it [00:52,  1.94it/s]Extractor Estimating: 99it [00:52,  1.86it/s]Extractor Estimating: 100it [00:53,  1.82it/s]Extractor Estimating: 101it [00:53,  1.75it/s]Extractor Estimating: 102it [00:54,  1.68it/s]Extractor Estimating: 103it [00:55,  1.67it/s]Extractor Estimating: 104it [00:55,  1.69it/s]Extractor Estimating: 105it [00:56,  1.67it/s]Extractor Estimating: 106it [00:56,  1.64it/s]Extractor Estimating: 107it [00:57,  1.70it/s]Extractor Estimating: 108it [00:58,  1.71it/s]Extractor Estimating: 109it [00:58,  1.70it/s]Extractor Estimating: 110it [00:59,  1.72it/s]Extractor Estimating: 111it [00:59,  1.71it/s]Extractor Estimating: 112it [01:00,  1.53it/s]Extractor Estimating: 113it [01:01,  1.56it/s]Extractor Estimating: 114it [01:01,  1.57it/s]Extractor Estimating: 115it [01:02,  1.63it/s]Extractor Estimating: 116it [01:03,  1.62it/s]Extractor Estimating: 117it [01:03,  1.64it/s]Extractor Estimating: 118it [01:04,  1.68it/s]Extractor Estimating: 119it [01:04,  1.71it/s]Extractor Estimating: 120it [01:05,  1.69it/s]Extractor Estimating: 121it [01:06,  1.62it/s]Extractor Estimating: 122it [01:06,  1.69it/s]Extractor Estimating: 123it [01:07,  1.54it/s]Extractor Estimating: 124it [01:07,  1.60it/s]Extractor Estimating: 125it [01:08,  1.58it/s]Extractor Estimating: 126it [01:09,  1.65it/s]Extractor Estimating: 127it [01:09,  1.67it/s]Extractor Estimating: 128it [01:10,  1.69it/s]Extractor Estimating: 129it [01:10,  1.68it/s]Extractor Estimating: 130it [01:11,  1.62it/s]Extractor Estimating: 131it [01:12,  1.66it/s]Extractor Estimating: 132it [01:12,  1.71it/s]Extractor Estimating: 133it [01:13,  1.69it/s]Extractor Estimating: 134it [01:13,  1.70it/s]Extractor Estimating: 135it [01:14,  1.71it/s]Extractor Estimating: 136it [01:14,  1.77it/s]Extractor Estimating: 137it [01:15,  1.76it/s]Extractor Estimating: 138it [01:16,  1.76it/s]Extractor Estimating: 139it [01:16,  1.67it/s]Extractor Estimating: 140it [01:17,  1.68it/s]Extractor Estimating: 141it [01:17,  1.67it/s]Extractor Estimating: 142it [01:18,  1.69it/s]Extractor Estimating: 143it [01:19,  1.66it/s]Extractor Estimating: 144it [01:19,  1.69it/s]Extractor Estimating: 145it [01:20,  1.72it/s]Extractor Estimating: 146it [01:20,  1.73it/s]Extractor Estimating: 147it [01:21,  1.67it/s]Extractor Estimating: 148it [01:22,  1.63it/s]Extractor Estimating: 149it [01:22,  1.64it/s]Extractor Estimating: 150it [01:23,  1.60it/s]Extractor Estimating: 151it [01:23,  1.65it/s]Extractor Estimating: 152it [01:24,  1.72it/s]Extractor Estimating: 153it [01:25,  1.71it/s]Extractor Estimating: 154it [01:25,  1.75it/s]Extractor Estimating: 155it [01:26,  1.75it/s]Extractor Estimating: 156it [01:26,  1.68it/s]Extractor Estimating: 157it [01:27,  1.71it/s]Extractor Estimating: 158it [01:28,  1.70it/s]Extractor Estimating: 159it [01:28,  1.76it/s]Extractor Estimating: 160it [01:29,  1.77it/s]Extractor Estimating: 161it [01:29,  1.78it/s]Extractor Estimating: 162it [01:30,  1.76it/s]Extractor Estimating: 163it [01:30,  1.72it/s]Extractor Estimating: 164it [01:31,  1.72it/s]Extractor Estimating: 165it [01:32,  1.69it/s]Extractor Estimating: 166it [01:32,  1.65it/s]Extractor Estimating: 167it [01:33,  1.65it/s]Extractor Estimating: 168it [01:33,  1.69it/s]Extractor Estimating: 169it [01:34,  1.73it/s]Extractor Estimating: 170it [01:34,  1.76it/s]Extractor Estimating: 171it [01:35,  1.75it/s]Extractor Estimating: 172it [01:36,  1.77it/s]Extractor Estimating: 173it [01:36,  1.72it/s]Extractor Estimating: 174it [01:37,  1.71it/s]Extractor Estimating: 175it [01:37,  1.69it/s]Extractor Estimating: 176it [01:38,  1.71it/s]Extractor Estimating: 177it [01:39,  1.73it/s]Extractor Estimating: 178it [01:39,  1.73it/s]Extractor Estimating: 179it [01:40,  1.73it/s]Extractor Estimating: 180it [01:40,  1.65it/s]Extractor Estimating: 181it [01:41,  1.70it/s]Extractor Estimating: 182it [01:42,  1.66it/s]Extractor Estimating: 183it [01:42,  1.72it/s]Extractor Estimating: 184it [01:43,  1.77it/s]Extractor Estimating: 185it [01:43,  1.64it/s]Extractor Estimating: 186it [01:44,  1.65it/s]Extractor Estimating: 187it [01:45,  1.65it/s]Extractor Estimating: 188it [01:45,  1.64it/s]Extractor Estimating: 189it [01:46,  1.69it/s]Extractor Estimating: 190it [01:46,  1.69it/s]Extractor Estimating: 191it [01:47,  1.55it/s]Extractor Estimating: 192it [01:48,  1.57it/s]Extractor Estimating: 193it [01:48,  1.64it/s]Extractor Estimating: 194it [01:49,  1.65it/s]Extractor Estimating: 195it [01:49,  1.68it/s]Extractor Estimating: 196it [01:50,  1.69it/s]Extractor Estimating: 197it [01:51,  1.68it/s]Extractor Estimating: 198it [01:51,  1.68it/s]Extractor Estimating: 199it [01:52,  1.67it/s]Extractor Estimating: 200it [01:52,  1.67it/s]Extractor Estimating: 201it [01:53,  1.70it/s]Extractor Estimating: 202it [01:54,  1.67it/s]Extractor Estimating: 203it [01:54,  1.67it/s]Extractor Estimating: 204it [01:55,  1.62it/s]Extractor Estimating: 205it [01:55,  1.65it/s]Extractor Estimating: 206it [01:56,  1.68it/s]Extractor Estimating: 207it [01:57,  1.68it/s]Extractor Estimating: 208it [01:57,  1.70it/s]Extractor Estimating: 209it [01:58,  1.66it/s]Extractor Estimating: 210it [01:58,  1.65it/s]Extractor Estimating: 211it [01:59,  1.68it/s]Extractor Estimating: 212it [02:00,  1.68it/s]Extractor Estimating: 213it [02:00,  1.67it/s]Extractor Estimating: 214it [02:01,  1.69it/s]Extractor Estimating: 215it [02:01,  1.68it/s]Extractor Estimating: 216it [02:02,  1.61it/s]Extractor Estimating: 217it [02:03,  1.60it/s]Extractor Estimating: 218it [02:03,  1.61it/s]Extractor Estimating: 219it [02:04,  1.62it/s]Extractor Estimating: 220it [02:04,  1.66it/s]Extractor Estimating: 221it [02:05,  1.68it/s]Extractor Estimating: 222it [02:06,  1.68it/s]Extractor Estimating: 223it [02:06,  1.70it/s]Extractor Estimating: 224it [02:07,  1.68it/s]Extractor Estimating: 225it [02:07,  1.69it/s]Extractor Estimating: 226it [02:08,  1.71it/s]Extractor Estimating: 227it [02:09,  1.69it/s]Extractor Estimating: 228it [02:09,  1.69it/s]Extractor Estimating: 229it [02:10,  1.66it/s]Extractor Estimating: 230it [02:10,  1.70it/s]Extractor Estimating: 231it [02:11,  1.66it/s]Extractor Estimating: 232it [02:12,  1.69it/s]Extractor Estimating: 233it [02:12,  1.66it/s]Extractor Estimating: 234it [02:13,  1.65it/s]Extractor Estimating: 235it [02:13,  1.66it/s]Extractor Estimating: 236it [02:14,  1.56it/s]Extractor Estimating: 237it [02:15,  1.60it/s]Extractor Estimating: 238it [02:15,  1.62it/s]Extractor Estimating: 239it [02:16,  1.64it/s]Extractor Estimating: 240it [02:16,  1.63it/s]Extractor Estimating: 241it [02:17,  1.63it/s]Extractor Estimating: 242it [02:18,  1.61it/s]Extractor Estimating: 243it [02:18,  1.59it/s]Extractor Estimating: 244it [02:19,  1.59it/s]Extractor Estimating: 245it [02:20,  1.61it/s]Extractor Estimating: 246it [02:20,  1.67it/s]Extractor Estimating: 247it [02:21,  1.64it/s]Extractor Estimating: 248it [02:21,  1.68it/s]Extractor Estimating: 249it [02:22,  1.66it/s]Extractor Estimating: 250it [02:23,  1.60it/s]Extractor Estimating: 251it [02:23,  1.59it/s]Extractor Estimating: 252it [02:24,  1.59it/s]Extractor Estimating: 253it [02:25,  1.60it/s]Extractor Estimating: 254it [02:25,  1.59it/s]Extractor Estimating: 255it [02:26,  1.60it/s]Extractor Estimating: 256it [02:26,  1.61it/s]Extractor Estimating: 257it [02:27,  1.60it/s]Extractor Estimating: 258it [02:28,  1.60it/s]Extractor Estimating: 259it [02:28,  1.54it/s]Extractor Estimating: 260it [02:29,  1.56it/s]Extractor Estimating: 261it [02:30,  1.44it/s]Extractor Estimating: 262it [02:30,  1.48it/s]Extractor Estimating: 263it [02:31,  1.51it/s]Extractor Estimating: 264it [02:32,  1.49it/s]Extractor Estimating: 265it [02:32,  1.53it/s]Extractor Estimating: 266it [02:33,  1.54it/s]Extractor Estimating: 267it [02:34,  1.55it/s]Extractor Estimating: 268it [02:34,  1.55it/s]Extractor Estimating: 269it [02:35,  1.54it/s]Extractor Estimating: 270it [02:36,  1.55it/s]Extractor Estimating: 271it [02:36,  1.56it/s]Extractor Estimating: 272it [02:37,  1.56it/s]Extractor Estimating: 273it [02:38,  1.56it/s]Extractor Estimating: 274it [02:38,  1.52it/s]Extractor Estimating: 275it [02:39,  1.59it/s]Extractor Estimating: 276it [02:39,  1.58it/s]Extractor Estimating: 277it [02:40,  1.59it/s]Extractor Estimating: 278it [02:41,  1.62it/s]Extractor Estimating: 279it [02:41,  1.58it/s]Extractor Estimating: 280it [02:42,  1.61it/s]Extractor Estimating: 281it [02:43,  1.58it/s]Extractor Estimating: 282it [02:43,  1.58it/s]Extractor Estimating: 283it [02:44,  1.58it/s]Extractor Estimating: 284it [02:44,  1.58it/s]Extractor Estimating: 285it [02:45,  1.63it/s]Extractor Estimating: 286it [02:46,  1.65it/s]Extractor Estimating: 287it [02:46,  1.64it/s]Extractor Estimating: 288it [02:47,  1.66it/s]Extractor Estimating: 289it [02:47,  1.65it/s]Extractor Estimating: 290it [02:48,  1.63it/s]Extractor Estimating: 291it [02:49,  1.62it/s]Extractor Estimating: 292it [02:49,  1.63it/s]Extractor Estimating: 293it [02:50,  1.60it/s]Extractor Estimating: 294it [02:51,  1.61it/s]Extractor Estimating: 295it [02:51,  1.70it/s]Extractor Estimating: 296it [02:52,  1.71it/s]Extractor Estimating: 297it [02:52,  1.72it/s]Extractor Estimating: 298it [02:53,  1.68it/s]Extractor Estimating: 299it [02:53,  1.67it/s]Extractor Estimating: 300it [02:54,  1.64it/s]Extractor Estimating: 301it [02:55,  1.69it/s]Extractor Estimating: 302it [02:55,  1.73it/s]Extractor Estimating: 303it [02:56,  1.74it/s]Extractor Estimating: 304it [02:56,  1.76it/s]Extractor Estimating: 305it [02:57,  1.75it/s]Extractor Estimating: 306it [02:57,  1.80it/s]Extractor Estimating: 307it [02:58,  1.83it/s]Extractor Estimating: 308it [02:58,  1.86it/s]Extractor Estimating: 309it [02:59,  1.79it/s]Extractor Estimating: 310it [03:00,  1.79it/s]Extractor Estimating: 311it [03:00,  1.79it/s]Extractor Estimating: 312it [03:01,  1.83it/s]Extractor Estimating: 313it [03:01,  1.86it/s]Extractor Estimating: 314it [03:02,  1.85it/s]Extractor Estimating: 315it [03:02,  1.85it/s]Extractor Estimating: 316it [03:03,  1.80it/s]Extractor Estimating: 317it [03:03,  1.83it/s]Extractor Estimating: 318it [03:04,  1.80it/s]Extractor Estimating: 319it [03:05,  1.79it/s]Extractor Estimating: 320it [03:05,  1.83it/s]Extractor Estimating: 321it [03:06,  1.76it/s]Extractor Estimating: 322it [03:06,  1.77it/s]Extractor Estimating: 323it [03:07,  1.76it/s]Extractor Estimating: 324it [03:07,  1.78it/s]Extractor Estimating: 325it [03:08,  1.75it/s]Extractor Estimating: 326it [03:09,  1.69it/s]Extractor Estimating: 327it [03:09,  1.62it/s]Extractor Estimating: 328it [03:10,  1.60it/s]Extractor Estimating: 329it [03:11,  1.59it/s]Extractor Estimating: 330it [03:11,  1.58it/s]Extractor Estimating: 331it [03:12,  1.58it/s]Extractor Estimating: 332it [03:12,  1.58it/s]Extractor Estimating: 333it [03:13,  1.55it/s]Extractor Estimating: 334it [03:14,  1.53it/s]Extractor Estimating: 335it [03:14,  1.52it/s]Extractor Estimating: 336it [03:15,  1.55it/s]Extractor Estimating: 337it [03:16,  1.56it/s]Extractor Estimating: 338it [03:16,  1.55it/s]Extractor Estimating: 339it [03:17,  1.55it/s]Extractor Estimating: 340it [03:18,  1.56it/s]Extractor Estimating: 341it [03:18,  1.55it/s]Extractor Estimating: 342it [03:19,  1.53it/s]Extractor Estimating: 343it [03:20,  1.54it/s]Extractor Estimating: 344it [03:20,  1.53it/s]Extractor Estimating: 345it [03:21,  1.54it/s]Extractor Estimating: 346it [03:22,  1.56it/s]Extractor Estimating: 347it [03:22,  1.56it/s]Extractor Estimating: 348it [03:23,  1.56it/s]Extractor Estimating: 349it [03:23,  1.61it/s]Extractor Estimating: 350it [03:24,  1.74it/s]Extractor Estimating: 351it [03:24,  1.84it/s]Extractor Estimating: 352it [03:25,  1.94it/s]Extractor Estimating: 353it [03:25,  1.94it/s]Extractor Estimating: 354it [03:26,  2.00it/s]Extractor Estimating: 355it [03:26,  1.96it/s]Extractor Estimating: 356it [03:27,  2.01it/s]Extractor Estimating: 357it [03:27,  2.06it/s]Extractor Estimating: 358it [03:28,  2.12it/s]Extractor Estimating: 359it [03:28,  2.15it/s]Extractor Estimating: 360it [03:29,  2.17it/s]Extractor Estimating: 361it [03:29,  2.09it/s]Extractor Estimating: 362it [03:30,  2.13it/s]Extractor Estimating: 363it [03:30,  2.14it/s]Extractor Estimating: 364it [03:30,  2.13it/s]Extractor Estimating: 365it [03:31,  2.15it/s]Extractor Estimating: 366it [03:32,  1.88it/s]Extractor Estimating: 367it [03:32,  1.93it/s]Extractor Estimating: 368it [03:33,  1.94it/s]Extractor Estimating: 369it [03:33,  2.00it/s]Extractor Estimating: 370it [03:34,  2.04it/s]Extractor Estimating: 371it [03:34,  2.08it/s]Extractor Estimating: 372it [03:35,  2.01it/s]Extractor Estimating: 373it [03:35,  2.03it/s]Extractor Estimating: 374it [03:35,  2.24it/s]Extractor Estimating: 374it [03:35,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:26,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:26,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:26,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:26,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:26,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:08:27,274 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:08:27,275 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:08:28,213 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:08:29,286 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:08:29,286 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:32,418 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:32,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:32,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:32,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:32,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:08:33,179 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:08:33,180 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:08:33,807 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:08:33,979 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:08:33,979 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:20:18,552 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:20:18,601 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7470 mean pseudo reward: 0.9805039771247483
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16059
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16159, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16159, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.957, loss:198.7109
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.022, loss:191.1232
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.966, loss:178.7893
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.965, loss:175.1958
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.961, loss:168.1846
>> valid entity prec:0.5510, rec:0.5649, f1:0.5578
>> valid relation prec:0.1923, rec:0.1213, f1:0.1488
>> valid relation with NER prec:0.1923, rec:0.1213, f1:0.1488
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.557, loss:171.6540
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.958, loss:169.7386
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.966, loss:172.5531
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.967, loss:173.4946
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.953, loss:164.1115
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5476, rec:0.5139, f1:0.5303
>> valid relation prec:0.1976, rec:0.1193, f1:0.1488
>> valid relation with NER prec:0.1976, rec:0.1193, f1:0.1488
g_step 1100, step 164, avg_time 2.552, loss:176.4429
g_step 1200, step 264, avg_time 0.975, loss:177.5533
g_step 1300, step 52, avg_time 0.957, loss:167.9864
g_step 1400, step 152, avg_time 0.972, loss:165.6172
g_step 1500, step 252, avg_time 0.968, loss:162.8301
>> valid entity prec:0.5651, rec:0.5361, f1:0.5503
>> valid relation prec:0.2074, rec:0.1074, f1:0.1415
>> valid relation with NER prec:0.2074, rec:0.1074, f1:0.1415
g_step 1600, step 40, avg_time 2.536, loss:159.4080
g_step 1700, step 140, avg_time 0.972, loss:158.8935
g_step 1800, step 240, avg_time 0.956, loss:152.6090
g_step 1900, step 28, avg_time 0.967, loss:141.6008
g_step 2000, step 128, avg_time 0.970, loss:158.8401
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5452, rec:0.5252, f1:0.5350
>> valid relation prec:0.1945, rec:0.1168, f1:0.1460
>> valid relation with NER prec:0.1945, rec:0.1168, f1:0.1460
g_step 2100, step 228, avg_time 2.542, loss:145.8389
g_step 2200, step 16, avg_time 0.952, loss:146.6076
g_step 2300, step 116, avg_time 0.964, loss:136.2410
g_step 2400, step 216, avg_time 0.953, loss:151.3149
g_step 2500, step 4, avg_time 0.971, loss:141.1718
>> valid entity prec:0.5385, rec:0.5599, f1:0.5490
>> valid relation prec:0.1876, rec:0.1107, f1:0.1392
>> valid relation with NER prec:0.1876, rec:0.1107, f1:0.1392
g_step 2600, step 104, avg_time 2.535, loss:143.7987
g_step 2700, step 204, avg_time 0.967, loss:139.1488
g_step 2800, step 304, avg_time 0.956, loss:131.6930
g_step 2900, step 92, avg_time 0.957, loss:131.6762
g_step 3000, step 192, avg_time 0.962, loss:131.9278
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5464, rec:0.5610, f1:0.5536
>> valid relation prec:0.1630, rec:0.0998, f1:0.1238
>> valid relation with NER prec:0.1630, rec:0.0998, f1:0.1238
g_step 3100, step 292, avg_time 2.540, loss:128.0397
g_step 3200, step 80, avg_time 0.966, loss:133.5376
g_step 3300, step 180, avg_time 0.961, loss:137.5693
g_step 3400, step 280, avg_time 0.953, loss:121.3335
g_step 3500, step 68, avg_time 0.963, loss:117.2325
>> valid entity prec:0.5403, rec:0.5136, f1:0.5266
>> valid relation prec:0.1700, rec:0.0998, f1:0.1258
>> valid relation with NER prec:0.1700, rec:0.0998, f1:0.1258
g_step 3600, step 168, avg_time 2.546, loss:125.4023
g_step 3700, step 268, avg_time 0.953, loss:125.3146
g_step 3800, step 56, avg_time 0.950, loss:119.4713
g_step 3900, step 156, avg_time 0.979, loss:119.8082
g_step 4000, step 256, avg_time 0.954, loss:118.9230
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5277, rec:0.5451, f1:0.5363
>> valid relation prec:0.1711, rec:0.0918, f1:0.1195
>> valid relation with NER prec:0.1711, rec:0.0918, f1:0.1195
g_step 4100, step 44, avg_time 2.527, loss:118.4621
g_step 4200, step 144, avg_time 0.961, loss:113.2316
g_step 4300, step 244, avg_time 0.959, loss:113.0312
g_step 4400, step 32, avg_time 0.966, loss:112.6966
g_step 4500, step 132, avg_time 0.951, loss:108.2161
>> valid entity prec:0.5340, rec:0.5418, f1:0.5379
>> valid relation prec:0.1516, rec:0.1078, f1:0.1260
>> valid relation with NER prec:0.1516, rec:0.1078, f1:0.1260
g_step 4600, step 232, avg_time 2.539, loss:106.1079
g_step 4700, step 20, avg_time 0.954, loss:118.6569
g_step 4800, step 120, avg_time 0.965, loss:103.2218
g_step 4900, step 220, avg_time 0.960, loss:109.5975
g_step 5000, step 8, avg_time 0.949, loss:104.0304
learning rate was adjusted to 0.0008
>> valid entity prec:0.5356, rec:0.5400, f1:0.5378
>> valid relation prec:0.1610, rec:0.1138, f1:0.1333
>> valid relation with NER prec:0.1610, rec:0.1138, f1:0.1333
g_step 5100, step 108, avg_time 2.537, loss:105.8342
g_step 5200, step 208, avg_time 0.966, loss:101.4706
g_step 5300, step 308, avg_time 0.961, loss:118.5955
g_step 5400, step 96, avg_time 0.945, loss:98.6738
g_step 5500, step 196, avg_time 0.971, loss:103.0951
>> valid entity prec:0.5447, rec:0.5078, f1:0.5256
>> valid relation prec:0.1749, rec:0.0998, f1:0.1271
>> valid relation with NER prec:0.1749, rec:0.0998, f1:0.1271
g_step 5600, step 296, avg_time 2.547, loss:105.0233
g_step 5700, step 84, avg_time 0.961, loss:95.8724
g_step 5800, step 184, avg_time 0.959, loss:102.2087
g_step 5900, step 284, avg_time 0.961, loss:105.1743
g_step 6000, step 72, avg_time 0.960, loss:103.7528
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5452, rec:0.4992, f1:0.5212
>> valid relation prec:0.1806, rec:0.1062, f1:0.1337
>> valid relation with NER prec:0.1806, rec:0.1062, f1:0.1337
g_step 6100, step 172, avg_time 2.533, loss:98.3226
g_step 6200, step 272, avg_time 0.954, loss:107.9705
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:20:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:20:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-20-18_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:20:19 - WARNING - datasets.builder -   Using custom data configuration default-6edca1563a057232
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6edca1563a057232/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:20:21,549 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:20:21,550 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:20:21,550 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:20:21,551 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:20:21,634 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,702 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,702 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,702 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,702 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,703 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:21,703 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:20:22,019 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:20:25,123 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:20:25,154 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6edca1563a057232/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.20ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.07ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.46ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.67ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.81ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.90ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.06ba/s]100%|██████████| 8/8 [00:01<00:00,  5.00ba/s]100%|██████████| 8/8 [00:01<00:00,  4.61ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.45ba/s] 40%|████      | 2/5 [00:00<00:00,  4.09ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.32ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.45ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]100%|██████████| 5/5 [00:01<00:00,  4.33ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.93ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.67ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.48ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.86ba/s]100%|██████████| 8/8 [00:00<00:00, 10.06ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.49ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.92ba/s]100%|██████████| 5/5 [00:00<00:00,  9.93ba/s]100%|██████████| 5/5 [00:00<00:00,  9.45ba/s]
[INFO|trainer.py:414] 2023-08-29 03:20:30,589 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:20:30,598 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:20:30,598 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 03:20:30,598 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:20:30,598 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:20:30,598 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:20:30,598 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:20:30,598 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.37it/s]  0%|          | 2/585 [00:00<02:51,  3.41it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:53,  3.34it/s]  1%|          | 5/585 [00:01<02:51,  3.38it/s]  1%|          | 6/585 [00:01<02:51,  3.38it/s]  1%|          | 7/585 [00:02<02:50,  3.39it/s]  1%|▏         | 8/585 [00:02<02:50,  3.39it/s]  2%|▏         | 9/585 [00:02<02:49,  3.39it/s]  2%|▏         | 10/585 [00:02<02:49,  3.39it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.39it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:52,  3.30it/s]  3%|▎         | 16/585 [00:04<02:51,  3.33it/s]  3%|▎         | 17/585 [00:05<02:49,  3.35it/s]  3%|▎         | 18/585 [00:05<02:48,  3.36it/s]  3%|▎         | 19/585 [00:05<02:47,  3.37it/s]  3%|▎         | 20/585 [00:05<02:47,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.38it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:48,  3.31it/s]  5%|▍         | 27/585 [00:08<02:47,  3.33it/s]  5%|▍         | 28/585 [00:08<02:46,  3.35it/s]  5%|▍         | 29/585 [00:08<02:45,  3.36it/s]  5%|▌         | 30/585 [00:08<02:44,  3.37it/s]  5%|▌         | 31/585 [00:09<02:43,  3.38it/s]  5%|▌         | 32/585 [00:09<02:43,  3.38it/s]  6%|▌         | 33/585 [00:09<02:43,  3.39it/s]  6%|▌         | 34/585 [00:10<02:42,  3.39it/s]  6%|▌         | 35/585 [00:10<02:42,  3.39it/s]  6%|▌         | 36/585 [00:10<02:41,  3.39it/s]  6%|▋         | 37/585 [00:10<02:44,  3.33it/s]  6%|▋         | 38/585 [00:11<02:43,  3.35it/s]  7%|▋         | 39/585 [00:11<02:42,  3.36it/s]  7%|▋         | 40/585 [00:11<02:41,  3.37it/s]  7%|▋         | 41/585 [00:12<02:41,  3.37it/s]  7%|▋         | 42/585 [00:12<02:40,  3.38it/s]  7%|▋         | 43/585 [00:12<02:40,  3.39it/s]  8%|▊         | 44/585 [00:13<02:39,  3.39it/s]  8%|▊         | 45/585 [00:13<02:39,  3.39it/s]  8%|▊         | 46/585 [00:13<02:38,  3.39it/s]  8%|▊         | 47/585 [00:13<02:38,  3.39it/s]  8%|▊         | 48/585 [00:14<02:42,  3.31it/s]  8%|▊         | 49/585 [00:14<02:40,  3.34it/s]  9%|▊         | 50/585 [00:14<02:39,  3.35it/s]  9%|▊         | 51/585 [00:15<02:38,  3.37it/s]  9%|▉         | 52/585 [00:15<02:37,  3.37it/s]  9%|▉         | 53/585 [00:15<02:37,  3.38it/s]  9%|▉         | 54/585 [00:16<02:36,  3.38it/s]  9%|▉         | 55/585 [00:16<02:36,  3.39it/s] 10%|▉         | 56/585 [00:16<02:36,  3.39it/s] 10%|▉         | 57/585 [00:16<02:35,  3.39it/s] 10%|▉         | 58/585 [00:17<02:35,  3.39it/s] 10%|█         | 59/585 [00:17<02:39,  3.30it/s] 10%|█         | 60/585 [00:17<02:37,  3.33it/s] 10%|█         | 61/585 [00:18<02:36,  3.34it/s] 11%|█         | 62/585 [00:18<02:35,  3.36it/s] 11%|█         | 63/585 [00:18<02:34,  3.37it/s] 11%|█         | 64/585 [00:18<02:34,  3.38it/s] 11%|█         | 65/585 [00:19<02:33,  3.38it/s] 11%|█▏        | 66/585 [00:19<02:33,  3.38it/s] 11%|█▏        | 67/585 [00:19<02:33,  3.39it/s] 12%|█▏        | 68/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 70/585 [00:20<02:33,  3.35it/s] 12%|█▏        | 71/585 [00:21<02:33,  3.36it/s] 12%|█▏        | 72/585 [00:21<02:32,  3.36it/s] 12%|█▏        | 73/585 [00:21<02:31,  3.37it/s] 13%|█▎        | 74/585 [00:21<02:31,  3.38it/s] 13%|█▎        | 75/585 [00:22<02:30,  3.38it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.38it/s] 13%|█▎        | 77/585 [00:22<02:31,  3.35it/s] 13%|█▎        | 78/585 [00:23<02:30,  3.36it/s] 14%|█▎        | 79/585 [00:23<02:30,  3.37it/s] 14%|█▎        | 80/585 [00:23<02:29,  3.38it/s] 14%|█▍        | 81/585 [00:24<02:29,  3.38it/s] 14%|█▍        | 82/585 [00:24<02:28,  3.38it/s] 14%|█▍        | 83/585 [00:24<02:28,  3.39it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.39it/s] 15%|█▍        | 85/585 [00:25<02:27,  3.39it/s] 15%|█▍        | 86/585 [00:25<02:27,  3.38it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.38it/s] 15%|█▌        | 88/585 [00:26<02:31,  3.29it/s] 15%|█▌        | 89/585 [00:26<02:29,  3.31it/s] 15%|█▌        | 90/585 [00:26<02:28,  3.34it/s] 16%|█▌        | 91/585 [00:27<02:27,  3.35it/s] 16%|█▌        | 92/585 [00:27<02:26,  3.36it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.37it/s] 16%|█▌        | 94/585 [00:27<02:25,  3.38it/s] 16%|█▌        | 95/585 [00:28<02:24,  3.38it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 98/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 99/585 [00:29<02:26,  3.32it/s] 17%|█▋        | 100/585 [00:29<02:25,  3.34it/s] 17%|█▋        | 101/585 [00:29<02:24,  3.35it/s] 17%|█▋        | 102/585 [00:30<02:24,  3.35it/s] 18%|█▊        | 103/585 [00:30<02:23,  3.36it/s] 18%|█▊        | 104/585 [00:30<02:22,  3.37it/s] 18%|█▊        | 105/585 [00:31<02:22,  3.37it/s] 18%|█▊        | 106/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:32<02:21,  3.38it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 110/585 [00:32<02:23,  3.30it/s] 19%|█▉        | 111/585 [00:32<02:22,  3.33it/s] 19%|█▉        | 112/585 [00:33<02:21,  3.34it/s] 19%|█▉        | 113/585 [00:33<02:20,  3.35it/s] 19%|█▉        | 114/585 [00:33<02:20,  3.36it/s] 20%|█▉        | 115/585 [00:34<02:19,  3.37it/s] 20%|█▉        | 116/585 [00:34<02:19,  3.37it/s] 20%|██        | 117/585 [00:34<02:18,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 03:21:05,381 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:21:05,381 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:21:05,381 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.52it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.42it/s][A
  3%|▎         | 17/611 [00:00<00:13, 45.12it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.26it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.76it/s][A
  5%|▌         | 32/611 [00:00<00:13, 42.79it/s][A
  6%|▌         | 37/611 [00:00<00:13, 43.15it/s][A
  7%|▋         | 42/611 [00:00<00:13, 43.44it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.79it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.21it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.37it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.51it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.54it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.07it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.17it/s][A
 15%|█▌        | 92/611 [00:02<00:17, 28.87it/s][A
 16%|█▌        | 97/611 [00:02<00:15, 32.44it/s][A
 17%|█▋        | 102/611 [00:02<00:14, 35.36it/s][A
 18%|█▊        | 107/611 [00:02<00:13, 37.68it/s][A
 18%|█▊        | 112/611 [00:02<00:12, 39.56it/s][A
 19%|█▉        | 117/611 [00:02<00:12, 41.08it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 42.14it/s][A
 21%|██        | 127/611 [00:03<00:11, 42.95it/s][A
 22%|██▏       | 132/611 [00:03<00:11, 43.20it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.16it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.43it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 43.70it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.94it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.02it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.40it/s][A
 28%|██▊       | 172/611 [00:04<00:09, 44.49it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.27it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.99it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.02it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.09it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.27it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.38it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.59it/s][A
 35%|███▍      | 212/611 [00:04<00:08, 44.63it/s][A
 36%|███▌      | 217/611 [00:05<00:08, 44.58it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.36it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.23it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.17it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.07it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.12it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.28it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.30it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.48it/s][A
 43%|████▎     | 262/611 [00:06<00:07, 44.48it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.37it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.23it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.14it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.29it/s][A
 48%|████▊     | 292/611 [00:06<00:08, 38.15it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 39.97it/s][A
 49%|████▉     | 302/611 [00:07<00:07, 41.31it/s][A
 50%|█████     | 307/611 [00:07<00:07, 42.39it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.13it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.58it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 43.82it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.00it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.67it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.64it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.81it/s][A
 57%|█████▋    | 347/611 [00:08<00:05, 44.04it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.35it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.47it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.63it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.49it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.44it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.14it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.03it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.00it/s][A
 64%|██████▍   | 392/611 [00:09<00:04, 44.28it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.43it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.58it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.64it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.51it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.38it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.20it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 40.59it/s][A
 71%|███████   | 432/611 [00:10<00:04, 41.76it/s][A
 72%|███████▏  | 437/611 [00:10<00:04, 42.60it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.22it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.56it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 43.94it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.05it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.90it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.59it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.52it/s][A
 78%|███████▊  | 477/611 [00:11<00:03, 43.82it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.13it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.40it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.52it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.63it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.45it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.31it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.11it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.99it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 44.09it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 44.37it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.56it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.64it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.62it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.41it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.30it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.20it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 40.41it/s][A
 93%|█████████▎| 567/611 [00:13<00:01, 41.71it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 42.63it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.32it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.83it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.10it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.07it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.96it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.74it/s][A
 99%|█████████▉| 607/611 [00:14<00:00, 43.78it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 43.78it/s][A 20%|██        | 117/585 [00:48<02:18,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:21:19,979 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 03:21:20,443 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:21:25,394 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:21:25,620 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:21:25,761 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:03<1:08:09,  8.76s/it] 20%|██        | 119/585 [01:03<48:20,  6.22s/it]   21%|██        | 120/585 [01:03<34:27,  4.45s/it] 21%|██        | 121/585 [01:04<24:44,  3.20s/it] 21%|██        | 122/585 [01:04<17:58,  2.33s/it] 21%|██        | 123/585 [01:04<13:13,  1.72s/it] 21%|██        | 124/585 [01:05<09:55,  1.29s/it] 21%|██▏       | 125/585 [01:05<07:36,  1.01it/s] 22%|██▏       | 126/585 [01:05<05:59,  1.28it/s] 22%|██▏       | 127/585 [01:05<04:51,  1.57it/s] 22%|██▏       | 128/585 [01:06<04:03,  1.87it/s] 22%|██▏       | 129/585 [01:06<03:30,  2.16it/s] 22%|██▏       | 130/585 [01:06<03:10,  2.38it/s] 22%|██▏       | 131/585 [01:07<02:53,  2.62it/s] 23%|██▎       | 132/585 [01:07<02:41,  2.81it/s] 23%|██▎       | 133/585 [01:07<02:32,  2.96it/s] 23%|██▎       | 134/585 [01:07<02:26,  3.08it/s] 23%|██▎       | 135/585 [01:08<02:22,  3.17it/s] 23%|██▎       | 136/585 [01:08<02:19,  3.23it/s] 23%|██▎       | 137/585 [01:08<02:16,  3.29it/s] 24%|██▎       | 138/585 [01:09<02:14,  3.33it/s] 24%|██▍       | 139/585 [01:09<02:12,  3.36it/s] 24%|██▍       | 140/585 [01:09<02:11,  3.39it/s] 24%|██▍       | 141/585 [01:10<02:15,  3.27it/s] 24%|██▍       | 142/585 [01:10<02:13,  3.31it/s] 24%|██▍       | 143/585 [01:10<02:12,  3.33it/s] 25%|██▍       | 144/585 [01:10<02:11,  3.35it/s] 25%|██▍       | 145/585 [01:11<02:11,  3.36it/s] 25%|██▍       | 146/585 [01:11<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:11<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:12<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:12<02:08,  3.38it/s] 26%|██▌       | 150/585 [01:12<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:13<02:08,  3.39it/s] 26%|██▌       | 152/585 [01:13<02:11,  3.29it/s] 26%|██▌       | 153/585 [01:13<02:10,  3.32it/s] 26%|██▋       | 154/585 [01:13<02:09,  3.34it/s] 26%|██▋       | 155/585 [01:14<02:08,  3.36it/s] 27%|██▋       | 156/585 [01:14<02:07,  3.37it/s] 27%|██▋       | 157/585 [01:14<02:06,  3.37it/s] 27%|██▋       | 158/585 [01:15<02:06,  3.39it/s] 27%|██▋       | 159/585 [01:15<02:05,  3.40it/s] 27%|██▋       | 160/585 [01:15<02:04,  3.41it/s] 28%|██▊       | 161/585 [01:15<02:03,  3.42it/s] 28%|██▊       | 162/585 [01:16<02:03,  3.42it/s] 28%|██▊       | 163/585 [01:16<02:04,  3.39it/s] 28%|██▊       | 164/585 [01:16<02:03,  3.40it/s] 28%|██▊       | 165/585 [01:17<02:02,  3.42it/s] 28%|██▊       | 166/585 [01:17<02:02,  3.42it/s] 29%|██▊       | 167/585 [01:17<02:01,  3.43it/s] 29%|██▊       | 168/585 [01:18<02:01,  3.43it/s] 29%|██▉       | 169/585 [01:18<02:01,  3.44it/s] 29%|██▉       | 170/585 [01:18<02:00,  3.43it/s] 29%|██▉       | 171/585 [01:18<02:00,  3.44it/s] 29%|██▉       | 172/585 [01:19<02:00,  3.44it/s] 30%|██▉       | 173/585 [01:19<01:59,  3.44it/s] 30%|██▉       | 174/585 [01:19<02:02,  3.36it/s] 30%|██▉       | 175/585 [01:20<02:01,  3.38it/s] 30%|███       | 176/585 [01:20<02:00,  3.40it/s] 30%|███       | 177/585 [01:20<01:59,  3.41it/s] 30%|███       | 178/585 [01:20<01:59,  3.42it/s] 31%|███       | 179/585 [01:21<01:58,  3.42it/s] 31%|███       | 180/585 [01:21<01:58,  3.43it/s] 31%|███       | 181/585 [01:21<01:57,  3.43it/s] 31%|███       | 182/585 [01:22<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:22<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:22<01:56,  3.44it/s] 32%|███▏      | 185/585 [01:23<01:57,  3.39it/s] 32%|███▏      | 186/585 [01:23<01:57,  3.41it/s] 32%|███▏      | 187/585 [01:23<01:56,  3.41it/s] 32%|███▏      | 188/585 [01:23<01:56,  3.42it/s] 32%|███▏      | 189/585 [01:24<01:55,  3.42it/s] 32%|███▏      | 190/585 [01:24<01:55,  3.43it/s] 33%|███▎      | 191/585 [01:24<01:54,  3.43it/s] 33%|███▎      | 192/585 [01:25<01:54,  3.43it/s] 33%|███▎      | 193/585 [01:25<01:54,  3.43it/s] 33%|███▎      | 194/585 [01:25<01:53,  3.43it/s] 33%|███▎      | 195/585 [01:25<01:53,  3.43it/s] 34%|███▎      | 196/585 [01:26<01:53,  3.44it/s] 34%|███▎      | 197/585 [01:26<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:26<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:27<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:27<01:54,  3.37it/s] 34%|███▍      | 201/585 [01:27<01:53,  3.39it/s] 35%|███▍      | 202/585 [01:27<01:52,  3.40it/s] 35%|███▍      | 203/585 [01:28<01:51,  3.41it/s] 35%|███▍      | 204/585 [01:28<01:51,  3.42it/s] 35%|███▌      | 205/585 [01:28<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:29<01:50,  3.43it/s] 35%|███▌      | 207/585 [01:29<01:50,  3.43it/s] 36%|███▌      | 208/585 [01:29<01:49,  3.43it/s] 36%|███▌      | 209/585 [01:30<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:30<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:30<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:30<01:49,  3.39it/s] 36%|███▋      | 213/585 [01:31<01:49,  3.41it/s] 37%|███▋      | 214/585 [01:31<01:48,  3.41it/s] 37%|███▋      | 215/585 [01:31<01:48,  3.42it/s] 37%|███▋      | 216/585 [01:32<01:47,  3.43it/s] 37%|███▋      | 217/585 [01:32<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:32<01:46,  3.43it/s] 37%|███▋      | 219/585 [01:32<01:46,  3.43it/s] 38%|███▊      | 220/585 [01:33<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:33<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:33<01:47,  3.37it/s] 38%|███▊      | 223/585 [01:34<01:46,  3.39it/s] 38%|███▊      | 224/585 [01:34<01:45,  3.41it/s] 38%|███▊      | 225/585 [01:34<01:45,  3.41it/s] 39%|███▊      | 226/585 [01:35<01:44,  3.42it/s] 39%|███▉      | 227/585 [01:35<01:44,  3.42it/s] 39%|███▉      | 228/585 [01:35<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:35<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:36<01:43,  3.43it/s] 39%|███▉      | 231/585 [01:36<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:36<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:37<01:48,  3.25it/s] 40%|████      | 234/585 [01:37<01:46,  3.30it/s][INFO|trainer.py:2140] 2023-08-29 03:22:08,029 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:22:08,029 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:22:08,029 >>   Batch size = 8
{'eval_loss': 1.0407958030700684, 'eval_runtime': 14.1252, 'eval_samples_per_second': 345.622, 'eval_steps_per_second': 43.256, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.29it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.38it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.90it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.15it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.24it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.60it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.24it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.12it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.21it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.21it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.43it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.63it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.63it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.45it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.31it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.15it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.15it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.48it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.53it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.82it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.00it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.10it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.18it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.36it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.34it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.43it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.36it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.35it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.25it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.16it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.19it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.35it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.38it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.37it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.12it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.17it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.05it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.16it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.07it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.14it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.21it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.38it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.41it/s][A
 40%|████      | 247/611 [00:05<00:08, 42.16it/s][A
 41%|████      | 252/611 [00:05<00:08, 42.86it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.34it/s][A
 43%|████▎     | 262/611 [00:05<00:08, 43.62it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.76it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.96it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.10it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.21it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.04it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.18it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.33it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.33it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.31it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.34it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.15it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.40it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.34it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.34it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.25it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.39it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.36it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.34it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.35it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.24it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.34it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.26it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 41.39it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 42.36it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 42.98it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.39it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.77it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.84it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.85it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.92it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 43.66it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.76it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.06it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.27it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.30it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.46it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.52it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.39it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.27it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.12it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.11it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.17it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.32it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.51it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.55it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.52it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.38it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.31it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.23it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 39.80it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 41.19it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 42.32it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.07it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 38.97it/s][A
 89%|████████▉ | 543/611 [00:12<00:01, 42.01it/s][A
 90%|████████▉ | 548/611 [00:12<00:01, 42.86it/s][A
 91%|█████████ | 553/611 [00:12<00:01, 43.11it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 43.11it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 43.47it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 43.82it/s][A
 94%|█████████▍| 573/611 [00:13<00:00, 44.11it/s][A
 95%|█████████▍| 578/611 [00:13<00:00, 44.24it/s][A
 95%|█████████▌| 583/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 588/611 [00:13<00:00, 44.27it/s][A
 97%|█████████▋| 593/611 [00:13<00:00, 44.45it/s][A
 98%|█████████▊| 598/611 [00:13<00:00, 44.13it/s][A
 99%|█████████▊| 603/611 [00:13<00:00, 44.08it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.14it/s][A 40%|████      | 234/585 [01:51<01:46,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:22:22,185 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 03:22:22,474 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:22:26,412 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:22:26,630 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:22:26,778 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:03<46:29,  7.97s/it] 40%|████      | 236/585 [02:03<33:01,  5.68s/it] 41%|████      | 237/585 [02:03<23:33,  4.06s/it] 41%|████      | 238/585 [02:04<16:57,  2.93s/it] 41%|████      | 239/585 [02:04<12:20,  2.14s/it] 41%|████      | 240/585 [02:04<09:07,  1.59s/it] 41%|████      | 241/585 [02:05<06:52,  1.20s/it] 41%|████▏     | 242/585 [02:05<05:18,  1.08it/s] 42%|████▏     | 243/585 [02:05<04:12,  1.35it/s] 42%|████▏     | 244/585 [02:05<03:26,  1.65it/s] 42%|████▏     | 245/585 [02:06<02:54,  1.95it/s] 42%|████▏     | 246/585 [02:06<02:31,  2.24it/s] 42%|████▏     | 247/585 [02:06<02:18,  2.45it/s] 42%|████▏     | 248/585 [02:07<02:06,  2.67it/s] 43%|████▎     | 249/585 [02:07<01:57,  2.85it/s] 43%|████▎     | 250/585 [02:07<01:51,  3.00it/s] 43%|████▎     | 251/585 [02:08<01:47,  3.10it/s] 43%|████▎     | 252/585 [02:08<01:44,  3.18it/s] 43%|████▎     | 253/585 [02:08<01:42,  3.24it/s] 43%|████▎     | 254/585 [02:08<01:40,  3.29it/s] 44%|████▎     | 255/585 [02:09<01:39,  3.31it/s] 44%|████▍     | 256/585 [02:09<01:38,  3.34it/s] 44%|████▍     | 257/585 [02:09<01:37,  3.35it/s] 44%|████▍     | 258/585 [02:10<01:39,  3.29it/s] 44%|████▍     | 259/585 [02:10<01:38,  3.32it/s] 44%|████▍     | 260/585 [02:10<01:37,  3.34it/s] 45%|████▍     | 261/585 [02:10<01:36,  3.37it/s] 45%|████▍     | 262/585 [02:11<01:35,  3.39it/s] 45%|████▍     | 263/585 [02:11<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:11<01:34,  3.41it/s] 45%|████▌     | 265/585 [02:12<01:33,  3.42it/s] 45%|████▌     | 266/585 [02:12<01:33,  3.43it/s] 46%|████▌     | 267/585 [02:12<01:32,  3.43it/s] 46%|████▌     | 268/585 [02:13<01:32,  3.44it/s] 46%|████▌     | 269/585 [02:13<01:34,  3.36it/s] 46%|████▌     | 270/585 [02:13<01:33,  3.39it/s] 46%|████▋     | 271/585 [02:13<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:14<01:31,  3.42it/s] 47%|████▋     | 273/585 [02:14<01:31,  3.42it/s] 47%|████▋     | 274/585 [02:14<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:15<01:30,  3.42it/s] 47%|████▋     | 276/585 [02:15<01:30,  3.43it/s] 47%|████▋     | 277/585 [02:15<01:29,  3.43it/s] 48%|████▊     | 278/585 [02:15<01:29,  3.43it/s] 48%|████▊     | 279/585 [02:16<01:29,  3.44it/s] 48%|████▊     | 280/585 [02:16<01:30,  3.39it/s] 48%|████▊     | 281/585 [02:16<01:29,  3.40it/s] 48%|████▊     | 282/585 [02:17<01:28,  3.41it/s] 48%|████▊     | 283/585 [02:17<01:28,  3.42it/s] 49%|████▊     | 284/585 [02:17<01:27,  3.42it/s] 49%|████▊     | 285/585 [02:18<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:18<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:18<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:18<01:26,  3.43it/s] 49%|████▉     | 289/585 [02:19<01:26,  3.43it/s] 50%|████▉     | 290/585 [02:19<01:25,  3.44it/s] 50%|████▉     | 291/585 [02:19<01:28,  3.33it/s] 50%|████▉     | 292/585 [02:20<01:27,  3.36it/s] 50%|█████     | 293/585 [02:20<01:26,  3.38it/s] 50%|█████     | 294/585 [02:20<01:25,  3.40it/s] 50%|█████     | 295/585 [02:20<01:25,  3.41it/s] 51%|█████     | 296/585 [02:21<01:24,  3.42it/s] 51%|█████     | 297/585 [02:21<01:24,  3.42it/s] 51%|█████     | 298/585 [02:21<01:23,  3.43it/s] 51%|█████     | 299/585 [02:22<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:22<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:22<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:23<01:24,  3.33it/s] 52%|█████▏    | 303/585 [02:23<01:23,  3.37it/s] 52%|█████▏    | 304/585 [02:23<01:23,  3.38it/s] 52%|█████▏    | 305/585 [02:23<01:22,  3.40it/s] 52%|█████▏    | 306/585 [02:24<01:21,  3.41it/s] 52%|█████▏    | 307/585 [02:24<01:21,  3.42it/s] 53%|█████▎    | 308/585 [02:24<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:25<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:25<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:25<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:25<01:19,  3.44it/s] 54%|█████▎    | 313/585 [02:26<01:19,  3.43it/s] 54%|█████▎    | 314/585 [02:26<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:26<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:27<01:18,  3.44it/s] 54%|█████▍    | 317/585 [02:27<01:17,  3.44it/s] 54%|█████▍    | 318/585 [02:27<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:27<01:17,  3.44it/s] 55%|█████▍    | 320/585 [02:28<01:17,  3.44it/s] 55%|█████▍    | 321/585 [02:28<01:16,  3.44it/s] 55%|█████▌    | 322/585 [02:28<01:17,  3.38it/s] 55%|█████▌    | 323/585 [02:29<01:17,  3.40it/s] 55%|█████▌    | 324/585 [02:29<01:16,  3.41it/s] 56%|█████▌    | 325/585 [02:29<01:16,  3.42it/s] 56%|█████▌    | 326/585 [02:30<01:15,  3.42it/s] 56%|█████▌    | 327/585 [02:30<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:30<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:30<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:31<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:31<01:13,  3.44it/s] 57%|█████▋    | 332/585 [02:31<01:13,  3.44it/s] 57%|█████▋    | 333/585 [02:32<01:13,  3.41it/s] 57%|█████▋    | 334/585 [02:32<01:13,  3.42it/s] 57%|█████▋    | 335/585 [02:32<01:13,  3.42it/s] 57%|█████▋    | 336/585 [02:32<01:12,  3.43it/s] 58%|█████▊    | 337/585 [02:33<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:33<01:11,  3.43it/s] 58%|█████▊    | 339/585 [02:33<01:11,  3.44it/s] 58%|█████▊    | 340/585 [02:34<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:34<01:11,  3.44it/s] 58%|█████▊    | 342/585 [02:34<01:10,  3.44it/s] 59%|█████▊    | 343/585 [02:34<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:35<01:11,  3.35it/s] 59%|█████▉    | 345/585 [02:35<01:11,  3.37it/s] 59%|█████▉    | 346/585 [02:35<01:10,  3.39it/s] 59%|█████▉    | 347/585 [02:36<01:09,  3.41it/s] 59%|█████▉    | 348/585 [02:36<01:09,  3.42it/s] 60%|█████▉    | 349/585 [02:36<01:08,  3.42it/s] 60%|█████▉    | 350/585 [02:37<01:09,  3.38it/s] 60%|██████    | 351/585 [02:37<01:08,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 03:23:07,976 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:23:07,976 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:23:07,976 >>   Batch size = 8
{'eval_loss': 1.061909794807434, 'eval_runtime': 13.9156, 'eval_samples_per_second': 350.829, 'eval_steps_per_second': 43.908, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.04it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.60it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.70it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.70it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.97it/s][A
  5%|▌         | 32/611 [00:00<00:13, 43.15it/s][A
  6%|▌         | 37/611 [00:00<00:13, 43.43it/s][A
  7%|▋         | 42/611 [00:00<00:13, 43.65it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.91it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.12it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.46it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.44it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.40it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.18it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.04it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.13it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.10it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.26it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.43it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.55it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.57it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.43it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.26it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.22it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.15it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.17it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.35it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.44it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.53it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.48it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.41it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.20it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.19it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.42it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.67it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.95it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.24it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.35it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.23it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.23it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.05it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.06it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.18it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.24it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.32it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.53it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.54it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.47it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.27it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.13it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.12it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.28it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.32it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.45it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.52it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.52it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.29it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.19it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 40.29it/s][A
 50%|█████     | 307/611 [00:06<00:07, 41.61it/s][A
 51%|█████     | 312/611 [00:07<00:07, 42.47it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.21it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 43.69it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 43.99it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.98it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.96it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.73it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.66it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.91it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.19it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.43it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.56it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.55it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.44it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.16it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.89it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 43.73it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.90it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.06it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.31it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.39it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.32it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.20it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.00it/s][A
 72%|███████▏  | 437/611 [00:09<00:04, 37.26it/s][A
 72%|███████▏  | 442/611 [00:10<00:04, 39.29it/s][A
 73%|███████▎  | 447/611 [00:10<00:04, 40.90it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 42.01it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 42.89it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.45it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.90it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.95it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.64it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.39it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.61it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.90it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.21it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.38it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.49it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.61it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.50it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.03it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.89it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.84it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.09it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 39.53it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 41.07it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 42.18it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 42.96it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.52it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 38.48it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 40.25it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 41.46it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 42.18it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 42.76it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.34it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.79it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.02it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.68it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.68it/s][A 60%|██████    | 351/585 [02:51<01:08,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:23:22,307 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 03:23:22,675 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:23:26,631 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:23:26,933 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:23:27,085 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:04<32:11,  8.29s/it] 60%|██████    | 353/585 [03:04<22:48,  5.90s/it] 61%|██████    | 354/585 [03:04<16:14,  4.22s/it] 61%|██████    | 355/585 [03:05<11:39,  3.04s/it] 61%|██████    | 356/585 [03:05<08:27,  2.22s/it] 61%|██████    | 357/585 [03:05<06:14,  1.64s/it] 61%|██████    | 358/585 [03:06<04:40,  1.24s/it] 61%|██████▏   | 359/585 [03:06<03:35,  1.05it/s] 62%|██████▏   | 360/585 [03:06<02:50,  1.32it/s] 62%|██████▏   | 361/585 [03:06<02:18,  1.62it/s] 62%|██████▏   | 362/585 [03:07<01:56,  1.92it/s] 62%|██████▏   | 363/585 [03:07<01:40,  2.21it/s] 62%|██████▏   | 364/585 [03:07<01:33,  2.36it/s] 62%|██████▏   | 365/585 [03:08<01:24,  2.59it/s] 63%|██████▎   | 366/585 [03:08<01:18,  2.79it/s] 63%|██████▎   | 367/585 [03:08<01:13,  2.95it/s] 63%|██████▎   | 368/585 [03:09<01:10,  3.07it/s] 63%|██████▎   | 369/585 [03:09<01:08,  3.16it/s] 63%|██████▎   | 370/585 [03:09<01:06,  3.22it/s] 63%|██████▎   | 371/585 [03:09<01:05,  3.28it/s] 64%|██████▎   | 372/585 [03:10<01:04,  3.31it/s] 64%|██████▍   | 373/585 [03:10<01:03,  3.33it/s] 64%|██████▍   | 374/585 [03:10<01:05,  3.20it/s] 64%|██████▍   | 375/585 [03:11<01:04,  3.25it/s] 64%|██████▍   | 376/585 [03:11<01:03,  3.30it/s] 64%|██████▍   | 377/585 [03:11<01:02,  3.32it/s] 65%|██████▍   | 378/585 [03:12<01:01,  3.34it/s] 65%|██████▍   | 379/585 [03:12<01:01,  3.36it/s] 65%|██████▍   | 380/585 [03:12<01:00,  3.37it/s] 65%|██████▌   | 381/585 [03:12<01:00,  3.38it/s] 65%|██████▌   | 382/585 [03:13<01:00,  3.38it/s] 65%|██████▌   | 383/585 [03:13<00:59,  3.38it/s] 66%|██████▌   | 384/585 [03:13<00:59,  3.39it/s] 66%|██████▌   | 385/585 [03:14<01:02,  3.22it/s] 66%|██████▌   | 386/585 [03:14<01:00,  3.27it/s] 66%|██████▌   | 387/585 [03:14<00:59,  3.31it/s] 66%|██████▋   | 388/585 [03:15<00:59,  3.33it/s] 66%|██████▋   | 389/585 [03:15<00:58,  3.35it/s] 67%|██████▋   | 390/585 [03:15<00:57,  3.37it/s] 67%|██████▋   | 391/585 [03:15<00:57,  3.37it/s] 67%|██████▋   | 392/585 [03:16<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:16<00:56,  3.38it/s] 67%|██████▋   | 394/585 [03:16<00:56,  3.39it/s] 68%|██████▊   | 395/585 [03:17<00:55,  3.40it/s] 68%|██████▊   | 396/585 [03:17<00:56,  3.33it/s] 68%|██████▊   | 397/585 [03:17<00:55,  3.37it/s] 68%|██████▊   | 398/585 [03:18<00:55,  3.39it/s] 68%|██████▊   | 399/585 [03:18<00:54,  3.40it/s] 68%|██████▊   | 400/585 [03:18<00:54,  3.41it/s] 69%|██████▊   | 401/585 [03:18<00:53,  3.42it/s] 69%|██████▊   | 402/585 [03:19<00:53,  3.42it/s] 69%|██████▉   | 403/585 [03:19<00:53,  3.43it/s] 69%|██████▉   | 404/585 [03:19<00:52,  3.43it/s] 69%|██████▉   | 405/585 [03:20<00:52,  3.43it/s] 69%|██████▉   | 406/585 [03:20<00:52,  3.43it/s] 70%|██████▉   | 407/585 [03:20<00:53,  3.35it/s] 70%|██████▉   | 408/585 [03:20<00:52,  3.37it/s] 70%|██████▉   | 409/585 [03:21<00:51,  3.40it/s] 70%|███████   | 410/585 [03:21<00:51,  3.41it/s] 70%|███████   | 411/585 [03:21<00:50,  3.42it/s] 70%|███████   | 412/585 [03:22<00:50,  3.42it/s] 71%|███████   | 413/585 [03:22<00:50,  3.43it/s] 71%|███████   | 414/585 [03:22<00:49,  3.43it/s] 71%|███████   | 415/585 [03:23<00:49,  3.43it/s] 71%|███████   | 416/585 [03:23<00:49,  3.43it/s] 71%|███████▏  | 417/585 [03:23<00:48,  3.44it/s] 71%|███████▏  | 418/585 [03:23<00:49,  3.35it/s] 72%|███████▏  | 419/585 [03:24<00:49,  3.38it/s] 72%|███████▏  | 420/585 [03:24<00:48,  3.40it/s] 72%|███████▏  | 421/585 [03:24<00:48,  3.41it/s] 72%|███████▏  | 422/585 [03:25<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:25<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:25<00:46,  3.43it/s] 73%|███████▎  | 425/585 [03:25<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:26<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:26<00:46,  3.43it/s] 73%|███████▎  | 428/585 [03:26<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:27<00:46,  3.37it/s] 74%|███████▎  | 430/585 [03:27<00:45,  3.39it/s] 74%|███████▎  | 431/585 [03:27<00:45,  3.41it/s] 74%|███████▍  | 432/585 [03:27<00:44,  3.41it/s] 74%|███████▍  | 433/585 [03:28<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:28<00:44,  3.43it/s] 74%|███████▍  | 435/585 [03:28<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:29<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:29<00:43,  3.44it/s] 75%|███████▍  | 438/585 [03:29<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:30<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:30<00:42,  3.38it/s] 75%|███████▌  | 441/585 [03:30<00:42,  3.40it/s] 76%|███████▌  | 442/585 [03:30<00:41,  3.41it/s] 76%|███████▌  | 443/585 [03:31<00:41,  3.42it/s] 76%|███████▌  | 444/585 [03:31<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:31<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:32<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:32<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:32<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:32<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:33<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:33<00:40,  3.32it/s] 77%|███████▋  | 452/585 [03:33<00:39,  3.35it/s] 77%|███████▋  | 453/585 [03:34<00:39,  3.37it/s] 78%|███████▊  | 454/585 [03:34<00:38,  3.39it/s] 78%|███████▊  | 455/585 [03:34<00:38,  3.40it/s] 78%|███████▊  | 456/585 [03:35<00:37,  3.41it/s] 78%|███████▊  | 457/585 [03:35<00:37,  3.42it/s] 78%|███████▊  | 458/585 [03:35<00:37,  3.42it/s] 78%|███████▊  | 459/585 [03:35<00:36,  3.42it/s] 79%|███████▊  | 460/585 [03:36<00:36,  3.42it/s] 79%|███████▉  | 461/585 [03:36<00:36,  3.42it/s] 79%|███████▉  | 462/585 [03:36<00:36,  3.34it/s] 79%|███████▉  | 463/585 [03:37<00:36,  3.30it/s] 79%|███████▉  | 464/585 [03:37<00:36,  3.34it/s] 79%|███████▉  | 465/585 [03:37<00:35,  3.37it/s] 80%|███████▉  | 466/585 [03:37<00:35,  3.39it/s] 80%|███████▉  | 467/585 [03:38<00:34,  3.40it/s] 80%|████████  | 468/585 [03:38<00:34,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 03:24:09,216 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:24:09,216 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:24:09,216 >>   Batch size = 8
{'eval_loss': 1.0809916257858276, 'eval_runtime': 14.0104, 'eval_samples_per_second': 348.454, 'eval_steps_per_second': 43.61, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.75it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.55it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.88it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.85it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.17it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.77it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.53it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.20it/s][A
  8%|▊         | 47/611 [00:01<00:13, 42.92it/s][A
  9%|▊         | 52/611 [00:01<00:12, 43.58it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.13it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.30it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.31it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.00it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.97it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.84it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.00it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.26it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.65it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.60it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.47it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.29it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.15it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.16it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.21it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.42it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.53it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.66it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.49it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.34it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.12it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.02it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.01it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.08it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.28it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.52it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.53it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.41it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.32it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.16it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.15it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.26it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.47it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.55it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.59it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.42it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.31it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.12it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.15it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.21it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.37it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.38it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.47it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.35it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.22it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.13it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.10it/s][A
 52%|█████▏    | 317/611 [00:07<00:07, 40.06it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 41.44it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 42.42it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.19it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.65it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.94it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.90it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.69it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.69it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.89it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.17it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.28it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.31it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.39it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.28it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.06it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.57it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.94it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.15it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.32it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.40it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.51it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.57it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.47it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.28it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.16it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 41.25it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 42.35it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.04it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.60it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.89it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.02it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.96it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.87it/s][A
 81%|████████  | 492/611 [00:11<00:02, 40.25it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 41.67it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 42.57it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.24it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.67it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.03it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.30it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.23it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.98it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.68it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.80it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.08it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.25it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.36it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.56it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.60it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.50it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.09it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.95it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 41.07it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 42.14it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 42.91it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.51it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.96it/s][A 80%|████████  | 468/585 [03:52<00:34,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:24:23,349 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 03:24:23,621 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:24:27,619 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:24:27,821 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:24:27,933 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:04<15:09,  7.84s/it] 80%|████████  | 470/585 [04:04<10:42,  5.58s/it] 81%|████████  | 471/585 [04:04<07:35,  4.00s/it] 81%|████████  | 472/585 [04:04<05:26,  2.89s/it] 81%|████████  | 473/585 [04:05<03:56,  2.11s/it] 81%|████████  | 474/585 [04:05<02:53,  1.56s/it] 81%|████████  | 475/585 [04:05<02:10,  1.18s/it] 81%|████████▏ | 476/585 [04:06<01:39,  1.09it/s] 82%|████████▏ | 477/585 [04:06<01:18,  1.37it/s] 82%|████████▏ | 478/585 [04:06<01:04,  1.67it/s] 82%|████████▏ | 479/585 [04:06<00:53,  1.97it/s] 82%|████████▏ | 480/585 [04:07<00:46,  2.25it/s] 82%|████████▏ | 481/585 [04:07<00:42,  2.47it/s] 82%|████████▏ | 482/585 [04:07<00:38,  2.69it/s] 83%|████████▎ | 483/585 [04:08<00:35,  2.87it/s] 83%|████████▎ | 484/585 [04:08<00:33,  3.01it/s] 83%|████████▎ | 485/585 [04:08<00:32,  3.11it/s] 83%|████████▎ | 486/585 [04:09<00:30,  3.20it/s] 83%|████████▎ | 487/585 [04:09<00:30,  3.26it/s] 83%|████████▎ | 488/585 [04:09<00:29,  3.31it/s] 84%|████████▎ | 489/585 [04:09<00:28,  3.35it/s] 84%|████████▍ | 490/585 [04:10<00:28,  3.38it/s] 84%|████████▍ | 491/585 [04:10<00:27,  3.40it/s] 84%|████████▍ | 492/585 [04:10<00:28,  3.30it/s] 84%|████████▍ | 493/585 [04:11<00:27,  3.34it/s] 84%|████████▍ | 494/585 [04:11<00:26,  3.37it/s] 85%|████████▍ | 495/585 [04:11<00:26,  3.39it/s] 85%|████████▍ | 496/585 [04:12<00:26,  3.41it/s] 85%|████████▍ | 497/585 [04:12<00:25,  3.42it/s] 85%|████████▌ | 498/585 [04:12<00:25,  3.43it/s] 85%|████████▌ | 499/585 [04:12<00:25,  3.43it/s] 85%|████████▌ | 500/585 [04:13<00:24,  3.43it/s]                                                  85%|████████▌ | 500/585 [04:13<00:24,  3.43it/s] 86%|████████▌ | 501/585 [04:13<00:24,  3.43it/s] 86%|████████▌ | 502/585 [04:13<00:24,  3.44it/s] 86%|████████▌ | 503/585 [04:14<00:24,  3.32it/s] 86%|████████▌ | 504/585 [04:14<00:24,  3.36it/s] 86%|████████▋ | 505/585 [04:14<00:23,  3.38it/s] 86%|████████▋ | 506/585 [04:14<00:23,  3.40it/s] 87%|████████▋ | 507/585 [04:15<00:22,  3.41it/s] 87%|████████▋ | 508/585 [04:15<00:22,  3.42it/s] 87%|████████▋ | 509/585 [04:15<00:22,  3.43it/s] 87%|████████▋ | 510/585 [04:16<00:21,  3.43it/s] 87%|████████▋ | 511/585 [04:16<00:21,  3.43it/s] 88%|████████▊ | 512/585 [04:16<00:21,  3.43it/s] 88%|████████▊ | 513/585 [04:16<00:20,  3.44it/s] 88%|████████▊ | 514/585 [04:17<00:21,  3.33it/s] 88%|████████▊ | 515/585 [04:17<00:20,  3.36it/s] 88%|████████▊ | 516/585 [04:17<00:20,  3.38it/s] 88%|████████▊ | 517/585 [04:18<00:20,  3.40it/s] 89%|████████▊ | 518/585 [04:18<00:19,  3.41it/s] 89%|████████▊ | 519/585 [04:18<00:19,  3.42it/s] 89%|████████▉ | 520/585 [04:19<00:18,  3.43it/s] 89%|████████▉ | 521/585 [04:19<00:18,  3.43it/s] 89%|████████▉ | 522/585 [04:19<00:18,  3.43it/s] 89%|████████▉ | 523/585 [04:19<00:18,  3.43it/s] 90%|████████▉ | 524/585 [04:20<00:17,  3.43it/s] 90%|████████▉ | 525/585 [04:20<00:17,  3.39it/s] 90%|████████▉ | 526/585 [04:20<00:17,  3.40it/s] 90%|█████████ | 527/585 [04:21<00:16,  3.41it/s] 90%|█████████ | 528/585 [04:21<00:16,  3.42it/s] 90%|█████████ | 529/585 [04:21<00:16,  3.43it/s] 91%|█████████ | 530/585 [04:21<00:16,  3.43it/s] 91%|█████████ | 531/585 [04:22<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:22<00:15,  3.43it/s] 91%|█████████ | 533/585 [04:22<00:15,  3.44it/s] 91%|█████████▏| 534/585 [04:23<00:14,  3.44it/s] 91%|█████████▏| 535/585 [04:23<00:14,  3.44it/s] 92%|█████████▏| 536/585 [04:23<00:14,  3.34it/s] 92%|█████████▏| 537/585 [04:24<00:14,  3.36it/s] 92%|█████████▏| 538/585 [04:24<00:13,  3.38it/s] 92%|█████████▏| 539/585 [04:24<00:13,  3.40it/s] 92%|█████████▏| 540/585 [04:24<00:13,  3.40it/s] 92%|█████████▏| 541/585 [04:25<00:12,  3.41it/s] 93%|█████████▎| 542/585 [04:25<00:12,  3.42it/s] 93%|█████████▎| 543/585 [04:25<00:12,  3.42it/s] 93%|█████████▎| 544/585 [04:26<00:11,  3.42it/s] 93%|█████████▎| 545/585 [04:26<00:11,  3.43it/s] 93%|█████████▎| 546/585 [04:26<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:26<00:11,  3.35it/s] 94%|█████████▎| 548/585 [04:27<00:10,  3.38it/s] 94%|█████████▍| 549/585 [04:27<00:10,  3.39it/s] 94%|█████████▍| 550/585 [04:27<00:10,  3.40it/s] 94%|█████████▍| 551/585 [04:28<00:09,  3.41it/s] 94%|█████████▍| 552/585 [04:28<00:09,  3.42it/s] 95%|█████████▍| 553/585 [04:28<00:09,  3.42it/s] 95%|█████████▍| 554/585 [04:29<00:09,  3.42it/s] 95%|█████████▍| 555/585 [04:29<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:29<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:29<00:08,  3.42it/s] 95%|█████████▌| 558/585 [04:30<00:07,  3.42it/s] 96%|█████████▌| 559/585 [04:30<00:07,  3.43it/s] 96%|█████████▌| 560/585 [04:30<00:07,  3.43it/s] 96%|█████████▌| 561/585 [04:31<00:06,  3.43it/s] 96%|█████████▌| 562/585 [04:31<00:06,  3.43it/s] 96%|█████████▌| 563/585 [04:31<00:06,  3.37it/s] 96%|█████████▋| 564/585 [04:31<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:32<00:05,  3.40it/s] 97%|█████████▋| 566/585 [04:32<00:05,  3.41it/s] 97%|█████████▋| 567/585 [04:32<00:05,  3.41it/s] 97%|█████████▋| 568/585 [04:33<00:04,  3.42it/s] 97%|█████████▋| 569/585 [04:33<00:04,  3.42it/s] 97%|█████████▋| 570/585 [04:33<00:04,  3.43it/s] 98%|█████████▊| 571/585 [04:33<00:04,  3.43it/s] 98%|█████████▊| 572/585 [04:34<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:34<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:34<00:03,  3.32it/s] 98%|█████████▊| 575/585 [04:35<00:02,  3.35it/s] 98%|█████████▊| 576/585 [04:35<00:02,  3.37it/s] 99%|█████████▊| 577/585 [04:35<00:02,  3.39it/s] 99%|█████████▉| 578/585 [04:36<00:02,  3.40it/s] 99%|█████████▉| 579/585 [04:36<00:01,  3.41it/s] 99%|█████████▉| 580/585 [04:36<00:01,  3.42it/s] 99%|█████████▉| 581/585 [04:37<00:01,  3.04it/s] 99%|█████████▉| 582/585 [04:37<00:00,  3.15it/s]100%|█████████▉| 583/585 [04:37<00:00,  3.22it/s]100%|█████████▉| 584/585 [04:37<00:00,  3.19it/s]100%|██████████| 585/585 [04:38<00:00,  3.26it/s][INFO|trainer.py:2140] 2023-08-29 03:25:08,870 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:25:08,870 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:25:08,870 >>   Batch size = 8
{'eval_loss': 1.0952880382537842, 'eval_runtime': 13.9184, 'eval_samples_per_second': 350.759, 'eval_steps_per_second': 43.899, 'epoch': 4.0}
{'loss': 0.3252, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.71it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.23it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.74it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.89it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.41it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.90it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.50it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.45it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.53it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.64it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.64it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.58it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.57it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.46it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.31it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.28it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.23it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.32it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.42it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.51it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.54it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.08it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.47it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.56it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.57it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.79it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.14it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.29it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.35it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.21it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.32it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.34it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.35it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.18it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.32it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.42it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.41it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.48it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.40it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.42it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.25it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.21it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.25it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.38it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.44it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.48it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.36it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.44it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.43it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.26it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.21it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.22it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.25it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.24it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.25it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.24it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.00it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.16it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.18it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.30it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.35it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.34it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.37it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.46it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.44it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.30it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.29it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.31it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.36it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.38it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.40it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.42it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.39it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 39.37it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 40.86it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 41.98it/s][A
 65%|██████▍   | 397/611 [00:08<00:05, 42.71it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.45it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.90it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.12it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.14it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 43.76it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.72it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.89it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.00it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.22it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.39it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.55it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.70it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.41it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.20it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.08it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.13it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.08it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.39it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.52it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.63it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.42it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.47it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 39.28it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 35.21it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 37.61it/s][A
 86%|████████▋ | 527/611 [00:12<00:02, 39.52it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 41.05it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 42.16it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 42.99it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.47it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.69it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.54it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.39it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.46it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 43.71it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.07it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.36it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.44it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.54it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.44it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.16it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.83it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.83it/s][A100%|██████████| 585/585 [04:52<00:00,  3.26it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:25:22,956 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 03:25:23,287 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:25:27,345 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:25:27,549 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:25:27,677 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:25:35,383 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:25:35,403 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 1.0407958030700684).
                                                 100%|██████████| 585/585 [05:15<00:00,  3.26it/s]100%|██████████| 585/585 [05:15<00:00,  1.85it/s]
[INFO|trainer.py:1894] 2023-08-29 03:25:46,143 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 03:25:46,286 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:25:49,192 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:25:49,323 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:25:49,408 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:25:49,994 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,994 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,995 >>   train_loss               =     0.3229
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,995 >>   train_runtime            = 0:05:15.50
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,995 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,995 >>   train_samples_per_second =    118.859
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:49,995 >>   train_steps_per_second   =      1.854
{'eval_loss': 1.1018191576004028, 'eval_runtime': 13.9358, 'eval_samples_per_second': 350.322, 'eval_steps_per_second': 43.844, 'epoch': 5.0}
{'train_runtime': 315.5004, 'train_samples_per_second': 118.859, 'train_steps_per_second': 1.854, 'train_loss': 0.3228885552822015, 'epoch': 5.0}
08/29/2023 03:25:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:25:50,258 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:25:50,258 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:25:50,258 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.45it/s]  2%|▏         | 12/611 [00:00<00:12, 48.78it/s]  3%|▎         | 17/611 [00:00<00:12, 47.18it/s]  4%|▎         | 22/611 [00:00<00:12, 46.26it/s]  4%|▍         | 27/611 [00:00<00:12, 45.93it/s]  5%|▌         | 32/611 [00:00<00:12, 45.69it/s]  6%|▌         | 37/611 [00:00<00:12, 45.50it/s]  7%|▋         | 42/611 [00:00<00:12, 45.05it/s]  8%|▊         | 47/611 [00:01<00:12, 44.41it/s]  9%|▊         | 52/611 [00:01<00:12, 44.11it/s]  9%|▉         | 57/611 [00:01<00:12, 44.18it/s] 10%|█         | 62/611 [00:01<00:12, 44.35it/s] 11%|█         | 67/611 [00:01<00:12, 44.49it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.67it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.84it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.81it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.65it/s] 15%|█▌        | 92/611 [00:02<00:12, 42.37it/s] 16%|█▌        | 97/611 [00:02<00:11, 42.96it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.39it/s] 18%|█▊        | 107/611 [00:02<00:11, 43.80it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.11it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.36it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.46it/s] 21%|██        | 127/611 [00:02<00:10, 44.49it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.23it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.10it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.13it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.22it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.34it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.48it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.56it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.61it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.60it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.35it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.22it/s] 31%|███       | 187/611 [00:04<00:09, 44.17it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.06it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.47it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.58it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.61it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.67it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.55it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.42it/s] 37%|███▋      | 227/611 [00:05<00:09, 42.59it/s] 38%|███▊      | 232/611 [00:05<00:08, 43.14it/s] 39%|███▉      | 237/611 [00:05<00:08, 43.54it/s] 40%|███▉      | 242/611 [00:05<00:08, 43.95it/s] 40%|████      | 247/611 [00:05<00:08, 44.21it/s] 41%|████      | 252/611 [00:05<00:08, 44.36it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.33it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.27it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.08it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.10it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.18it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.32it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.45it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.60it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.61it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.47it/s] 50%|█████     | 307/611 [00:06<00:06, 44.38it/s] 51%|█████     | 312/611 [00:07<00:06, 44.22it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.17it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.14it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.31it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.51it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.60it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.52it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.55it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.38it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.28it/s] 59%|█████▉    | 362/611 [00:08<00:05, 43.35it/s] 60%|██████    | 367/611 [00:08<00:05, 43.65it/s] 61%|██████    | 372/611 [00:08<00:05, 43.94it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.29it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.51it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.54it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.44it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.40it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.15it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.08it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.19it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.33it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.52it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.44it/s] 71%|███████   | 432/611 [00:09<00:04, 44.52it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.45it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.30it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s] 74%|███████▍  | 452/611 [00:10<00:03, 43.45it/s] 75%|███████▍  | 457/611 [00:10<00:03, 43.78it/s] 76%|███████▌  | 462/611 [00:10<00:03, 43.98it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.28it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.47it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.45it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.33it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.21it/s] 81%|████████  | 492/611 [00:11<00:02, 44.03it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.16it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.21it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.36it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.49it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.60it/s] 85%|████████▌ | 522/611 [00:11<00:01, 44.63it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.48it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.27it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.21it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.19it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.14it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.28it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.46it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.58it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.46it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.41it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.27it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.18it/s] 96%|█████████▌| 587/611 [00:13<00:00, 42.84it/s] 97%|█████████▋| 592/611 [00:13<00:00, 43.35it/s] 98%|█████████▊| 597/611 [00:13<00:00, 43.88it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.22it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.23it/s]100%|██████████| 611/611 [00:13<00:00, 44.33it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:26:04,059 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   eval_loss               =     1.0408
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   eval_runtime            = 0:00:13.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   eval_samples_per_second =    353.754
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   eval_steps_per_second   =     44.274
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:26:04,059 >>   perplexity              =     2.8315
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:13,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:13,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:13,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:13,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:13,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:26:14,412 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:26:14,413 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:14,982 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:16,045 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:16,045 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:19,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:19,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:19,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:19,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:19,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:26:19,723 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:26:19,724 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:20,335 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:20,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:20,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:11,  1.66it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.71it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:13,  1.76it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:14,  1.74it/s]Extractor Predicting: 25it [00:15,  1.71it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.71it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:19,  1.71it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:22,  1.70it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:23,  1.71it/s]Extractor Predicting: 41it [00:24,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:26,  1.72it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:29,  1.63it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:32,  1.60it/s]Extractor Predicting: 56it [00:33,  1.57it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.58it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:36,  1.59it/s]Extractor Predicting: 62it [00:37,  1.44it/s]Extractor Predicting: 63it [00:38,  1.48it/s]Extractor Predicting: 64it [00:38,  1.52it/s]Extractor Predicting: 65it [00:39,  1.52it/s]Extractor Predicting: 66it [00:40,  1.49it/s]Extractor Predicting: 67it [00:40,  1.52it/s]Extractor Predicting: 68it [00:41,  1.57it/s]Extractor Predicting: 69it [00:42,  1.57it/s]Extractor Predicting: 70it [00:42,  1.54it/s]Extractor Predicting: 71it [00:43,  1.54it/s]Extractor Predicting: 72it [00:44,  1.51it/s]Extractor Predicting: 73it [00:44,  1.52it/s]Extractor Predicting: 74it [00:45,  1.54it/s]Extractor Predicting: 75it [00:46,  1.55it/s]Extractor Predicting: 76it [00:46,  1.56it/s]Extractor Predicting: 77it [00:47,  1.56it/s]Extractor Predicting: 78it [00:47,  1.59it/s]Extractor Predicting: 79it [00:48,  1.61it/s]Extractor Predicting: 80it [00:49,  1.60it/s]Extractor Predicting: 81it [00:49,  1.61it/s]Extractor Predicting: 82it [00:50,  1.56it/s]Extractor Predicting: 83it [00:51,  1.57it/s]Extractor Predicting: 84it [00:51,  1.57it/s]Extractor Predicting: 85it [00:52,  1.57it/s]Extractor Predicting: 86it [00:52,  1.56it/s]Extractor Predicting: 87it [00:53,  1.56it/s]Extractor Predicting: 88it [00:54,  1.52it/s]Extractor Predicting: 89it [00:54,  1.50it/s]Extractor Predicting: 90it [00:55,  1.51it/s]Extractor Predicting: 91it [00:56,  1.52it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:57,  1.65it/s]Extractor Predicting: 94it [00:58,  1.64it/s]Extractor Predicting: 95it [00:58,  1.64it/s]Extractor Predicting: 96it [00:59,  1.64it/s]Extractor Predicting: 97it [00:59,  1.66it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.54it/s]Extractor Predicting: 100it [01:01,  1.58it/s]Extractor Predicting: 101it [01:02,  1.50it/s]Extractor Predicting: 102it [01:03,  1.49it/s]Extractor Predicting: 103it [01:03,  1.52it/s]Extractor Predicting: 104it [01:04,  1.55it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:05,  1.60it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.63it/s]Extractor Predicting: 110it [01:08,  1.63it/s]Extractor Predicting: 111it [01:08,  1.67it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.61it/s]Extractor Predicting: 116it [01:11,  1.57it/s]Extractor Predicting: 117it [01:12,  1.56it/s]Extractor Predicting: 118it [01:13,  1.56it/s]Extractor Predicting: 119it [01:13,  1.54it/s]Extractor Predicting: 120it [01:14,  1.51it/s]Extractor Predicting: 121it [01:15,  1.53it/s]Extractor Predicting: 122it [01:15,  1.52it/s]Extractor Predicting: 123it [01:16,  1.55it/s]Extractor Predicting: 124it [01:17,  1.56it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:20,  1.60it/s]Extractor Predicting: 130it [01:20,  1.53it/s]Extractor Predicting: 131it [01:21,  1.55it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:23,  1.42it/s]Extractor Predicting: 134it [01:23,  1.46it/s]Extractor Predicting: 135it [01:24,  1.47it/s]Extractor Predicting: 136it [01:24,  1.50it/s]Extractor Predicting: 137it [01:25,  1.49it/s]Extractor Predicting: 138it [01:26,  1.50it/s]Extractor Predicting: 139it [01:26,  1.49it/s]Extractor Predicting: 140it [01:27,  1.49it/s]Extractor Predicting: 141it [01:28,  1.52it/s]Extractor Predicting: 142it [01:28,  1.53it/s]Extractor Predicting: 143it [01:29,  1.51it/s]Extractor Predicting: 144it [01:30,  1.55it/s]Extractor Predicting: 145it [01:30,  1.57it/s]Extractor Predicting: 146it [01:31,  1.56it/s]Extractor Predicting: 147it [01:32,  1.53it/s]Extractor Predicting: 148it [01:32,  1.53it/s]Extractor Predicting: 149it [01:33,  1.52it/s]Extractor Predicting: 150it [01:34,  1.51it/s]Extractor Predicting: 151it [01:34,  1.51it/s]Extractor Predicting: 152it [01:35,  1.51it/s]Extractor Predicting: 153it [01:36,  1.49it/s]Extractor Predicting: 154it [01:36,  1.50it/s]Extractor Predicting: 155it [01:37,  1.51it/s]Extractor Predicting: 156it [01:38,  1.46it/s]Extractor Predicting: 157it [01:38,  1.43it/s]Extractor Predicting: 158it [01:39,  1.39it/s]Extractor Predicting: 159it [01:40,  1.43it/s]Extractor Predicting: 160it [01:41,  1.47it/s]Extractor Predicting: 161it [01:41,  1.49it/s]Extractor Predicting: 162it [01:42,  1.50it/s]Extractor Predicting: 163it [01:42,  1.50it/s]Extractor Predicting: 164it [01:43,  1.53it/s]Extractor Predicting: 165it [01:44,  1.53it/s]Extractor Predicting: 166it [01:44,  1.56it/s]Extractor Predicting: 167it [01:45,  1.56it/s]Extractor Predicting: 168it [01:46,  1.53it/s]Extractor Predicting: 169it [01:46,  1.54it/s]Extractor Predicting: 170it [01:47,  1.55it/s]Extractor Predicting: 171it [01:48,  1.57it/s]Extractor Predicting: 172it [01:48,  1.60it/s]Extractor Predicting: 173it [01:49,  1.55it/s]Extractor Predicting: 174it [01:49,  1.57it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:51,  1.55it/s]Extractor Predicting: 177it [01:51,  1.52it/s]Extractor Predicting: 178it [01:52,  1.54it/s]Extractor Predicting: 179it [01:53,  1.54it/s]Extractor Predicting: 180it [01:53,  1.54it/s]Extractor Predicting: 181it [01:54,  1.54it/s]Extractor Predicting: 182it [01:55,  1.54it/s]Extractor Predicting: 183it [01:55,  1.50it/s]Extractor Predicting: 184it [01:56,  1.49it/s]Extractor Predicting: 185it [01:57,  1.59it/s]Extractor Predicting: 185it [01:57,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:34,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:34,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:34,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:34,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:34,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:28:35,580 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:28:35,582 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:28:36,223 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:28:37,276 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:28:37,276 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:40,281 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:40,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:40,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:40,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:40,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:28:40,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:28:40,956 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:28:41,548 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:28:41,703 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:28:41,704 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.28979272585060617,
  "recall": 0.1517820565342073,
  "score": 0.199220325312542,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.71it/s]Extractor Predicting: 32it [00:19,  1.70it/s]Extractor Predicting: 33it [00:20,  1.70it/s]Extractor Predicting: 34it [00:20,  1.70it/s]Extractor Predicting: 35it [00:21,  1.71it/s]Extractor Predicting: 36it [00:22,  1.68it/s]Extractor Predicting: 37it [00:22,  1.70it/s]Extractor Predicting: 38it [00:23,  1.68it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.72it/s]Extractor Predicting: 46it [00:28,  1.74it/s]Extractor Predicting: 47it [00:28,  1.73it/s]Extractor Predicting: 48it [00:29,  1.73it/s]Extractor Predicting: 49it [00:29,  1.72it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:31,  1.72it/s]Extractor Predicting: 52it [00:31,  1.72it/s]Extractor Predicting: 53it [00:32,  1.70it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.64it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:35,  1.75it/s]Extractor Predicting: 59it [00:35,  1.71it/s]Extractor Predicting: 60it [00:36,  1.69it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:38,  1.56it/s]Extractor Predicting: 64it [00:39,  1.50it/s]Extractor Predicting: 65it [00:39,  1.51it/s]Extractor Predicting: 66it [00:40,  1.50it/s]Extractor Predicting: 67it [00:41,  1.49it/s]Extractor Predicting: 68it [00:41,  1.51it/s]Extractor Predicting: 69it [00:42,  1.52it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:43,  1.53it/s]Extractor Predicting: 72it [00:44,  1.55it/s]Extractor Predicting: 73it [00:44,  1.61it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.63it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.66it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:48,  1.70it/s]Extractor Predicting: 80it [00:48,  1.73it/s]Extractor Predicting: 81it [00:49,  1.68it/s]Extractor Predicting: 82it [00:50,  1.70it/s]Extractor Predicting: 83it [00:50,  1.66it/s]Extractor Predicting: 84it [00:51,  1.65it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.58it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:54,  1.58it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:55,  1.58it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:57,  1.61it/s]Extractor Predicting: 94it [00:57,  1.45it/s]Extractor Predicting: 95it [00:58,  1.48it/s]Extractor Predicting: 96it [00:59,  1.49it/s]Extractor Predicting: 97it [00:59,  1.53it/s]Extractor Predicting: 98it [01:00,  1.55it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:01,  1.55it/s]Extractor Predicting: 101it [01:02,  1.58it/s]Extractor Predicting: 102it [01:03,  1.59it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:04,  1.58it/s]Extractor Predicting: 105it [01:04,  1.60it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:08,  1.63it/s]Extractor Predicting: 112it [01:09,  1.59it/s]Extractor Predicting: 113it [01:09,  1.58it/s]Extractor Predicting: 114it [01:10,  1.58it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:11,  1.59it/s]Extractor Predicting: 117it [01:12,  1.62it/s]Extractor Predicting: 118it [01:13,  1.60it/s]Extractor Predicting: 119it [01:13,  1.59it/s]Extractor Predicting: 120it [01:14,  1.61it/s]Extractor Predicting: 121it [01:14,  1.65it/s]Extractor Predicting: 122it [01:15,  1.63it/s]Extractor Predicting: 123it [01:16,  1.61it/s]Extractor Predicting: 124it [01:16,  1.58it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.59it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:19,  1.57it/s]Extractor Predicting: 130it [01:20,  1.60it/s]Extractor Predicting: 131it [01:21,  1.58it/s]Extractor Predicting: 132it [01:21,  1.56it/s]Extractor Predicting: 133it [01:22,  1.55it/s]Extractor Predicting: 134it [01:23,  1.55it/s]Extractor Predicting: 135it [01:23,  1.57it/s]Extractor Predicting: 136it [01:24,  1.61it/s]Extractor Predicting: 137it [01:25,  1.54it/s]Extractor Predicting: 138it [01:25,  1.57it/s]Extractor Predicting: 139it [01:26,  1.60it/s]Extractor Predicting: 140it [01:26,  1.61it/s]Extractor Predicting: 141it [01:27,  1.60it/s]Extractor Predicting: 142it [01:28,  1.62it/s]Extractor Predicting: 143it [01:28,  1.52it/s]Extractor Predicting: 144it [01:29,  1.59it/s]Extractor Predicting: 145it [01:30,  1.54it/s]Extractor Predicting: 146it [01:30,  1.58it/s]Extractor Predicting: 147it [01:31,  1.64it/s]Extractor Predicting: 148it [01:31,  1.62it/s]Extractor Predicting: 149it [01:32,  1.66it/s]Extractor Predicting: 150it [01:33,  1.67it/s]Extractor Predicting: 151it [01:33,  1.69it/s]Extractor Predicting: 152it [01:34,  1.65it/s]Extractor Predicting: 153it [01:34,  1.64it/s]Extractor Predicting: 154it [01:35,  1.60it/s]Extractor Predicting: 155it [01:36,  1.61it/s]Extractor Predicting: 156it [01:36,  1.66it/s]Extractor Predicting: 157it [01:37,  1.62it/s]Extractor Predicting: 158it [01:38,  1.61it/s]Extractor Predicting: 159it [01:38,  1.62it/s]Extractor Predicting: 160it [01:39,  1.62it/s]Extractor Predicting: 161it [01:39,  1.66it/s]Extractor Predicting: 162it [01:40,  1.60it/s]Extractor Predicting: 163it [01:41,  1.60it/s]Extractor Predicting: 164it [01:41,  1.60it/s]Extractor Predicting: 165it [01:42,  1.55it/s]Extractor Predicting: 166it [01:43,  1.53it/s]Extractor Predicting: 167it [01:43,  1.54it/s]Extractor Predicting: 168it [01:44,  1.56it/s]Extractor Predicting: 169it [01:44,  1.59it/s]Extractor Predicting: 170it [01:45,  1.59it/s]Extractor Predicting: 171it [01:46,  1.61it/s]Extractor Predicting: 172it [01:46,  1.64it/s]Extractor Predicting: 173it [01:47,  1.64it/s]Extractor Predicting: 174it [01:47,  1.63it/s]Extractor Predicting: 175it [01:48,  1.63it/s]Extractor Predicting: 176it [01:49,  1.64it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:50,  1.59it/s]Extractor Predicting: 179it [01:51,  1.57it/s]Extractor Predicting: 180it [01:51,  1.61it/s]Extractor Predicting: 181it [01:52,  1.62it/s]Extractor Predicting: 182it [01:52,  1.64it/s]Extractor Predicting: 183it [01:53,  1.67it/s]Extractor Predicting: 184it [01:54,  1.65it/s]Extractor Predicting: 185it [01:54,  1.66it/s]Extractor Predicting: 186it [01:55,  1.59it/s]Extractor Predicting: 187it [01:56,  1.61it/s]Extractor Predicting: 188it [01:56,  1.62it/s]Extractor Predicting: 189it [01:57,  1.62it/s]Extractor Predicting: 190it [01:57,  1.64it/s]Extractor Predicting: 191it [01:58,  1.64it/s]Extractor Predicting: 192it [01:59,  1.69it/s]Extractor Predicting: 193it [01:59,  1.66it/s]Extractor Predicting: 194it [02:00,  1.67it/s]Extractor Predicting: 195it [02:00,  1.64it/s]Extractor Predicting: 196it [02:01,  1.64it/s]Extractor Predicting: 197it [02:02,  1.63it/s]Extractor Predicting: 198it [02:02,  1.62it/s]Extractor Predicting: 199it [02:03,  1.61it/s]Extractor Predicting: 200it [02:03,  1.61it/s]Extractor Predicting: 201it [02:04,  1.63it/s]Extractor Predicting: 202it [02:05,  1.64it/s]Extractor Predicting: 203it [02:05,  1.65it/s]Extractor Predicting: 204it [02:06,  1.64it/s]Extractor Predicting: 205it [02:06,  1.65it/s]Extractor Predicting: 206it [02:07,  1.64it/s]Extractor Predicting: 207it [02:08,  1.66it/s]Extractor Predicting: 208it [02:08,  1.62it/s]Extractor Predicting: 209it [02:09,  1.41it/s]Extractor Predicting: 210it [02:10,  1.52it/s]Extractor Predicting: 211it [02:10,  1.52it/s]Extractor Predicting: 212it [02:11,  1.56it/s]Extractor Predicting: 213it [02:12,  1.58it/s]Extractor Predicting: 214it [02:12,  1.63it/s]Extractor Predicting: 215it [02:13,  1.62it/s]Extractor Predicting: 216it [02:14,  1.60it/s]Extractor Predicting: 217it [02:14,  1.62it/s]Extractor Predicting: 218it [02:15,  1.60it/s]Extractor Predicting: 219it [02:15,  1.60it/s]Extractor Predicting: 220it [02:16,  1.61it/s]Extractor Predicting: 221it [02:17,  1.63it/s]Extractor Predicting: 222it [02:17,  1.57it/s]Extractor Predicting: 223it [02:18,  1.52it/s]Extractor Predicting: 224it [02:19,  1.55it/s]Extractor Predicting: 225it [02:19,  1.57it/s]Extractor Predicting: 226it [02:20,  1.61it/s]Extractor Predicting: 227it [02:20,  1.60it/s]Extractor Predicting: 228it [02:21,  1.63it/s]Extractor Predicting: 229it [02:22,  1.66it/s]Extractor Predicting: 230it [02:22,  1.67it/s]Extractor Predicting: 231it [02:23,  1.68it/s]Extractor Predicting: 232it [02:23,  1.66it/s]Extractor Predicting: 233it [02:24,  1.65it/s]Extractor Predicting: 234it [02:25,  1.66it/s]Extractor Predicting: 235it [02:25,  1.63it/s]Extractor Predicting: 236it [02:26,  1.63it/s]Extractor Predicting: 237it [02:26,  1.63it/s]Extractor Predicting: 238it [02:27,  1.60it/s]Extractor Predicting: 239it [02:28,  1.62it/s]Extractor Predicting: 240it [02:28,  1.61it/s]Extractor Predicting: 241it [02:29,  1.63it/s]Extractor Predicting: 242it [02:30,  1.60it/s]Extractor Predicting: 243it [02:30,  1.56it/s]Extractor Predicting: 244it [02:31,  1.56it/s]Extractor Predicting: 245it [02:32,  1.60it/s]Extractor Predicting: 246it [02:32,  1.60it/s]Extractor Predicting: 247it [02:33,  1.64it/s]Extractor Predicting: 248it [02:33,  1.57it/s]Extractor Predicting: 249it [02:34,  1.57it/s]Extractor Predicting: 250it [02:35,  1.59it/s]Extractor Predicting: 251it [02:35,  1.60it/s]Extractor Predicting: 252it [02:36,  1.58it/s]Extractor Predicting: 253it [02:37,  1.54it/s]Extractor Predicting: 254it [02:37,  1.53it/s]Extractor Predicting: 255it [02:38,  1.55it/s]Extractor Predicting: 256it [02:39,  1.56it/s]Extractor Predicting: 257it [02:39,  1.58it/s]Extractor Predicting: 258it [02:40,  1.59it/s]Extractor Predicting: 259it [02:40,  1.59it/s]Extractor Predicting: 260it [02:41,  1.54it/s]Extractor Predicting: 261it [02:42,  1.57it/s]Extractor Predicting: 262it [02:42,  1.56it/s]Extractor Predicting: 263it [02:43,  1.54it/s]Extractor Predicting: 264it [02:44,  1.56it/s]Extractor Predicting: 265it [02:44,  1.59it/s]Extractor Predicting: 266it [02:45,  1.54it/s]Extractor Predicting: 267it [02:46,  1.54it/s]Extractor Predicting: 268it [02:46,  1.51it/s]Extractor Predicting: 269it [02:47,  1.53it/s]Extractor Predicting: 270it [02:48,  1.53it/s]Extractor Predicting: 271it [02:48,  1.53it/s]Extractor Predicting: 272it [02:49,  1.53it/s]Extractor Predicting: 273it [02:49,  1.55it/s]Extractor Predicting: 274it [02:50,  1.52it/s]Extractor Predicting: 275it [02:51,  1.53it/s]Extractor Predicting: 276it [02:51,  1.54it/s]Extractor Predicting: 277it [02:52,  1.55it/s]Extractor Predicting: 278it [02:53,  1.58it/s]Extractor Predicting: 279it [02:53,  1.55it/s]Extractor Predicting: 280it [02:54,  1.56it/s]Extractor Predicting: 281it [02:55,  1.52it/s]Extractor Predicting: 282it [02:55,  1.52it/s]Extractor Predicting: 283it [02:56,  1.54it/s]Extractor Predicting: 284it [02:57,  1.56it/s]Extractor Predicting: 285it [02:57,  1.55it/s]Extractor Predicting: 286it [02:58,  1.58it/s]Extractor Predicting: 287it [02:59,  1.52it/s]Extractor Predicting: 288it [02:59,  1.56it/s]Extractor Predicting: 289it [03:00,  1.58it/s]Extractor Predicting: 290it [03:00,  1.54it/s]Extractor Predicting: 291it [03:01,  1.51it/s]Extractor Predicting: 292it [03:02,  1.53it/s]Extractor Predicting: 293it [03:02,  1.54it/s]Extractor Predicting: 294it [03:03,  1.52it/s]Extractor Predicting: 295it [03:04,  1.52it/s]Extractor Predicting: 296it [03:04,  1.56it/s]Extractor Predicting: 297it [03:05,  1.57it/s]Extractor Predicting: 298it [03:06,  1.55it/s]Extractor Predicting: 299it [03:06,  1.55it/s]Extractor Predicting: 300it [03:07,  1.54it/s]Extractor Predicting: 301it [03:08,  1.56it/s]Extractor Predicting: 302it [03:08,  1.53it/s]Extractor Predicting: 303it [03:09,  1.56it/s]Extractor Predicting: 304it [03:10,  1.58it/s]Extractor Predicting: 305it [03:10,  1.61it/s]Extractor Predicting: 306it [03:11,  1.64it/s]Extractor Predicting: 307it [03:11,  1.61it/s]Extractor Predicting: 308it [03:12,  1.63it/s]Extractor Predicting: 309it [03:13,  1.59it/s]Extractor Predicting: 310it [03:13,  1.58it/s]Extractor Predicting: 311it [03:14,  1.54it/s]Extractor Predicting: 312it [03:15,  1.57it/s]Extractor Predicting: 313it [03:16,  1.34it/s]Extractor Predicting: 314it [03:16,  1.39it/s]Extractor Predicting: 315it [03:17,  1.44it/s]Extractor Predicting: 316it [03:17,  1.50it/s]Extractor Predicting: 317it [03:18,  1.53it/s]Extractor Predicting: 318it [03:19,  1.58it/s]Extractor Predicting: 319it [03:19,  1.55it/s]Extractor Predicting: 320it [03:20,  1.52it/s]Extractor Predicting: 321it [03:21,  1.53it/s]Extractor Predicting: 322it [03:21,  1.55it/s]Extractor Predicting: 323it [03:22,  1.49it/s]Extractor Predicting: 324it [03:23,  1.51it/s]Extractor Predicting: 325it [03:23,  1.55it/s]Extractor Predicting: 326it [03:24,  1.54it/s]Extractor Predicting: 327it [03:25,  1.56it/s]Extractor Predicting: 328it [03:25,  1.52it/s]Extractor Predicting: 329it [03:26,  1.54it/s]Extractor Predicting: 330it [03:27,  1.52it/s]Extractor Predicting: 331it [03:27,  1.52it/s]Extractor Predicting: 332it [03:28,  1.52it/s]Extractor Predicting: 333it [03:29,  1.51it/s]Extractor Predicting: 334it [03:29,  1.54it/s]Extractor Predicting: 335it [03:30,  1.53it/s]Extractor Predicting: 336it [03:31,  1.45it/s]Extractor Predicting: 337it [03:31,  1.47it/s]Extractor Predicting: 338it [03:32,  1.48it/s]Extractor Predicting: 339it [03:33,  1.48it/s]Extractor Predicting: 340it [03:33,  1.49it/s]Extractor Predicting: 341it [03:34,  1.49it/s]Extractor Predicting: 342it [03:35,  1.50it/s]Extractor Predicting: 343it [03:35,  1.53it/s]Extractor Predicting: 344it [03:36,  1.54it/s]Extractor Predicting: 345it [03:37,  1.49it/s]Extractor Predicting: 346it [03:37,  1.50it/s]Extractor Predicting: 347it [03:38,  1.51it/s]Extractor Predicting: 348it [03:38,  1.53it/s]Extractor Predicting: 349it [03:39,  1.54it/s]Extractor Predicting: 350it [03:40,  1.54it/s]Extractor Predicting: 351it [03:40,  1.57it/s]Extractor Predicting: 352it [03:41,  1.52it/s]Extractor Predicting: 353it [03:42,  1.50it/s]Extractor Predicting: 354it [03:42,  1.51it/s]Extractor Predicting: 355it [03:43,  1.54it/s]Extractor Predicting: 356it [03:44,  1.54it/s]Extractor Predicting: 357it [03:44,  1.55it/s]Extractor Predicting: 358it [03:45,  1.56it/s]Extractor Predicting: 359it [03:46,  1.56it/s]Extractor Predicting: 360it [03:46,  1.53it/s]Extractor Predicting: 361it [03:47,  1.52it/s]Extractor Predicting: 362it [03:48,  1.51it/s]Extractor Predicting: 363it [03:48,  1.56it/s]Extractor Predicting: 364it [03:49,  1.57it/s]Extractor Predicting: 365it [03:50,  1.55it/s]Extractor Predicting: 366it [03:50,  1.51it/s]Extractor Predicting: 367it [03:51,  1.51it/s]Extractor Predicting: 368it [03:52,  1.49it/s]Extractor Predicting: 369it [03:52,  1.45it/s]Extractor Predicting: 370it [03:53,  1.44it/s]Extractor Predicting: 371it [03:54,  1.47it/s]Extractor Predicting: 372it [03:54,  1.74it/s]Extractor Predicting: 372it [03:54,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:47,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:47,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:47,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:47,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:47,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:32:47,794 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:32:47,795 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:32:48,490 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:32:49,542 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:32:49,542 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:52,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:52,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:52,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:52,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:52,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:32:53,003 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:32:53,004 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:32:53,362 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:32:53,529 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:32:53,529 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2678628135916164,
  "recall": 0.18929533213644525,
  "score": 0.221827744904668,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:17,  1.52it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.26it/s]Extractor Predicting: 35it [00:22,  1.32it/s]Extractor Predicting: 36it [00:23,  1.39it/s]Extractor Predicting: 37it [00:23,  1.42it/s]Extractor Predicting: 38it [00:24,  1.40it/s]Extractor Predicting: 39it [00:25,  1.39it/s]Extractor Predicting: 40it [00:26,  1.41it/s]Extractor Predicting: 41it [00:26,  1.43it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.46it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:34,  1.43it/s]Extractor Predicting: 53it [00:34,  1.45it/s]Extractor Predicting: 54it [00:35,  1.44it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:38,  1.46it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.41it/s]Extractor Predicting: 62it [00:41,  1.41it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 63it [00:41,  1.51it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.46698564593301434,
  "recall": 0.1462391369493557,
  "score": 0.2227293473299863,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
